[
  {
    "url": "/docs/latest/install-and-configure/configuration/",
    "title": "Configuring OpenSearch",
    "content": "Most OpenSearch configuration can take place in the cluster settings API. Certain operations require you to modify opensearch.yml and restart the cluster.\nWhenever possible, use the cluster settings API instead; opensearch.yml is local to each node, whereas the API applies the setting to all nodes in the cluster. Certain settings, however, require opensearch.yml. In general, these settings relate to networking, cluster formation, and the local file system. To learn more, see Cluster formation.\nSpecify settings as environment variables\nYou can specify environment variables as arguments using -E when launching OpenSearch:./opensearch -Ecluster.name = opensearch-cluster -Enode.name = opensearch-node1 -Ehttp.host = 0.0.0.0 -Ediscovery.type = single-node Update cluster settings using the API\nThe first step in changing a setting is to view the current settings: GET _cluster/settings?include_defaults=true For a more concise summary of non-default settings: GET _cluster/settings Three categories of setting exist in the cluster settings API: persistent, transient, and default. Persistent settings, well, persist after a cluster restart. After a restart, OpenSearch clears transient settings.\nIf you specify the same setting in multiple places, OpenSearch uses the following precedence:\nTransient settings\nPersistent settings\nSettings from opensearch.yml Default settings\nTo change a setting, just specify the new one as either persistent or transient. This example shows the flat settings form: PUT _cluster/settings { \"persistent\": { \"action.auto_create_index\": false } } You can also use the expanded form, which lets you copy and paste from the GET response and change existing values: PUT _cluster/settings { \"persistent\": { \"action\": { \"auto_create_index\": false } } } For more information about the Cluster Settings API, see Cluster settings.\nConfiguration file\nYou can find opensearch.yml in /usr/share/opensearch/config/opensearch.yml (Docker) or /etc/opensearch/opensearch.yml (most Linux distributions) on each node.\nYou can edit the OPENSEARCH_PATH_CONF=/etc/opensearch to change the config directory location. This variable is sourced from /etc/default/opensearch (Debian package) and /etc/sysconfig/opensearch (RPM package).\nIf you set your customized OPENSEARCH_PATH_CONF variable, be aware that other default environment variables will not be loaded.\nYou don’t mark settings in opensearch.yml as persistent or transient, and settings use the flat form: cluster.name: my-application action.auto_create_index: true compatibility.override_main_response_version: true The demo configuration includes a number of settings for the Security plugin that you should modify before using OpenSearch for a production workload. To learn more, see Security.\n(Optional) CORS header configuration\nIf you are working on a client application running against an OpenSearch cluster on a different domain, you can configure headers in opensearch.yml to allow for developing a local application on the same machine. Use Cross Origin Resource Sharing so your application can make calls to the OpenSearch API running locally. Add the following lines in your custom-opensearch.yml file (note that the “-“ must be the first character in each line). - http.host:0.0.0.0 - http.port:9200 - http.cors.allow-origin:\"http://localhost\" - http.cors.enabled:true - http.cors.allow-headers:X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization - http.cors.allow-credentials:true",
    "ancestors": [
      "Install and upgrade"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-dashboards/debian/",
    "title": "Debian",
    "content": "Installing OpenSearch Dashboards using the Advanced Packaging Tool (APT) package manager simplifies the process considerably compared to the Tarball method. For example, the package manager handles several technical considerations, such as the installation path, location of configuration files, and creation of a service managed by systemd.\nBefore installing OpenSearch Dashboards you must configure an OpenSearch cluster. Refer to the OpenSearch Debian installation guide for steps.\nThis guide assumes that you are comfortable working from the Linux command line interface (CLI). You should understand how to input commands, navigate between directories, and edit text files. Some example commands reference the vi text editor, but you may use any text editor available.\nInstalling OpenSearch Dashboards from a package\nDownload the Debian package for the desired version directly from the OpenSearch downloads page. The Debian package can be downloaded for both x64 and arm64 architectures.\nFrom the CLI, install using dpkg. # x64 sudo dpkg -i opensearch-dashboards-2.7.0-linux-x64.deb # arm64 sudo dpkg -i opensearch-dashboards-2.7.0-linux-arm64.deb After the installation completes, reload the systemd manager configuration. sudo systemctl daemon-reload Enable OpenSearch as a service. sudo systemctl enable opensearch-dashboards Start the OpenSearch service. sudo systemctl start opensearch-dashboards Verify that OpenSearch launched correctly. sudo systemctl status opensearch-dashboards Fingerprint verification\nThe Debian package is not signed. If you would like to verify the fingerprint, the OpenSearch Project provides a.sig file as well as the.deb package for use with GNU Privacy Guard (GPG).\nDownload the desired Debian package. curl -SLO https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-x64.deb Download the corresponding signature file. curl -SLO https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-x64.deb.sig Download and import the GPG key. curl -o- https://artifacts.opensearch.org/publickeys/opensearch.pgp | gpg --import - Verify the signature. gpg --verify opensearch-dashboards-2.7.0-linux-x64.deb.sig opensearch-dashboards-2.7.0-linux-x64.deb Installing OpenSearch Dashboards from an APT repository\nAPT, the primary package management tool for Debian–based operating systems, allows you to download and install the Debian package from the APT repository.\nImport the public GPG key. This key is used to verify that the APT repository is signed. curl -o- https://artifacts.opensearch.org/publickeys/opensearch.pgp | sudo apt-key add - Create an APT repository for OpenSearch. echo \"deb https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/opensearch-dashboards-2.x.list Verify that the repository was created successfully. sudo apt-get update With the repository information added, list all available versions of OpenSearch: sudo apt list -a opensearch-dashboards Choose the version of OpenSearch you want to install:\nUnless otherwise indicated, the latest available version of OpenSearch is installed. sudo apt-get install opensearch-dashboards To install a specific version of OpenSearch Dashboards, pass a version number after the package name. # Specify the version manually using opensearch=&lt;version&gt; sudo apt-get install opensearch-dashboards = 2.7.0 Once complete, enable OpenSearch. sudo systemctl enable opensearch-dashboards Start OpenSearch. sudo systemctl start opensearch-dashboards Verify that OpenSearch launched correctly. sudo systemctl status opensearch-dashboards Exploring OpenSearch Dashboards\nBy default, OpenSearch Dashboards, like OpenSearch, binds to localhost when you initially install it. As a result, OpenSearch Dashboards is not reachable from a remote host unless the configuration is updated.\nOpen opensearch_dashboards.yml. sudo vi /etc/opensearch-dashboards/opensearch_dashboards.yml Specify a network interface that OpenSearch Dashboards should bind to. # Use 0.0.0.0 to bind to any available interface. server.host: 0.0.0.0 Save and quit.\nRestart OpenSearch Dashboards to apply the configuration change. sudo systemctl restart opensearch-dashboards From a web browser, navigate to OpenSearch Dashboards. The default port is 5601.\nLog in with the default username admin and the default password admin.\nVisit Getting started with OpenSearch Dashboards to learn more.",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-dashboards/docker/",
    "title": "Docker",
    "content": "You can start OpenSearch Dashboards using docker run after creating a Docker network and starting OpenSearch, but the process of connecting OpenSearch Dashboards to OpenSearch is significantly easier with a Docker Compose file.\nRun docker pull opensearchproject/opensearch-dashboards:2.7.0.\nCreate a docker-compose.yml file appropriate for your environment. A sample file that includes OpenSearch Dashboards is available on the OpenSearch Docker installation page.\nJust like opensearch.yml, you can pass a custom opensearch_dashboards.yml to the container in the Docker Compose file.\nRun docker-compose up.\nWait for the containers to start. Then see the OpenSearch Dashboards documentation.\nWhen finished, run docker-compose down.",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-dashboards/helm/",
    "title": "Helm",
    "content": "Helm is a package manager that allows you to easily install and manage OpenSearch Dashboards in a Kubernetes cluster. You can define your OpenSearch configurations in a YAML file and use Helm to deploy your applications in a version-controlled and reproducible way.\nThe Helm chart contains the resources described in the following table. Resource Description Chart.yaml Information about the chart. values.yaml Default configuration values for the chart. templates Templates that combine with values to generate the Kubernetes manifest files. The specification in the default Helm chart supports many standard use cases and setups. You can modify the default chart to configure your desired specifications and set Transport Layer Security (TLS) and role-based access control (RBAC).\nFor information about the default configuration, steps to configure security, and configurable parameters, see the README.\nThe instructions here assume you have a Kubernetes cluster with Helm preinstalled. See the Kubernetes documentation for steps to configure a Kubernetes cluster and the Helm documentation to install Helm.\nPrerequisites\nBefore you get started, you must first use Helm to install OpenSearch.\nMake sure that you can send requests to your OpenSearch pod: $ curl -XGET https://localhost: 9200 -u 'admin:admin' --insecure { \"name\": \"opensearch-cluster-master-1\", \"cluster_name\": \"opensearch-cluster\", \"cluster_uuid\": \"hP2gq5bPS3SLp8Z7wXm8YQ\", \"version\": { \"distribution\": \"opensearch\", \"number\": \"1.0.0\", \"build_type\": \"tar\", \"build_hash\": \"34550c5b17124ddc59458ef774f6b43a086522e3\", \"build_date\": \"2021-07-02T23:22:21.383695Z\", \"build_snapshot\": false, \"lucene_version\": \"8.8.2\", \"minimum_wire_compatibility_version\": \"6.8.0\", \"minimum_index_compatibility_version\": \"6.0.0-beta1\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Install OpenSearch Dashboards using Helm\nChange to the opensearch-dashboards directory: cd opensearch-dashboards Package the Helm chart: helm package. Deploy OpenSearch Dashboards: helm install --generate-name opensearch-dashboards-1.0.0.tgz The output shows you the specifications instantiated from the install.\nTo customize the deployment, pass in the values that you want to override with a custom YAML file: helm install --values = customvalues.yaml opensearch-dashboards-1.0.0.tgz Sample output NAME: opensearch-dashboards-1-1629223356 LAST DEPLOYED: Tue Aug 17 18:02:37 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=opensearch-dashboards,app.kubernetes.io/instance=op ensearch-dashboards-1-1629223356\" -o jsonpath=\"{.items[0].metadata.name}\") export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\") echo \"Visit http://127.0.0.1:8080 to use your application\" kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT To make sure your OpenSearch Dashboards pod is up and running, run the following command: $ kubectl get pods\nNAME READY STATUS RESTARTS AGE\nopensearch-cluster-master-0 1/1 Running 0 4m35s\nopensearch-cluster-master-1 1/1 Running 0 4m35s\nopensearch-cluster-master-2 1/1 Running 0 4m35s\nopensearch-dashboards-1-1629223356-758bd8747f-8www5 1/1 Running 0 66s To set up port forwarding to access OpenSearch Dashboards, exit the OpenSearch shell and run the following command: $ kubectl port-forward deployment/opensearch-dashboards-1-1629223356 5601 You can now access OpenSearch Dashboards from your browser at: http://localhost:5601.\nUninstall using Helm\nTo identify the OpenSearch Dashboards deployment that you want to delete: $ helm list\nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION\nopensearch-1-1629223146 default 1 2021-08-17 17:59:07.664498239 +0000 UTCdeployedopensearch-1.0.0 1.0.0\nopensearch-dashboards-1-1629223356 default 1 2021-08-17 18:02:37.600796946 +0000 UTCdepl\noyedopensearch-dashboards-1.0.0 1.0.0 To delete or uninstall a deployment, run the following command: helm delete opensearch-dashboards-1-1629223356",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-dashboards/index/",
    "title": "Installing OpenSearch Dashboards",
    "content": "This section details how to install and configure OpenSearch Dashboards.\nInstallation options\nOpenSearch Dashboards has the following installation options: Docker Tarball RPM Debian Helm Windows Browser compatibility\nOpenSearch Dashboards supports the following web browsers:\nChrome\nFirefox\nSafari\nEdge (Chromium)\nOther Chromium-based browsers might work, as well. Internet Explorer and Microsoft Edge Legacy are not supported.\nConfiguration\nTo learn how to configure TLS for OpenSearch Dashboards, see Configure TLS.",
    "ancestors": [
      "Install and upgrade"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-dashboards/plugins/",
    "title": "Managing OpenSearch Dashboards plugins",
    "content": "OpenSearch Dashboards provides a command line tool called opensearch-plugin for managing plugins. This tool allows you to:\nList installed plugins.\nInstall plugins.\nRemove an installed plugin.\nPlugin compatibility\nMajor, minor, and patch plugin versions must match OpenSearch major, minor, and patch versions in order to be compatible. For example, plugins versions 2.3.0.x work only with OpenSearch 2.3.0.\nPrerequisites\nA compatible OpenSearch cluster\nThe corresponding OpenSearch plugins installed on that cluster The corresponding version of OpenSearch Dashboards (for example, OpenSearch Dashboards 2.3.0 works with OpenSearch 2.3.0)\nAvailable plugins\nThe following table lists available OpenSearch Dashboards plugins. Plugin Name Repository Earliest Available Version Alerting Dashboards alerting-dashboards-plugin 1.0.0\nAnomaly Detection Dashboards anomaly-detection-dashboards-plugin 1.0.0\nCustom Import Maps Dashboards dashboards-maps 2.2.0\nSearch Relevance Dashboards dashboards-search-relevance 2.4.0\nGantt Chart Dashboards gantt-chart 1.0.0\nIndex Management Dashboards index-management-dashboards-plugin 1.0.0\nNotebooks Dashboards dashboards-notebooks 1.0.0\nNotifications Dashboards notifications 2.0.0\nObservability Dashboards dashboards-observability 2.0.0\nQuery Workbench Dashboards query-workbench 1.0.0\nReports Dashboards dashboards-reporting 1.0.0\nSecurity Analytics Dashboards security-analytics-dashboards-plugin 2.4.0\nSecurity Dashboards security-dashboards-plugin 1.0.0 Install\nNavigate to the OpenSearch Dashboards home directory (for example, /usr/share/opensearch-dashboards) and run the install command for each plugin.\nViewing a list of installed plugins\nTo view the list of installed plugins from the command line, use the following command: sudo bin/opensearch-dashboards-plugin list Remove plugins\nTo remove a plugin: sudo bin/opensearch-dashboards-plugin remove &lt;plugin-name&gt; Then remove all associated entries from opensearch_dashboards.yml.\nFor certain plugins, you must also remove the “optimze” bundle. This is a sample command for the Anomaly Detection plugin: sudo rm /usr/share/opensearch-dashboards/optimize/bundles/opensearch-anomaly-detection-opensearch-dashboards. * Then restart OpenSearch Dashboards. After you remove any plugin, OpenSearch Dashboards performs an optimize operation the next time you start it. This operation takes several minutes even on fast machines, so be patient.\nUpdating plugins\nOpenSearch Dashboards doesn’t update plugins. Instead, you have to remove the old version and its optimized bundle, reinstall them, and restart OpenSearch Dashboards:\nRemove the old version: sudo bin/opensearch-dashboards-plugin remove &lt;plugin-name&gt; Remove the optimized bundle: sudo rm /usr/share/opensearch-dashboards/optimize/bundles/&lt;bundle-name&gt; Reinstall the new version: sudo bin/opensearch-dashboards-plugin install &lt;plugin-name&gt; Restart OpenSearch Dashboards.\nFor example, to remove and reinstall the anomaly detection plugin: sudo bin/opensearch-plugin remove opensearch-anomaly-detection sudo rm /usr/share/opensearch-dashboards/optimize/bundles/opensearch-anomaly-detection-opensearch-dashboards. * sudo bin/opensearch-dashboards-plugin install &lt;AD OpenSearch Dashboards plugin artifact URL&gt;",
    "ancestors": [
      "Install and upgrade"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-dashboards/rpm/",
    "title": "RPM",
    "content": "OpenSearch Dashboards is the default visualization tool for data in OpenSearch. It also serves as a user interface for many of the OpenSearch plugins, including security, alerting, Index State Management, SQL, and more.\nInstall OpenSearch Dashboards from a package\nDownload the RPM package for the desired version directly from the OpenSearch downloads page. The RPM package can be download for both x64 and arm64 architectures.\nImport the public GPG key. This key verifies that your OpenSearch instance is signed. sudo rpm --import https://artifacts.opensearch.org/publickeys/opensearch.pgp From the command line interface (CLI), you can install the package with rpm or yum. x64 # Install the x64 package using yum. sudo yum install opensearch-dashboards-2.7.0-linux-x64.rpm # Install the x64 package using rpm. sudo rpm -ivh opensearch-dashboards-2.7.0-linux-x64.rpm arm64 # Install the arm64 package using yum. sudo yum install opensearch-dashboards-2.7.0-linux-arm64.rpm # Install the arm64 package using rpm. sudo rpm -ivh opensearch-dashboards-2.7.0-linux-arm64.rpm After the installation succeeds, enable OpenSearch Dashboards as a service. sudo systemctl enable opensearch-dashboards Start OpenSearch Dashboards. sudo systemctl start opensearch-dashboards Verify that OpenSearch Dashboards launched correctly. sudo systemctl status opensearch-dashboards Install OpenSearch Dashboards from a local YUM repository\nYUM, the primary package management tool for Red Hat-based operating systems, allows you to download and install the RPM package from the YUM repository library.\nCreate a local repository file for OpenSearch Dashboards: sudo curl -SL https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo -o /etc/yum.repos.d/opensearch-dashboards-2.x.repo Verify that the repository was created successfully. sudo yum repolist Clean your YUM cache, to ensure a smooth installation: sudo yum clean all With the repository file downloaded, list all available versions of OpenSearch-Dashboards: sudo yum list opensearch-dashboards --showduplicates Choose the version of OpenSearch Dashboards you want to install:\nUnless otherwise indicated, the highest minor version of OpenSearch installs. sudo yum install opensearch-dashboards To install a specific version of OpenSearch Dashboards: sudo yum install 'opensearch-dashboards-2.7.0' During installation, the installer will present you with the GPG key fingerprint. Verify that the information matches the following: Fingerprint: c5b7 4989 65ef d1c2 924b a9d5 39d3 1987 9310 d3fc If correct, enter yes or y. The OpenSearch installation continues.\nOnce complete, you can run OpenSearch Dashboards. sudo systemctl start opensearch-dashboards",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-dashboards/tar/",
    "title": "Tarball",
    "content": "Download the tarball from the OpenSearch downloads page.\nExtract the TAR file to a directory and change to that directory: # x64 tar -zxf opensearch-dashboards-2.7.0-linux-x64.tar.gz cd opensearch-dashboards # ARM64 tar -zxf opensearch-dashboards-2.7.0-linux-arm64.tar.gz cd opensearch-dashboards If desired, modify config/opensearch_dashboards.yml.\nRun OpenSearch Dashboards:./bin/opensearch-dashboards",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-dashboards/tls/",
    "title": "Configure TLS",
    "content": "By default, for ease of testing and getting started, OpenSearch Dashboards runs over HTTP. To enable TLS for HTTPS, update the following settings in opensearch_dashboards.yml. Setting Description opensearch.ssl.verificationMode\nThis setting is for communications between OpenSearch and OpenSearch Dashboards. Valid values are full, certificate, or none. We recommend full if you enable TLS, which enables hostname verification. certificate just checks the certificate, not the hostname, and none performs no checks (suitable for HTTP). Default is full.\nopensearch.ssl.certificateAuthorities\nIf opensearch.ssl.verificationMode is full or certificate, specify the full path to one or more CA certificates that comprise a trusted chain for your OpenSearch cluster. For example, you might need to include a root CA and an intermediate CA if you used the intermediate CA to issue your admin, client, and node certificates.\nserver.ssl.enabled\nThis setting is for communications between OpenSearch Dashboards and the web browser. Set to true for HTTPS, false for HTTP.\nserver.ssl.certificate\nIf server.ssl.enabled is true, specify the full path to a valid client certificate for your OpenSearch cluster. You can generate your own or get one from a certificate authority.\nserver.ssl.key\nIf server.ssl.enabled is true, specify the full path (e.g. /usr/share/opensearch-dashboards-1.0.0/config/my-client-cert-key.pem to the key for your client certificate. You can generate your own or get one from a certificate authority.\nopensearch_security.cookie.secure\nIf you enable TLS for OpenSearch Dashboards, change this setting to true. For HTTP, set it to false. This opensearch_dashboards.yml configuration shows OpenSearch and OpenSearch Dashboards running on the same machine with the demo configuration: opensearch.hosts: [ \" https://localhost:9200\"] opensearch.ssl.verificationMode: full opensearch.username: \" kibanaserver\" opensearch.password: \" kibanaserver\" opensearch.requestHeadersAllowlist: [ authorization, securitytenant] server.ssl.enabled: true server.ssl.certificate: /usr/share/opensearch-dashboards/config/client-cert.pem server.ssl.key: /usr/share/opensearch-dashboards/config/client-cert-key.pem opensearch.ssl.certificateAuthorities: [ \" /usr/share/opensearch-dashboards/config/root-ca.pem\", \" /usr/share/opensearch-dashboards/config/intermediate-ca.pem\"] opensearch_security.multitenancy.enabled: true opensearch_security.multitenancy.tenants.preferred: [ \" Private\", \" Global\"] opensearch_security.readonly_mode.roles: [ \" kibana_read_only\"] opensearch_security.cookie.secure: true If you use the Docker install, you can pass a custom opensearch_dashboards.yml to the container. To learn more, see the Docker installation page.\nAfter enabling these settings and starting OpenSearch Dashboards, you can connect to it at https://localhost:5601. You might have to acknowledge a browser warning if your certificates are self-signed. To avoid this sort of warning (or outright browser incompatibility), best practice is to use certificates from trusted certificate authority.",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-dashboards/windows/",
    "title": "Windows",
    "content": "Perform the following steps to install OpenSearch Dashboards on Windows.\nMake sure you have a zip utility installed.\nDownload the opensearch-dashboards-2.7.0-windows-x64.zip archive.\nTo extract the archive contents, right-click to select Extract All. Note: Some versions of the Windows operating system limit the file path length. If you encounter a path-length-related error when unzipping the archive, perform the following steps to enable long path support:\nOpen Powershell by entering powershell in the search box next to Start on the taskbar.\nRun the following command in Powershell: Set -ItemProperty -Path HKLM:\\SYSTEM\\CurrentControlSet\\Control\\FileSystem LongPathsEnabled -Type DWORD -Value 1 -Force Restart your computer.\nRun OpenSearch Dashboards.\nThere are two ways of running OpenSearch Dashboards:\nRun the batch script using the Windows UI:\nNavigate to the top directory of your OpenSearch Dashboards installation and open the opensearch-dashboards-2.7.0 folder.\nIf desired, modify opensearch_dashboards.yml located in the config folder, to change the default OpenSearch Dashboards settings.\nOpen the bin folder and run the batch script by double-clicking the opensearch-dashboards.bat file. This opens a command prompt with an OpenSearch Dashboards instance running.\nRun the batch script from Command Prompt or Powershell:\nOpen Command Prompt by entering cmd, or Powershell by entering powershell, in the search box next to Start on the taskbar.\nChange to the top directory of your OpenSearch Dashboards installation. cd \\path\\to\\opensearch -dashboards - 2.7.0 If desired, modify config\\opensearch_dashboards.yml.\nRun the batch script to start OpenSearch Dashboards..\\bin\\opensearch -dashboards.bat To stop OpenSearch Dashboards, press Ctrl+C in Command Prompt or Powershell, or simply close the Command Prompt or Powershell window.",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-opensearch/ansible/",
    "title": "Ansible playbook",
    "content": "You can use an Ansible playbook to install and configure a production-ready OpenSearch cluster along with OpenSearch Dashboards.\nThe Ansible playbook only supports deployment of OpenSearch and OpenSearch Dashboards to CentOS7 hosts.\nPrerequisites\nMake sure you have Ansible and Java 8 installed.\nConfiguration\nClone the OpenSearch ansible-playbook repository: git clone https://github.com/opensearch-project/ansible-playbook copy Configure the node properties in the inventories/opensearch/hosts file: ansible_host = &lt;Public IP address&gt; ansible_user = root ip = &lt;Private IP address / 0.0.0.0&gt; copy where: ansible_host is the IP address of the target node that you want the Ansible playbook to install OpenSearch and OpenSearch DashBoards on. ip is the IP address that you want OpenSearch and OpenSearch DashBoards to bind to. You can specify the private IP of the target node, or localhost, or 0.0.0.0.\nYou can modify the default configuration values in the inventories/opensearch/group_vars/all/all.yml file. For example, you can increase the Java memory heap size: xms_value: 8\nxmx_value: 8 copy Make sure you have direct SSH access into the root user of the target node.\nRun OpenSearch and OpenSearch Dashboards using Ansible playbook\nRun the Ansible playbook with root privileges: ansible-playbook -i inventories/opensearch/hosts opensearch.yml --extra-vars \"admin_password=Test@123 kibanaserver_password=Test@6789\" copy You can set the passwords for reserved users ( admin and kibanaserver) using the admin_password and kibanaserver_password variables.\nAfter the deployment process is complete, you can access OpenSearch and OpenSearch Dashboards with the username admin and the password that you set for the admin_password variable.\nIf you bind ip to a private IP or localhost, make sure you’re logged into the server that deployed the playbook to access OpenSearch and OpenSearch Dashboards: curl https://localhost:9200 -u 'admin:Test@123' --insecure copy If you bind ip to 0.0.0.0, then replace localhost with the public IP or the private IP (if it’s in the same network).",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-opensearch/debian/",
    "title": "Debian",
    "content": "Installing OpenSearch using the Advanced Packaging Tool (APT) package manager simplifies the process considerably compared to the Tarball method. Several technical considerations, such as the installation path, location of configuration files, and creation of a service managed by systemd, as examples, are handled automatically by the package manager.\nGenerally speaking, installing OpenSearch from the Debian distribution can be broken down into a few steps: Download and install OpenSearch. Install manually from a Debian package or from an APT repository. (Optional) Test OpenSearch. Confirm that OpenSearch is able to run before you apply any custom configuration.\nThis can be done without any security (no password, no certificates) or with a demo security configuration that can be applied by a packaged script. Configure OpenSearch for your environment. Apply basic settings to OpenSearch and start using it in your environment.\nThe Debian distribution provides everything you need to run OpenSearch inside Debian-based Linux Distributions, such as Ubuntu.\nThis guide assumes that you are comfortable working from the Linux command line interface (CLI). You should understand how to input commands, navigate between directories, and edit text files. Some example commands reference the vi text editor, but you may use any text editor available.\nStep 1: Download and install OpenSearch\nInstall OpenSearch from a package\nDownload the Debian package for the desired version directly from the OpenSearch downloads page. The Debian package can be downloaded for both x64 and arm64 architectures.\nFrom the CLI, install using dpkg. # x64 sudo dpkg -i opensearch-2.7.0-linux-x64.deb # arm64 sudo dpkg -i opensearch-2.7.0-linux-arm64.deb After the installation succeeds, enable OpenSearch as a service. sudo systemctl enable opensearch copy Start the OpenSearch service. sudo systemctl start opensearch copy Verify that OpenSearch launched correctly. sudo systemctl status opensearch copy Fingerprint verification\nThe Debian package is not signed. If you would like to verify the fingerprint, the OpenSearch Project provides a.sig file as well as the.deb package for use with GNU Privacy Guard (GPG).\nDownload the desired Debian package. curl -SLO https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.deb copy Download the corresponding signature file. curl -SLO https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.deb.sig copy Download and import the GPG key. curl -o- https://artifacts.opensearch.org/publickeys/opensearch.pgp | gpg --import - copy Verify the signature. gpg --verify opensearch-2.7.0-linux-x64.deb.sig opensearch-2.7.0-linux-x64.deb copy Install OpenSearch from an APT repository\nAPT, the primary package management tool for Debian–based operating systems, allows you to download and install the Debian package from the APT repository.\nImport the public GPG key. This key is used to verify that the APT repository is signed. curl -o- https://artifacts.opensearch.org/publickeys/opensearch.pgp | sudo apt-key add - copy Create an APT repository for OpenSearch: echo \"deb https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/opensearch-2.x.list copy Verify that the repository was created successfully. sudo apt-get update copy With the repository information added, list all available versions of OpenSearch: sudo apt list -a opensearch copy Choose the version of OpenSearch you want to install:\nUnless otherwise indicated, the latest available version of OpenSearch is installed. sudo apt-get install opensearch copy To install a specific version of OpenSearch: # Specify the version manually using opensearch=&lt;version&gt; sudo apt-get install opensearch = 2.7.0 During installation, the installer will present you with the GPG key fingerprint. Verify that the information matches the following: Fingerprint: c5b7 4989 65ef d1c2 924b a9d5 39d3 1987 9310 d3fc copy Once complete, enable OpenSearch. sudo systemctl enable opensearch copy Start OpenSearch. sudo systemctl start opensearch copy Verify that OpenSearch launched correctly. sudo systemctl status opensearch copy Step 2: (Optional) Test OpenSearch\nBefore proceeding with any configuration, you should test your installation of OpenSearch. Otherwise, it can be difficult to determine whether future problems are due to installation issues or custom settings you applied after installation.\nWhen OpenSearch is installed using the Debian package, some demo security settings are automatically applied. This includes self-signed TLS certificates and several users and roles. If you would like to configure these yourself, see Set up OpenSearch in your environment.\nAn OpenSearch node in its default configuration (with demo certificates and users with default passwords) is not suitable for a production environment. If you plan to use the node in a production environment, you should, at a minimum, replace the demo TLS certificates with your own TLS certificates and update the list of internal users and passwords. See Security configuration for additional guidance to ensure that your nodes are configured according to your security requirements.\nSend requests to the server to verify that OpenSearch is running. Note the use of the --insecure flag, which is required because the TLS certificates are self-signed.\nSend a request to port 9200: curl -X GET https://localhost:9200 -u 'admin:admin' --insecure copy You should get a response that looks like this: { \"name\": \"hostname\", \"cluster_name\": \"opensearch\", \"cluster_uuid\": \"QqgpHCbnSRKcPAizqjvoOw\", \"version\": { \"distribution\": \"opensearch\", \"number\":&lt;version&gt;, \"build_type\":&lt;build-type&gt;, \"build_hash\":&lt;build-hash&gt;, \"build_date\":&lt;build-date&gt;, \"build_snapshot\":false, \"lucene_version\":&lt;lucene-version&gt;, \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Query the plugins endpoint: curl -X GET https://localhost:9200/_cat/plugins?v -u 'admin:admin' --insecure copy The response should look like this: name component version hostname opensearch-alerting 2.7.0 hostname opensearch-anomaly-detection 2.7.0 hostname opensearch-asynchronous-search 2.7.0 hostname opensearch-cross-cluster-replication 2.7.0 hostname opensearch-geospatial 2.7.0 hostname opensearch-index-management 2.7.0 hostname opensearch-job-scheduler 2.7.0 hostname opensearch-knn 2.7.0 hostname opensearch-ml 2.7.0 hostname opensearch-neural-search 2.7.0 hostname opensearch-notifications 2.7.0 hostname opensearch-notifications-core 2.7.0 hostname opensearch-observability 2.7.0 hostname opensearch-performance-analyzer 2.7.0 hostname opensearch-reports-scheduler 2.7.0 hostname opensearch-security 2.7.0 hostname opensearch-security-analytics 2.7.0 hostname opensearch-sql 2.7.0 Step 3: Set up OpenSearch in your environment\nUsers who do not have prior experience with OpenSearch may want a list of recommended settings in order to get started with the service. By default, OpenSearch is not bound to a network interface and cannot be reached by external hosts. Additionally, security settings are populated by default user names and passwords. The following recommendations will enable a user to bind OpenSearch to a network interface, create and sign TLS certificates, and configure basic authentication.\nThe following recommended settings will allow you to:\nBind OpenSearch to an IP or network interface on the host.\nSet initial and maximum JVM heap sizes.\nDefine an environment variable that points to the bundled JDK.\nConfigure your own TLS certificates—no third-party certificate authority (CA) is required.\nCreate an admin user with a custom password.\nIf you ran the security demo script, then you will need to manually reconfigure settings that were modified. Refer to Security configuration for guidance before proceeding.\nBefore modifying any configuration files, it’s always a good idea to save a backup copy before making changes. The backup file can be used to mitigate any issues caused by a bad configuration.\nOpen opensearch.yml. sudo vi /etc/opensearch/opensearch.yml copy Add the following lines: # Bind OpenSearch to the correct network interface. Use 0.0.0.0 # to include all available interfaces or specify an IP address # assigned to a specific interface. network.host: 0.0.0.0 # Unless you have already configured a cluster, you should set # discovery.type to single-node, or the bootstrap checks will # fail when you try to start the service. discovery.type: single-node # If you previously disabled the Security plugin in opensearch.yml, # be sure to re-enable it. Otherwise you can skip this setting. plugins.security.disabled: false copy Save your changes and close the file.\nSpecify initial and maximum JVM heap sizes.\nOpen jvm.options. vi /etc/opensearch/jvm.options copy Modify the values for initial and maximum heap sizes. As a starting point, you should set these values to half of the available system memory. For dedicated hosts this value can be increased based on your workflow requirements.\nAs an example, if the host machine has 8 GB of memory, then you might want to set the initial and maximum heap sizes to 4 GB: -Xms4g -Xmx4g copy Save your changes and close the file.\nConfigure TLS\nTLS certificates provide additional security for your cluster by allowing clients to confirm the identity of hosts and encrypt traffic between the client and host. For more information, refer to Configure TLS Certificates and Generate Certificates, which are included in the Security plugin documentation. For work performed in a development environment, self-signed certificates are usually adequate. This section will guide you through the basic steps required to generate your own TLS certificates and apply them to your OpenSearch host.\nNavigate to the directory where the certificates will be stored. cd /etc/opensearch copy Delete the demo certificates. sudo rm -f * pem copy Generate a root certificate. This is what you will use to sign your other certificates. # Create a private key for the root certificate sudo openssl genrsa -out root-ca-key.pem 2048 # Use the private key to create a self-signed root certificate. Be sure to # replace the arguments passed to -subj so they reflect your specific host. sudo openssl req -new -x509 -sha256 -key root-ca-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=ROOT\" -out root-ca.pem -days 730 Next, create the admin certificate. This certificate is used to gain elevated rights for performing administrative tasks relating to the Security plugin. # Create a private key for the admin certificate. sudo openssl genrsa -out admin-key-temp.pem 2048 # Convert the private key to PKCS#8. sudo openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem # Create the certficiate signing request (CSR). A common name (CN) of \"A\" is acceptable because this certificate is # used for authenticating elevated access and is not tied to a host. sudo openssl req -new -key admin-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=A\" -out admin.csr # Sign the admin certificate with the root certificate and private key you created earlier. sudo openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem -days 730 Create a certificate for the node being configured. # Create a private key for the node certificate. sudo openssl genrsa -out node1-key-temp.pem 2048 # Convert the private key to PKCS#8. sudo openssl pkcs8 -inform PEM -outform PEM -in node1-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node1-key.pem # Create the CSR and replace the arguments passed to -subj so they reflect your specific host. # The CN should match a DNS A record for the host-do not use the hostname. sudo openssl req -new -key node1-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.dns.a-record\" -out node1.csr # Create an extension file that defines a SAN DNS name for the host. This # should match the DNS A record of the host. sudo sh -c 'echo subjectAltName=DNS:node1.dns.a-record &gt; node1.ext' # Sign the node certificate with the root certificate and private key that you created earlier. sudo openssl x509 -req -in node1.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node1.pem -days 730 -extfile node1.ext Remove temporary files that are no longer required. sudo rm -f * temp.pem * csr * ext copy Make sure the remaining certificates are owned by the opensearch user. sudo chown opensearch:opensearch admin-key.pem admin.pem node1-key.pem node1.pem root-ca-key.pem root-ca.pem root-ca.srl copy Add these certificates to opensearch.yml as described in Generate Certificates. Advanced users might also choose to append the settings using a script: #! /bin/bash # Before running this script, make sure to replace the CN in the # node's distinguished name with a real DNS A record. echo \"plugins.security.ssl.transport.pemcert_filepath: /etc/opensearch/node1.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.transport.pemkey_filepath: /etc/opensearch/node1-key.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.transport.pemtrustedcas_filepath: /etc/opensearch/root-ca.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.enabled: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemcert_filepath: /etc/opensearch/node1.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemkey_filepath: /etc/opensearch/node1-key.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemtrustedcas_filepath: /etc/opensearch/root-ca.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.allow_default_init_securityindex: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.authcz.admin_dn:\" | sudo tee -a /etc/opensearch/opensearch.yml echo \" - 'CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.nodes_dn:\" | sudo tee -a /etc/opensearch/opensearch.yml echo \" - 'CN=node1.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.audit.type: internal_opensearch\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.enable_snapshot_restore_privilege: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.check_snapshot_restore_write_privileges: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.restapi.roles_enabled: [ \\\" all_access \\\", \\\" security_rest_api_access \\\"]\" | sudo tee -a /etc/opensearch/opensearch.yml copy (Optional) Add trust for the self-signed root certificate. # Copy the root certificate to the correct directory sudo cp /etc/opensearch/root-ca.pem /etc/pki/ca-trust/source/anchors/ # Add trust sudo update-ca-trust Configure a user\nUsers are defined and authenticated by OpenSearch in a variety of ways. One method that does not require additional backend infrastructure is to manually configure users in internal_users.yml. See YAML files for more information about configuring users. The following steps explain how to remove all demo users except for the admin user and how to replace the admin default password using a script.\nNavigate to the Security plugins tools directory. cd /usr/share/opensearch/plugins/opensearch-security/tools copy Run hash.sh to generate a new password.\nThis script will fail if a path to the JDK has not been defined. # Example output if a JDK isn't found... $./hash.sh ************************************************************************** ** This tool will be deprecated in the next major release of OpenSearch ** ** https://github.com/opensearch-project/security/issues/1755 ** ************************************************************************** which: no java in ( /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/user/.local/bin:/home/user/bin) WARNING: nor OPENSEARCH_JAVA_HOME nor JAVA_HOME is set, will use./hash.sh: line 35: java: command not found copy Declare an environment variable when you invoke the script in order to avoid issues: OPENSEARCH_JAVA_HOME = /usr/share/opensearch/jdk./hash.sh copy Enter the desired password at the prompt and make a note of the output hash.\nOpen internal_users.yml. sudo vi /etc/opensearch/opensearch-security/internal_users.yml copy Remove all demo users except for admin and replace the hash with the output provided by hash.sh in a previous step. The file should look similar to the following example: --- # This is the internal user database # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh _meta: type: \"internalusers\" config_version: 2 # Define your internal users here admin: hash: \" $2y$1EXAMPLEQqwS8TUcoEXAMPLEeZ3lEHvkEXAMPLERqjyh1icEXAMPLE.\" reserved: true backend_roles:\n- \"admin\" description: \"Admin user\" copy Apply changes\nNow that TLS certificates are installed and demo users were removed or assigned new passwords, the last step is to apply the configuration changes. This last configuration step requires invoking securityadmin.sh while OpenSearch is running on the host.\nOpenSearch must be running for securityadmin.sh to apply changes. If you made changes to opensearch.yml, restart OpenSearch. sudo systemctl restart opensearch copy Open a separate terminal session with the host and navigate to the directory containing securityadmin.sh. # Change to the correct directory cd /usr/share/opensearch/plugins/opensearch-security/tools Invoke the script. See Apply changes using securityadmin.sh for definitions of the arguments you must pass. # You can omit the environment variable if you declared this in your $PATH. OPENSEARCH_JAVA_HOME = /usr/share/opensearch/jdk./securityadmin.sh -cd /etc/opensearch/opensearch-security/ -cacert /etc/opensearch/root-ca.pem -cert /etc/opensearch/admin.pem -key /etc/opensearch/admin-key.pem -icl -nhnv copy Verify that the service is running\nOpenSearch is now running on your host with custom TLS certificates and a secure user for basic authentication. You can verify external connectivity by sending an API request to your OpenSearch node from another host.\nDuring the previous test you directed requests to localhost. Now that TLS certificates have been applied and the new certificates reference your host’s actual DNS record, requests to localhost will fail the CN check and the certificate will be considered invalid. Instead, requests should be sent to the address you specified while generating the certificate.\nYou should add trust for the root certificate to your client before sending requests. If you do not add trust, then you must use the -k option so that cURL ignores CN and root certificate validation. $ curl https://your.host.address:9200 -u admin:yournewpassword -k { \"name\": \"hostname\", \"cluster_name\": \"opensearch\", \"cluster_uuid\": \"QqgpHCbnSRKcPAizqjvoOw\", \"version\": { \"distribution\": \"opensearch\", \"number\":&lt;version&gt;, \"build_type\":&lt;build-type&gt;, \"build_hash\":&lt;build-hash&gt;, \"build_date\":&lt;build-date&gt;, \"build_snapshot\":false, \"lucene_version\":&lt;lucene-version&gt;, \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Upgrade to a newer version\nOpenSearch instances installed using dpkg or apt-get can be easily upgraded to a newer version.\nManual upgrade with DPKG\nDownload the Debian package for the desired upgrade version directly from the OpenSearch downloads page.\nNavigate to the directory containing the distribution and run the following command: sudo dpkg -i opensearch-2.7.0-linux-x64.deb copy APT-GET\nTo upgrade to the latest version of OpenSearch using apt-get: sudo apt-get upgrade opensearch copy You can also upgrade to a specific OpenSearch version: sudo apt-get upgrade opensearch = &lt;version&gt; copy Related links OpenSearch configuration Install and configure OpenSearch Dashboards OpenSearch plugin installation About the Security plugin",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-opensearch/docker/",
    "title": "Docker",
    "content": "Docker greatly simplifies the process of configuring and managing your OpenSearch clusters. You can pull official images from Docker Hub or Amazon Elastic Container Registry (Amazon ECR) and quickly deploy a cluster using Docker Compose and any of the sample Docker Compose files included in this guide. Experienced OpenSearch users can further customize their deployment by creating a custom Docker Compose file.\nDocker containers are portable and will run on any compatible host that supports Docker (such as Linux, MacOS, or Windows). The portability of a Docker container offers flexibility over other installations methods, like RPM or a manual Tarball installation, which both require additional configuration after downloading and unpacking.\nThis guide assumes that you are comfortable working from the Linux command line interface (CLI). You should understand how to input commands, navigate between directories, and edit text files. For help with Docker or Docker Compose, refer to the official documentation on their websites.\nInstall Docker and Docker Compose\nVisit Get Docker for guidance on installing and configuring Docker for your environment. If you are installing Docker Engine using the CLI, then Docker, by default, will not have any constraints on available host resources. Depending on your environment, you may wish to configure resource limits in Docker. See Runtime options with Memory, CPUs, and GPUs for information.\nDocker Desktop users should set host memory utilization to a minimum of 4 GB by opening Docker Desktop and selecting Settings → Resources.\nDocker Compose is a utility that allows users to launch multiple containers with a single command. You pass a file to Docker Compose when you invoke it. Docker Compose reads those settings and starts the requested containers. Docker Compose is installed automatically with Docker Desktop, but users operating in a command line environment must install Docker Compose manually. You can find information about installing Docker Compose on the official Docker Compose GitHub page.\nIf you need to install Docker Compose manually and your host supports Python, you can use pip to install the Docker Compose package automatically.\nImportant host settings\nBefore launching OpenSearch you should review some important system settings that can impact the performance of your services.\nDisable memory paging and swapping performance on the host to improve performance. sudo swapoff -a Increase the number of memory maps available to OpenSearch. # Edit the sysctl config file sudo vi /etc/sysctl.conf # Add a line to define the desired value # or change the value if the key exists, # and then save your changes. vm.max_map_count = 262144 # Reload the kernel parameters using sysctl sudo sysctl -p # Verify that the change was applied by checking the value cat /proc/sys/vm/max_map_count Run OpenSearch in a Docker container\nOfficial OpenSearch images are hosted on Docker Hub and Amazon ECR. If you want to inspect the images you can pull them individually using docker pull, such as in the following examples. Docker Hub: docker pull opensearchproject/opensearch:latest\ndocker pull opensearchproject/opensearch-dashboards:latest Amazon ECR: docker pull public.ecr.aws/opensearchproject/opensearch:latest\ndocker pull public.ecr.aws/opensearchproject/opensearch-dashboards:latest To download a specific version of OpenSearch or OpenSearch Dashboards other than the latest available version, modify the image tag where it is referenced (either in the command line or in a Docker Compose file). For example, opensearchproject/opensearch:2.7.0 will pull OpenSearch version 2.7.0. Refer to the official image repositories for available versions.\nBefore continuing, you should verify that Docker is working correctly by deploying OpenSearch in a single container.\nRun the following command: # This command maps ports 9200 and 9600, sets the discovery type to \"single-node\" and requests the newest image of OpenSearch docker run -d -p 9200:9200 -p 9600:9600 -e \"discovery.type=single-node\" opensearchproject/opensearch:latest Send a request to port 9200. The default username and password are admin. curl https://localhost:9200 -ku 'admin:admin' copy You should get a response that looks like this: { \"name\": \"a937e018cee5\", \"cluster_name\": \"docker-cluster\", \"cluster_uuid\": \"GLAjAG6bTeWErFUy_d-CLw\", \"version\": { \"distribution\": \"opensearch\", \"number\": &lt;version&gt;, \"build_type\": &lt;build-type&gt;, \"build_hash\": &lt;build-hash&gt;, \"build_date\": &lt;build-date&gt;, \"build_snapshot\": false, \"lucene_version\": &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Before stopping the running container, display a list of all running containers and copy the container ID for the OpenSearch node you are testing. In the following example, the container ID is a937e018cee5: $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\na937e018cee5 opensearchproject/opensearch:latest \"./opensearch-docker…\" 19 minutes ago Up 19 minutes 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9650/tcp wonderful_boyd Stop the running container by passing the container ID to docker stop. docker stop &lt;containerId&gt; copy Remember that docker container ls does not list stopped containers. If you would like to review stopped containers, use docker container ls -a. You can remove unneeded containers manually with docker container rm &lt;containerId_1&gt; &lt;containerId_2&gt; &lt;containerId_3&gt; [...] (pass all container IDs you wish to stop, separated by spaces), or if you want to remove all stopped containers, you can use the shorter command docker prune.\nDeploy an OpenSearch cluster using Docker Compose\nAlthough it is technically possible to build an OpenSearch cluster by creating containers one command at a time, it is far easier to define your environment in a YAML file and let Docker Compose manage the cluster. The following section contains example YAML files that you can use to launch a predefined cluster with OpenSearch and OpenSearch Dashboards. These examples are useful for testing and development, but are not suitable for a production environment. If you don’t have prior experience using Docker Compose, you may wish to review the Docker Compose specification for guidance on syntax and formatting before making any changes to the dictionary structures in the examples.\nThe YAML file that defines the environment is referred to as a Docker Compose file. By default, docker-compose commands will first check your current directory for a file that matches any of the following names: docker-compose.yml docker-compose.yaml compose.yml compose.yaml If none of those files exist in your current directory, the docker-compose command fails.\nYou can specify a custom file location and name when invoking docker-compose with the -f flag: # Use a relative or absolute path to the file. docker-compose -f /path/to/your-file.yml up If this is your first time launching an OpenSearch cluster using Docker Compose, use the following example docker-compose.yml file. Save it in the home directory of your host and name it docker-compose.yml. This file will create a cluster that contains three containers: two containers running the OpenSearch service and a single container running OpenSearch Dashboards. These containers will communicate over a bridge network called opensearch-net and use two volumes, one for each OpenSearch node. Because this file does not explicitly disable the demo security configuration, self-signed TLS certificates are installed and internal users with default names and passwords are created.\nSample docker-compose.yml version: ' 3' services: opensearch-node1: # This is also the hostname of the container within the Docker network (i.e. https://opensearch-node1/) image: opensearchproject/opensearch:latest # Specifying the latest available image - modify if you want a specific version container_name: opensearch-node1 environment: - cluster.name=opensearch-cluster # Name the cluster - node.name=opensearch-node1 # Name the node that will run in this container - discovery.seed_hosts=opensearch-node1,opensearch-node2 # Nodes to look for when discovering the cluster - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 # Nodes eligible to serve as cluster manager - bootstrap.memory_lock=true # Disable JVM heap memory swapping - \" OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # Set min and max JVM heap sizes to at least 50% of system RAM ulimits: memlock: soft: -1 # Set memlock to unlimited (no soft or hard limit) hard: -1 nofile: soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536 hard: 65536 volumes: - opensearch-data1:/usr/share/opensearch/data # Creates volume called opensearch-data1 and mounts it to the container ports: - 9200:9200 # REST API - 9600:9600 # Performance Analyzer networks: - opensearch-net # All of the containers will join the same Docker bridge network opensearch-node2: image: opensearchproject/opensearch:latest # This should be the same image used for opensearch-node1 to avoid issues container_name: opensearch-node2 environment: - cluster.name=opensearch-cluster - node.name=opensearch-node2 - discovery.seed_hosts=opensearch-node1,opensearch-node2 - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 - bootstrap.memory_lock=true - \" OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 volumes: - opensearch-data2:/usr/share/opensearch/data networks: - opensearch-net opensearch-dashboards: image: opensearchproject/opensearch-dashboards:latest # Make sure the version of opensearch-dashboards matches the version of opensearch installed on other nodes container_name: opensearch-dashboards ports: - 5601:5601 # Map host port 5601 to container port 5601 expose: - \" 5601\" # Expose port 5601 for web access to OpenSearch Dashboards environment: OPENSEARCH_HOSTS: ' [\"https://opensearch-node1:9200\",\"https://opensearch-node2:9200\"]' # Define the OpenSearch nodes that OpenSearch Dashboards will query networks: - opensearch-net volumes: opensearch-data1: opensearch-data2: networks: opensearch-net: copy If you override opensearch_dashboards.yml settings using environment variables in your compose file, use all uppercase letters and replace periods with underscores (for example, for opensearch.hosts, use OPENSEARCH_HOSTS). This behavior is inconsistent with overriding opensearch.yml settings, where the conversion is just a change to the assignment operator (for example, discovery.type: single-node in opensearch.yml is defined as discovery.type=single-node in docker-compose.yml).\nFrom the home directory of your host (containing docker-compose.yml), create and start the containers in detached mode: docker-compose up -d copy Verify that the service containers started correctly: docker-compose ps copy If a container failed to start, you can review the service logs: # If you don't pass a service name, docker-compose will show you logs from all of the nodes docker-compose logs &lt;serviceName&gt; copy Verify access to OpenSearch Dashboards by connecting to http://localhost:5601 from a browser. The default username and password are admin. We do not recommend using this configuration on hosts that are accessible from the public internet until you have customized the security configuration of your deployment.\nRemember that localhost cannot be accessed remotely. If you are deploying these containers to a remote host, then you will need to establish a network connection and replace localhost with the IP or DNS record corresponding to the host.\nStop the running containers in your cluster: docker-compose down copy docker-compose down will stop the running containers, but it will not remove the Docker volumes that exist on the host. If you don’t care about the contents of these volumes, use the -v option to delete all volumes, for example, docker-compose down -v.\nConfigure OpenSearch\nUnlike the RPM distribution of OpenSearch, which requires a large amount of post-installation configuration, running OpenSearch clusters with Docker allows you to define the environment before the containers are even created. This is possible whether you use Docker or Docker Compose.\nFor example, take a look at the following command: docker run \\ -p 9200:9200 -p 9600:9600 \\ -e \"discovery.type=single-node\" \\ -v /path/to/custom-opensearch.yml:/usr/share/opensearch/config/opensearch.yml \\ opensearchproject/opensearch:latest copy By reviewing each part of the command, you can see that it:\nMaps ports 9200 and 9600 ( HOST_PORT: CONTAINER_PORT).\nSets discovery.type to single-node so that bootstrap checks don’t fail for this single-node deployment.\nUses the -v flag to pass a local file called custom-opensearch.yml to the container, replacing the opensearch.yml file included with the image.\nRequests the opensearchproject/opensearch:latest image from Docker Hub.\nRuns the container.\nIf you compare this command to the Sample docker-compose.yml file, you might notice some common settings, such as the port mappings and the image reference. The command, however, is only deploying a single container running OpenSearch and will not create a container for OpenSearch Dashboards. Furthermore, if you want to use custom TLS certificates, users, or roles, or define additional volumes and networks, then this “one-line” command rapidly grows to an impractical size. That is where the utility of Docker Compose becomes useful.\nWhen you build your OpenSearch cluster with Docker Compose you might find it easier to pass custom configuration files from your host to the container, as opposed to enumerating every individual setting in docker-compose.yml. Similar to how the example docker run command mounted a volume from the host to the container using the -v flag, compose files can specify volumes to mount as a sub-option to the corresponding service. The following truncated YAML file demonstrates how to mount a file or directory to the container. Refer to the official Docker documentation on volumes for comprehensive information about volume usage and syntax. services: opensearch-node1: volumes: - opensearch-data1:/usr/share/opensearch/data -./custom-opensearch.yml:/usr/share/opensearch/config/opensearch.yml opensearch-node2: volumes: - opensearch-data2:/usr/share/opensearch/data -./custom-opensearch.yml:/usr/share/opensearch/config/opensearch.yml opensearch-dashboards: volumes: -./custom-opensearch_dashboards.yml:/usr/share/opensearch-dashboards/config/opensearch_dashboards.yml copy Sample Docker Compose file for development\nIf you want to build your own compose file from an example, review the following sample docker-compose.yml file. This sample file creates two OpenSearch nodes and one OpenSearch Dashboards node with the Security plugin disabled. You can use this sample file as a starting point while reviewing Configuring basic security settings. version: ' 3' services: opensearch-node1: image: opensearchproject/opensearch:latest container_name: opensearch-node1 environment: - cluster.name=opensearch-cluster # Name the cluster - node.name=opensearch-node1 # Name the node that will run in this container - discovery.seed_hosts=opensearch-node1,opensearch-node2 # Nodes to look for when discovering the cluster - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 # Nodes eligibile to serve as cluster manager - bootstrap.memory_lock=true # Disable JVM heap memory swapping - \" OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # Set min and max JVM heap sizes to at least 50% of system RAM - \" DISABLE_INSTALL_DEMO_CONFIG=true\" # Prevents execution of bundled demo script which installs demo certificates and security configurations to OpenSearch - \" DISABLE_SECURITY_PLUGIN=true\" # Disables Security plugin ulimits: memlock: soft: -1 # Set memlock to unlimited (no soft or hard limit) hard: -1 nofile: soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536 hard: 65536 volumes: - opensearch-data1:/usr/share/opensearch/data # Creates volume called opensearch-data1 and mounts it to the container ports: - 9200:9200 # REST API - 9600:9600 # Performance Analyzer networks: - opensearch-net # All of the containers will join the same Docker bridge network opensearch-node2: image: opensearchproject/opensearch:latest container_name: opensearch-node2 environment: - cluster.name=opensearch-cluster # Name the cluster - node.name=opensearch-node2 # Name the node that will run in this container - discovery.seed_hosts=opensearch-node1,opensearch-node2 # Nodes to look for when discovering the cluster - cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 # Nodes eligibile to serve as cluster manager - bootstrap.memory_lock=true # Disable JVM heap memory swapping - \" OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # Set min and max JVM heap sizes to at least 50% of system RAM - \" DISABLE_INSTALL_DEMO_CONFIG=true\" # Prevents execution of bundled demo script which installs demo certificates and security configurations to OpenSearch - \" DISABLE_SECURITY_PLUGIN=true\" # Disables Security plugin ulimits: memlock: soft: -1 # Set memlock to unlimited (no soft or hard limit) hard: -1 nofile: soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536 hard: 65536 volumes: - opensearch-data2:/usr/share/opensearch/data # Creates volume called opensearch-data2 and mounts it to the container networks: - opensearch-net # All of the containers will join the same Docker bridge network opensearch-dashboards: image: opensearchproject/opensearch-dashboards:latest container_name: opensearch-dashboards ports: - 5601:5601 # Map host port 5601 to container port 5601 expose: - \" 5601\" # Expose port 5601 for web access to OpenSearch Dashboards environment: - ' OPENSEARCH_HOSTS=[\"http://opensearch-node1:9200\",\"http://opensearch-node2:9200\"]' - \" DISABLE_SECURITY_DASHBOARDS_PLUGIN=true\" # disables security dashboards plugin in OpenSearch Dashboards networks: - opensearch-net volumes: opensearch-data1: opensearch-data2: networks: opensearch-net: copy Configuring basic security settings\nBefore making your OpenSearch cluster available to external hosts, it’s a good idea to review the deployment’s security configuration. You may recall from the first Sample docker-compose.yml file that, unless disabled by setting DISABLE_SECURITY_PLUGIN=true, a bundled script will apply a default demo security configuration to the nodes in the cluster. Because this configuration is used for demo purposes, the default usernames and passwords are known. For that reason, we recommend that you create your own security configuration files and use volumes to pass these files to the containers. For specific guidance on OpenSearch security settings, see Security configuration.\nTo use your own certificates in your configuration, add all of the necessary certificates to the volumes section of the compose file: volumes: -./root-ca.pem:/usr/share/opensearch/config/root-ca.pem -./admin.pem:/usr/share/opensearch/config/admin.pem -./admin-key.pem:/usr/share/opensearch/config/admin-key.pem -./node1.pem:/usr/share/opensearch/config/node1.pem -./node1-key.pem:/usr/share/opensearch/config/node1-key.pem copy When you add TLS certificates to your OpenSearch nodes with Docker Compose volumes, you should also include a custom opensearch.yml file that defines those certificates. For example: volumes: -./root-ca.pem:/usr/share/opensearch/config/root-ca.pem -./admin.pem:/usr/share/opensearch/config/admin.pem -./admin-key.pem:/usr/share/opensearch/config/admin-key.pem -./node1.pem:/usr/share/opensearch/config/node1.pem -./node1-key.pem:/usr/share/opensearch/config/node1-key.pem -./custom-opensearch.yml:/usr/share/opensearch/config/opensearch.yml copy Remember that the certificates you specify in your compose file must be the same as the certificates defined in your custom opensearch.yml file. You should replace the root, admin, and node certificates with your own. For more information see Configure TLS certificates. plugins.security.ssl.transport.pemcert_filepath: node1.pem plugins.security.ssl.transport.pemkey_filepath: node1-key.pem plugins.security.ssl.transport.pemtrustedcas_filepath: root-ca.pem plugins.security.ssl.http.pemcert_filepath: node1.pem plugins.security.ssl.http.pemkey_filepath: node1-key.pem plugins.security.ssl.http.pemtrustedcas_filepath: root-ca.pem plugins.security.authcz.admin_dn: - CN=admin,OU=SSL,O=Test,L=Test,C=DE copy After configuring security settings, your custom opensearch.yml file might look something like the following example, which adds TLS certificates and the distinguished name (DN) of the admin certificate, defines a few permissions, and enables verbose audit logging: plugins.security.ssl.transport.pemcert_filepath: node1.pem plugins.security.ssl.transport.pemkey_filepath: node1-key.pem plugins.security.ssl.transport.pemtrustedcas_filepath: root-ca.pem plugins.security.ssl.transport.enforce_hostname_verification: false plugins.security.ssl.http.enabled: true plugins.security.ssl.http.pemcert_filepath: node1.pem plugins.security.ssl.http.pemkey_filepath: node1-key.pem plugins.security.ssl.http.pemtrustedcas_filepath: root-ca.pem plugins.security.allow_default_init_securityindex: true plugins.security.authcz.admin_dn: - CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA plugins.security.nodes_dn: - ' CN=N,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' plugins.security.audit.type: internal_opensearch plugins.security.enable_snapshot_restore_privilege: true plugins.security.check_snapshot_restore_write_privileges: true plugins.security.restapi.roles_enabled: [ \" all_access\", \" security_rest_api_access\"] cluster.routing.allocation.disk.threshold_enabled: false opendistro_security.audit.config.disabled_rest_categories: NONE opendistro_security.audit.config.disabled_transport_categories: NONE copy For a full list of settings, see Security.\nUse the same process to specify a Backend configuration in /usr/share/opensearch/config/opensearch-security/config.yml as well as new internal users, roles, mappings, action groups, and tenants in their respective YAML files.\nAfter replacing the certificates and creating your own internal users, roles, mappings, action groups, and tenants, use Docker Compose to start the cluster: docker-compose up -d copy Working with plugins\nTo use the OpenSearch image with a custom plugin, you must first create a Dockerfile. Review the official Docker documentation for information about creating a Dockerfile. FROM opensearchproject/opensearch:latest\nRUN /usr/share/opensearch/bin/opensearch-plugin install --batch &lt;pluginId&gt; Then run the following commands: # Build an image from a Dockerfile docker build --tag = opensearch-custom-plugin. # Start the container from the custom image docker run -p 9200:9200 -p 9600:9600 -v /usr/share/opensearch/data opensearch-custom-plugin Alternatively, you might want to remove a plugin from an image before deploying it. This example Dockerfile removes the Security plugin: FROM opensearchproject/opensearch:latest\nRUN /usr/share/opensearch/bin/opensearch-plugin remove opensearch-security copy You can also use a Dockerfile to pass your own certificates for use with the Security plugin: FROM opensearchproject/opensearch:latest\nCOPY --chown=opensearch:opensearch opensearch.yml /usr/share/opensearch/config/\nCOPY --chown=opensearch:opensearch my-key-file.pem /usr/share/opensearch/config/\nCOPY --chown=opensearch:opensearch my-certificate-chain.pem /usr/share/opensearch/config/\nCOPY --chown=opensearch:opensearch my-root-cas.pem /usr/share/opensearch/config/ copy Related links OpenSearch configuration Performance analyzer Install and configure OpenSearch Dashboards About Security in OpenSearch",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-opensearch/helm/",
    "title": "Helm",
    "content": "Helm is a package manager that allows you to easily install and manage OpenSearch in a Kubernetes cluster. You can define your OpenSearch configurations in a YAML file and use Helm to deploy your applications in a version-controlled and reproducible way.\nThe Helm chart contains the resources described in the following table. Resource Description Chart.yaml Information about the chart. values.yaml Default configuration values for the chart. templates Templates that combine with values to generate the Kubernetes manifest files. The specification in the default Helm chart supports many standard use cases and setups. You can modify the default chart to configure your desired specifications and set Transport Layer Security (TLS) and role-based access control (RBAC).\nFor information about the default configuration, steps to configure security, and configurable parameters, see the README.\nThe instructions here assume you have a Kubernetes cluster with Helm preinstalled. See the Kubernetes documentation for steps to configure a Kubernetes cluster and the Helm documentation to install Helm.\nPrerequisites\nThe default Helm chart deploys a three-node cluster. We recommend that you have at least 8 GiB of memory available for this deployment. You can expect the deployment to fail if, say, you have less than 4 GiB of memory available.\nInstall OpenSearch using Helm\nAdd opensearch helm-charts repository to Helm: helm repo add opensearch https://opensearch-project.github.io/helm-charts/ copy Update the available charts locally from charts repositories: helm repo update copy To search for the OpenSearch-related Helm charts: helm search repo opensearch copy NAME CHART VERSION\tAPP VERSION\tDESCRIPTION\nopensearch/opensearch 1.0.7 1.0.0 A Helm chart for OpenSearch\nopensearch/opensearch-dashboards\t1.0.4 1.0.0 A Helm chart for OpenSearch Dashboards Deploy OpenSearch: helm install my-deployment opensearch/opensearch copy You can also build the opensearch-1.0.0.tgz file manually:\nChange to the opensearch directory: cd charts/opensearch copy Package the Helm chart: helm package. copy Deploy OpenSearch: helm install --generate-name opensearch-1.0.0.tgz copy The output shows you the specifications instantiated from the install.\nTo customize the deployment, pass in the values that you want to override with a custom YAML file: helm install --values = customvalues.yaml opensearch-1.0.0.tgz copy Sample output NAME: opensearch-1-1629223146 LAST DEPLOYED: Tue Aug 17 17:59:07 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Watch all cluster members come up. $ kubectl get pods --namespace=default -l app=opensearch-cluster-master -w To make sure your OpenSearch pod is up and running, run the following command: $ kubectl get pods\nNAME READY STATUS RESTARTS AGE\nopensearch-cluster-master-0 1/1 Running 0 3m56s\nopensearch-cluster-master-1 1/1 Running 0 3m56s\nopensearch-cluster-master-2 1/1 Running 0 3m56s To access the OpenSearch shell: $ kubectl exec -it opensearch-cluster-master-0 -- /bin/bash copy You can send requests to the pod to verify that OpenSearch is up and running: $ curl -XGET https://localhost: 9200 -u 'admin:admin' --insecure { \"name\": \"opensearch-cluster-master-1\", \"cluster_name\": \"opensearch-cluster\", \"cluster_uuid\": \"hP2gq5bPS3SLp8Z7wXm8YQ\", \"version\": { \"distribution\": \"opensearch\", \"number\": &lt;version&gt;, \"build_type\": &lt;build-type&gt;, \"build_hash\": &lt;build-hash&gt;, \"build_date\": &lt;build-date&gt;, \"build_snapshot\": false, \"lucene_version\": &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\": \"6.8.0\", \"minimum_index_compatibility_version\": \"6.0.0-beta1\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Uninstall using Helm\nTo identify the OpenSearch deployment that you want to delete: $ helm list\nNAME NAMESPACEREVISIONUPDATED STATUS CHART APP VERSION\nopensearch-1-1629223146 default 1 2021-08-17 17:59:07.664498239 +0000 UTCdeployedopensearch-1.0.0 1.0.0 To delete or uninstall a deployment, run the following command: helm delete opensearch-1-1629223146 copy For steps to install OpenSearch Dashboards, see Helm to install OpenSearch Dashboards.",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-opensearch/index/",
    "title": "Installing OpenSearch",
    "content": "This section details how to install OpenSearch on your host, including which operating systems are compatible with OpenSearch, which ports to open, and which important settings to configure on your host.\nOperating system compatibility\nWe recommend installing OpenSearch on Red Hat Enterprise Linux (RHEL) or Debian-based Linux distributions that use systemd, such as CentOS, Amazon Linux 2, or Ubuntu Long-Term Support (LTS). OpenSearch should work on most Linux distributions, but we only test a handful. We recommend RHEL 7 or 8, CentOS 7 or 8, Amazon Linux 2, or Ubuntu 16.04, 18.04, or 20.04 for any version of OpenSearch. OpenSearch also supports Windows Server 2019.\nFile system recommendations\nAvoid using a network file system for node storage in a production workflow. Using a network file system for node storage can cause performance issues in your cluster due to factors such as network conditions (like latency or limited throughput) or read/write speeds. You should use solid-state drives (SSDs) installed on the host for node storage where possible.\nJava compatibility\nThe OpenSearch distribution for Linux ships with a compatible Adoptium JDK version of Java in the jdk directory. To find the JDK version, run./jdk/bin/java -version. For example, the OpenSearch 1.0.0 tarball ships with Java 15.0.1+9 (non-LTS), OpenSearch 1.3.0 ships with Java 11.0.14.1+1 (LTS), and OpenSearch 2.0.0 ships with Java 17.0.2+8 (LTS). OpenSearch is tested with all compatible Java versions. OpenSearch Version Compatible Java Versions Bundled Java Version 1.0 - 1.2.x\n11, 15\n15.0.1+9\n1.3.x\n8, 11, 14\n11.0.14.1+1\n2.0.0\n11, 17\n17.0.2+8 To use a different Java installation, set the OPENSEARCH_JAVA_HOME or JAVA_HOME environment variable to the Java install location. For example: export OPENSEARCH_JAVA_HOME = /path/to/opensearch-2.7.0/jdk Network requirements\nThe following ports need to be open for OpenSearch components. Port number OpenSearch component 443\nOpenSearch Dashboards in AWS OpenSearch Service with encryption in transit (TLS)\n5601\nOpenSearch Dashboards\n9200\nOpenSearch REST API\n9250\nCross-cluster search\n9300\nNode communication and transport\n9600\nPerformance Analyzer Important settings\nFor production workloads, make sure the Linux setting vm.max_map_count is set to at least 262144. Even if you use the Docker image, set this value on the host machine. To check the current value, run this command: cat /proc/sys/vm/max_map_count To increase the value, add the following line to /etc/sysctl.conf: vm.max_map_count=262144 Then run sudo sysctl -p to reload.\nThe sample docker-compose.yml file also contains several key settings: bootstrap.memory_lock=true Disables swapping (along with memlock). Swapping can dramatically decrease performance and stability, so you should ensure it is disabled on production clusters. OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m Sets the size of the Java heap (we recommend half of system RAM). nofile 65536 Sets a limit of 65536 open files for the OpenSearch user. port 9600 Allows you to access Performance Analyzer on port 9600.\nDo not declare the same JVM options in multiple locations because it can result in unexpected behavior or a failure of the OpenSearch service to start. If you declare JVM options using an environment variable, such as OPENSEARCH_JAVA_OPTS=-Xms3g -Xmx3g, then you should comment out any references to that JVM option in config/jvm.options. Conversely, if you define JVM options in config/jvm.options, then you should not define those JVM options using environment variables.",
    "ancestors": [
      "Install and upgrade"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-opensearch/rpm/",
    "title": "RPM",
    "content": "Installing OpenSearch using RPM Package Manager (RPM) simplifies the process considerably compared to the Tarball method. Several technical considerations, such as the installation path, location of configuration files, and creation of a service managed by systemd, as examples, are handled automatically by the package manager.\nGenerally speaking, installing OpenSearch from the RPM distribution can be broken down into a few steps: Download and install OpenSearch. Install manually from an RPM package or from a YUM repository. (Optional) Test OpenSearch. Confirm that OpenSearch is able to run before you apply any custom configuration.\nThis can be done without any security (no password, no certificates) or with a demo security configuration that can be applied by a packaged script. Configure OpenSearch for your environment. Apply basic settings to OpenSearch and start using it in your environment.\nThe RPM distribution provides everything you need to run OpenSearch inside Red Hat or Red Hat–based Linux Distributions, such as supported CentOS and RHEL versions, and Amazon Linux 2. If you have your own Java installation and set JAVA_HOME in your terminal application, macOS works, as well.\nThis guide assumes that you are comfortable working from the Linux command line interface (CLI). You should understand how to input commands, navigate between directories, and edit text files. Some example commands reference the vi text editor, but you may use any text editor available.\nStep 1: Download and install OpenSearch\nInstall OpenSearch from a package\nDownload the RPM package for the desired version directly from the OpenSearch downloads page. The RPM package can be downloaded for both x64 and arm64 architectures.\nImport the public GNU Privacy Guard (GPG) key. This key verifies that your OpenSearch instance is signed. sudo rpm --import https://artifacts.opensearch.org/publickeys/opensearch.pgp copy From the CLI, you can install the package with rpm or yum. # Install the x64 package using yum. sudo yum install opensearch-2.7.0-linux-x64.rpm # Install the x64 package using rpm. sudo rpm -ivh opensearch-2.7.0-linux-x64.rpm # Install the arm64 package using yum. sudo yum install opensearch-2.7.0-linux-x64.rpm # Install the arm64 package using rpm. sudo rpm -ivh opensearch-2.7.0-linux-x64.rpm After the installation succeeds, enable OpenSearch as a service. sudo systemctl enable opensearch copy Start OpenSearch. sudo systemctl start opensearch copy Verify that OpenSearch launched correctly. sudo systemctl status opensearch copy Install OpenSearch from a YUM repository\nYUM, the primary package management tool for Red Hat–based operating systems, allows you to download and install the RPM package from the YUM repository.\nCreate a local repository file for OpenSearch: sudo curl -SL https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo -o /etc/yum.repos.d/opensearch-2.x.repo copy Clean your YUM cache to ensure a smooth installation: sudo yum clean all copy Verify that the repository was created successfully. sudo yum repolist copy With the repository file downloaded, list all available versions of OpenSearch: sudo yum list opensearch --showduplicates copy Choose the version of OpenSearch you want to install:\nUnless otherwise indicated, the latest available version of OpenSearch is installed. sudo yum install opensearch copy To install a specific version of OpenSearch: sudo yum install 'opensearch-2.7.0' copy During installation, the installer will present you with the GPG key fingerprint. Verify that the information matches the following: Fingerprint: c5b7 4989 65ef d1c2 924b a9d5 39d3 1987 9310 d3fc copy If correct, enter yes or y. The OpenSearch installation continues.\nOnce complete, you can run OpenSearch. sudo systemctl start opensearch copy Verify that OpenSearch launched correctly. sudo systemctl status opensearch copy Step 2: (Optional) Test OpenSearch\nBefore proceeding with any configuration, you should test your installation of OpenSearch. Otherwise, it can be difficult to determine whether future problems are due to installation issues or custom settings you applied after installation.\nWhen OpenSearch is installed using the RPM package, some demo security settings are automatically applied. This includes self-signed TLS certificates and several users and roles. If you would like to configure these yourself, see Set up OpenSearch in your environment.\nAn OpenSearch node in its default configuration (with demo certificates and users with default passwords) is not suitable for a production environment. If you plan to use the node in a production environment, you should, at a minimum, replace the demo TLS certificates with your own TLS certificates and update the list of internal users and passwords. See Security configuration for additional guidance to ensure that your nodes are configured according to your security requirements.\nSend requests to the server to verify that OpenSearch is running. Note the use of the --insecure flag, which is required because the TLS certificates are self-signed.\nSend a request to port 9200: curl -X GET https://localhost:9200 -u 'admin:admin' --insecure copy You should get a response that looks like this: { \"name\": \"hostname\", \"cluster_name\": \"opensearch\", \"cluster_uuid\": \"6XNc9m2gTUSIoKDqJit0PA\", \"version\": { \"distribution\": \"opensearch\", \"number\": &lt;version&gt;, \"build_type\": &lt;build-type&gt;, \"build_hash\": &lt;build-hash&gt;, \"build_date\": &lt;build-date&gt;, \"build_snapshot\": false, \"lucene_version\": &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Query the plugins endpoint: curl -X GET https://localhost:9200/_cat/plugins?v -u 'admin:admin' --insecure copy The response should look like this: name component version hostname opensearch-alerting 2.7.0 hostname opensearch-anomaly-detection 2.7.0 hostname opensearch-asynchronous-search 2.7.0 hostname opensearch-cross-cluster-replication 2.7.0 hostname opensearch-index-management 2.7.0 hostname opensearch-job-scheduler 2.7.0 hostname opensearch-knn 2.7.0 hostname opensearch-ml 2.7.0 hostname opensearch-notifications 2.7.0 hostname opensearch-notifications-core 2.7.0 hostname opensearch-observability 2.7.0 hostname opensearch-performance-analyzer 2.7.0 hostname opensearch-reports-scheduler 2.7.0 hostname opensearch-security 2.7.0 hostname opensearch-sql 2.7.0 Step 3: Set up OpenSearch in your environment\nUsers who do not have prior experience with OpenSearch may want a list of recommended settings in order to get started with the service. By default, OpenSearch is not bound to a network interface and cannot be reached by external hosts. Additionally, security settings are populated by default user names and passwords. The following recommendations will enable a user to bind OpenSearch to a network interface, create and sign TLS certificates, and configure basic authentication.\nThe following recommended settings will allow you to:\nBind OpenSearch to an IP or network interface on the host.\nSet initial and maximum JVM heap sizes.\nDefine an environment variable that points to the bundled JDK.\nConfigure your own TLS certificates—no third-party certificate authority (CA) is required.\nCreate an admin user with a custom password.\nIf you ran the security demo script, then you will need to manually reconfigure settings that were modified. Refer to Security configuration for guidance before proceeding.\nBefore modifying any configuration files, it’s always a good idea to save a backup copy before making changes. The backup file can be used to mitigate any issues caused by a bad configuration.\nOpen opensearch.yml. sudo vi /etc/opensearch/opensearch.yml copy Add the following lines: # Bind OpenSearch to the correct network interface. Use 0.0.0.0 # to include all available interfaces or specify an IP address # assigned to a specific interface. network.host: 0.0.0.0 # Unless you have already configured a cluster, you should set # discovery.type to single-node, or the bootstrap checks will # fail when you try to start the service. discovery.type: single-node # If you previously disabled the Security plugin in opensearch.yml, # be sure to re-enable it. Otherwise you can skip this setting. plugins.security.disabled: false copy Save your changes and close the file.\nSpecify initial and maximum JVM heap sizes.\nOpen jvm.options. vi /etc/opensearch/jvm.options copy Modify the values for initial and maximum heap sizes. As a starting point, you should set these values to half of the available system memory. For dedicated hosts this value can be increased based on your workflow requirements.\nAs an example, if the host machine has 8 GB of memory, then you might want to set the initial and maximum heap sizes to 4 GB: -Xms4g -Xmx4g copy Save your changes and close the file.\nConfigure TLS\nTLS certificates provide additional security for your cluster by allowing clients to confirm the identity of hosts and encrypt traffic between the client and host. For more information, refer to Configure TLS Certificates and Generate Certificates, which are included in the Security plugin documentation. For work performed in a development environment, self-signed certificates are usually adequate. This section will guide you through the basic steps required to generate your own TLS certificates and apply them to your OpenSearch host.\nNavigate to the directory where the certificates will be stored. cd /etc/opensearch copy Delete the demo certificates. sudo rm -f * pem copy Generate a root certificate. This is what you will use to sign your other certificates. # Create a private key for the root certificate sudo openssl genrsa -out root-ca-key.pem 2048 # Use the private key to create a self-signed root certificate. Be sure to # replace the arguments passed to -subj so they reflect your specific host. sudo openssl req -new -x509 -sha256 -key root-ca-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=ROOT\" -out root-ca.pem -days 730 Next, create the admin certificate. This certificate is used to gain elevated rights for performing administrative tasks relating to the Security plugin. # Create a private key for the admin certificate. sudo openssl genrsa -out admin-key-temp.pem 2048 # Convert the private key to PKCS#8. sudo openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem # Create the certficiate signing request (CSR). A common name (CN) of \"A\" is acceptable because this certificate is # used for authenticating elevated access and is not tied to a host. sudo openssl req -new -key admin-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=A\" -out admin.csr # Sign the admin certificate with the root certificate and private key you created earlier. sudo openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem -days 730 Create a certificate for the node being configured. # Create a private key for the node certificate. sudo openssl genrsa -out node1-key-temp.pem 2048 # Convert the private key to PKCS#8. sudo openssl pkcs8 -inform PEM -outform PEM -in node1-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node1-key.pem # Create the CSR and replace the arguments passed to -subj so they reflect your specific host. # The CN should match a DNS A record for the host-do not use the hostname. sudo openssl req -new -key node1-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.dns.a-record\" -out node1.csr # Create an extension file that defines a SAN DNS name for the host. This # should match the DNS A record of the host. sudo sh -c 'echo subjectAltName=DNS:node1.dns.a-record &gt; node1.ext' # Sign the node certificate with the root certificate and private key that you created earlier. sudo openssl x509 -req -in node1.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node1.pem -days 730 -extfile node1.ext Remove temporary files that are no longer required. sudo rm -f * temp.pem * csr * ext copy Make sure the remaining certificates are owned by the opensearch user. sudo chown opensearch:opensearch admin-key.pem admin.pem node1-key.pem node1.pem root-ca-key.pem root-ca.pem root-ca.srl copy Add these certificates to opensearch.yml as described in Generate Certificates. Advanced users might also choose to append the settings using a script: #! /bin/bash # Before running this script, make sure to replace the CN in the # node's distinguished name with a real DNS A record. echo \"plugins.security.ssl.transport.pemcert_filepath: /etc/opensearch/node1.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.transport.pemkey_filepath: /etc/opensearch/node1-key.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.transport.pemtrustedcas_filepath: /etc/opensearch/root-ca.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.enabled: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemcert_filepath: /etc/opensearch/node1.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemkey_filepath: /etc/opensearch/node1-key.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.ssl.http.pemtrustedcas_filepath: /etc/opensearch/root-ca.pem\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.allow_default_init_securityindex: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.authcz.admin_dn:\" | sudo tee -a /etc/opensearch/opensearch.yml echo \" - 'CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.nodes_dn:\" | sudo tee -a /etc/opensearch/opensearch.yml echo \" - 'CN=node1.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.audit.type: internal_opensearch\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.enable_snapshot_restore_privilege: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.check_snapshot_restore_write_privileges: true\" | sudo tee -a /etc/opensearch/opensearch.yml echo \"plugins.security.restapi.roles_enabled: [ \\\" all_access \\\", \\\" security_rest_api_access \\\"]\" | sudo tee -a /etc/opensearch/opensearch.yml copy (Optional) Add trust for the self-signed root certificate. # Copy the root certificate to the correct directory sudo cp /etc/opensearch/root-ca.pem /etc/pki/ca-trust/source/anchors/ # Add trust sudo update-ca-trust Configure a user\nUsers are defined and authenticated by OpenSearch in a variety of ways. One method that does not require additional backend infrastructure is to manually configure users in internal_users.yml. See YAML files for more information about configuring users. The following steps explain how to remove all demo users except for the admin user and how to replace the admin default password using a script.\nNavigate to the Security plugins tools directory. cd /usr/share/opensearch/plugins/opensearch-security/tools copy Run hash.sh to generate a new password.\nThis script will fail if a path to the JDK has not been defined. # Example output if a JDK isn't found... $./hash.sh ************************************************************************** ** This tool will be deprecated in the next major release of OpenSearch ** ** https://github.com/opensearch-project/security/issues/1755 ** ************************************************************************** which: no java in ( /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/user/.local/bin:/home/user/bin) WARNING: nor OPENSEARCH_JAVA_HOME nor JAVA_HOME is set, will use./hash.sh: line 35: java: command not found Declare an environment variable when you invoke the script in order to avoid issues: OPENSEARCH_JAVA_HOME = /usr/share/opensearch/jdk./hash.sh copy Enter the desired password at the prompt and make a note of the output hash.\nOpen internal_users.yml. sudo vi /etc/opensearch/opensearch-security/internal_users.yml copy Remove all demo users except for admin and replace the hash with the output provided by hash.sh in a previous step. The file should look similar to the following example: --- # This is the internal user database # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh _meta: type: \"internalusers\" config_version: 2 # Define your internal users here admin: hash: \" $2y$1EXAMPLEQqwS8TUcoEXAMPLEeZ3lEHvkEXAMPLERqjyh1icEXAMPLE.\" reserved: true backend_roles:\n- \"admin\" description: \"Admin user\" copy Apply changes\nNow that TLS certificates are installed and demo users were removed or assigned new passwords, the last step is to apply the configuration changes. This last configuration step requires invoking securityadmin.sh while OpenSearch is running on the host.\nOpenSearch must be running for securityadmin.sh to apply changes. If you made changes to opensearch.yml, restart OpenSearch. sudo systemctl restart opensearch Open a separate terminal session with the host and navigate to the directory containing securityadmin.sh. # Change to the correct directory cd /usr/share/opensearch/plugins/opensearch-security/tools Invoke the script. See Apply changes using securityadmin.sh for definitions of the arguments you must pass. # You can omit the environment variable if you declared this in your $PATH. OPENSEARCH_JAVA_HOME = /usr/share/opensearch/jdk./securityadmin.sh -cd /etc/opensearch/opensearch-security/ -cacert /etc/opensearch/root-ca.pem -cert /etc/opensearch/admin.pem -key /etc/opensearch/admin-key.pem -icl -nhnv Verify that the service is running\nOpenSearch is now running on your host with custom TLS certificates and a secure user for basic authentication. You can verify external connectivity by sending an API request to your OpenSearch node from another host.\nDuring the previous test you directed requests to localhost. Now that TLS certificates have been applied and the new certificates reference your host’s actual DNS record, requests to localhost will fail the CN check and the certificate will be considered invalid. Instead, requests should be sent to the address you specified while generating the certificate.\nYou should add trust for the root certificate to your client before sending requests. If you do not add trust, then you must use the -k option so that cURL ignores CN and root certificate validation. $ curl https://your.host.address:9200 -u admin:yournewpassword -k { \"name\": \"hostname-here\", \"cluster_name\": \"opensearch\", \"cluster_uuid\": \"efC0ANNMQlGQ5TbhNflVPg\", \"version\": { \"distribution\": \"opensearch\", \"number\": &lt;version&gt;, \"build_type\": &lt;build-type&gt;, \"build_hash\": &lt;build-hash&gt;, \"build_date\": &lt;build-date&gt;, \"build_snapshot\": false, \"lucene_version\": &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Upgrade to a newer version\nOpenSearch instances installed using RPM or YUM can be easily upgraded to a newer version. We recommend updating with YUM, but you can also upgrade using RPM.\nManual upgrade with RPM\nDownload the RPM package for the desired upgrade version directly from the OpenSearch downloads page.\nNavigate to the directory containing the distribution and run the following command: rpm -Uvh opensearch-2.7.0-linux-x64.rpm copy YUM\nTo upgrade to the latest version of OpenSearch using YUM: sudo yum update copy You can also upgrade to a specific OpenSearch version: sudo yum update opensearch-&lt;version-number&gt; copy Related links OpenSearch configuration Install and configure OpenSearch Dashboards OpenSearch plugin installation About the Security plugin",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-opensearch/tar/",
    "title": "Tarball",
    "content": "Installing OpenSearch from a tarball, also known as a tar archive, may appeal to users who want granular control over installation details like file permissions and installation paths.\nGenerally speaking, the installation of OpenSearch from a tarball can be broken down into a few steps: Download and unpack OpenSearch. Configure important system settings. These settings are applied to the host before modifying any OpenSearch files. (Optional) Test OpenSearch. Confirm that OpenSearch is able to run before you apply any custom configuration.\nThis can be done without any security (no password, no certificates) or with a demo security configuration that can be applied by a packaged script. Configure OpenSearch for your environment. Apply basic settings to OpenSearch and start using it in your environment.\nThe tarball is a self-contained directory with everything needed to run OpenSearch, including an integrated Java Development Kit (JDK). This installation method is compatible with most Linux distributions, including CentOS 7, Amazon Linux 2, and Ubuntu 18.04. If you have your own Java installation and set the environment variable JAVA_HOME in the terminal, macOS works as well.\nThis guide assumes that you are comfortable working from the Linux command line interface (CLI). You should understand how to input commands, navigate between directories, and edit text files. Some example commands reference the vi text editor, but you may use any text editor available.\nStep 1: Download and unpack OpenSearch\nDownload the appropriate tar.gz archive from the OpenSearch downloads page or by using the command line (such as with wget). # x64 wget https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.tar.gz # ARM64 wget https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-arm64.tar.gz Extract the contents of the tarball. # x64 tar -xvf opensearch-2.7.0-linux-x64.tar.gz # ARM64 tar -xvf opensearch-2.7.0-linux-arm64.tar.gz Step 2: Configure important system settings\nBefore launching OpenSearch you should review some important system settings.\nDisable memory paging and swapping performance on the host to improve performance. sudo swapoff -a copy Increase the number of memory maps available to OpenSearch. # Edit the sysctl config file sudo vi /etc/sysctl.conf # Add a line to define the desired value # or change the value if the key exists, # and then save your changes. vm.max_map_count = 262144 # Reload the kernel parameters using sysctl sudo sysctl -p # Verify that the change was applied by checking the value cat /proc/sys/vm/max_map_count Step 3: (Optional) Test OpenSearch\nBefore proceeding you should test your installation of OpenSearch. Otherwise, it can be difficult to determine whether future problems are due to installation issues or custom settings you applied after installation. There are two quick methods for testing OpenSearch at this stage: (Security enabled) Apply a generic configuration using the demo security script included in the tar archive. (Security disabled) Manually disable the Security plugin and test the instance before applying your own custom security settings.\nThe demo security script will apply a generic configuration to your instance of OpenSearch. This configuration defines some environment variables and also applies self-signed TLS certificates. If you would like to configure these yourself, see Step 4: Set up OpenSearch in your environment.\nIf you only want to verify that the service is properly configured and you intend to configure security settings yourself, then you may want to disable the Security plugin and launch the service without encryption or authentication.\nAn OpenSearch node configured by the demo security script is not suitable for a production environment. If you plan to use the node in a production environment after running opensearch-tar-install.sh, you should, at a minimum, replace the demo TLS certificates with your own TLS certificates and update the list of internal users and passwords. See Security configuration for additional guidance to ensure that your nodes are configured according to your security requirements.\nOption 1: Test your Opensearch settings with security enabled\nChange to the top directory of your OpenSearch installation. cd /path/to/opensearch-2.7.0 copy Run the demo security script../opensearch-tar-install.sh copy Open another terminal session and send requests to the server to verify that OpenSearch is running. Note the use of the --insecure flag, which is required because the TLS certificates are self-signed.\nSend a request to port 9200: curl -X GET https://localhost:9200 -u 'admin:admin' --insecure copy You should get a response that looks like this: { \"name\": \"hostname\", \"cluster_name\": \"opensearch\", \"cluster_uuid\": \"6XNc9m2gTUSIoKDqJit0PA\", \"version\": { \"distribution\": \"opensearch\", \"number\": &lt;version&gt;, \"build_type\": &lt;build-type&gt;, \"build_hash\": &lt;build-hash&gt;, \"build_date\": &lt;build-date&gt;, \"build_snapshot\": false, \"lucene_version\": &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Query the plugins endpoint: curl -X GET https://localhost:9200/_cat/plugins?v -u 'admin:admin' --insecure copy The response should look like this: name component version hostname opensearch-alerting 2.7.0 hostname opensearch-anomaly-detection 2.7.0 hostname opensearch-asynchronous-search 2.7.0 hostname opensearch-cross-cluster-replication 2.7.0 hostname opensearch-index-management 2.7.0 hostname opensearch-job-scheduler 2.7.0 hostname opensearch-knn 2.7.0 hostname opensearch-ml 2.7.0 hostname opensearch-notifications 2.7.0 hostname opensearch-notifications-core 2.7.0 hostname opensearch-observability 2.7.0 hostname opensearch-performance-analyzer 2.7.0 hostname opensearch-reports-scheduler 2.7.0 hostname opensearch-security 2.7.0 hostname opensearch-sql 2.7.0 Return to the original terminal session and stop the process by pressing CTRL + C.\nOption 2: Test your OpenSearch settings with security disabled\nOpen the configuration file. vi /path/to/opensearch-2.7.0/config/opensearch.yml copy Add the following line to disable the Security plugin: plugins.security.disabled: true copy Save the change and close the file.\nOpen another terminal session and send requests to the server to verify that OpenSearch is running. Because the Security plugin has been disabled, you will be sending commands using HTTP rather than HTTPS.\nSend a request to port 9200. curl -X GET http://localhost:9200 copy You should get a response that looks like this: { \"name\": \"hostname\", \"cluster_name\": \"opensearch\", \"cluster_uuid\": \"6XNc9m2gTUSIoKDqJit0PA\", \"version\": { \"distribution\": \"opensearch\", \"number\": &lt;version&gt;, \"build_type\": &lt;build-type&gt;, \"build_hash\": &lt;build-hash&gt;, \"build_date\": &lt;build-date&gt;, \"build_snapshot\": false, \"lucene_version\": &lt;lucene-version&gt;, \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Query the plugins endpoint. curl -X GET http://localhost:9200/_cat/plugins?v copy The response should look like this: name component version hostname opensearch-alerting 2.7.0 hostname opensearch-anomaly-detection 2.7.0 hostname opensearch-asynchronous-search 2.7.0 hostname opensearch-cross-cluster-replication 2.7.0 hostname opensearch-index-management 2.7.0 hostname opensearch-job-scheduler 2.7.0 hostname opensearch-knn 2.7.0 hostname opensearch-ml 2.7.0 hostname opensearch-notifications 2.7.0 hostname opensearch-notifications-core 2.7.0 hostname opensearch-observability 2.7.0 hostname opensearch-performance-analyzer 2.7.0 hostname opensearch-reports-scheduler 2.7.0 hostname opensearch-security 2.7.0 hostname opensearch-sql 2.7.0 Step 4: Set up OpenSearch in your environment\nUsers who do not have prior experience with OpenSearch may want a list of recommended settings in order to get started with the service. By default, OpenSearch is not bound to a network interface and cannot be reached by external hosts. Additionally, security settings are either undefined (greenfield install) or populated by default usernames and passwords if you ran the security demo script by invoking opensearch-tar-install.sh. The following recommendations will enable a user to bind OpenSearch to a network interface, create and sign TLS certificates, and configure basic authentication.\nThe following recommended settings will allow you to:\nBind OpenSearch to an IP or network interface on the host.\nSet initial and maximum JVM heap sizes.\nDefine an environment variable that points to the bundled JDK.\nConfigure your own TLS certificates - no third-party certificate authority (CA) is required.\nCreate an admin user with a custom password.\nIf you ran the security demo script, then you will need to manually reconfigure settings that were modified. Refer to Security configuration for guidance before proceeding.\nBefore modifying any configuration files, it’s always a good idea to save a backup copy before making changes. The backup file can be used to revert any issues caused by a bad configuration.\nOpen opensearch.yml. vi /path/to/opensearch-2.7.0/config/opensearch.yml copy Add the following lines. # Bind OpenSearch to the correct network interface. Use 0.0.0.0 # to include all available interfaces or specify an IP address # assigned to a specific interface. network.host: 0.0.0.0 # Unless you have already configured a cluster, you should set # discovery.type to single-node, or the bootstrap checks will # fail when you try to start the service. discovery.type: single-node # If you previously disabled the Security plugin in opensearch.yml, # be sure to re-enable it. Otherwise you can skip this setting. plugins.security.disabled: false copy Save your changes and close the file.\nSpecify initial and maximum JVM heap sizes.\nOpen jvm.options. vi /path/to/opensearch-2.7.0/config/jvm.options copy Modify the values for initial and maximum heap sizes. As a starting point, you should set these values to half of the available system memory. For dedicated hosts this value can be increased based on your workflow requirements.\nAs an example, if the host machine has 8 GB of memory then you might want to set the initial and maximum heap sizes to 4 GB: -Xms4g -Xmx4g copy Save your changes and close the file.\nSpecify the location of the included JDK. export OPENSEARCH_JAVA_HOME = /path/to/opensearch-2.7.0/jdk copy Configure TLS\nTLS certificates provide additional security for your cluster by allowing clients to confirm the identity of hosts and encrypt traffic between the client and host. For more information, refer to Configure TLS Certificates and Generate Certificates, which are included in the Security plugin documentation. For work performed in a development environment, self-signed certificates are usually adequate. This section will guide you through the basic steps required to generate your own TLS certificates and apply them to your OpenSearch host.\nNavigate to the OpenSearch config directory. This is where the certificates will be stored. cd /path/to/opensearch-2.7.0/config/ copy Generate a root certificate. This is what you will use to sign your other certificates. # Create a private key for the root certificate openssl genrsa -out root-ca-key.pem 2048 # Use the private key to create a self-signed root certificate. Be sure to # replace the arguments passed to -subj so they reflect your specific host. openssl req -new -x509 -sha256 -key root-ca-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=ROOT\" -out root-ca.pem -days 730 Next, create the admin certificate. This certificate is used to gain elevated rights for performing administrative tasks relating to the Security plugin. # Create a private key for the admin certificate. openssl genrsa -out admin-key-temp.pem 2048 # Convert the private key to PKCS#8. openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem # Create the CSR. A common name (CN) of \"A\" is acceptable because this certificate is # used for authenticating elevated access and is not tied to a host. openssl req -new -key admin-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=A\" -out admin.csr # Sign the admin certificate with the root certificate and private key you created earlier. openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem -days 730 Create a certificate for the node being configured. # Create a private key for the node certificate. openssl genrsa -out node1-key-temp.pem 2048 # Convert the private key to PKCS#8. openssl pkcs8 -inform PEM -outform PEM -in node1-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node1-key.pem # Create the CSR and replace the arguments passed to -subj so they reflect your specific host. # The CN should match a DNS A record for the host--do not use the hostname. openssl req -new -key node1-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.dns.a-record\" -out node1.csr # Create an extension file that defines a SAN DNS name for the host. This # should match the DNS A record of the host. echo 'subjectAltName=DNS:node1.dns.a-record' &gt; node1.ext # Sign the node certificate with the root certificate and private key that you created earlier. openssl x509 -req -in node1.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node1.pem -days 730 -extfile node1.ext Remove temporary files that are no longer required. rm * temp.pem * csr * ext copy Add these certificates to opensearch.yml as described in Generate Certificates. Advanced users might also choose to append the settings using a script: #! /bin/bash # Before running this script, make sure to replace the /path/to your OpenSearch directory, # and remember to replace the CN in the node's distinguished name with a real # DNS A record. echo \"plugins.security.ssl.transport.pemcert_filepath: /path/to/opensearch-2.7.0/config/node1.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.transport.pemkey_filepath: /path/to/opensearch-2.7.0/config/node1-key.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.transport.pemtrustedcas_filepath: /path/to/opensearch-2.7.0/config/root-ca.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.http.enabled: true\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.http.pemcert_filepath: /path/to/opensearch-2.7.0/config/node1.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.http.pemkey_filepath: /path/to/opensearch-2.7.0/config/node1-key.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.ssl.http.pemtrustedcas_filepath: /path/to/opensearch-2.7.0/config/root-ca.pem\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.allow_default_init_securityindex: true\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.authcz.admin_dn:\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \" - 'CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.nodes_dn:\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \" - 'CN=node1.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA'\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.audit.type: internal_opensearch\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.enable_snapshot_restore_privilege: true\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.check_snapshot_restore_write_privileges: true\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml echo \"plugins.security.restapi.roles_enabled: [ \\\" all_access \\\", \\\" security_rest_api_access \\\"]\" | sudo tee -a /path/to/opensearch-2.7.0/config/opensearch.yml copy (Optional) Add trust for the self-signed root certificate. # Copy the root certificate to the correct directory sudo cp /path/to/opensearch-2.7.0/config/root-ca.pem /etc/pki/ca-trust/source/anchors/ # Add trust sudo update-ca-trust Configure a user\nUsers are defined and authenticated by OpenSearch in a variety of ways. One method, which does not require additional backend infrastructure, is to manually configure users in internal_users.yml. See YAML files for more information about configuring users. The following steps explain how to remove all demo users except for the admin user and how to replace the admin default password using a script.\nMake the Security plugin scripts executable. chmod 755 /path/to/opensearch-2.7.0/plugins/opensearch-security/tools/ *.sh copy Run hash.sh to generate a new password.\nThis script will fail if a path to the JDK has not been defined. # Example output if a JDK isn't found... $./hash.sh ************************************************************************** ** This tool will be deprecated in the next major release of OpenSearch ** ** https://github.com/opensearch-project/security/issues/1755 ** ************************************************************************** which: no java in ( /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/user/.local/bin:/home/user/bin) WARNING: nor OPENSEARCH_JAVA_HOME nor JAVA_HOME is set, will use./hash.sh: line 35: java: command not found copy Declare an environment variable when you invoke the script in order to avoid issues: OPENSEARCH_JAVA_HOME = /path/to/opensearch-2.7.0/jdk./hash.sh copy Enter the desired password at the prompt and make a note of the output hash.\nOpen internal_users.yml. vi /path/to/opensearch-2.7.0/config/opensearch-security/internal_users.yml copy Remove all demo users except for admin and replace the hash with the output provided by hash.sh in a previous step. The file should look similar to the following example: --- # This is the internal user database # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh _meta: type: \"internalusers\" config_version: 2 # Define your internal users here admin: hash: \" $2y$1EXAMPLEQqwS8TUcoEXAMPLEeZ3lEHvkEXAMPLERqjyh1icEXAMPLE.\" reserved: true backend_roles:\n- \"admin\" description: \"Admin user\" copy Apply changes\nNow that TLS certificates are installed and demo users were removed or assigned new passwords, the last step is to apply the configuration changes. This last configuration step requires invoking securityadmin.sh while OpenSearch is running on the host.\nStart OpenSearch. It must be running for securityadmin.sh to apply changes. # Change directories cd /path/to/opensearch-2.7.0/bin # Run the service in the foreground./opensearch Open a separate terminal session with the host and navigate to the directory containing securityadmin.sh. # Change to the correct directory cd /path/to/opensearch-2.7.0/plugins/opensearch-security/tools Invoke the script. See Apply changes using securityadmin.sh for definitions of the arguments you must pass. # You can omit the environment variable if you declared this in your $PATH. OPENSEARCH_JAVA_HOME = /path/to/opensearch-2.7.0/jdk./securityadmin.sh -cd /path/to/opensearch-2.7.0/config/opensearch-security/ -cacert /path/to/opensearch-2.7.0/config/root-ca.pem -cert /path/to/opensearch-2.7.0/config/admin.pem -key /path/to/opensearch-2.7.0/config/admin-key.pem -icl -nhnv Stop and restart the running OpenSearch process to apply the changes.\nVerify that the service is running\nOpenSearch is now running on your host with custom TLS certificates and a secure user for basic authentication. You can verify external connectivity by sending an API request to your OpenSearch node from another host.\nDuring previous tests you directed requests to localhost. Now that TLS certificates have been applied and the new certificates reference your host’s actual DNS record, requests to localhost will fail the CN check and the certificate will be considered invalid. Instead, requests should be sent to the address you specified while generating the certificate.\nYou should add trust for the root certificate to your client before sending requests. If you do not add trust, then you must use the -k option so that cURL ignores CN and root certificate validation. $ curl https://your.host.address:9200 -u admin:yournewpassword -k { \"name\": \"hostname-here\", \"cluster_name\": \"opensearch\", \"cluster_uuid\": \"efC0ANNMQlGQ5TbhNflVPg\", \"version\": { \"distribution\": \"opensearch\", \"number\": \"2.1.0\", \"build_type\": \"tar\", \"build_hash\": \"388c80ad94529b1d9aad0a735c4740dce2932a32\", \"build_date\": \"2022-06-30T21:31:04.823801692Z\", \"build_snapshot\": false, \"lucene_version\": \"9.2.0\", \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Run OpenSearch as a service with systemd\nThis section will guide you through creating a service for OpenSearch and registering it with systemd. After the service has been defined, you can enable, start, and stop the OpenSearch service using systemctl commands. The commands in this section reflect an environment where OpenSearch has been installed to /opt/opensearch and should be changed depending on your installation path.\nThe following configuration is only suitable for testing in a non-production environment. We do not recommend using the following configuration in a production environment. You should install OpenSearch with the RPM distribution if you want to run OpenSearch as a systemd-managed service on your host. The tarball installation does not define a specific installation path, users, roles, or permissions. Failure to properly secure your host environment can result in unexpected behavior.\nCreate a user for the OpenSearch service. sudo adduser --system --shell /bin/bash -U --no-create-home opensearch copy Add your user to the opensearch user group. sudo usermod -aG opensearch $USER copy Change the file owner to opensearch. Make sure to change the path if your OpenSearch files are in a different directory. sudo chown -R opensearch /opt/opensearch/ copy Create the service file and open it for editing. sudo vi /etc/systemd/system/opensearch.service copy Enter the following example service configuration. Make sure to change references to the path if your OpenSearch files are in a different directory. [ Unit] Description = OpenSearch Wants = network-online.target After = network-online.target [ Service] Type = forking RuntimeDirectory = data WorkingDirectory = /opt/opensearch ExecStart = /opt/opensearch/bin/opensearch -d User = opensearch Group = opensearch StandardOutput = journal StandardError = inherit LimitNOFILE = 65535 LimitNPROC = 4096 LimitAS = infinity LimitFSIZE = infinity TimeoutStopSec = 0 KillSignal = SIGTERM KillMode = process SendSIGKILL = no SuccessExitStatus = 143 TimeoutStartSec = 75 [ Install] WantedBy = multi-user.target copy Reload systemd manager configuration. sudo systemctl daemon-reload copy Enable the OpenSearch service. sudo systemctl enable opensearch.service copy Start the OpenSearch service. sudo systemctl start opensearch copy Verify that the service is running. sudo systemctl status opensearch copy Related links OpenSearch configuration Configure Performance Analyzer for Tarball Installation Install and configure OpenSearch Dashboards OpenSearch plugin installation About the Security plugin",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/install-opensearch/windows/",
    "title": "Windows",
    "content": "The following sections describe installing OpenSearch on Windows from a zip archive.\nGenerally speaking, the installation of OpenSearch from a zip archive can be broken down into a few steps: Download and unpack OpenSearch. (Optional) Test OpenSearch. Confirm that OpenSearch is able to run before you apply any custom configuration.\nThis can be done without any security (no password, no certificates) or with a demo security configuration that can be applied by a packaged script. Configure OpenSearch for your environment. Apply basic settings to OpenSearch and start using it in your environment.\nThe Windows OpenSearch archive is a self-contained directory with everything needed to run OpenSearch, including an integrated Java Development Kit (JDK). If you have your own Java installation and set the environment variable JAVA_HOME, OpenSearch will use that installation if the OPENSEARCH_JAVA_HOME environment variable is not set. To learn how to set the OPENSEARCH_JAVA_HOME environment variable, see Step 3: Set up OpenSearch in your environment.\nPrerequisites\nMake sure you have a zip utility installed.\nStep 1: Download and unpack OpenSearch\nPerform the following steps to install OpenSearch on Windows.\nDownload the opensearch-2.7.0-windows-x64.zip archive.\nTo extract the archive contents, right-click to select Extract All.\nStep 2: (Optional) Test OpenSearch\nBefore proceeding with any configuration, you should test your installation of OpenSearch. Otherwise, it can be difficult to determine whether future problems are due to installation issues or custom settings you applied after installation. There are two quick methods for testing OpenSearch at this stage: (Security enabled) Apply a generic configuration using the batch script included in the Windows archive. (Security disabled) Manually disable the Security plugin and test the instance before applying your own custom security settings.\nThe batch script will apply a generic configuration to your instance of OpenSearch. This configuration defines some environment variables and also applies self-signed TLS certificates. Alternatively, you can choose to configure these yourself.\nIf you only want to verify that the service is properly configured and you intend to configure security settings yourself, then you may want to disable the Security plugin and launch the service without encryption or authentication.\nAn OpenSearch node in its default configuration (with demo certificates and users with default passwords) is not suitable for a production environment. If you plan to use the node in a production environment, you should, at a minimum, replace the demo TLS certificates with your own TLS certificates and update the list of internal users and passwords. See Security configuration for additional guidance to ensure that your nodes are configured according to your security requirements.\nOption 1: Test your OpenSearch settings with security enabled\nRun the demo batch script.\nThere are two ways of running the batch script:\nRun the batch script using the Windows UI:\nNavigate to the top directory of your OpenSearch installation and open the opensearch-2.7.0 folder.\nRun the batch script by double-clicking the opensearch-windows-install.bat file. This opens a command prompt with an OpenSearch instance running.\nRun the batch script from Command prompt or Powershell:\nOpen Command Prompt by entering cmd, or Powershell by entering powershell, in the search box next to Start on the taskbar.\nChange to the top directory of your OpenSearch installation. cd \\path\\to\\opensearch - 2.7.0 copy Run the batch script..\\opensearch -windows-install.bat copy Open a new command prompt and send requests to the server to verify that OpenSearch is running. Note the use of the --insecure flag, which is required because the TLS certificates are self-signed.\nSend a request to port 9200: curl.exe -X GET https://localhost:9200 -u \"admin:admin\" --insecure copy You should get a response that looks like this: { \"name\": \"hostname-here\", \"cluster_name\": \"opensearch\", \"cluster_uuid\": \"7Nqtr0LrQTOveFcBb7Kufw\", \"version\": { \"distribution\": \"opensearch\", \"number\": &lt; version &gt;, \"build_type\": &lt; build -type &gt;, \"build_hash\": &lt; build -hash &gt;, \"build_date\": &lt; build -date &gt;, \"build_snapshot\": false, \"lucene_version\": &lt; lucene -version &gt;, \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Query the plugins endpoint: curl.exe -X GET https://localhost:9200/_cat/plugins? v -u \"admin:admin\" --insecure copy The response should look like this: hostname opensearch -alerting 2.7.0 hostname opensearch -anomaly-detection 2.7.0 hostname opensearch -asynchronous-search 2.7.0 hostname opensearch -cross-cluster-replication 2.7.0 hostname opensearch -geospatial 2.7.0 hostname opensearch -index-management 2.7.0 hostname opensearch -job-scheduler 2.7.0 hostname opensearch -knn 2.7.0 hostname opensearch -ml 2.7.0 hostname opensearch -neural-search 2.7.0 hostname opensearch -notifications 2.7.0 hostname opensearch -notifications-core 2.7.0 hostname opensearch -observability 2.7.0 hostname opensearch -reports-scheduler 2.7.0 hostname opensearch -security 2.7.0 hostname opensearch -security-analytics 2.7.0 hostname opensearch -sql 2.7.0 Option 2: Test your OpenSearch settings with security disabled\nOpen the opensearch-2.7.0\\config folder.\nOpen the opensearch.yml file with a text editor.\nAdd the following line to disable the Security plugin: plugins.security.disabled: true copy Save the change and close the file.\nNavigate to the top directory of your OpenSearch installation and open the opensearch-2.7.0 folder.\nRun the default by double-clicking the opensearch-windows-install.bat file. This opens a command prompt with an OpenSearch instance running.\nOpen a new command prompt and send requests to the server to verify that OpenSearch is running. Because the Security plugin has been disabled, you will be sending commands using HTTP rather than HTTPS.\nSend a request to port 9200: curl.exe -X GET http://localhost:9200 copy You should get a response that looks like this: { \"name\": \"hostname-here\", \"cluster_name\": \"opensearch\", \"cluster_uuid\": \"7Nqtr0LrQTOveFcBb7Kufw\", \"version\": { \"distribution\": \"opensearch\", \"number\": \"2.4.0\", \"build_type\": \"zip\", \"build_hash\": \"77ef9e304dd6ee95a600720a387a9735bbcf7bc9\", \"build_date\": \"2022-11-05T05:50:15.404072800Z\", \"build_snapshot\": false, \"lucene_version\": \"9.4.1\", \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Query the plugins endpoint: curl.exe -X GET http://localhost:9200/_cat/plugins? v copy The response should look like this: hostname opensearch -alerting 2.7.0 hostname opensearch -anomaly-detection 2.7.0 hostname opensearch -asynchronous-search 2.7.0 hostname opensearch -cross-cluster-replication 2.7.0 hostname opensearch -geospatial 2.7.0 hostname opensearch -index-management 2.7.0 hostname opensearch -job-scheduler 2.7.0 hostname opensearch -knn 2.7.0 hostname opensearch -ml 2.7.0 hostname opensearch -neural-search 2.7.0 hostname opensearch -notifications 2.7.0 hostname opensearch -notifications-core 2.7.0 hostname opensearch -observability 2.7.0 hostname opensearch -reports-scheduler 2.7.0 hostname opensearch -security 2.7.0 hostname opensearch -security-analytics 2.7.0 hostname opensearch -sql 2.7.0 To stop OpenSearch, press Ctrl+C in Command Prompt or Powershell, or simply close the Command Prompt or Powershell window.\nStep 3: Set up OpenSearch in your environment\nUsers who do not have prior experience with OpenSearch may want a list of recommended settings in order to get started with the service. By default, OpenSearch is not bound to a network interface and cannot be reached by external hosts. Additionally, security settings are either undefined (greenfield install) or populated by default usernames and passwords if you ran the security demo script by invoking opensearch-windows-install.bat. The following recommendations will enable a user to bind OpenSearch to a network interface.\nThe following recommended settings will allow you to:\nBind OpenSearch to an IP or network interface on the host.\nSet initial and maximum JVM heap sizes.\nDefine an environment variable that points to the bundled JDK.\nIf you ran the security demo script, then you will need to manually reconfigure settings that were modified. Refer to Security configuration for guidance before proceeding.\nBefore modifying any configuration files, it’s always a good idea to save a backup copy before making changes. The backup file can be used to revert any issues caused by a bad configuration.\nOpen the opensearch-2.7.0\\config folder.\nOpen the opensearch.yml file with a text editor.\nAdd the following lines: # Bind OpenSearch to the correct network interface. Use 0.0.0.0 # to include all available interfaces or specify an IP address # assigned to a specific interface. network.host: 0.0.0.0 # Unless you have already configured a cluster, you should set # discovery.type to single-node, or the bootstrap checks will # fail when you try to start the service. discovery.type: single-node # If you previously disabled the Security plugin in opensearch.yml, # be sure to re-enable it. Otherwise you can skip this setting. plugins.security.disabled: false copy Save your changes and close the file.\nSpecify initial and maximum JVM heap sizes.\nOpen the opensearch-2.7.0\\config folder.\nOpen the jvm.options file with a text editor.\nModify the values for initial and maximum heap sizes. As a starting point, you should set these values to half of the available system memory. For dedicated hosts this value can be increased based on your workflow requirements. As an example, if the host machine has 8 GB of memory, then you might want to set the initial and maximum heap sizes to 4 GB: -Xms4g -Xmx4g copy Save your changes and close the file.\nSpecify the location of the included JDK.\nIn the search box next to Start on the taskbar, enter edit environment variables for your account or edit the system environment variables. To edit the system environment variables, you need admin rights. User environment variables take precedence over system environment variables.\nSelect Edit environment variables for your account or Edit the system environment variables.\nIf the System Properties dialog opens, in the Advanced tab, select Environment Variables.\nUnder User variables or System variables, select New.\nIn Variable name, enter OPENSEARCH_JAVA_HOME.\nIn Variable value, enter \\path\\to\\opensearch-2.7.0\\jdk.\nSelect OK to close all dialogs.\nPlugin compatibility\nThe Performance Analyzer plugin is not available on Windows. All other OpenSearch plugins, including the k-NN plugin, are available. For a complete list of plugins, see Available plugins.\nRelated links OpenSearch configuration OpenSearch plugin installation About the Security plugin",
    "ancestors": [
      "Install and upgrade",
      "Installing OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/plugins/",
    "title": "Installing plugins",
    "content": "You can install individual plugins for OpenSearch based on your needs. For information about available plugins, see Available plugins.\nManaging plugins\nOpenSearch uses a command line tool called opensearch-plugin for managing plugins. This tool allows you to: List installed plugins. Install plugins. Remove an installed plugin.\nPrint help text by passing -h or --help. Depending on your host configuration, you might also need to run the command with sudo privileges.\nIf you are running OpenSearch in a Docker container, plugins must be installed, removed, and configured by modifying the Docker image. For information, see Working with plugins List\nUse list to see a list of plugins that have already been installed.\nUsage: bin/opensearch-plugin list Example: $./opensearch-plugin list\nopensearch-alerting\nopensearch-anomaly-detection\nopensearch-asynchronous-search\nopensearch-cross-cluster-replication\nopensearch-geospatial\nopensearch-index-management\nopensearch-job-scheduler\nopensearch-knn\nopensearch-ml\nopensearch-notifications\nopensearch-notifications-core\nopensearch-observability\nopensearch-performance-analyzer\nopensearch-reports-scheduler\nopensearch-security\nopensearch-sql You can also list installed plugins by using the CAT API.\nPath and HTTP method GET _cat/plugins Sample response opensearch-node1 opensearch-alerting 2.0.1.0\nopensearch-node1 opensearch-anomaly-detection 2.0.1.0\nopensearch-node1 opensearch-asynchronous-search 2.0.1.0\nopensearch-node1 opensearch-cross-cluster-replication 2.0.1.0\nopensearch-node1 opensearch-index-management 2.0.1.0\nopensearch-node1 opensearch-job-scheduler 2.0.1.0\nopensearch-node1 opensearch-knn 2.0.1.0\nopensearch-node1 opensearch-ml 2.0.1.0\nopensearch-node1 opensearch-notifications 2.0.1.0\nopensearch-node1 opensearch-notifications-core 2.0.1.0 Install\nThere are three ways to install plugins using the opensearch-plugin: Install a plugin by name Install a plugin by from a zip file Install a plugin using Maven coordinates Install a plugin by name:\nFor a list of plugins that can be installed by name, see Additional plugins.\nUsage: bin/opensearch-plugin install &lt;plugin-name&gt; Example: $ sudo./opensearch-plugin install analysis-icu\n-&gt; Installing analysis-icu\n-&gt; Downloading analysis-icu from opensearch [=================================================] 100%\n-&gt; Installed analysis-icu with folder name analysis-icu Install a plugin from a zip file:\nRemote zip files can be installed by replacing &lt;zip-file&gt; with the URL of the hosted file. The tool only supports downloading over HTTP/HTTPS protocols. For local zip files, replace &lt;zip-file&gt; with file: followed by the absolute or relative path to the plugin zip file as in the second example below.\nUsage: bin/opensearch-plugin install &lt;zip-file&gt; Example: # Zip file is hosted on a remote server - in this case, Maven central repository. $ sudo./opensearch-plugin install https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip\n-&gt; Installing https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip\n-&gt; Downloading https://repo1.maven.org/maven2/org/opensearch/plugin/opensearch-anomaly-detection/2.2.0.0/opensearch-anomaly-detection-2.2.0.0.zip [=================================================] 100%\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: plugin requires additional permissions @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ * java.lang.RuntimePermission accessClassInPackage.sun.misc * java.lang.RuntimePermission accessDeclaredMembers * java.lang.RuntimePermission getClassLoader * java.lang.RuntimePermission setContextClassLoader * java.lang.reflect.ReflectPermission suppressAccessChecks * java.net.SocketPermission * connect,resolve * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name = pool,type = GenericObjectPool] registerMBean * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name = pool,type = GenericObjectPool] unregisterMBean * javax.management.MBeanServerPermission createMBeanServer * javax.management.MBeanTrustPermission register\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks.\nContinue with installation? [ y/N]y\n-&gt; Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection # Zip file in a local directory. $ sudo./opensearch-plugin install file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip\n-&gt; Installing file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip\n-&gt; Downloading file:/home/user/opensearch-anomaly-detection-2.2.0.0.zip [=================================================] 100%\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: plugin requires additional permissions @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ * java.lang.RuntimePermission accessClassInPackage.sun.misc * java.lang.RuntimePermission accessDeclaredMembers * java.lang.RuntimePermission getClassLoader * java.lang.RuntimePermission setContextClassLoader * java.lang.reflect.ReflectPermission suppressAccessChecks * java.net.SocketPermission * connect,resolve * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name = pool,type = GenericObjectPool] registerMBean * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name = pool,type = GenericObjectPool] unregisterMBean * javax.management.MBeanServerPermission createMBeanServer * javax.management.MBeanTrustPermission register\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks.\nContinue with installation? [ y/N]y\n-&gt; Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection Install a plugin using Maven coordinates:\nThe opensearch-plugin install tool also accepts Maven coordinates for available artifacts and versions hosted on Maven Central. opensearch-plugin will parse the Maven coordinates you provide and construct a URL. As a result, the host must be able to connect directly to Maven Central. The plugin installation will fail if you pass coordinates to a proxy or local repository.\nUsage: bin/opensearch-plugin install &lt;groupId&gt;:&lt;artifactId&gt;:&lt;version&gt; Example: $ sudo./opensearch-plugin install org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0\n-&gt; Installing org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0\n-&gt; Downloading org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0 from maven central [=================================================] 100%\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@ WARNING: plugin requires additional permissions @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ * java.lang.RuntimePermission accessClassInPackage.sun.misc * java.lang.RuntimePermission accessDeclaredMembers * java.lang.RuntimePermission getClassLoader * java.lang.RuntimePermission setContextClassLoader * java.lang.reflect.ReflectPermission suppressAccessChecks * java.net.SocketPermission * connect,resolve * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name = pool,type = GenericObjectPool] registerMBean * javax.management.MBeanPermission org.apache.commons.pool2.impl.GenericObjectPool#-[org.apache.commons.pool2:name = pool,type = GenericObjectPool] unregisterMBean * javax.management.MBeanServerPermission createMBeanServer * javax.management.MBeanTrustPermission register\nSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks.\nContinue with installation? [ y/N]y\n-&gt; Installed opensearch-anomaly-detection with folder name opensearch-anomaly-detection Restart your OpenSearch node after installing a plugin.\nRemove\nYou can remove a plugin that has already been installed with the remove option.\nUsage: bin/opensearch-plugin remove &lt;plugin-name&gt; Example: $ sudo $./opensearch-plugin remove opensearch-anomaly-detection\n-&gt; removing [ opensearch-anomaly-detection]... Restart your OpenSearch node after removing a plugin.\nBatch mode\nWhen installing plugins that require additional privileges not included by default, the plugins will prompt the user for confirmation of the required privileges. To grant all requested privileges, use batch mode to skip the confirmation prompt.\nTo force batch mode when installing plugins, add the -b or --batch option: bin/opensearch-plugin install --batch &lt;plugin-name&gt; Available plugins\nMajor, minor, and patch plugin versions must match OpenSearch major, minor, and patch versions in order to be compatible. For example, plugins versions 2.3.0.x work only with OpenSearch 2.3.0.\nBundled Plugins\nThe following plugins are bundled with all OpenSearch distributions except for minimum distribution packages. Plugin Name Repository Earliest Available Version Alerting opensearch-alerting 1.0.0\nAnomaly Detection opensearch-anomaly-detection 1.0.0\nAsynchronous Search opensearch-asynchronous-search 1.0.0\nCross Cluster Replication opensearch-cross-cluster-replication 1.1.0\nNotebooks 1 opensearch-notebooks 1.0.0 to 1.1.0\nNotifications notifications 2.0.0\nReports Scheduler opensearch-reports-scheduler 1.0.0\nGeospatial opensearch-geospatial 2.2.0\nIndex Management opensearch-index-management 1.0.0\nJob Scheduler opensearch-job-scheduler 1.0.0\nk-NN opensearch-knn 1.0.0\nML Commons opensearch-ml 1.3.0\nNeural Search neural-search 2.4.0\nObservability opensearch-observability 1.2.0\nPerformance Analyzer 2 opensearch-performance-analyzer 1.0.0\nSecurity opensearch-security 1.0.0\nSecurity Analytics opensearch-security-analytics 2.4.0\nSQL opensearch-sql 1.0.0 1 Dashboard Notebooks was merged in to the Observability plugin with the release of OpenSearch 1.2.0. 2 Performance Analyzer is not available on Windows. Additional plugins\nMembers of the OpenSearch community have built countless plugins for the service. Although it isn’t possible to build an exhaustive list of every plugin, since many plugins are not maintained within the OpenSearch GitHub repository, the following list of plugins are available to be installed by name using bin/opensearch-plugin install &lt;plugin-name&gt;. Plugin Name Earliest Available Version analysis-icu\n1.0.0\nanalysis-kuromoji\n1.0.0\nanalysis-nori\n1.0.0\nanalysis-phonetic\n1.0.0\nanalysis-smartcn\n1.0.0\nanalysis-stempel\n1.0.0\nanalysis-ukrainian\n1.0.0\ndiscovery-azure-classic\n1.0.0\ndiscovery-ec2\n1.0.0\ndiscovery-gce\n1.0.0\ningest-attachment\n1.0.0\nmapper-annotated-text\n1.0.0\nmapper-murmur3\n1.0.0\nmapper-size\n1.0.0\nrepository-azure\n1.0.0\nrepository-gcs\n1.0.0\nrepository-hdfs\n1.0.0\nrepository-s3\n1.0.0\nstore-smb\n1.0.0\ntransport-nio\n1.0.0 Related links About Observability About security analytics About the Security plugin Alerting Anomaly detection Asynchronous search Cross-cluster replication Index State Management k-NN ML Commons plugin Neural Search Notifications OpenSearch Dashboards Performance Analyzer SQL",
    "ancestors": [
      "Install and upgrade"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/upgrade-opensearch/appendix/index/",
    "title": "Upgrades appendix",
    "content": "Use the upgrades appendix to find additional supporting documentation, such as labs that include example API requests and configuration files to supplement the related process documentation. Specific procedures outlined in the appendix section can be used in a variety of ways:\nNew OpenSearch users can use the steps and example resources we provide to learn about configuring and using OpenSearch and OpenSearch Dashboards.\nSystem administrators who work with OpenSearch clusters can use the examples we provide to simulate cluster maintenance in a test environment before applying any changes to a production workload.\nIf you would like to request a specific topic, please comment on issue #2830 in the OpenSearch Project on GitHub.\nThe specific commands included in this appendix serve as examples of interacting with the OpenSearch API, and the underlying host, in order to demonstrate the steps described in the related upgrade process documents. The intention is not to be overly prescriptive but instead to add context for users who are new to OpenSearch and want to see practical examples.",
    "ancestors": [
      "Install and upgrade",
      "Upgrading OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/upgrade-opensearch/appendix/rolling-upgrade-lab/",
    "title": "Rolling upgrade lab",
    "content": "<!--\nTesting out tabs for code blocks to identify example outputs and file names.\nTo use, invoke class=\"codeblock-label\"\n-->\nYou can follow these steps on your own compatible host to recreate the same cluster state the OpenSearch Project used for testing rolling upgrades. This exercise is useful if you want to test the upgrade process in a development environment.\nThe steps used in this lab were validated on an arbitrarily chosen Amazon Elastic Compute Cloud (Amazon EC2) t2.large instance using Amazon Linux 2 kernel version Linux 5.10.162-141.675.amzn2.x86_64 and Docker version 20.10.17, build 100c701. The instance was provisioned with an attached 20 GiB gp2 Amazon EBS root volume. These specifications are included for informational purposes and do not represent hardware requirements for OpenSearch or OpenSearch Dashboards.\nReferences in this procedure to the $HOME path on the host machine in this procedure are represented by the tilde character (“~”) to make the instructions more portable. If you would prefer to specify an absolute path, modify the volume paths defined in upgrade-demo-cluster.sh and used throughout relevant commands in this document to reflect your environment.\nSetting up the environment\nAs you follow the steps in this document, you will define several Docker resources, including containers, volumes, and a dedicated Docker network, using a script we provide. You can clean up your environment with the following command if you want to restart the process: docker container stop $( docker container ls -aqf name = os-); \\ docker container rm $( docker container ls -aqf name = os-); \\ docker volume rm -f $( docker volume ls -q | egrep 'data-0|repo-0'); \\ docker network rm opensearch-dev-net copy The command removes container names matching the regular expression os-*, data volumes matching data-0* and repo-0*, and the Docker network named opensearch-dev-net. If you have other Docker resources running on your host, then you should review and modify the command to avoid removing other resources unintentionally. This command does not revert host configuration changes, like memory swapping behavior.\nAfter selecting a host, you can begin the lab:\nInstall the appropriate version of Docker Engine for your Linux distribution and system architecture.\nConfigure important system settings on your host:\nDisable memory paging and swapping on the host to improve performance: sudo swapoff -a copy Increase the number of memory maps available to OpenSearch. Open the sysctl configuration file for editing. This example command uses the vim text editor, but you can use any available text editor: sudo vim /etc/sysctl.conf copy Add the following line to /etc/sysctl.conf: vm.max_map_count = 262144 copy Save and quit. If you use the vi or vim text editors, you save and quit by switching to command mode, and entering:wq! or ZZ.\nApply the configuration change: sudo sysctl -p copy Create a new directory called deploy in your home directory, then navigate to it. You will use ~/deploy for paths in the deployment script, configuration files, and TLS certificates: mkdir ~/deploy &amp;&amp; cd ~/deploy copy Download upgrade-demo-cluster.sh from the OpenSearch Project documentation-website repository: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/main/assets/examples/upgrade-demo-cluster.sh copy Run the script without any modifications in order to deploy four containers running OpenSearch and one container running OpenSearch Dashboards, with custom, self-signed TLS certificates and a pre-defined set of internal users: sh upgrade-demo-cluster.sh copy Confirm that the containers were launched successfully: docker container ls copy Example response CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n6e5218c8397d opensearchproject/opensearch-dashboards:1.3.7 \"./opensearch-dashbo…\" 24 seconds ago Up 22 seconds 0.0.0.0:5601-&gt;5601/tcp,:::5601-&gt;5601/tcp os-dashboards-01\ncb5188308b21 opensearchproject/opensearch:1.3.7 \"./opensearch-docker…\" 25 seconds ago Up 24 seconds 9300/tcp, 9650/tcp, 0.0.0.0:9204-&gt;9200/tcp,:::9204-&gt;9200/tcp, 0.0.0.0:9604-&gt;9600/tcp,:::9604-&gt;9600/tcp os-node-04\n71b682aa6671 opensearchproject/opensearch:1.3.7 \"./opensearch-docker…\" 26 seconds ago Up 25 seconds 9300/tcp, 9650/tcp, 0.0.0.0:9203-&gt;9200/tcp,:::9203-&gt;9200/tcp, 0.0.0.0:9603-&gt;9600/tcp,:::9603-&gt;9600/tcp os-node-03\nf894054a9378 opensearchproject/opensearch:1.3.7 \"./opensearch-docker…\" 27 seconds ago Up 26 seconds 9300/tcp, 9650/tcp, 0.0.0.0:9202-&gt;9200/tcp,:::9202-&gt;9200/tcp, 0.0.0.0:9602-&gt;9600/tcp,:::9602-&gt;9600/tcp os-node-02\n2e9c91c959cd opensearchproject/opensearch:1.3.7 \"./opensearch-docker…\" 28 seconds ago Up 27 seconds 9300/tcp, 9650/tcp, 0.0.0.0:9201-&gt;9200/tcp,:::9201-&gt;9200/tcp, 0.0.0.0:9601-&gt;9600/tcp,:::9601-&gt;9600/tcp os-node-01 The amount of time OpenSearch needs to initialize the cluster varies depending on the performance capabilities of the underlying host. You can follow container logs to see what OpenSearch is doing during the bootstrap process:\nEnter the following command to display logs for container os-node-01 in the terminal window: docker logs -f os-node-01 copy You will see a log entry resembling the following example when the node is ready:\nExample [INFO][o.o.s.c.ConfigurationRepository] [os-node-01] Node 'os-node-01' initialized Press Ctrl+C to stop following container logs and return to the command prompt.\nUse cURL to query the OpenSearch REST API. In the following command, os-node-01 is queried by sending the request to host port 9201, which is mapped to port 9200 on the container: curl -s \"https://localhost:9201\" -ku admin:admin copy Example response { \"name\": \"os-node-01\", \"cluster_name\": \"opensearch-dev-cluster\", \"cluster_uuid\": \"g1MMknuDRuuD9IaaNt56KA\", \"version\": { \"distribution\": \"opensearch\", \"number\": \"1.3.7\", \"build_type\": \"tar\", \"build_hash\": \"db18a0d5a08b669fb900c00d81462e221f4438ee\", \"build_date\": \"2022-12-07T22:59:20.186520Z\", \"build_snapshot\": false, \"lucene_version\": \"8.10.1\", \"minimum_wire_compatibility_version\": \"6.8.0\", \"minimum_index_compatibility_version\": \"6.0.0-beta1\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Tip: Use the -s option with curl to hide the progress meter and error messages.\nAdding data and configuring OpenSearch Security\nNow that the OpenSearch cluster is running, it’s time to add data and configure some OpenSearch Security settings. The data you add and settings you configure will be validated again after the version upgrade is complete.\nThis section can be broken down into two parts: Indexing data with the REST API Adding data using OpenSearch Dashboards Indexing data with the REST API\nDownload the sample field mappings file: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/main/assets/examples/ecommerce-field_mappings.json copy Next, download the bulk data that you will ingest into this index: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/main/assets/examples/ecommerce.json copy Use the Create index API to create an index using the mappings defined in ecommerce-field_mappings.json: curl -H \"Content-Type: application/x-ndjson\" \\ -X PUT \"https://localhost:9201/ecommerce?pretty\" \\ --data-binary \"@ecommerce-field_mappings.json\" \\ -ku admin:admin copy Example response { \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"ecommerce\" } Use the Bulk API to add data to the new ecommerce index from ecommerce.json: curl -H \"Content-Type: application/x-ndjson\" \\ -X PUT \"https://localhost:9201/ecommerce/_bulk?pretty\" \\ --data-binary \"@ecommerce.json\" \\ -ku admin:admin copy Example response (truncated) { \"took\": 3323, \"errors\": false, \"items\": [... \"index\": { \"_index\": \"ecommerce\", \"_type\": \"_doc\", \"_id\": \"4674\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 4674, \"_primary_term\": 1, \"status\": 201 }] } A search query can also confirm that the data was indexed successfully. The following query returns the number of documents in which keyword `customer_first_name` equals `Sonya`: curl -H 'Content-Type: application/json' \\ -X GET \"https://localhost:9201/ecommerce/_search?pretty=true&amp;filter_path=hits.total\" \\ -d '{\"query\":{\"match\":{\"customer_first_name\":\"Sonya\"}}}' \\ -ku admin:admin copy Example response { \"hits\": { \"total\": { \"value\": 106, \"relation\": \"eq\" } } } Adding data using OpenSearch Dashboards\nOpen a web browser and navigate to port 5601 on your Docker host (for example, https:// HOST_ADDRESS:5601). If OpenSearch Dashboards is running and you have network access to the host from your browser client, then you will be redirected to a login page.\nIf the web browser throws an error because the TLS certificates are self-signed, then you might need to bypass certificate checks in your browser. Refer to the browser’s documentation for information about bypassing certificate checks. The common name (CN) for each certificate is generated according to the container and node names for intracluster communication, so connecting to the host from a browser will still result in an “invalid CN” warning.\nEnter the default username ( admin) and password ( admin).\nOn the OpenSearch Dashboards Home page, select Add sample data.\nUnder Sample web logs, select Add data. Optional: Select View data to review the [Logs] Web Traffic dashboard.\nSelect the Menu button to open the Navigation pane, then go to Security &gt; Internal users.\nSelect Create internal user.\nProvide a Username and Password.\nIn the Backend role field, enter admin.\nSelect Create.\nBacking up important files\nAlways create backups before making changes to your cluster, especially if the cluster is running in a production environment.\nIn this section you will be: Registering a snapshot repository. Creating a snapshot. Backing up security settings.\nRegistering a snapshot repository\nRegister a repository using the volume that was mapped by upgrade-demo-cluster.sh: curl -H 'Content-Type: application/json' \\ -X PUT \"https://localhost:9201/_snapshot/snapshot-repo?pretty\" \\ -d '{\"type\":\"fs\",\"settings\":{\"location\":\"/usr/share/opensearch/snapshots\"}}' \\ -ku admin:admin copy Example response { \"acknowledged\": true } Optional: Perform an additional check to verify that the repository was created successfully: curl -H 'Content-Type: application/json' \\ -X POST \"https://localhost:9201/_snapshot/snapshot-repo/_verify?timeout=0s&amp;master_timeout=50s&amp;pretty\" \\ -ku admin:admin copy Example response { \"nodes\": { \"UODBXfAlRnueJ67grDxqgw\": { \"name\": \"os-node-03\" }, \"14I_OyBQQXio8nmk0xsVcQ\": { \"name\": \"os-node-04\" }, \"tQp3knPRRUqHvFNKpuD2vQ\": { \"name\": \"os-node-02\" }, \"rPe8D6ssRgO5twIP00wbCQ\": { \"name\": \"os-node-01\" } } } Creating a snapshot\nSnapshots are backups of a cluster’s indexes and state. See Snapshots to learn more.\nCreate a snapshot that includes all indexes and the cluster state: curl -H 'Content-Type: application/json' \\ -X PUT \"https://localhost:9201/_snapshot/snapshot-repo/cluster-snapshot-v137?wait_for_completion=true&amp;pretty\" \\ -ku admin:admin copy Example response { \"snapshot\": { \"snapshot\": \"cluster-snapshot-v137\", \"uuid\": \"-IYB8QNPShGOTnTtMjBjNg\", \"version_id\": 135248527, \"version\": \"1.3.7\", \"indices\": [ \"opensearch_dashboards_sample_data_logs\", \".opendistro_security\", \"security-auditlog-2023.02.27\", \".kibana_1\", \".kibana_92668751_admin_1\", \"ecommerce\", \"security-auditlog-2023.03.06\", \"security-auditlog-2023.02.28\", \"security-auditlog-2023.03.07\"], \"data_streams\": [], \"include_global_state\": true, \"state\": \"SUCCESS\", \"start_time\": \"2023-03-07T18:33:00.656Z\", \"start_time_in_millis\": 1678213980656, \"end_time\": \"2023-03-07T18:33:01.471Z\", \"end_time_in_millis\": 1678213981471, \"duration_in_millis\": 815, \"failures\": [], \"shards\": { \"total\": 9, \"failed\": 0, \"successful\": 9 } } } Backing up security settings\nCluster administrators can modify OpenSearch Security settings by using any of the following methods:\nModifying YAML files and running securityadmin.sh Making REST API requests using the admin certificate\nMaking changes with OpenSearch Dashboards\nRegardless of the method you choose, OpenSearch Security writes your configuration to a special system index called.opendistro_security. This system index is preserved through the upgrade process, and it is also saved in the snapshot you created. However, restoring system indexes requires elevated access granted by the admin certificate. To learn more, see System indexes and Configuring TLS certificates.\nYou can also export your OpenSearch Security settings as YAML files by running securityadmin.sh with the -backup option on any of your OpenSearch nodes. These YAML files can be used to reinitialize the.opendistro_security index with your existing configuration. The following steps will guide you through generating these backup files and copying them to your host for storage:\nOpen an interactive pseudo-TTY session with os-node-01: docker exec -it os-node-01 bash copy Create a directory called backups and navigate to it: mkdir /usr/share/opensearch/backups &amp;&amp; cd /usr/share/opensearch/backups copy Use securityadmin.sh to create backups of your OpenSearch Security settings in /usr/share/opensearch/backups/: /usr/share/opensearch/plugins/opensearch-security/tools/securityadmin.sh \\ -backup /usr/share/opensearch/backups \\ -icl \\ -nhnv \\ -cacert /usr/share/opensearch/config/root-ca.pem \\ -cert /usr/share/opensearch/config/admin.pem \\ -key /usr/share/opensearch/config/admin-key.pem copy Example response Security Admin v7\nWill connect to localhost:9300... done Connected as CN = A,OU = DOCS,O = OPENSEARCH,L = PORTLAND,ST = OREGON,C = US\nOpenSearch Version: 1.3.7\nOpenSearch Security Version: 1.3.7.0\nContacting opensearch cluster 'opensearch' and wait for YELLOW clusterstate...\nClustername: opensearch-dev-cluster\nClusterstate: GREEN\nNumber of nodes: 4\nNumber of data nodes: 4.opendistro_security index already exists, so we do not need to create one.\nWill retrieve '/config' into /usr/share/opensearch/backups/config.yml\nSUCC: Configuration for 'config' stored in /usr/share/opensearch/backups/config.yml\nWill retrieve '/roles' into /usr/share/opensearch/backups/roles.yml\nSUCC: Configuration for 'roles' stored in /usr/share/opensearch/backups/roles.yml\nWill retrieve '/rolesmapping' into /usr/share/opensearch/backups/roles_mapping.yml\nSUCC: Configuration for 'rolesmapping' stored in /usr/share/opensearch/backups/roles_mapping.yml\nWill retrieve '/internalusers' into /usr/share/opensearch/backups/internal_users.yml\nSUCC: Configuration for 'internalusers' stored in /usr/share/opensearch/backups/internal_users.yml\nWill retrieve '/actiongroups' into /usr/share/opensearch/backups/action_groups.yml\nSUCC: Configuration for 'actiongroups' stored in /usr/share/opensearch/backups/action_groups.yml\nWill retrieve '/tenants' into /usr/share/opensearch/backups/tenants.yml\nSUCC: Configuration for 'tenants' stored in /usr/share/opensearch/backups/tenants.yml\nWill retrieve '/nodesdn' into /usr/share/opensearch/backups/nodes_dn.yml\nSUCC: Configuration for 'nodesdn' stored in /usr/share/opensearch/backups/nodes_dn.yml\nWill retrieve '/whitelist' into /usr/share/opensearch/backups/whitelist.yml\nSUCC: Configuration for 'whitelist' stored in /usr/share/opensearch/backups/whitelist.yml\nWill retrieve '/audit' into /usr/share/opensearch/backups/audit.yml\nSUCC: Configuration for 'audit' stored in /usr/share/opensearch/backups/audit.yml Optional: Create a backup directory for TLS certificates and store copies of the certificates. Repeat this for each node if you use unique TLS certificates: mkdir /usr/share/opensearch/backups/certs &amp;&amp; cp /usr/share/opensearch/config/ * pem /usr/share/opensearch/backups/certs/ copy Terminate the pseudo-TTY session: exit copy Copy the files to your host: docker cp os-node-01:/usr/share/opensearch/backups ~/deploy/ copy Performing the upgrade\nNow that the cluster is configured and you have made backups of important files and settings, it’s time to begin the version upgrade.\nSome steps included in this section, like disabling shard replication and flushing the transaction log, will not impact the performance of your cluster. These steps are included as best practices and can significantly improve cluster performance in situations where clients continue interacting with the OpenSearch cluster throughout the upgrade, such as by querying existing data or indexing documents.\nDisable shard replication to stop the movement of Lucene index segments within your cluster: curl -H 'Content-type: application/json' \\ -X PUT \"https://localhost:9201/_cluster/settings?pretty\" \\ -d '{\"persistent\":{\"cluster.routing.allocation.enable\":\"primaries\"}}' \\ -ku admin:admin copy Example response { \"acknowledged\": true, \"persistent\": { \"cluster\": { \"routing\": { \"allocation\": { \"enable\": \"primaries\" } } } }, \"transient\": { } } Perform a flush operation on the cluster to commit transaction log entries to the Lucene index: curl -X POST \"https://localhost:9201/_flush?pretty\" -ku admin:admin copy Example response { \"_shards\": { \"total\": 20, \"successful\": 20, \"failed\": 0 } } Select a node to upgrade. You can upgrade nodes in any order because all of the nodes in this demo cluster are eligible cluster managers. The following command will stop and remove container os-node-01 without removing the mounted data volume: docker stop os-node-01 &amp;&amp; docker container rm os-node-01 copy Start a new container named os-node-01 with the opensearchproject/opensearch:2.5.0 image and using the same mapped volumes as the original container: docker run -d \\ -p 9201:9200 -p 9601:9600 \\ -e \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" \\ --ulimit nofile = 65536:65536 --ulimit memlock = -1:-1 \\ -v data-01:/usr/share/opensearch/data \\ -v repo-01:/usr/share/opensearch/snapshots \\ -v ~/deploy/opensearch-01.yml:/usr/share/opensearch/config/opensearch.yml \\ -v ~/deploy/root-ca.pem:/usr/share/opensearch/config/root-ca.pem \\ -v ~/deploy/admin.pem:/usr/share/opensearch/config/admin.pem \\ -v ~/deploy/admin-key.pem:/usr/share/opensearch/config/admin-key.pem \\ -v ~/deploy/os-node-01.pem:/usr/share/opensearch/config/os-node-01.pem \\ -v ~/deploy/os-node-01-key.pem:/usr/share/opensearch/config/os-node-01-key.pem \\ --network opensearch-dev-net \\ --ip 172.20.0.11 \\ --name os-node-01 \\ opensearchproject/opensearch:2.5.0 copy Example response d26d0cb2e1e93e9c01bb00f19307525ef89c3c3e306d75913860e6542f729ea4 Optional: Query the cluster to determine which node is acting as the cluster manager. You can run this command at any time during the process to see when a new cluster manager is elected: curl -s \"https://localhost:9201/_cat/nodes?v&amp;h=name,version,node.role,master\" \\ -ku admin:admin | column -t copy Example response name version node.role master\nos-node-01 2.5.0 dimr -\nos-node-04 1.3.7 dimr * os-node-02 1.3.7 dimr -\nos-node-03 1.3.7 dimr - Optional: Query the cluster to see how shard allocation changes as nodes are removed and replaced. You can run this command at any time during the process to see how shard statuses change: curl -s \"https://localhost:9201/_cat/shards\" \\ -ku admin:admin copy Example response security-auditlog-2023.03.06 0 p STARTED 53 214.5kb 172.20.0.13 os-node-03\nsecurity-auditlog-2023.03.06 0 r UNASSIGNED.kibana_1 0 p STARTED 3 14.5kb 172.20.0.12 os-node-02.kibana_1 0 r STARTED 3 14.5kb 172.20.0.13 os-node-03\necommerce 0 p STARTED 4675 3.9mb 172.20.0.12 os-node-02\necommerce 0 r STARTED 4675 3.9mb 172.20.0.14 os-node-04\nsecurity-auditlog-2023.03.07 0 p STARTED 37 175.7kb 172.20.0.14 os-node-04\nsecurity-auditlog-2023.03.07 0 r UNASSIGNED.opendistro_security 0 p STARTED 10 67.9kb 172.20.0.12 os-node-02.opendistro_security 0 r STARTED 10 67.9kb 172.20.0.13 os-node-03.opendistro_security 0 r STARTED 10 64.5kb 172.20.0.14 os-node-04.opendistro_security 0 r UNASSIGNED\nsecurity-auditlog-2023.02.27 0 p STARTED 4 80.5kb 172.20.0.12 os-node-02\nsecurity-auditlog-2023.02.27 0 r UNASSIGNED\nsecurity-auditlog-2023.02.28 0 p STARTED 6 104.1kb 172.20.0.14 os-node-04\nsecurity-auditlog-2023.02.28 0 r UNASSIGNED\nopensearch_dashboards_sample_data_logs 0 p STARTED 14074 9.1mb 172.20.0.12 os-node-02\nopensearch_dashboards_sample_data_logs 0 r STARTED 14074 8.9mb 172.20.0.13 os-node-03.kibana_92668751_admin_1 0 r STARTED 33 37.3kb 172.20.0.13 os-node-03.kibana_92668751_admin_1 0 p STARTED 33 37.3kb 172.20.0.14 os-node-04 Stop os-node-02: docker stop os-node-02 &amp;&amp; docker container rm os-node-02 copy Start a new container named os-node-02 with the opensearchproject/opensearch:2.5.0 image and using the same mapped volumes as the original container: docker run -d \\ -p 9202:9200 -p 9602:9600 \\ -e \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" \\ --ulimit nofile = 65536:65536 --ulimit memlock = -1:-1 \\ -v data-02:/usr/share/opensearch/data \\ -v repo-01:/usr/share/opensearch/snapshots \\ -v ~/deploy/opensearch-02.yml:/usr/share/opensearch/config/opensearch.yml \\ -v ~/deploy/root-ca.pem:/usr/share/opensearch/config/root-ca.pem \\ -v ~/deploy/admin.pem:/usr/share/opensearch/config/admin.pem \\ -v ~/deploy/admin-key.pem:/usr/share/opensearch/config/admin-key.pem \\ -v ~/deploy/os-node-02.pem:/usr/share/opensearch/config/os-node-02.pem \\ -v ~/deploy/os-node-02-key.pem:/usr/share/opensearch/config/os-node-02-key.pem \\ --network opensearch-dev-net \\ --ip 172.20.0.12 \\ --name os-node-02 \\ opensearchproject/opensearch:2.5.0 copy Example response 7b802865bd6eb420a106406a54fc388ed8e5e04f6cbd908c2a214ea5ce72ac00 Stop os-node-03: docker stop os-node-03 &amp;&amp; docker container rm os-node-03 copy Start a new container named os-node-03 with the opensearchproject/opensearch:2.5.0 image and using the same mapped volumes as the original container: docker run -d \\ -p 9203:9200 -p 9603:9600 \\ -e \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" \\ --ulimit nofile = 65536:65536 --ulimit memlock = -1:-1 \\ -v data-03:/usr/share/opensearch/data \\ -v repo-01:/usr/share/opensearch/snapshots \\ -v ~/deploy/opensearch-03.yml:/usr/share/opensearch/config/opensearch.yml \\ -v ~/deploy/root-ca.pem:/usr/share/opensearch/config/root-ca.pem \\ -v ~/deploy/admin.pem:/usr/share/opensearch/config/admin.pem \\ -v ~/deploy/admin-key.pem:/usr/share/opensearch/config/admin-key.pem \\ -v ~/deploy/os-node-03.pem:/usr/share/opensearch/config/os-node-03.pem \\ -v ~/deploy/os-node-03-key.pem:/usr/share/opensearch/config/os-node-03-key.pem \\ --network opensearch-dev-net \\ --ip 172.20.0.13 \\ --name os-node-03 \\ opensearchproject/opensearch:2.5.0 copy Example response d7f11726841a89eb88ff57a8cbecab392399f661a5205f0c81b60a995fc6c99d Stop os-node-04: docker stop os-node-04 &amp;&amp; docker container rm os-node-04 copy Start a new container named os-node-04 with the opensearchproject/opensearch:2.5.0 image and using the same mapped volumes as the original container: docker run -d \\ -p 9204:9200 -p 9604:9600 \\ -e \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" \\ --ulimit nofile = 65536:65536 --ulimit memlock = -1:-1 \\ -v data-04:/usr/share/opensearch/data \\ -v repo-01:/usr/share/opensearch/snapshots \\ -v ~/deploy/opensearch-04.yml:/usr/share/opensearch/config/opensearch.yml \\ -v ~/deploy/root-ca.pem:/usr/share/opensearch/config/root-ca.pem \\ -v ~/deploy/admin.pem:/usr/share/opensearch/config/admin.pem \\ -v ~/deploy/admin-key.pem:/usr/share/opensearch/config/admin-key.pem \\ -v ~/deploy/os-node-04.pem:/usr/share/opensearch/config/os-node-04.pem \\ -v ~/deploy/os-node-04-key.pem:/usr/share/opensearch/config/os-node-04-key.pem \\ --network opensearch-dev-net \\ --ip 172.20.0.14 \\ --name os-node-04 \\ opensearchproject/opensearch:2.5.0 copy Example response 26f8286ab11e6f8dcdf6a83c95f265172f9557578a1b292af84c6f5ef8738e1d Confirm that your cluster is running the new version: curl -s \"https://localhost:9201/_cat/nodes?v&amp;h=name,version,node.role,master\" \\ -ku admin:admin | column -t copy Example response name version node.role master\nos-node-01 2.5.0 dimr * os-node-02 2.5.0 dimr -\nos-node-04 2.5.0 dimr -\nos-node-03 2.5.0 dimr - The last component you should upgrade is the OpenSearch Dashboards node. First, stop and remove the old container: docker stop os-dashboards-01 &amp;&amp; docker rm os-dashboards-01 copy Create a new container running the target version of OpenSearch Dashboards: docker run -d \\ -p 5601:5601 --expose 5601 \\ -v ~/deploy/opensearch_dashboards.yml:/usr/share/opensearch-dashboards/config/opensearch_dashboards.yml \\ -v ~/deploy/root-ca.pem:/usr/share/opensearch-dashboards/config/root-ca.pem \\ -v ~/deploy/os-dashboards-01.pem:/usr/share/opensearch-dashboards/config/os-dashboards-01.pem \\ -v ~/deploy/os-dashboards-01-key.pem:/usr/share/opensearch-dashboards/config/os-dashboards-01-key.pem \\ --network opensearch-dev-net \\ --ip 172.20.0.10 \\ --name os-dashboards-01 \\ opensearchproject/opensearch-dashboards:2.5.0 copy Example response 310de7a24cf599ca0b39b241db07fa8865592ebe15b6f5fda26ad19d8e1c1e09 Make sure the OpenSearch Dashboards container started properly. A command like the following can be used to confirm that requests to https:// HOST_ADDRESS:5601 are redirected (HTTP status code 302) to /app/login?: curl https://localhost:5601 -kI copy Example response HTTP/1.1 302 Found\nlocation: /app/login?\nosd-name: opensearch-dashboards-dev\ncache-control: private, no-cache, no-store, must-revalidate\nset-cookie: security_authentication =; Max-Age = 0; Expires = Thu, 01 Jan 1970 00:00:00 GMT; Secure; HttpOnly; Path = /\ncontent-length: 0\nDate: Wed, 08 Mar 2023 15:36:53 GMT\nConnection: keep-alive\nKeep-Alive: timeout = 120 Re-enable allocation of replica shards: curl -H 'Content-type: application/json' \\ -X PUT \"https://localhost:9201/_cluster/settings?pretty\" \\ -d '{\"persistent\":{\"cluster.routing.allocation.enable\":\"all\"}}' \\ -ku admin:admin copy Example response { \"acknowledged\": true, \"persistent\": { \"cluster\": { \"routing\": { \"allocation\": { \"enable\": \"all\" } } } }, \"transient\": { } } Validating the upgrade\nYou successfully deployed a secure OpenSearch cluster, indexed data, created a dashboard populated with sample data, created a new internal user, backed up your important files, and upgraded the cluster from version 1.3.7 to 2.5.0. Before you continue exploring and experimenting with OpenSearch and OpenSearch Dashboards, you should validate the outcome of the upgrade.\nFor this cluster, post-upgrade validation steps can include verifying the following: Running version Health and shard allocation Data consistency Verifying the new running version\nVerify the current running version of your OpenSearch nodes: curl -s \"https://localhost:9201/_cat/nodes?v&amp;h=name,version,node.role,master\" \\ -ku admin:admin | column -t copy Example response name version node.role master\nos-node-01 2.5.0 dimr * os-node-02 2.5.0 dimr -\nos-node-04 2.5.0 dimr -\nos-node-03 2.5.0 dimr - Verify the current running version of OpenSearch Dashboards: Option 1: Verify the OpenSearch Dashboards version from the web interface.\nOpen a web browser and navigate to port 5601 on your Docker host (for example, https:// HOST_ADDRESS:5601).\nLog in with the default username ( admin) and default password ( admin).\nSelect the Help button in the upper-right corner. The version is displayed in a pop-up window.\nSelect the Help button again to close the pop-up window. Option 2: Verify the OpenSearch Dashboards version by inspecting manifest.yml.\nFrom the command line, open an interactive pseudo-TTY session with the OpenSearch Dashboards container: docker exec -it os-dashboards-01 bash copy Check manifest.yml for the version: head -n 5 manifest.yml copy Example response --- schema-version: '1.1' build:\nname: OpenSearch Dashboards\nversion: 2.5.0 Terminate the pseudo-TTY session: exit copy Verifying cluster health and shard allocation\nQuery the Cluster health API endpoint to see information about the health of your cluster. You should see a status of green, which indicates that all primary and replica shards are allocated: curl -s \"https://localhost:9201/_cluster/health?pretty\" -ku admin:admin copy Example response { \"cluster_name\": \"opensearch-dev-cluster\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 4, \"number_of_data_nodes\": 4, \"discovered_master\": true, \"discovered_cluster_manager\": true, \"active_primary_shards\": 16, \"active_shards\": 36, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0, \"delayed_unassigned_shards\": 0, \"number_of_pending_tasks\": 0, \"number_of_in_flight_fetch\": 0, \"task_max_waiting_in_queue_millis\": 0, \"active_shards_percent_as_number\": 100.0 } Query the CAT shards API endpoint to see how shards are allocated after the cluster is upgrade: curl -s \"https://localhost:9201/_cat/shards\" -ku admin:admin copy Example response security-auditlog-2023.02.27 0 r STARTED 4 80.5kb 172.20.0.13 os-node-03\nsecurity-auditlog-2023.02.27 0 p STARTED 4 80.5kb 172.20.0.11 os-node-01\nsecurity-auditlog-2023.03.08 0 p STARTED 30 95.2kb 172.20.0.13 os-node-03\nsecurity-auditlog-2023.03.08 0 r STARTED 30 123.8kb 172.20.0.11 os-node-01\necommerce 0 p STARTED 4675 3.9mb 172.20.0.12 os-node-02\necommerce 0 r STARTED 4675 3.9mb 172.20.0.13 os-node-03.kibana_1 0 p STARTED 3 5.9kb 172.20.0.12 os-node-02.kibana_1 0 r STARTED 3 5.9kb 172.20.0.11 os-node-01.kibana_92668751_admin_1 0 p STARTED 33 37.3kb 172.20.0.13 os-node-03.kibana_92668751_admin_1 0 r STARTED 33 37.3kb 172.20.0.11 os-node-01\nopensearch_dashboards_sample_data_logs 0 p STARTED 14074 9.1mb 172.20.0.12 os-node-02\nopensearch_dashboards_sample_data_logs 0 r STARTED 14074 9.1mb 172.20.0.14 os-node-04\nsecurity-auditlog-2023.02.28 0 p STARTED 6 26.2kb 172.20.0.11 os-node-01\nsecurity-auditlog-2023.02.28 0 r STARTED 6 26.2kb 172.20.0.14 os-node-04.opendistro-reports-definitions 0 p STARTED 0 208b 172.20.0.12 os-node-02.opendistro-reports-definitions 0 r STARTED 0 208b 172.20.0.13 os-node-03.opendistro-reports-definitions 0 r STARTED 0 208b 172.20.0.14 os-node-04\nsecurity-auditlog-2023.03.06 0 r STARTED 53 174.6kb 172.20.0.12 os-node-02\nsecurity-auditlog-2023.03.06 0 p STARTED 53 174.6kb 172.20.0.14 os-node-04.kibana_101107607_newuser_1 0 r STARTED 1 5.1kb 172.20.0.13 os-node-03.kibana_101107607_newuser_1 0 p STARTED 1 5.1kb 172.20.0.11 os-node-01.opendistro_security 0 r STARTED 10 64.5kb 172.20.0.12 os-node-02.opendistro_security 0 r STARTED 10 64.5kb 172.20.0.13 os-node-03.opendistro_security 0 r STARTED 10 64.5kb 172.20.0.11 os-node-01.opendistro_security 0 p STARTED 10 64.5kb 172.20.0.14 os-node-04.kibana_-152937574_admintenant_1 0 r STARTED 1 5.1kb 172.20.0.12 os-node-02.kibana_-152937574_admintenant_1 0 p STARTED 1 5.1kb 172.20.0.14 os-node-04\nsecurity-auditlog-2023.03.07 0 r STARTED 37 175.7kb 172.20.0.12 os-node-02\nsecurity-auditlog-2023.03.07 0 p STARTED 37 175.7kb 172.20.0.14 os-node-04.kibana_92668751_admin_2 0 p STARTED 34 38.6kb 172.20.0.13 os-node-03.kibana_92668751_admin_2 0 r STARTED 34 38.6kb 172.20.0.11 os-node-01.kibana_2 0 p STARTED 3 6kb 172.20.0.13 os-node-03.kibana_2 0 r STARTED 3 6kb 172.20.0.14 os-node-04.opendistro-reports-instances 0 r STARTED 0 208b 172.20.0.12 os-node-02.opendistro-reports-instances 0 r STARTED 0 208b 172.20.0.11 os-node-01.opendistro-reports-instances 0 p STARTED 0 208b 172.20.0.14 os-node-04 Verifying data consistency\nYou need to query the ecommerce index again in order to confirm that the sample data is still present:\nCompare the response to this query with the response you received in the last step of Indexing data with the REST API: curl -H 'Content-Type: application/json' \\ -X GET \"https://localhost:9201/ecommerce/_search?pretty=true&amp;filter_path=hits.total\" \\ -d '{\"query\":{\"match\":{\"customer_first_name\":\"Sonya\"}}}' \\ -ku admin:admin copy Example response { \"hits\": { \"total\": { \"value\": 106, \"relation\": \"eq\" } } } Open a web browser and navigate to port 5601 on your Docker host (for example, https:// HOST_ADDRESS:5601).\nEnter the default username ( admin) and password ( admin).\nOn the OpenSearch Dashboards Home page, select the Menu button in the upper-left corner of the web interface to open the Navigation pane.\nSelect Dashboard.\nChoose [Logs] Web Traffic to open the dashboard that was created when you added sample data earlier in the process.\nWhen you are done reviewing the dashboard, select the Profile button. Choose Log out so you can log in as a different user.\nEnter the username and password you created before upgrading, then select Log in.\nNext steps\nReview the following resoures to learn more about how OpenSearch works: REST API reference Quickstart guide for OpenSearch Dashboards About Security in OpenSearch",
    "ancestors": [
      "Install and upgrade",
      "Upgrading OpenSearch",
      "Upgrades appendix"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/upgrade-opensearch/index/",
    "title": "Upgrading OpenSearch",
    "content": "The OpenSearch Project releases regular updates that include new features, enhancements, and bug fixes. OpenSearch uses Semantic Versioning, which means that breaking changes are only introduced between major version releases. To learn about upcoming features and fixes, review the OpenSearch Project Roadmap on GitHub. To view a list of previous releases or to learn more about how OpenSearch uses versioning, see Release Schedule and Maintenance Policy.\nWe recognize that users are excited about upgrading OpenSearch in order to enjoy the latest features, and we will continue to expand on these upgrade and migration documents to cover additional topics, such as upgrading OpenSearch Dashboards and preserving custom configurations, such as for plugins. To see what’s coming next or to make a request for future content, leave a comment on the upgrade and migration documentation meta issue in the OpenSearch Project on GitHub.\nIf you would like a specific process to be added or would like to contribute, create an issue on GitHub. See the Contributor Guidelines to learn how you can help.\nWorkflow considerations\nTake time to plan the process before making any changes to your cluster. For example, consider the following questions:\nHow long will the upgrade process take?\nIf your cluster is being used in production, how impactful is downtime?\nDo you have infrastructure in place to stand up the new cluster in a testing or development environment before you move it into production, or do you need to upgrade the production hosts directly?\nThe answers to questions like these will help you determine which upgrade path will work best in your environment.\nAt a minimum, you should be: Reviewing breaking changes. Reviewing the OpenSearch tools compatibility matrices. Reviewing plugin compatibility. Backing up configuration files. Creating a snapshot.\nStop any nonessential indexing before you begin the upgrade procedure to eliminate unnecessary resource demands on the cluster while you perform the upgrade.\nReviewing breaking changes\nIt’s important to determine how the new version of OpenSearch will integreate with your environment. Review Breaking changes before beginning any upgrade procedures to determine whether you will need to make adjustments to your workflow. For example, upstream or downstream components might need to be modified to be compatible with an API change (see meta issue #2589).\nReviewing the OpenSearch tools compatibility matrices\nIf your OpenSearch cluster interacts with other services in your environment, like Logstash or Beats, then you should check the OpenSearch tools compatibility matrices to determine whether other components will need to be upgraded.\nReviewing plugin compatibility\nReview the plugins you use to determine compatibility with the target version of OpenSearch. Official OpenSearch Project plugins can be found in the OpenSearch Project repository on GitHub. If you use any third-party plugins, then you should check the documentation for those plugins to determine whether they are compatible.\nGo to Available plugins to see a reference table that highlights version compatibility for bundled OpenSearch plugins.\nMajor, minor, and patch plugin versions must match OpenSearch major, minor, and patch versions in order to be compatible. For example, plugin versions 2.3.0.x work only with OpenSearch 2.3.0.\nBacking up configuration files\nMitigate the risk of data loss by backing up any important files before you start an upgrade. Generally, these files will be located in either of two directories: opensearch/config opensearch-dashboards/config Some examples include opensearch.yml, opensearch_dashboards.yml, plugin configuration files, and TLS certificates. Once you identify which files you want to back up, copy them to remote storage for safety.\nIf you use security features, make sure to read A word of caution for information about backing up and restoring your security settings.\nCreating a snapshot\nWe recommend that you back up your cluster state and indexes using snapshots. Snapshots you take before an upgrade can be used as restore points if you need to roll back the cluster to its original version.\nYou can further reduce the risk of data loss by storing your snapshots on external storage, such as a mounted Network File System (NFS) or a cloud storage solution like those listed in the following table. Snapshot repository location Required OpenSearch plugin Amazon Simple Storage Service (Amazon S3) repository-s3 Google Cloud Storage (GCS) repository-gcs Apache Hadoop Distributed File System (HDFS) repository-hdfs Microsoft Azure Blob Storage repository-azure Upgrade methods\nChoose an appropriate method for upgrading your cluster to a new version of OpenSearch based on your requirements:\nA rolling upgrade upgrades nodes one at a time without stopping the cluster.\nA cluster restart upgrade upgrades services while the cluster is stopped.\nUpgrades spanning more than a single major version of OpenSearch will require additional effort due to the need for reindexing. For more information, refer to the Reindex API. See the Index compatibility reference table included later in this guide for help planning your data migration.\nRolling upgrade\nA rolling upgrade is a great option if you want to keep your cluster operational throughout the process. Data may continue to be ingested, analyzed, and queried as nodes are individually stopped, upgraded, and restarted. A variation of the rolling upgrade referred to as “node replacement” follows exactly the same process except that hosts and containers are not reused for the new node. You might perform node replacement if you are upgrading the underlying host(s) as well.\nOpenSearch nodes cannot join a cluster if the cluster manager is running a newer version of OpenSearch than the node requesting membership. To avoid this issue, upgrade the cluster-manager-eligible nodes last.\nSee Rolling Upgrade for more information about the process.\nCluster restart upgrade\nOpenSearch administrators might choose to perform a cluster restart upgrade for several reasons, such as if the administrator doesn’t want to perform maintenance on a running cluster or if the cluster is being migrated to a different environment.\nUnlike a rolling upgrade, where only one node is offline at a time, a cluster restart upgrade requires you to stop OpenSearch and OpenSearch Dashboards on all nodes in the cluster before proceeding. After the nodes are stopped, a new version of OpenSearch is installed. Then OpenSearch is started and the cluster bootstraps to the new version.\nCompatibility\nOpenSearch nodes are compatible with other OpenSearch nodes running any other minor version within the same major version release. For example, 1.1.0 is compatible with 1.3.7 because they are part of the same major version (1.x). Additionally, OpenSearch nodes and indexes are backward compatible with the previous major version. That means, for example, that an index created by an OpenSearch node running any 1.x version can be restored from a snapshot to an OpenSearch cluster running any 2.x version.\nOpenSearch 1.x nodes are compatible with nodes running Elasticsearch 7.x, but the longevity of a mixed-version environment should not extend beyond cluster upgrade activities.\nIndex compatibility is determined by the version of Apache Lucene that created the index. If an index was created by an OpenSearch cluster running version 1.0.0, then the index can be used by any other OpenSearch cluster running up to the latest 1.x or 2.x release. See the Index compatibility reference table for Lucene versions running in OpenSearch 1.0.0 and later and Elasticsearch 6.8 and later.\nIf your upgrade path spans more than a single major version and you want to retain any existing indexes, then you can use the Reindex API to make your indexes compatible with the target version of OpenSearch before upgrading. For example, if your cluster is currently running Elasticsearch 6.8 and you want to upgrade to OpenSearch 2.x, then you must first upgrade to OpenSearch 1.x, recreate your indexes using the Reindex API, and finally upgrade to 2.x. One alternative to reindexing is to reingest data from the origin, such as by replaying a data stream or ingesting data from a database.\nIndex compatibility reference\nIf you plan to retain old indexes after the OpenSearch version upgrade, then you might need to reindex or reingest the data. Refer to the following table for Lucene versions across recent OpenSearch and Elasticsearch releases. Lucene Version OpenSearch Version Elasticsearch Version 9.4.2\n2.5.0 2.4.1\n8.6\n9.4.1\n2.4.0\n&#8212;\n9.4.0\n&#8212;\n8.5\n9.3.0\n2.3.0 2.2.x\n8.4\n9.2.0\n2.1.0\n8.3\n9.1.0\n2.0.x\n8.2\n9.0.0\n&#8212;\n8.1 8.0\n8.11.1\n&#8212;\n7.17\n8.10.1\n1.3.x 1.2.x\n7.16\n8.9.0\n1.1.0\n7.15 7.14\n8.8.2\n1.0.0\n7.13\n8.8.0\n&#8212;\n7.12\n8.7.0\n&#8212;\n7.11 7.10\n8.6.2\n&#8212;\n7.9\n8.5.1\n&#8212;\n7.8 7.7\n8.4.0\n&#8212;\n7.6\n8.3.0\n&#8212;\n7.5\n8.2.0\n&#8212;\n7.4\n8.1.0\n&#8212;\n7.3\n8.0.0\n&#8212;\n7.2 7.1\n7.7.3\n&#8212;\n6.8 A dash (&#8212;) indicates that there is no product version containing the specified version of Apache Lucene.",
    "ancestors": [
      "Install and upgrade"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/install-and-configure/upgrade-opensearch/rolling-upgrade/",
    "title": "Rolling Upgrade",
    "content": "Rolling upgrades, sometimes referred to as “node replacement upgrades,” can be performed on running clusters with virtually no downtime. Nodes are individually stopped and upgraded in place. Alternatively, nodes can be stopped and replaced, one at a time, by hosts running the new version. During this process you can continue to index and query data in your cluster.\nThis document serves as a high-level, platform-agnostic overview of the rolling upgrade procedure. For specific examples of commands, scripts, and configuration files, refer to the Appendix.\nPreparing to upgrade\nReview Upgrading OpenSearch for recommendations about backing up your configuration files and creating a snapshot of the cluster state and indexes before you make any changes to your OpenSearch cluster. Important: OpenSearch nodes cannot be downgraded. If you need to revert the upgrade, then you will need to perform a fresh installation of OpenSearch and restore the cluster from a snapshot. Take a snapshot and store it in a remote repository before beginning the upgrade procedure.\nPerforming the upgrade\nVerify the health of your OpenSearch cluster before you begin. You should resolve any index or shard allocation issues prior to upgrading to ensure that your data is preserved. A status of green indicates that all primary and replica shards are allocated. See Cluster health for more information. The following command queries the _cluster/health API endpoint: GET \"/_cluster/health?pretty\" The response should look similar to the following example: { \"cluster_name\": \"opensearch-dev-cluster\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 4, \"number_of_data_nodes\": 4, \"active_primary_shards\": 1, \"active_shards\": 4, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0, \"delayed_unassigned_shards\": 0, \"number_of_pending_tasks\": 0, \"number_of_in_flight_fetch\": 0, \"task_max_waiting_in_queue_millis\": 0, \"active_shards_percent_as_number\": 100.0 } Disable shard replication to prevent shard replicas from being created while nodes are being taken offline. This stops the movement of Lucene index segments on nodes in your cluster. You can disable shard replication by querying the _cluster/settings API endpoint: PUT \"/_cluster/settings?pretty\" { \"persistent\": { \"cluster.routing.allocation.enable\": \"primaries\" } } The response should look similar to the following example: { \"acknowledged\": true, \"persistent\": { \"cluster\": { \"routing\": { \"allocation\": { \"enable\": \"primaries\" } } } }, \"transient\": { } } Perform a flush operation on the cluster to commit transaction log entries to the Lucene index: POST \"/_flush?pretty\" The response should look similar to the following example: { \"_shards\": { \"total\": 4, \"successful\": 4, \"failed\": 0 } } Review your cluster and identify the first node to upgrade. Eligible cluster manager nodes should be upgraded last because OpenSearch nodes can join a cluster with manager nodes running an older version, but they cannot join a cluster with all manager nodes running a newer version.\nQuery the _cat/nodes endpoint to identify which node was promoted to cluster manager. The following command includes additional query parameters that request only the name, version, node.role, and master headers. Note that OpenSearch 1.x versions use the term “master,” which has been deprecated and replaced by “cluster_manager” in OpenSearch 2.x and later. GET \"/_cat/nodes?v&amp;h=name,version,node.role,master\" | column -t The response should look similar to the following example: name version node.role master\nos-node-01 7.10.2 dimr -\nos-node-04 7.10.2 dimr -\nos-node-03 7.10.2 dimr -\nos-node-02 7.10.2 dimr * Stop the node you are upgrading. Do not delete the volume associated with the container when you delete the container. The new OpenSearch container will use the existing volume. Deleting the volume will result in data loss.\nConfirm that the associated node has been dismissed from the cluster by querying the _cat/nodes API endpoint: GET \"/_cat/nodes?v&amp;h=name,version,node.role,master\" | column -t The response should look similar to the following example: name version node.role master\nos-node-02 7.10.2 dimr * os-node-04 7.10.2 dimr -\nos-node-03 7.10.2 dimr - os-node-01 is no longer listed because the container has been stopped and deleted.\nDeploy a new container running the desired version of OpenSearch and mapped to the same volume as the container you deleted.\nQuery the _cat/nodes endpoint after OpenSearch is running on the new node to confirm that it has joined the cluster: GET \"/_cat/nodes?v&amp;h=name,version,node.role,master\" | column -t The response should look similar to the following example: name version node.role master\nos-node-02 7.10.2 dimr * os-node-04 7.10.2 dimr -\nos-node-01 7.10.2 dimr -\nos-node-03 7.10.2 dimr - In the example output, the new OpenSearch node reports a running version of 7.10.2 to the cluster. This is the result of compatibility.override_main_response_version, which is used when connecting to a cluster with legacy clients that check for a version. You can manually confirm the version of the node by calling the /_nodes API endpoint, as in the following command. Replace &lt;nodeName&gt; with the name of your node. See Nodes API to learn more. GET \"/_nodes/&lt;nodeName&gt;?pretty=true\" | jq -r '.nodes |.[] | \"\\(.name) v\\(.version)\"' The response should look similar to the following example: os-node-01 v1.3.7 Repeat steps 5 through 9 for each node in your cluster. Remember to upgrade an eligible cluster manager node last. After replacing the last node, query the _cat/nodes endpoint to confirm that all nodes have joined the cluster. The cluster is now bootstrapped to the new version of OpenSearch. You can verify the cluster version by querying the _cat/nodes API endpoint: GET \"/_cat/nodes?v&amp;h=name,version,node.role,master\" | column -t The response should look similar to the following example: name version node.role master\nos-node-04 1.3.7 dimr -\nos-node-02 1.3.7 dimr * os-node-01 1.3.7 dimr -\nos-node-03 1.3.7 dimr - Reenable shard replication: PUT \"/_cluster/settings?pretty\" { \"persistent\": { \"cluster.routing.allocation.enable\": \"all\" } } The response should look similar to the following example: { \"acknowledged\": true, \"persistent\": { \"cluster\": { \"routing\": { \"allocation\": { \"enable\": \"all\" } } } }, \"transient\": { } } Confirm that the cluster is healthy: GET \"/_cluster/health?pretty\" The response should look similar to the following example: { \"cluster_name\": \"opensearch-dev-cluster\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 4, \"number_of_data_nodes\": 4, \"discovered_master\": true, \"active_primary_shards\": 1, \"active_shards\": 4, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0, \"delayed_unassigned_shards\": 0, \"number_of_pending_tasks\": 0, \"number_of_in_flight_fetch\": 0, \"task_max_waiting_in_queue_millis\": 0, \"active_shards_percent_as_number\": 100.0 } The upgrade is now complete, and you can begin enjoying the latest features and fixes!\nRelated articles OpenSearch configuration Performance analyzer Install and configure OpenSearch Dashboards About Security in OpenSearch",
    "ancestors": [
      "Install and upgrade",
      "Upgrading OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/upgrade-to/dashboards-upgrade-to/",
    "title": "Migrating from Kibana OSS to OpenSearch Dashboards",
    "content": "Kibana OSS stores its visualizations and dashboards in one or more indexes (.kibana*) on the Elasticsearch OSS cluster. As such, the most important step is to leave those indexes intact as you migrate from Elasticsearch OSS to OpenSearch.\nConsider exporting all Kibana objects prior to starting the migration. In Kibana, choose Stack Management, Saved Objects, Export objects.\nAfter you migrate your Elasticsearch OSS cluster to OpenSearch, stop Kibana.\nFor safety, make a backup copy of &lt;kibana-dir&gt;/config/kibana.yml.\nExtract the OpenSearch Dashboards tarball to a new directory.\nPort your settings from &lt;kibana-dir&gt;/config/kibana.yml to &lt;dashboards-dir&gt;/config/opensearch_dashboards.yml.\nIn general, settings with elasticsearch in their names map to opensearch (for example, elasticsearch.shardTimeout and opensearch.shardTimeout) and settings with kibana in their names map to opensearchDashboards (for example, kibana.defaultAppId and opensearchDashboards.defaultAppId). Most other settings use the same names.\nFor a full list of OpenSearch Dashboards settings, see opensearch_dashboards.yml.\nIf your OpenSearch cluster uses the Security plugin, preserve and modify the default settings in opensearch_dashboards.yml, particularly opensearch.username and opensearch.password.\nIf you disabled the Security plugin on your OpenSearch cluster, remove or comment out all opensearch_security settings. Then run rm -rf plugins/security-dashboards/ to remove the Security plugin.\nStart OpenSearch Dashboards:./bin/opensearch-dashboards Log in, and verify that your saved searches, visualizations, and dashboards are present.",
    "ancestors": [
      "Migrate to OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/upgrade-to/docker-upgrade-to/",
    "title": "Migrating Docker clusters to OpenSearch",
    "content": "If you use a container orchestration system like Kubernetes (or manage your containers manually) and want to avoid downtime, think of the process not as an upgrade of each node, but as a decommissioning and replacement of each node. One by one, add OpenSearch nodes to the cluster and remove Elasticsearch OSS nodes, pointing to existing data volumes as necessary and allowing time for all indexes to return to a green status prior to proceeding.\nIf you use Docker Compose, we highly recommend that you perform what amounts to a cluster restart upgrade. Update your cluster configuration with new images, new settings, and new environment variables, and test it. Then stop and start the cluster. This process requires downtime, but takes very few steps and lets you continue to treat the cluster as a single entity that you can reliably deploy and redeploy.\nThe most important step is to leave your data volumes intact. Don’t run docker-compose down -v.",
    "ancestors": [
      "Migrate to OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/upgrade-to/index/",
    "title": "About the migration process",
    "content": "The process of migrating from Elasticsearch OSS to OpenSearch varies depending on your current version of Elasticsearch OSS, installation type, tolerance for downtime, and cost-sensitivity. Rather than concrete steps to cover every situation, we have general guidance for the process.\nThree approaches exist:\nUse a snapshot to migrate your Elasticsearch OSS data to a new OpenSearch cluster. This method may incur downtime.\nPerform a restart upgrade or a rolling upgrade on your existing nodes. A restart upgrade involves upgrading the entire cluster and restarting it, whereas a rolling upgrade requires upgrading and restarting nodes in the cluster one by one.\nReplace existing Elasticsearch OSS nodes with new OpenSearch nodes. Node replacement is most popular when upgrading Docker clusters.\nRegardless of your approach, to safeguard against data loss, we recommend that you take a snapshot of all indexes prior to any migration.\nIf your existing clients include a version check, such as recent versions of Logstash OSS and Filebeat OSS, check compatibility before upgrading.\nUpgrading from Open Distro\nFor steps to upgrade from Open Distro to OpenSearch, refer to the blog post How To: Upgrade from Open Distro to OpenSearch.",
    "ancestors": [
      "Migrate to OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/upgrade-to/snapshot-migrate/",
    "title": "Using snapshots to migrate data",
    "content": "One popular approach is to take a snapshot of your Elasticsearch OSS 6.x or 7.x indexes, create an OpenSearch cluster, restore the snapshot on the new cluster, and point your clients to the new host.\nThe snapshot approach can mean running two clusters in parallel, but lets you validate that the OpenSearch cluster is working in a way that meets your needs prior to modifying the Elasticsearch OSS cluster.",
    "ancestors": [
      "Migrate to OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/upgrade-to/upgrade-to/",
    "title": "Migrating from Elasticsearch OSS to OpenSearch",
    "content": "If you want to migrate from an existing Elasticsearch OSS cluster to OpenSearch and find the snapshot approach unappealing, you can migrate your existing nodes from Elasticsearch OSS to OpenSearch.\nIf your existing cluster runs an older version of Elasticsearch OSS, the first step is to upgrade to version 6.x or 7.x. Elasticsearch OSS supports two types of upgrades: rolling and cluster restart.\nRolling upgrades let you shut down one node at a time for minimal disruption of service.\nRolling upgrades work between minor versions (for example, 6.5 to 6.8) and also support a single path to the next major version (for example, 6.8 to 7.10.2). Performing these upgrades might require intermediate upgrades to arrive at your desired version and can affect cluster performance as nodes leave and rejoin, but the cluster remains available throughout the process.\nCluster restart upgrades require you to shut down all nodes, perform the upgrade, and restart the cluster.\nCluster restart upgrades work between minor versions (for example, 6.5 to 6.8) and the next major version (for example, 6.x to 7.10.2). Cluster restart upgrades are faster to perform and require fewer intermediate upgrades, but require downtime.\nMigration paths Elasticsearch OSS version Rolling upgrade path Cluster restart upgrade path 5.x\nUpgrade to 5.6, upgrade to 6.8, reindex all 5.x indexes, upgrade to 7.10.2, and migrate to OpenSearch.\nUpgrade to 6.8, reindex all 5.x indexes, and migrate to OpenSearch.\n6.x\nUpgrade to 6.8, upgrade to 7.10.2, and migrate to OpenSearch.\nMigrate to OpenSearch.\n7.x\nMigrate to OpenSearch.\nMigrate to OpenSearch. If you are migrating an Open Distro for Elasticsearch cluster, we recommend first upgrading to ODFE 1.13 and then migrating to OpenSearch.\nUpgrade Elasticsearch OSS\nDisable shard allocation to prevent Elasticsearch OSS from replicating shards as you shut down nodes: PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"primaries\" } } Stop Elasticsearch OSS on one node (rolling upgrade) or all nodes (cluster restart upgrade).\nOn Linux distributions that use systemd, use this command: sudo systemctl stop elasticsearch.service For tarball installations, find the process ID ( ps aux) and kill it ( kill &lt;pid&gt;).\nUpgrade the node (rolling) or all nodes (cluster restart).\nThe exact command varies by package manager, but likely looks something like this: sudo yum install elasticsearch-oss-7.10.2 --enablerepo = elasticsearch For tarball installations, extract to a new directory to ensure you do not overwrite your config, data, and logs directories. Ideally, these directories should have their own, independent paths and not be colocated with the Elasticsearch application directory. Then set the ES_PATH_CONF environment variable to the directory that contains elasticsearch.yml (for example, /etc/elasticesarch/). In elasticsearch.yml, set path.data and path.logs to your data and logs directories (for example, /var/lib/elasticsearch and /var/log/opensearch).\nRestart Elasticsearch OSS on the node (rolling) or all nodes (cluster restart).\nOn Linux distributions that use systemd, use this command: sudo systemctl start elasticsearch.service For tarball installations, run./bin/elasticsearch -d.\nWait for the node to rejoin the cluster (rolling) or for the cluster to start (cluster restart). Check the _nodes summary to verify that all nodes are available and running the expected version: # Elasticsearch OSS curl -XGET 'localhost:9200/_nodes/_all?pretty=true' # Open Distro for Elasticsearch with Security plugin enabled curl -XGET 'https://localhost:9200/_nodes/_all?pretty=true' -u 'admin:admin' -k Specifically, check the nodes.&lt;node-id&gt;.version portion of the response. Also check _cat/indices?v for a green status on all indexes.\n(Rolling) Repeat steps 2–5 until all nodes are using the new version.\nAfter all nodes are using the new version, re-enable shard allocation: PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"all\" } } If you upgraded from 5.x to 6.x, reindex all indexes.\nRepeat all steps as necessary until you arrive at your desired Elasticsearch OSS version.\nMigrate to OpenSearch\nDisable shard allocation to prevent Elasticsearch OSS from replicating shards as you shut down nodes: PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"primaries\" } } Stop Elasticsearch OSS on one node (rolling upgrade) or all nodes (cluster restart upgrade).\nOn Linux distributions that use systemd, use this command: sudo systemctl stop elasticsearch.service For tarball installations, find the process ID ( ps aux) and kill it ( kill &lt;pid&gt;).\nUpgrade the node (rolling) or all nodes (cluster restart).\nExtract the OpenSearch tarball to a new directory to ensure you do not overwrite your Elasticsearch OSS config, data, and logs directories.\n(Optional) Copy or move your Elasticsearch OSS data and logs directories to new paths. For example, you might move /var/lib/elasticsearch to /var/lib/opensearch.\nSet the OPENSEARCH_PATH_CONF environment variable to the directory that contains opensearch.yml (for example, /etc/opensearch).\nIn opensearch.yml, set path.data and path.logs. You might also want to disable the Security plugin for now. opensearch.yml might look something like this: path.data: /var/lib/opensearch path.logs: /var/log/opensearch plugins.security.disabled: true Port your settings from elasticsearch.yml to opensearch.yml. Most settings use the same names. At a minimum, specify cluster.name, node.name, discovery.seed_hosts, and cluster.initial_cluster_manager_nodes.\n(Optional) If you’re actively connecting to the cluster with legacy clients that check for a particular version number, such as Logstash OSS, add a compatibility setting to opensearch.yml: compatibility.override_main_response_version: true (Optional) Add your certificates to your config directory, add them to opensearch.yml, and initialize the Security plugin.\nStart OpenSearch on the node (rolling) or all nodes (cluster restart).\nFor the tarball, run./bin/opensearch -d.\nWait for the OpenSearch node to rejoin the cluster (rolling) or for the cluster to start (cluster restart). Check the _nodes summary to verify that all nodes are available and running the expected version: # Security plugin disabled curl -XGET 'localhost:9200/_nodes/_all?pretty=true' # Security plugin enabled curl -XGET -k -u 'admin:admin' 'https://localhost:9200/_nodes/_all?pretty=true' Specifically, check the nodes.&lt;node-id&gt;.version portion of the response. Also check _cat/indices?v for a green status on all indexes.\n(Rolling) Repeat steps 2–5 until all nodes are using OpenSearch.\nAfter all nodes are using the new version, re-enable shard allocation: PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"all\" } } Upgrade tool\nThe opensearch-upgrade tool lets you automate some of the steps in Migrate to OpenSearch, eliminating the need for error-prone manual operations.\nThe opensearch-upgrade tool performs the following functions:\nImports any existing configurations and applies it to the new installation of OpenSearch.\nInstalls any existing core plugins.\nLimitations\nThe opensearch-upgrade tool doesn’t perform an end-to-end upgrade:\nYou need to run the tool on each node of the cluster individually as part of the upgrade process.\nThe tool doesn’t provide a rollback option after you’ve upgraded a node, so make sure you follow best practices and take backups.\nYou must install all community plugins (if available) manually.\nThe tool only validates any keystore settings at service start-up time, so you must manually remove any unsupported settings for the service to start.\nUsing the upgrade tool\nTo perform a rolling upgrade using the OpenSearch tarball distribution:\nCheck Migration paths to make sure that the version you’re upgrading to is supported and whether you need to upgrade to a supported Elasticsearch OSS version first.\nDisable shard allocation to prevent Elasticsearch OSS from replicating shards as you shut down nodes: PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"primaries\" } } On any one of the nodes, download and extract the OpenSearch tarball to a new directory.\nMake sure the following environment variables are set: ES_HOME - Path to the existing Elasticsearch installation home. export ES_HOME = /home/workspace/upgrade-demo/node1/elasticsearch-7.10.2 ES_PATH_CONF - Path to the existing Elasticsearch config directory. export ES_PATH_CONF = /home/workspace/upgrade-demo/node1/os-config OPENSEARCH_HOME - Path to the OpenSearch installation home. export OPENSEARCH_HOME = /home/workspace/upgrade-demo/node1/opensearch-1.0.0 OPENSEARCH_PATH_CONF - Path to the OpenSearch config directory. export OPENSEARCH_PATH_CONF = /home/workspace/upgrade-demo/node1/opensearch-config The opensearch-upgrade tool is in the bin directory of the distribution. Run the following command from the distribution home:\nMake sure you run this tool as the same user running the current Elasticsearch service../bin/opensearch-upgrade Stop Elasticsearch OSS on the node.\nOn Linux distributions that use systemd, use this command: sudo systemctl stop elasticsearch.service For tarball installations, find the process ID ( ps aux) and kill it ( kill &lt;pid&gt;).\nStart OpenSearch on the node:./bin/opensearch -d. Repeat steps 2–6 until all nodes are using the new version.\nAfter all nodes are using the new version, re-enable shard allocation: PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"all\" } } How it works\nBehind the scenes, the opensearch-upgrade tool performs the following tasks in sequence:\nLooks for a valid Elasticsearch installation on the current node. After it finds the installation, it reads the elasticsearch.yml file to get the endpoint details and connects to the locally running Elasticsearch service. If the tool can’t find an Elasticsearch installation, it tries to get the path from the ES_HOME location.\nVerifies if the existing version of Elasticsearch is compatible with the OpenSearch version. It prints a summary of the information gathered to the console and prompts you for a confirmation to proceed.\nImports the settings from the elasticsearch.yml config file into the opensearch.yml config file.\nCopies across any custom JVM options from the $ES_PATH_CONF/jvm.options.d directory into the $OPENSEARCH_PATH_CONF/jvm.options.d directory. Similarly, it also imports the logging configurations from the $ES_PATH_CONF/log4j2.properties file into the $OPENSEARCH_PATH_CONF/log4j2.properties file.\nInstalls the core plugins that you’ve currently installed in the $ES_HOME/plugins directory. You must install all other third-party community plugins manually.\nImports the secure settings from the elasticsearch.keystore file (if any) into the opensearch.keystore file. If the keystore file is password protected, the opensearch-upgrade tool prompts you to enter the password.",
    "ancestors": [
      "Migrate to OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/data-streams/",
    "title": "Data streams",
    "content": "If you’re ingesting continuously generated time-series data such as logs, events, and metrics into OpenSearch, you’re likely in a scenario where the number of documents grows rapidly and you don’t need to update older documents.\nA typical workflow to manage time-series data involves multiple steps, such as creating a rollover index alias, defining a write index, and defining common mappings and settings for the backing indices.\nData streams simplify this process and enforce a setup that best suits time-series data, such as being designed primarily for append-only data and ensuring that each document has a timestamp field.\nA data stream is internally composed of multiple backing indices. Search requests are routed to all the backing indices, while indexing requests are routed to the latest write index. ISM policies let you automatically handle index rollovers or deletions.\nGet started with data streams\nStep 1: Create an index template\nTo create a data stream, you first need to create an index template that configures a set of indices as a data stream. The data_stream object indicates that it’s a data stream and not a regular index template. The index pattern matches with the name of the data stream: PUT _index_template/logs-template { \"index_patterns\": [ \"my-data-stream\", \"logs-*\"], \"data_stream\": {}, \"priority\": 100 } In this case, each ingested document must have an @timestamp field.\nYou also have the ability to define your own custom timestamp field as a property in the data_stream object. You can also add index mappings and other settings here, just as you would for a regular index template. PUT _index_template/logs-template-nginx { \"index_patterns\": \"logs-nginx\", \"data_stream\": { \"timestamp_field\": { \"name\": \"request_time\" } }, \"priority\": 200, \"template\": { \"settings\": { \"number_of_shards\": 1, \"number_of_replicas\": 0 } } } In this case, logs-nginx index matches both the logs-template and logs-template-nginx templates. When you have a tie, OpenSearch selects the matching index template with the higher priority value.\nStep 2: Create a data stream\nAfter you create an index template, you can create a data stream.\nYou can use the data stream API to explicitly create a data stream. The data stream API initializes the first backing index: PUT _data_stream/logs-redis PUT _data_stream/logs-nginx You can also directly start ingesting data without creating a data stream.\nBecause we have a matching index template with a data_stream object, OpenSearch automatically creates the data stream: POST logs-staging/_doc { \"message\": \"login attempt failed\", \"@timestamp\": \"2013-03-01T00:00:00\" } To see information about a specific data stream: GET _data_stream/logs-nginx Example response { \"data_streams\": [ { \"name\": \"logs-nginx\", \"timestamp_field\": { \"name\": \"request_time\" }, \"indices\": [ { \"index_name\": \".ds-logs-nginx-000001\", \"index_uuid\": \"-VhmuhrQQ6ipYCmBhn6vLw\" }], \"generation\": 1, \"status\": \"GREEN\", \"template\": \"logs-template-nginx\" }] } You can see the name of the timestamp field, the list of the backing indices, and the template that’s used to create the data stream. You can also see the health of the data stream, which represents the lowest status of all its backing indices.\nTo see more insights about the data stream, use the _stats endpoint: GET _data_stream/logs-nginx/_stats Example response { \"_shards\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"data_stream_count\": 1, \"backing_indices\": 1, \"total_store_size_bytes\": 208, \"data_streams\": [ { \"data_stream\": \"logs-nginx\", \"backing_indices\": 1, \"store_size_bytes\": 208, \"maximum_timestamp\": 0 }] } To see information about all data streams, use the following request: GET _data_stream Step 3: Ingest data into the data stream\nTo ingest data into a data stream, you can use the regular indexing APIs. Make sure every document that you index has a timestamp field. If you try to ingest a document that doesn’t have a timestamp field, you get an error. POST logs-redis/_doc { \"message\": \"login attempt\", \"@timestamp\": \"2013-03-01T00:00:00\" } Step 4: Searching a data stream\nYou can search a data stream just like you search a regular index or an index alias.\nThe search operation applies to all of the backing indices (all data present in the stream). GET logs-redis/_search { \"query\": { \"match\": { \"message\": \"login\" } } } Example response { \"took\": 514, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 0.2876821, \"hits\": [ { \"_index\": \".ds-logs-redis-000001\", \"_type\": \"_doc\", \"_id\": \"-rhVmXoBL6BAVWH3mMpC\", \"_score\": 0.2876821, \"_source\": { \"message\": \"login attempt\", \"@timestamp\": \"2013-03-01T00:00:00\" } }] } } Step 5: Rollover a data stream\nA rollover operation creates a new backing index that becomes the data stream’s new write index.\nTo perform manual rollover operation on the data stream: POST logs-redis/_rollover Example response { \"acknowledged\": true, \"shards_acknowledged\": true, \"old_index\": \".ds-logs-redis-000001\", \"new_index\": \".ds-logs-redis-000002\", \"rolled_over\": true, \"dry_run\": false, \"conditions\": { } } If you now perform a GET operation on the logs-redis data stream, you see that the generation ID is incremented from 1 to 2.\nYou can also set up an Index State Management (ISM) policy to automate the rollover process for the data stream.\nThe ISM policy is applied to the backing indices at the time of their creation. When you associate a policy to a data stream, it only affects the future backing indices of that data stream.\nYou also don’t need to provide the rollover_alias setting, because the ISM policy infers this information from the backing index.\nStep 6: Manage data streams in OpenSearch Dashboards\nTo manage data streams from OpenSearch Dashboards, open OpenSearch Dashboards, choose Index Management, select Indices or Policy managed indices.\nYou see a toggle switch for data streams that you can use to show or hide indices belonging to a data stream.\nWhen you enable this switch, you see a data stream multi-select dropdown menu that you can use for filtering data streams.\nYou also see a data stream column that shows you the name of the data stream the index is contained in. You can select one or more data streams and apply an ISM policy on them. You can also apply a policy on any individual backing index.\nYou can performing visualizations on a data stream just like you would on a regular index or index alias.\nStep 7: Delete a data stream\nThe delete operation first deletes the backing indices of a data stream and then deletes the data stream itself.\nTo delete a data stream and all of its hidden backing indices: DELETE _data_stream/&lt;name_of_data_stream&gt; You can use wildcards to delete more than one data stream.\nWe recommend deleting data from a data stream using an ISM policy.\nYou can also use asynchronous search, SQL, and PPL to query your data stream directly. You can also use the Security plugin to define granular permissions for the data stream name.",
    "ancestors": [
      "Managing Indexes"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-alias/",
    "title": "Index aliases",
    "content": "An alias is a virtual index name that can point to one or more indexes.\nIf your data is spread across multiple indexes, rather than keeping track of which indexes to query, you can create an alias and query it instead.\nFor example, if you’re storing logs into indexes based on the month and you frequently query the logs for the previous two months, you can create a last_2_months alias and update the indexes it points to each month.\nBecause you can change the indexes an alias points to at any time, referring to indexes using aliases in your applications allows you to reindex your data without any downtime.\nTable of contents Create aliases Add or remove indexes Manage aliases Add aliases at index creation Create filtered aliases Index alias options Delete aliases Create aliases\nTo create an alias, use a POST request: POST _aliases Use the actions method to specify the list of actions that you want to perform. This command creates an alias named alias1 and adds index-1 to this alias: POST _aliases { \"actions\": [ { \"add\": { \"index\": \"index-1\", \"alias\": \"alias1\" } }] } You should see the following response: { \"acknowledged\": true } If this request fails, make sure the index that you’re adding to the alias already exists.\nYou can also create an alias using one of the following requests: PUT &lt;index&gt;/_aliases/&lt;alias name&gt; POST &lt;index&gt;/_aliases/&lt;alias name&gt; PUT &lt;index&gt;/_alias/&lt;alias name&gt; POST &lt;index&gt;/_alias/&lt;alias name&gt; The &lt;index&gt; in the above requests can be an index name, a comma-separated list of index names, or a wildcard expression. Use _all to refer to all indexes.\nTo check if alias1 refers to index-1, run one of the following commands: GET /_alias/alias 1 GET /index -1 /_alias/alias 1 To get the mappings and settings information of the indexes that the alias references, run the following command: GET alias 1 Add or remove indexes\nYou can perform multiple actions in the same _aliases operation.\nFor example, the following command removes index-1 and adds index-2 to alias1: POST _aliases { \"actions\": [ { \"remove\": { \"index\": \"index-1\", \"alias\": \"alias1\" } }, { \"add\": { \"index\": \"index-2\", \"alias\": \"alias1\" } }] } The add and remove actions occur atomically, which means that at no point will alias1 point to both index-1 and index-2.\nYou can also add indexes based on an index pattern: POST _aliases { \"actions\": [ { \"add\": { \"index\": \"index*\", \"alias\": \"alias1\" } }] } Manage aliases\nTo list the mapping of aliases to indexes, run the following command: GET _cat/aliases?v Example response alias index filter routing.index routing.search alias 1 index -1 * - - To check which indexes an alias points to, run the following command: GET _alias/alias 1 Example response { \"index-2\": { \"aliases\": { \"alias1\": {} } } } Conversely, to find which alias points to a specific index, run the following command: GET /index -2 /_alias/* To get all index names and their aliases, run the following command: GET /_alias To check if an alias exists, run one of the following commands: HEAD /alias 1 /_alias/ HEAD /_alias/alias 1 / HEAD index -1 /_alias/alias 1 / Add aliases at index creation\nYou can add an index to an alias as you create the index: PUT index -1 { \"aliases\": { \"alias1\": {} } } Create filtered aliases\nYou can create a filtered alias to access a subset of documents or fields from the underlying indexes.\nThis command adds only a specific timestamp field to alias1: POST _aliases { \"actions\": [ { \"add\": { \"index\": \"index-1\", \"alias\": \"alias1\", \"filter\": { \"term\": { \"timestamp\": \"1574641891142\" } } } }] } Index alias options\nYou can specify the options shown in the following table. Option Valid values Description Required index String\nThe name of the index that the alias points to.\nYes alias String\nThe name of the alias.\nNo filter Object\nAdd a filter to the alias.\nNo routing String\nLimit search to an associated shard value. You can specify search_routing and index_routing independently.\nNo is_write_index String\nSpecify the index that accepts any write operations to the alias. If this value is not specified, then no write operations are allowed.\nNo Delete aliases\nTo delete one or more aliases from an index, use the following request: DELETE &lt;index&gt;/_alias/&lt;alias&gt; DELETE &lt;index&gt;/_aliases/&lt;alias&gt; Both &lt;index&gt; and &lt;alias&gt; in the above request support comma-separated lists and wildcard expressions. Use _all in place of &lt;alias&gt; to delete all aliases for the indexes listed in &lt;index&gt;.\nFor example, if alias1 refers to index-1 and index-2, you can run the following command to remove alias1 from index-1: DELETE index -1 /_alias/alias 1 After you run the request above, alias1 no longer refers to index-1, but still refers to index-2.",
    "ancestors": [
      "Managing Indexes"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-rollups/index/",
    "title": "Index rollups",
    "content": "Time series data increases storage costs, strains cluster health, and slows down aggregations over time. Index rollup lets you periodically reduce data granularity by rolling up old data into summarized indexes.\nYou pick the fields that interest you and use index rollup to create a new index with only those fields aggregated into coarser time buckets. You can store months or years of historical data at a fraction of the cost with the same query performance.\nFor example, say you collect CPU consumption data every five seconds and store it on a hot node. Instead of moving older data to a read-only warm node, you can roll up or compress this data with only the average CPU consumption per day or with a 10% decrease in its interval every week.\nYou can use index rollup in three ways:\nUse the index rollup API for an on-demand index rollup job that operates on an index that’s not being actively ingested such as a rolled-over index. For example, you can perform an index rollup operation to reduce data collected at a five minute interval to a weekly average for trend analysis.\nUse the OpenSearch Dashboards UI to create an index rollup job that runs on a defined schedule. You can also set it up to roll up your indexes as it’s being actively ingested. For example, you can continuously roll up Logstash indexes from a five second interval to a one hour interval.\nSpecify the index rollup job as an ISM action for complete index management. This allows you to roll up an index after a certain event such as a rollover, index age reaching a certain point, index becoming read-only, and so on. You can also have rollover and index rollup jobs running in sequence, where the rollover first moves the current index to a warm node and then the index rollup job creates a new index with the minimized data on the hot node.\nCreate an Index Rollup Job\nTo get started, choose Index Management in OpenSearch Dashboards.\nSelect Rollup Jobs and choose Create rollup job.\nStep 1: Set up indexes\nIn the Job name and description section, specify a unique name and an optional description for the index rollup job.\nIn the Indices section, select the source and target index. The source index is the one that you want to roll up. The source index remains as is, the index rollup job creates a new index referred to as a target index. The target index is where the index rollup results are saved. For target index, you can either type in a name for a new index or you select an existing index.\nChoose Next After you create an index rollup job, you can’t change your index selections.\nStep 2: Define aggregations and metrics\nSelect the attributes with the aggregations (terms and histograms) and metrics (avg, sum, max, min, and value count) that you want to roll up. Make sure you don’t add a lot of highly granular attributes, because you won’t save much space.\nFor example, consider a dataset of cities and demographics within those cities. You can aggregate based on cities and specify demographics within a city as metrics.\nThe order in which you select attributes is critical. A city followed by a demographic is different from a demographic followed by a city.\nIn the Time aggregation section, select a timestamp field. Choose between a Fixed or Calendar interval type and specify the interval and timezone. The index rollup job uses this information to create a date histogram for the timestamp field.\n(Optional) Add additional aggregations for each field. You can choose terms aggregation for all field types and histogram aggregation only for numeric fields.\n(Optional) Add additional metrics for each field. You can choose between All, Min, Max, Sum, Avg, or Value Count.\nChoose Next.\nStep 3: Specify schedule\nSpecify a schedule to roll up your indexes as it’s being ingested. The index rollup job is enabled by default.\nSpecify if the data is continuous or not.\nFor roll up execution frequency, select Define by fixed interval and specify the Rollup interval and the time unit or Define by cron expression and add in a cron expression to select the interval. To learn how to define a cron expression, see Alerting.\nSpecify the number of pages per execution process. A larger number means faster execution and more cost for memory.\n(Optional) Add a delay to the roll up executions. This is the amount of time the job waits for data ingestion to accommodate any processing time. For example, if you set this value to 10 minutes, an index rollup that executes at 2 PM to roll up 1 PM to 2 PM of data starts at 2:10 PM.\nChoose Next.\nStep 4: Review and create\nReview your configuration and select Create.\nStep 5: Search the target index\nYou can use the standard _search API to search the target index. Make sure that the query matches the constraints of the target index. For example, if you don’t set up terms aggregations on a field, you don’t receive results for terms aggregations. If you don’t set up the maximum aggregations, you don’t receive results for maximum aggregations.\nYou can’t access the internal structure of the data in the target index because the plugin automatically rewrites the query in the background to suit the target index. This is to make sure you can use the same query for the source and target index.\nTo query the target index, set size to 0: GET target_index/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"avg_cpu\": { \"avg\": { \"field\": \"cpu_usage\" } } } } Consider a scenario where you collect rolled up data from 1 PM to 9 PM in hourly intervals and live data from 7 PM to 11 PM in minutely intervals. If you execute an aggregation over these in the same query, for 7 PM to 9 PM, you see an overlap of both rolled up data and live data because they get counted twice in the aggregations.\nSample Walkthrough\nThis walkthrough uses the OpenSearch Dashboards sample e-commerce data. To add that sample data, log in to OpenSearch Dashboards, choose Home and Try our sample data. For Sample eCommerce orders, choose Add data.\nThen run a search: GET opensearch_dashboards_sample_data_ecommerce/_search Example response { \"took\": 23, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4675, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"_type\": \"_doc\", \"_id\": \"jlMlwXcBQVLeQPrkC_kQ\", \"_score\": 1, \"_source\": { \"category\": [ \"Women's Clothing\", \"Women's Accessories\"], \"currency\": \"EUR\", \"customer_first_name\": \"Selena\", \"customer_full_name\": \"Selena Mullins\", \"customer_gender\": \"FEMALE\", \"customer_id\": 42, \"customer_last_name\": \"Mullins\", \"customer_phone\": \"\", \"day_of_week\": \"Saturday\", \"day_of_week_i\": 5, \"email\": \"selena@mullins-family.zzz\", \"manufacturer\": [ \"Tigress Enterprises\"], \"order_date\": \"2021-02-27T03:56:10+00:00\", \"order_id\": 581553, \"products\": [ { \"base_price\": 24.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 19240, \"category\": \"Women's Clothing\", \"sku\": \"ZO0064500645\", \"taxless_price\": 24.99, \"unit_discount_amount\": 0, \"min_price\": 12.99, \"_id\": \"sold_product_581553_19240\", \"discount_amount\": 0, \"created_on\": \"2016-12-24T03:56:10+00:00\", \"product_name\": \"Blouse - port royal\", \"price\": 24.99, \"taxful_price\": 24.99, \"base_unit_price\": 24.99 }, { \"base_price\": 10.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 17221, \"category\": \"Women's Accessories\", \"sku\": \"ZO0085200852\", \"taxless_price\": 10.99, \"unit_discount_amount\": 0, \"min_price\": 5.06, \"_id\": \"sold_product_581553_17221\", \"discount_amount\": 0, \"created_on\": \"2016-12-24T03:56:10+00:00\", \"product_name\": \"Snood - rose\", \"price\": 10.99, \"taxful_price\": 10.99, \"base_unit_price\": 10.99 }], \"sku\": [ \"ZO0064500645\", \"ZO0085200852\"], \"taxful_total_price\": 35.98, \"taxless_total_price\": 35.98, \"total_quantity\": 2, \"total_unique_products\": 2, \"type\": \"order\", \"user\": \"selena\", \"geoip\": { \"country_iso_code\": \"MA\", \"location\": { \"lon\": -8, \"lat\": 31.6 }, \"region_name\": \"Marrakech-Tensift-Al Haouz\", \"continent_name\": \"Africa\", \"city_name\": \"Marrakesh\" }, \"event\": { \"dataset\": \"sample_ecommerce\" } } }] } }... Create an index rollup job.\nThis example picks the order_date, customer_gender, geoip.city_name, geoip.region_name, and day_of_week fields and rolls them into an example_rollup target index: PUT _plugins/_rollup/jobs/example { \"rollup\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"last_updated_time\": 1602100553, \"description\": \"An example policy that rolls up the sample ecommerce data\", \"source_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"target_index\": \"example_rollup\", \"page_size\": 1000, \"delay\": 0, \"continuous\": false, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"order_date\", \"fixed_interval\": \"60m\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"customer_gender\" } }, { \"terms\": { \"source_field\": \"geoip.city_name\" } }, { \"terms\": { \"source_field\": \"geoip.region_name\" } }, { \"terms\": { \"source_field\": \"day_of_week\" } }], \"metrics\": [ { \"source_field\": \"taxless_total_price\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} }] }, { \"source_field\": \"total_quantity\", \"metrics\": [ { \"avg\": {} }, { \"max\": {} }] }] } } You can query the example_rollup index for the terms aggregations on the fields set up in the rollup job.\nYou get back the same response that you would on the original opensearch_dashboards_sample_data_ecommerce source index: POST example_rollup/_search { \"size\": 0, \"query\": { \"bool\": { \"must\": { \"term\": { \"geoip.region_name\": \"California\" } } } }, \"aggregations\": { \"daily_numbers\": { \"terms\": { \"field\": \"day_of_week\" }, \"aggs\": { \"per_city\": { \"terms\": { \"field\": \"geoip.city_name\" }, \"aggregations\": { \"average quantity\": { \"avg\": { \"field\": \"total_quantity\" } } } }, \"total_revenue\": { \"sum\": { \"field\": \"taxless_total_price\" } } } } } } Sample Response { \"took\": 14, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 281, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"daily_numbers\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Friday\", \"doc_count\": 59, \"total_revenue\": { \"value\": 4858.84375 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 59, \"average quantity\": { \"value\": 2.305084745762712 } }] } }, { \"key\": \"Saturday\", \"doc_count\": 46, \"total_revenue\": { \"value\": 3547.203125 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 46, \"average quantity\": { \"value\": 2.260869565217391 } }] } }, { \"key\": \"Tuesday\", \"doc_count\": 45, \"total_revenue\": { \"value\": 3983.28125 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 45, \"average quantity\": { \"value\": 2.2888888888888888 } }] } }, { \"key\": \"Sunday\", \"doc_count\": 44, \"total_revenue\": { \"value\": 3308.1640625 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 44, \"average quantity\": { \"value\": 2.090909090909091 } }] } }, { \"key\": \"Thursday\", \"doc_count\": 40, \"total_revenue\": { \"value\": 2876.125 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 40, \"average quantity\": { \"value\": 2.3 } }] } }, { \"key\": \"Monday\", \"doc_count\": 38, \"total_revenue\": { \"value\": 2673.453125 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 38, \"average quantity\": { \"value\": 2.1578947368421053 } }] } }, { \"key\": \"Wednesday\", \"doc_count\": 38, \"total_revenue\": { \"value\": 3202.453125 }, \"per_city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Los Angeles\", \"doc_count\": 38, \"average quantity\": { \"value\": 2.236842105263158 } }] } }] } } } The doc_count field\nThe doc_count field in bucket aggregations contains the number of documents collected in each bucket. When calculating the bucket’s doc_count, the number of documents is incremented by the number of the pre-aggregated documents in each summary document. The doc_count returned from rollup searches represents the total number of matching documents from the source index. The document count for each bucket is the same whether you search the source index or the rollup target index.\nQuery string queries\nTo take advantage of shorter and more easily written strings in Query DSL, you can use query strings to simplify search queries in rollup indexes. To use query strings, add the following fields to your rollup search request: \"query\": { \"query_string\": { \"query\": \"field_name:field_value\" } } The following example uses a query string with a * wildcard operator to search inside a rollup index called my_server_logs_rollup: GET my_server_logs_rollup/_search { \"size\": 0, \"query\": { \"query_string\": { \"query\": \"email* OR inventory\", \"default_field\": \"service_name\" } }, \"aggs\": { \"service_name\": { \"terms\": { \"field\": \"service_name\" }, \"aggs\": { \"region\": { \"terms\": { \"field\": \"region\" }, \"aggs\": { \"average quantity\": { \"avg\": { \"field\": \"cpu_usage\" } } } } } } } } For more information about query string query parameters, see Query string query.\nDynamic target index\nIn ISM rollup, the target_index field may contain a template that is compiled at the time of each rollup indexing. For example, if you specify the target_index field as rollup_ndx-{{ctx.source_index}}, the source index log-000001 will roll up into a target index rollup_ndx-log-000001. This allows you to roll up data into multiple time-based indexes, with one rollup job created for each source index.\nThe source_index parameter in {{ctx.source_index}} cannot contain wildcards.\nSearching multiple rollup indexes\nWhen data is rolled up into multiple target indexes, you can run one search across all of the rollup indexes. To search multiple target indexes that have the same rollup, specify the index names as a comma-separated list or a wildcard pattern. For example, with target_index as rollup_ndx-{{ctx.source_index}} and source indexes that start with log, specify the rollup_ndx-log* pattern. Or, to search for rolled up log-000001 and log-000002 indexes, specify the rollup_ndx-log-000001,rollup_ndx-log-000002 list.\nYou cannot search a mix of rollup and non-rollup indexes with the same query.\nExample\nThe following example demonstrates the doc_count field, dynamic index names, and searching multiple rollup indexes with the same rollup. Step 1: Add an index template for ISM to manage the rolling over of the indexes aliased by log: PUT _index_template/ism_rollover { \"index_patterns\": [ \"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } Step 2: Set up an ISM rollover policy to roll over any index whose name starts with log* after one document is uploaded to it, and then roll up the individual backing index. The target index name is dynamically generated from the source index name by prepending the string rollup_ndx- to the source index name. PUT _plugins/_ism/policies/rollover_policy { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } }], \"transitions\": [ { \"state_name\": \"rp\" }] }, { \"name\": \"rp\", \"actions\": [ { \"rollup\": { \"ism_rollup\": { \"target_index\": \"rollup_ndx-{{ctx.source_index}}\", \"description\": \"Example rollup job\", \"page_size\": 200, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"ts\", \"fixed_interval\": \"60m\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"message.keyword\" } }], \"metrics\": [ { \"source_field\": \"msg_size\", \"metrics\": [ { \"sum\": {} }] }] } } }], \"transitions\": [] }], \"ism_template\": { \"index_patterns\": [ \"log*\"], \"priority\": 100 } } } Step 3: Create an index named log-000001 and set up an alias log for it. PUT log -000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } Step 4: Index four documents into the index created above. Two of the documents have the message “Success”, and two have the message “Error”. POST log/_doc?refresh= true { \"ts\": \"2022-08-26T09:28:48-04:00\", \"message\": \"Success\", \"msg_size\": 10 } POST log/_doc?refresh= true { \"ts\": \"2022-08-26T10:06:25-04:00\", \"message\": \"Error\", \"msg_size\": 20 } POST log/_doc?refresh= true { \"ts\": \"2022-08-26T10:23:54-04:00\", \"message\": \"Error\", \"msg_size\": 30 } POST log/_doc?refresh= true { \"ts\": \"2022-08-26T10:53:41-04:00\", \"message\": \"Success\", \"msg_size\": 40 } Once you index the first document, the rollover action is executed. This action creates the index log-000002 with rollover_policy attached to it. Then the rollup action is executed, which creates the rollup index rollup_ndx-log-000001.\nTo monitor the status of rollover and rollup index creation, you can use the ISM explain API: GET _plugins/_ism/explain Step 5: Search the rollup index. GET rollup_ndx-log-*/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggregations\": { \"message_numbers\": { \"terms\": { \"field\": \"message.keyword\" }, \"aggs\": { \"per_message\": { \"terms\": { \"field\": \"message.keyword\" }, \"aggregations\": { \"sum_message\": { \"sum\": { \"field\": \"msg_size\" } } } } } } } } The response contains two buckets, “Error” and “Success”, and the document count for each bucket is 2: { \"took\": 30, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"message_numbers\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Success\", \"doc_count\": 2, \"per_message\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Success\", \"doc_count\": 2, \"sum_message\": { \"value\": 50.0 } }] } }, { \"key\": \"Error\", \"doc_count\": 2, \"per_message\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Error\", \"doc_count\": 2, \"sum_message\": { \"value\": 50.0 } }] } }] } } }",
    "ancestors": [
      "Managing Indexes"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-rollups/rollup-api/",
    "title": "Index rollups API",
    "content": "Use the index rollup operations to programmatically work with index rollup jobs.\nTable of contents Create or update an index rollup job Get an index rollup job Delete an index rollup job Start or stop an index rollup job Explain an index rollup job Create or update an index rollup job\nIntroduced 1.0\nCreates or updates an index rollup job.\nYou must provide the seq_no and primary_term parameters.\nRequest PUT _plugins/_rollup/jobs/&lt;rollup_id&gt; // Create PUT _plugins/_rollup/jobs/&lt;rollup_id&gt;?if_seq_no= 1 &amp;if_primary_term= 1 // Update { \"rollup\": { \"source_index\": \"nyc-taxi-data\", \"target_index\": \"rollup-nyc-taxi-data\", \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Days\" } }, \"description\": \"Example rollup job\", \"enabled\": true, \"page_size\": 200, \"delay\": 0, \"roles\": [ \"rollup_all\", \"nyc_taxi_all\", \"example_rollup_index_all\"], \"continuous\": false, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"tpep_pickup_datetime\", \"fixed_interval\": \"1h\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"PULocationID\" } }], \"metrics\": [ { \"source_field\": \"passenger_count\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} }] }] } } You can specify the following options. Options Description Type Required source_index The name of the detector.\nString\nYes target_index Specify the target index that the rolled up data is ingested into. You can either create a new target index or use an existing index. The target index cannot be a combination of raw and rolled up data. This field supports dynamically generated index names like rollup_{{ctx.source_index}}, where source_index cannot contain wildcards.\nString\nYes schedule Schedule of the index rollup job which can be an interval or a cron expression.\nObject\nYes schedule.interval Specify the frequency of execution of the rollup job.\nObject\nNo schedule.interval.start_time Start time of the interval.\nTimestamp\nYes schedule.interval.period Define the interval period.\nString\nYes schedule.interval.unit Specify the time unit of the interval.\nString\nYes schedule.interval.cron Optionally, specify a cron expression to define therollup frequency.\nList\nNo schedule.interval.cron.expression Specify a Unix cron expression.\nString\nYes schedule.interval.cron.timezone Specify timezones as defined by the IANA Time Zone Database. Defaults to UTC.\nString\nNo description Optionally, describe the rollup job.\nString\nNo enabled When true, the index rollup job is scheduled. Default is true.\nBoolean\nYes continuous Specify whether or not the index rollup job continuously rolls up data forever or just executes over the current data set once and stops. Default is false.\nBoolean\nYes error_notification Set up a Mustache message template for error notifications. For example, if an index rollup job fails, the system sends a message to a Slack channel.\nObject\nNo page_size Specify the number of buckets to paginate at a time during rollup.\nNumber\nYes delay The number of milliseconds to delay execution of the index rollup job.\nLong\nNo dimensions Specify aggregations to create dimensions for the roll up time window. Supported groups are terms, histogram, and date_histogram. For more information, see Bucket Aggregations.\nArray\nYes metrics Specify a list of objects that represent the fields and metrics that you want to calculate. Supported metrics are sum, max, min, value_count and avg. For more information, see Metric Aggregations.\nArray\nNo Example response { \"_id\": \"&lt;rollup_id&gt;\", \"_version\": 3, \"_seq_no\": 1, \"_primary_term\": 1, \"rollup\": { \"rollup_id\": \"&lt;rollup_id&gt;\", \"enabled\": true, \"schedule\": { \"interval\": { \"start_time\": 1680159934649, \"period\": 1, \"unit\": \"Days\", \"schedule_delay\": 0 } }, \"last_updated_time\": 1680159934649, \"enabled_time\": 1680159934649, \"description\": \"Example rollup job\", \"schema_version\": 17, \"source_index\": \"nyc-taxi-data\", \"target_index\": \"rollup-nyc-taxi-data\", \"metadata_id\": null, \"page_size\": 200, \"delay\": 0, \"continuous\": false, \"dimensions\": [ { \"date_histogram\": { \"fixed_interval\": \"1h\", \"source_field\": \"tpep_pickup_datetime\", \"target_field\": \"tpep_pickup_datetime\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"PULocationID\", \"target_field\": \"PULocationID\" } }], \"metrics\": [ { \"source_field\": \"passenger_count\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} }] }] } } Get an index rollup job\nIntroduced 1.0\nReturns all information about an index rollup job based on the rollup_id.\nRequest GET _plugins/_rollup/jobs/&lt;rollup_id&gt; Example response { \"_id\": \"my_rollup\", \"_seqNo\": 1, \"_primaryTerm\": 1, \"rollup\": {... } } Delete an index rollup job\nIntroduced 1.0\nDeletes an index rollup job based on the rollup_id.\nRequest DELETE _plugins/_rollup/jobs/&lt;rollup_id&gt; Example response 200 OK Start or stop an index rollup job\nIntroduced 1.0\nStart or stop an index rollup job.\nRequest POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_start POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_stop Example response 200 OK Explain an index rollup job\nIntroduced 1.0\nReturns detailed metadata information about the index rollup job and its current progress.\nRequest GET _plugins/_rollup/jobs/&lt;rollup_id&gt;/_explain Example response { \"example_rollup\": { \"rollup_id\": \"example_rollup\", \"last_updated_time\": 1602014281, \"continuous\": { \"next_window_start_time\": 1602055591, \"next_window_end_time\": 1602075591 }, \"status\": \"running\", \"failure_reason\": null, \"stats\": { \"pages_processed\": 342, \"documents_processed\": 489359, \"rollups_indexed\": 3420, \"index_time_in_ms\": 30495, \"search_time_in_ms\": 584922 } } }",
    "ancestors": [
      "Managing Indexes",
      "Index rollups"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-rollups/settings/",
    "title": "Settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases.\nAll settings are available using the OpenSearch _cluster/settings operation. None require a restart, and all can be marked persistent or transient. Setting Default Description plugins.rollup.search.backoff_millis 1000 milliseconds\nThe backoff time between retries for failed rollup jobs. plugins.rollup.search.backoff_count 5\nHow many retries the plugin should attempt for failed rollup jobs. plugins.rollup.search.search_all_jobs false\nWhether OpenSearch should return all jobs that match all specified search terms. If disabled, OpenSearch returns just one, as opposed to all, of the jobs that matches the search terms. plugins.rollup.dashboards.enabled true\nWhether rollups are enabled in OpenSearch Dashboards. plugins.rollup.enabled true\nWhether the rollup plugin is enabled. plugins.ingest.backoff_millis 1000 milliseconds\nThe backoff time between data ingestions for rollup jobs. plugins.ingest.backoff_count 5\nHow many retries the plugin should attempt for failed ingestions.",
    "ancestors": [
      "Managing Indexes",
      "Index rollups"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-templates/",
    "title": "Index templates",
    "content": "Index templates let you initialize new indexes with predefined mappings and settings. For example, if you continuously index log data, you can define an index template so that all of these indexes have the same number of shards and replicas.\nCreate a template\nTo create an index template, use a PUT or POST request: PUT _index_template/&lt;template name&gt; POST _index_template/&lt;template name&gt; This command creates a template named daily_logs and applies it to any new index whose name matches the pattern logs-2020-01-* and also adds it to the my_logs alias: PUT _index_template/daily_logs { \"index_patterns\": [ \"logs-2020-01-*\"], \"template\": { \"aliases\": { \"my_logs\": {} }, \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 1 }, \"mappings\": { \"properties\": { \"timestamp\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" }, \"value\": { \"type\": \"double\" } } } } } You should see the following response: { \"acknowledged\": true } If you create an index named logs-2020-01-01, you can see that it has the mappings and settings from the template: PUT logs -2020-01-01 GET logs -2020-01-01 { \"logs-2020-01-01\": { \"aliases\": { \"my_logs\": {} }, \"mappings\": { \"properties\": { \"timestamp\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" }, \"value\": { \"type\": \"double\" } } }, \"settings\": { \"index\": { \"creation_date\": \"1578107970779\", \"number_of_shards\": \"2\", \"number_of_replicas\": \"1\", \"uuid\": \"U1vMDMOHSAuS2IzPcPHpOA\", \"version\": { \"created\": \"7010199\" }, \"provided_name\": \"logs-2020-01-01\" } } } } Any additional indexes that match this pattern— logs-2020-01-02, logs-2020-01-03, and so on—will inherit the same mappings and settings.\nIndex patterns cannot contain any of the following characters::, \", +, /, \\, |,?, #, &gt;, and &lt;.\nRetrieve a template\nTo list all index templates: GET _cat/templates GET /_index_template To find a template by its name: GET _index_template/daily_logs To get a list of all templates that match a pattern: GET _index_template/daily* To check if a specific template exists: HEAD _index_template/&lt;name&gt; Configure multiple templates\nYou can create multiple index templates for your indexes. If the index name matches more than one template, OpenSearch takes the mappings and settings from the template with the highest priority and applies it to the index.\nFor example, say you have the following two templates that both match the logs-2020-01-02 index and there’s a conflict in the number_of_shards field:\nTemplate 1 PUT _index_template/template -01 { \"index_patterns\": [ \"logs*\"], \"priority\": 0, \"template\": { \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 2 } } } Template 2 PUT _index_template/template -02 { \"index_patterns\": [ \"logs-2020-01-*\"], \"priority\": 1, \"template\": { \"settings\": { \"number_of_shards\": 3 } } } Because template-02 has a higher priority value, it takes precedence over template-01. The logs-2020-01-02 index would have the number_of_shards value as 3 and the number_of_replicas as the default value 1.\nDelete a template\nYou can delete an index template using its name: DELETE _index_template/daily_logs Composable index templates\nManaging multiple index templates has the following challenges:\nIf you have duplication between index templates, storing these index templates results in a bigger cluster state.\nIf you want to make a change across all your index templates, you have to manually make the change for each template.\nYou can use composable index templates to overcome these challenges. Composable index templates let you abstract common settings, mappings, and aliases into a reusable building block called a component template.\nYou can combine component templates to compose an index template.\nSettings and mappings that you specify directly in the create index request override any settings or mappings specified in an index template and its component templates.\nCreate a component template\nLet’s define two component templates⁠— component_template_1 and component_template_2:\nComponent template 1 PUT _component_template/component_template_ 1 { \"template\": { \"mappings\": { \"properties\": { \"@timestamp\": { \"type\": \"date\" } } } } } Component template 2 PUT _component_template/component_template_ 2 { \"template\": { \"mappings\": { \"properties\": { \"ip_address\": { \"type\": \"ip\" } } } } } Use component templates to create an index template\nWhen creating index templates, you need to include the component templates in a composed_of list.\nOpenSearch applies the component templates in the order in which you specify them within the index template. The settings, mappings, and aliases that you specify inside the index template are applied last. PUT _index_template/daily_logs { \"index_patterns\": [ \"logs-2020-01-*\"], \"template\": { \"aliases\": { \"my_logs\": {} }, \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\": 1 }, \"mappings\": { \"properties\": { \"timestamp\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" }, \"value\": { \"type\": \"double\" } } } }, \"priority\": 200, \"composed_of\": [ \"component_template_1\", \"component_template_2\"], \"version\": 3, \"_meta\": { \"description\": \"using component templates\" } } If you create an index named logs-2020-01-01, you can see that it derives its mappings and settings from both the component templates: PUT logs -2020-01-01 GET logs -2020-01-01 Example response { \"logs-2020-01-01\": { \"aliases\": { \"my_logs\": {} }, \"mappings\": { \"properties\": { \"@timestamp\": { \"type\": \"date\" }, \"ip_address\": { \"type\": \"ip\" }, \"timestamp\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" }, \"value\": { \"type\": \"double\" } } }, \"settings\": { \"index\": { \"creation_date\": \"1625382479459\", \"number_of_shards\": \"2\", \"number_of_replicas\": \"1\", \"uuid\": \"rYUlpOXDSUSuZifQLPfa5A\", \"version\": { \"created\": \"7100299\" }, \"provided_name\": \"logs-2020-01-01\" } } } } Index template options\nYou can specify the following template options: Option Type Description Required template Object Specify index settings, mappings, and aliases.\nNo priority Integer The priority of the index template.\nNo composed_of String array The names of component templates applied on a new index together with the current template.\nNo version Integer Specify a version number to simplify template management. Default is null.\nNo _meta Object Specify meta information about the template.\nNo",
    "ancestors": [
      "Managing Indexes"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-transforms/index/",
    "title": "Index transforms",
    "content": "Whereas index rollup jobs let you reduce data granularity by rolling up old data into condensed indexes, transform jobs let you create a different, summarized view of your data centered around certain fields, so you can visualize or analyze the data in different ways.\nFor example, suppose that you have airline data that’s scattered across multiple fields and categories, and you want to view a summary of the data that’s organized by airline, quarter, and then price. You can use a transform job to create a new, summarized index that’s organized by those specific categories.\nYou can use transform jobs in two ways:\nUse the OpenSearch Dashboards UI to specify the index you want to transform and any optional data filters you want to use to filter the original index. Then select the fields you want to transform and the aggregations to use in the transformation. Finally, define a schedule for your job to follow.\nUse the transforms API to specify all the details about your job: the index you want to transform, target groups you want the transformed index to have, any aggregations you want to use to group columns, and a schedule for your job to follow.\nOpenSearch Dashboards provides a detailed summary of the jobs you created and their relevant information, such as associated indexes and job statuses. You can review and edit your job’s details and selections before creation, and even preview a transformed index’s data as you’re choosing which fields to transform. However, you can also use the REST API to create transform jobs and preview transform job results, but you must know all of the necessary settings and parameters to submit them as part of the HTTP request body. Submitting your transform job configurations as JSON scripts offers you more portability, allowing you to share and replicate your transform jobs, which is harder to do using OpenSearch Dashboards.\nYour use cases will help you decide which method to use to create transform jobs.\nCreate a transform job\nIf you don’t have any data in your cluster, you can use the sample flight data within OpenSearch Dashboards to try out transform jobs. Otherwise, after launching OpenSearch Dashboards, choose Index Management. Select Transform Jobs, and choose Create Transform Job.\nStep 1: Choose indexes\nIn the Job name and description section, specify a name and an optional description for your job.\nIn the Indices section, select the source and target index. You can either select an existing target index or create a new one by entering a name for your new index. If you want to transform just a subset of your source index, choose Edit data filter, and use the OpenSearch query DSL to specify a subset of your source index. For more information about the OpenSearch query DSL, see query DSL.\nChoose Next.\nStep 2: Select fields to transform\nAfter specifying the indexes, you can select the fields you want to use in your transform job, as well as whether to use groupings or aggregations.\nYou can use groupings to place your data into separate buckets in your transformed index. For example, if you want to group all of the airport destinations within the sample flight data, you can group the DestAirportID field into a target field of DestAirportID_terms field, and you can find the grouped airport IDs in your transformed index after the transform job finishes.\nOn the other hand, aggregations let you perform simple calculations. For example, you can include an aggregation in your transform job to define a new field of sum_of_total_ticket_price that calculates the sum of all airplane tickets, and then analyze the newly summer data within your transformed index.\nIn the data table, select the fields you want to transform and expand the drop-down menu within the column header to choose the grouping or aggregation you want to use.\nCurrently, transform jobs support histogram, date_histogram, and terms groupings. For more information about groupings, see Bucket Aggregations. In terms of aggregations, you can select from sum, avg, max, min, value_count, percentiles, and scripted_metric. For more information about aggregations, see Metric Aggregations.\nRepeat step 1 for any other fields that you want to transform.\nAfter selecting the fields that you want to transform and verifying the transformation, choose Next.\nStep 3: Specify a schedule\nYou can configure transform jobs to run once or multiple times on a schedule. Transform jobs are enabled by default.\nChoose whether the job should be continuous. Continuous jobs execute at each transform execution interval and incrementally transform newly modified buckets, which can include new data added to the source indexes. Non-continuous jobs execute only once.\nFor transformation execution interval, specify a transform interval in minutes, hours, or days. This interval dicatates how often continuous jobs should execute, and non-continuous jobs execute once after the interval elapses.\nUnder Advanced, specify an optional amount for Pages per execution. A larger number means more data is processed in each search request, but also uses more memory and causes higher latency. Exceeding allowed memory limits can cause exceptions and errors to occur.\nChoose Next.\nStep 4: Review and confirm details\nAfter confirming your transform job’s details are correct, choose Create Transform Job. If you want to edit any part of the job, choose Edit of the section you want to change, and make the necessary changes. You can’t change aggregations or groupings after creating a job.\nStep 5: Search through the transformed index.\nOnce the transform job finishes, you can use the _search API operation to search the target index. GET &lt;target_index&gt;/_search For example, after running a transform job that transforms the flight data based on a DestAirportID field, you can run the following request that returns all of the fields that have a value of SFO. Sample Request GET finished_flight_job/_search { \"query\": { \"match\": { \"DestAirportID_terms\": \"SFO\" } } } Sample Response { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4, \"relation\": \"eq\" }, \"max_score\": 3.845883, \"hits\": [ { \"_index\": \"finished_flight_job\", \"_id\": \"dSNKGb8U3OJOmC4RqVCi1Q\", \"_score\": 3.845883, \"_source\": { \"transform._id\": \"sample_flight_job\", \"transform._doc_count\": 14, \"Carrier_terms\": \"Dashboards Airlines\", \"DestAirportID_terms\": \"SFO\" } }, { \"_index\": \"finished_flight_job\", \"_id\": \"_D7oqOy7drx9E-MG96U5RA\", \"_score\": 3.845883, \"_source\": { \"transform._id\": \"sample_flight_job\", \"transform._doc_count\": 14, \"Carrier_terms\": \"Logstash Airways\", \"DestAirportID_terms\": \"SFO\" } }, { \"_index\": \"finished_flight_job\", \"_id\": \"YuZ8tOt1OsBA54e84WuAEw\", \"_score\": 3.6988301, \"_source\": { \"transform._id\": \"sample_flight_job\", \"transform._doc_count\": 11, \"Carrier_terms\": \"ES-Air\", \"DestAirportID_terms\": \"SFO\" } }, { \"_index\": \"finished_flight_job\", \"_id\": \"W_-e7bVmH6eu8veJeK8ZxQ\", \"_score\": 3.6988301, \"_source\": { \"transform._id\": \"sample_flight_job\", \"transform._doc_count\": 10, \"Carrier_terms\": \"JetBeats\", \"DestAirportID_terms\": \"SFO\" } }] } }",
    "ancestors": [
      "Managing Indexes"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index-transforms/transforms-apis/",
    "title": "Transforms APIs",
    "content": "Aside from using OpenSearch Dashboards, you can also use the REST API to create, start, stop, and complete other operations relative to transform jobs.\nTable of contents Create a transform job Request format Path parameters Request body fields Update a transform job Request format Query parameters Request body fields Get a transform job’s details Request format Query parameters Start a transform job Request format Stop a transform job Request format Get the status of a transform job Request format Preview a transform job’s results Delete a transform job Request format Create a transform job\nIntroduced 1.0\nCreates a transform job.\nRequest format PUT _plugins/_transform/&lt;transform_id&gt; Path parameters Parameter Data Type Description transform_id\nString\nTransform ID Request body fields\nYou can specify the following options in the HTTP request body: Option Data Type Description Required enabled\nBoolean\nIf true, the transform job is enabled at creation.\nNo\ncontinuous\nBoolean\nSpecifies whether the transform job should be continuous. Continuous jobs execute every time they are scheduled according to the schedule field and run based off of newly transformed buckets as well as any new data added to source indexes. Non-continuous jobs execute only once. Default is false.\nNo\nschedule\nObject\nThe schedule for the transform job.\nYes\nstart_time\nInteger\nThe Unix epoch time of the transform job’s start time.\nYes\ndescription\nString\nDescribes the transform job.\nNo\nmetadata_id\nString\nAny metadata to be associated with the transform job.\nNo\nsource_index\nString\nThe source index containing the data to be transformed.\nYes\ntarget_index\nString\nThe target index the newly transformed data is added to. You can create a new index or update an existing one.\nYes\ndata_selection_query\nObject\nThe query DSL to use to filter a subset of the source index for the transform job. See query domain-specific language(DSL) for more information.\nYes\npage_size\nInteger\nThe number of buckets IM processes and indexes concurrently. A higher number results in better performance, but it requires more memory. If your machine runs out of memory, Index Management (IM) automatically adjusts this field and retries until the operation succeeds.\nYes\ngroups\nArray\nSpecifies the grouping(s) to use in the transform job. Supported groups are terms, histogram, and date_histogram. For more information, see Bucket Aggregations.\nYes if not using aggregations.\nsource_field\nString\nThe field(s) to transform.\nYes\naggregations\nObject\nThe aggregations to use in the transform job. Supported aggregations are sum, max, min, value_count, avg, scripted_metric, and percentiles. For more information, see Metric Aggregations.\nYes if not using groups. Sample Request\nThe following request creates a transform job with the id sample: PUT _plugins/_transform/sample { \"transform\": { \"enabled\": true, \"continuous\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } Sample Response { \"_id\": \"sample\", \"_version\": 7, \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } Update a transform job\nIntroduced 1.0\nUpdates the transform job if transform_id already exists. For this request you must specify the sequence number and primary term of the transform to be updated. To get these, use the Get a transform job’s details API call.\nRequest format PUT _plugins/_transform/&lt;transform_id&gt;?if_seq_no=&lt;seq_no&gt;&amp;if_primary_term=&lt;primary_term&gt; Query parameters\nThe update operation supports the following query parameters: Parameter Description Required seq_no Only perform the transform operation if the last operation that changed the transform job has the specified sequence number.\nYes primary_term Only perform the transform operation if the last operation that changed the transform job has the specified sequence term.\nYes Request body fields\nYou can update the following fields. Option Data Type Description schedule\nObject\nThe schedule for the transform job. Contains the fields interval.start_time, interval.period, and interval.unit.\nstart_time\nInteger\nThe Unix epoch start time of the transform job.\nperiod\nInteger\nHow often to execute the transform job.\nunit\nString\nThe unit of time associated with the execution period. Available options are Minutes, Hours, and Days.\ndescription\nInteger\nDescribes the transform job.\npage_size\nInteger\nThe number of buckets IM processes and indexes concurrently. A higher number results in better performance, but it requires more memory. If your machine runs out of memory, IM automatically adjusts this field and retries until the operation succeeds. Sample Request\nThe following request updates a transform job with the id sample, sequence number 13, and primary term 1: PUT _plugins/_transform/sample?if_seq_no= 13 &amp;if_primary_term= 1 { \"transform\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } Sample Response PUT _plugins/_transform/sample?if_seq_no= 13 &amp;if_primary_term= 1 { \"transform\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } Get a transform job’s details\nIntroduced 1.0\nReturns a transform job’s details.\nRequest format GET _plugins/_transform/&lt;transform_id&gt; Sample Request\nThe following request returns the details of the transform job with the id sample: GET _plugins/_transform/sample Sample Response { \"_id\": \"sample\", \"_version\": 7, \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } You can also get details of all transform jobs by omitting transform_id.\nSample Request\nThe following request returns the details of all transform jobs: GET _plugins/_transform/ Sample Response { \"total_transforms\": 1, \"transforms\": [ { \"_id\": \"sample\", \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } }] } Query parameters\nYou can specify the following GET API operation’s query parameters to filter the results. Parameter Description Required from\nThe starting transform to return. Default is 0.\nNo\nsize\nSpecifies the number of transforms to return. Default is 10.\nNo\nsearch\nThe search term to use to filter results.\nNo\nsortField\nThe field to sort results with.\nNo\nsortDirection\nSpecifies the direction to sort results in. Can be ASC or DESC. Default is ASC.\nNo Sample Request\nThe following request returns two results starting from transform 8: GET _plugins/_transform?size= 2 &amp;from= 8 Sample Response { \"total_transforms\": 18, \"transforms\": [ { \"_id\": \"sample8\", \"_seq_no\": 93, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample8\", \"schema_version\": 7, \"schedule\": { \"interval\": { \"start_time\": 1622063596812, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": \"y4hFAB2ZURQ2dzY7BAMxWA\", \"updated_at\": 1622063657233, \"enabled\": false, \"enabled_at\": null, \"description\": \"Sample transform job\", \"source_index\": \"sample_index3\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target3\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } }, { \"_id\": \"sample9\", \"_seq_no\": 98, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample9\", \"schema_version\": 7, \"schedule\": { \"interval\": { \"start_time\": 1622063598065, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": \"x8tCIiYMTE3veSbIJkit5A\", \"updated_at\": 1622063658388, \"enabled\": false, \"enabled_at\": null, \"description\": \"Sample transform job\", \"source_index\": \"sample_index4\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target4\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } }] } Start a transform job\nIntroduced 1.0\nTransform jobs created using the API are automatically enabled, but if you ever need to enable a job, you can use the start API operation.\nRequest format POST _plugins/_transform/&lt;transform_id&gt;/_start Sample Request\nThe following request starts the transform job with the ID sample: POST _plugins/_transform/sample/_start Sample Response { \"acknowledged\": true } Stop a transform job\nIntroduced 1.0\nStops a transform job.\nRequest format POST _plugins/_transform/&lt;transform_id&gt;/_stop Sample Request\nThe following request stops the transform job with the ID sample: POST _plugins/_transform/sample/_stop Sample Response { \"acknowledged\": true } Get the status of a transform job\nIntroduced 1.0\nReturns the status and metadata of a transform job.\nRequest format GET _plugins/_transform/&lt;transform_id&gt;/_explain Sample Request\nThe following request returns the details of the transform job with the ID sample: GET _plugins/_transform/sample/_explain Sample Response { \"sample\": { \"metadata_id\": \"PzmjweME5xbgkenl9UpsYw\", \"transform_metadata\": { \"continuous_stats\": { \"last_timestamp\": 1621883525672, \"documents_behind\": { \"sample_index\": 72 } }, \"transform_id\": \"sample\", \"last_updated_at\": 1621883525873, \"status\": \"finished\", \"failure_reason\": \"null\", \"stats\": { \"pages_processed\": 0, \"documents_processed\": 0, \"documents_indexed\": 0, \"index_time_in_millis\": 0, \"search_time_in_millis\": 0 } } } } Preview a transform job’s results\nIntroduced 1.0\nReturns a preview of what a transformed index would look like.\nSample Request POST _plugins/_transform/_preview { \"transform\": { \"enabled\": false, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"test transform\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 10, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } }], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } Sample Response { \"documents\": [ { \"quantity\": 862.0, \"gender\": \"FEMALE\", \"day\": \"Friday\" }, { \"quantity\": 682.0, \"gender\": \"FEMALE\", \"day\": \"Monday\" }, { \"quantity\": 772.0, \"gender\": \"FEMALE\", \"day\": \"Saturday\" }, { \"quantity\": 669.0, \"gender\": \"FEMALE\", \"day\": \"Sunday\" }, { \"quantity\": 887.0, \"gender\": \"FEMALE\", \"day\": \"Thursday\" }] } Delete a transform job\nIntroduced 1.0\nDeletes a transform job. This operation does not delete the source or target indexes.\nRequest format DELETE _plugins/_transform/&lt;transform_id&gt; Sample Request\nThe following request deletes the transform job with the ID sample: DELETE _plugins/_transform/sample Sample Response { \"took\": 205, \"errors\": false, \"items\": [ { \"delete\": { \"_index\": \".opensearch-ism-config\", \"_id\": \"sample\", \"_version\": 4, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 6, \"_primary_term\": 1, \"status\": 200 } }] }",
    "ancestors": [
      "Managing Indexes",
      "Index transforms"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/index/",
    "title": "Managing indexes",
    "content": "OpenSearch Dashboards\nYou index data using the OpenSearch REST API. Two APIs exist: the index API and the _bulk API.\nFor situations in which new data arrives incrementally (for example, customer orders from a small business), you might use the index API to add documents individually as they arrive. For situations in which the flow of data is less frequent (for example, weekly updates to a marketing website), you might prefer to generate a file and send it to the _bulk API. For large numbers of documents, lumping requests together and using the _bulk API offers superior performance. If your documents are enormous, however, you might need to index them individually.\nIntroduction to indexing\nBefore you can search data, you must index it. Indexing is the method by which search engines organize data for fast retrieval. The resulting structure is called, fittingly, an index.\nIn OpenSearch, the basic unit of data is a JSON document. Within an index, OpenSearch identifies each document using a unique ID.\nA request to the index API looks like this: PUT &lt;index&gt;/_doc/&lt;id&gt; { \"A JSON\": \"document\" } A request to the _bulk API looks a little different, because you specify the index and ID in the bulk data: POST _bulk { \"index\": { \"_index\": \"&lt;index&gt;\", \"_id\": \"&lt;id&gt;\" } } { \"A JSON\": \"document\" } Bulk data must conform to a specific format, which requires a newline character ( \\n) at the end of every line, including the last line. This is the basic format: Action and metadata\\n\nOptional document\\n\nAction and metadata\\n\nOptional document\\n The document is optional, because delete actions don’t require a document. The other actions ( index, create, and update) all require a document. If you specifically want the action to fail if the document already exists, use the create action instead of the index action.\nTo index bulk data using the curl command, navigate to the folder where you have your file saved and run the following command: curl -H \"Content-Type: application/x-ndjson\" -POST https://localhost: 9200 /data/_bulk -u 'admin:admin' --insecure --data-binary \"@data.json\" If any one of the actions in the _bulk API fail, OpenSearch continues to execute the other actions. Examine the items array in the response to figure out what went wrong. The entries in the items array are in the same order as the actions specified in the request.\nOpenSearch automatically creates an index when you add a document to an index that doesn’t already exist. It also automatically generates an ID if you don’t specify an ID in the request. This simple example automatically creates the movies index, indexes the document, and assigns it a unique ID: POST movies/_doc { \"title\": \"Spirited Away\" } Automatic ID generation has a clear downside: because the indexing request didn’t specify a document ID, you can’t easily update the document at a later time. Also, if you run this request 10 times, OpenSearch indexes this document as 10 different documents with unique IDs. To specify an ID of 1, use the following request (note the use of PUT instead of POST): PUT movies/_doc/ 1 { \"title\": \"Spirited Away\" } Because you must specify an ID, if you run this command 10 times, you still have just one document indexed with the _version field incremented to 10.\nIndexes default to one primary shard and one replica. If you want to specify non-default settings, create the index before adding documents: PUT more-movies { \"settings\": { \"number_of_shards\": 6, \"number_of_replicas\": 2 } } Naming restrictions for indexes\nOpenSearch indexes have the following naming restrictions:\nAll letters must be lowercase.\nIndex names can’t begin with underscores ( _) or hyphens ( -).\nIndex names can’t contain spaces, commas, or the following characters::, \", *, +, /, \\, |,?, #, &gt;, or &lt; Read data\nAfter you index a document, you can retrieve it by sending a GET request to the same endpoint that you used for indexing: GET movies/_doc/ 1 { \"_index\": \"movies\", \"_type\": \"_doc\", \"_id\": \"1\", \"_version\": 1, \"_seq_no\": 0, \"_primary_term\": 1, \"found\": true, \"_source\": { \"title\": \"Spirited Away\" } } You can see the document in the _source object. If the document is not found, the found key is false and the _source object is not part of the response.\nTo retrieve multiple documents with a single command, use the _mget operation.\nThe format for retrieving multiple documents is similar to the _bulk operation, where you must specify the index and ID in the request body: GET _mget { \"docs\": [ { \"_index\": \"&lt;index&gt;\", \"_id\": \"&lt;id&gt;\" }, { \"_index\": \"&lt;index&gt;\", \"_id\": \"&lt;id&gt;\" }] } To only return specific fields in a document: GET _mget { \"docs\": [ { \"_index\": \"&lt;index&gt;\", \"_id\": \"&lt;id&gt;\", \"_source\": \"field1\" }, { \"_index\": \"&lt;index&gt;\", \"_id\": \"&lt;id&gt;\", \"_source\": \"field2\" }] } To check if a document exists: HEAD movies/_doc/&lt;doc-id&gt; If the document exists, you get back a 200 OK response, and if it doesn’t, you get back a 404 - Not Found error.\nUpdate data\nTo update existing fields or to add new fields, send a POST request to the _update operation with your changes in a doc object: POST movies/_update/ 1 { \"doc\": { \"title\": \"Castle in the Sky\", \"genre\": [ \"Animation\", \"Fantasy\"] } } Note the updated title field and new genre field: GET movies/_doc/ 1 { \"_index\": \"movies\", \"_type\": \"_doc\", \"_id\": \"1\", \"_version\": 2, \"_seq_no\": 1, \"_primary_term\": 1, \"found\": true, \"_source\": { \"title\": \"Castle in the Sky\", \"genre\": [ \"Animation\", \"Fantasy\"] } } The document also has an incremented _version field. Use this field to keep track of how many times a document is updated.\nPOST requests make partial updates to documents. To altogether replace a document, use a PUT request: PUT movies/_doc/ 1 { \"title\": \"Spirited Away\" } The document with ID of 1 will contain only the title field, because the entire document will be replaced with the document indexed in this PUT request.\nUse the upsert object to conditionally update documents based on whether they already exist. Here, if the document exists, its title field changes to Castle in the Sky. If it doesn’t, OpenSearch indexes the document in the upsert object. POST movies/_update/ 2 { \"doc\": { \"title\": \"Castle in the Sky\" }, \"upsert\": { \"title\": \"Only Yesterday\", \"genre\": [ \"Animation\", \"Fantasy\"], \"date\": 1993 } } Example response { \"_index\": \"movies\", \"_type\": \"_doc\", \"_id\": \"2\", \"_version\": 2, \"result\": \"updated\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 3, \"_primary_term\": 1 } Each update operation for a document has a unique combination of the _seq_no and _primary_term values.\nOpenSearch first writes your updates to the primary shard and then sends this change to all the replica shards. An uncommon issue can occur if multiple users of your OpenSearch-based application make updates to existing documents in the same index. In this situation, another user can read and update a document from a replica before it receives your update from the primary shard. Your update operation then ends up updating an older version of the document. In the best case, you and the other user make the same changes, and the document remains accurate. In the worst case, the document now contains out-of-date information.\nTo prevent this situation, use the _seq_no and _primary_term values in the request header: POST movies/_update/ 2?if_seq_no= 3 &amp;if_primary_term= 1 { \"doc\": { \"title\": \"Castle in the Sky\", \"genre\": [ \"Animation\", \"Fantasy\"] } } If the document is updated after we retrieved it, the _seq_no and _primary_term values are different and our update operation fails with a 409 — Conflict error.\nWhen using the _bulk API, specify the _seq_no and _primary_term values within the action metadata.\nDelete data\nTo delete a document from an index, use a DELETE request: DELETE movies/_doc/ 1 The DELETE operation increments the _version field. If you add the document back to the same ID, the _version field increments again. This behavior occurs because OpenSearch deletes the document _source, but retains its metadata.\nNext steps\nThe Index Management (IM) plugin lets you automate recurring index management activities and reduce storage costs. For more information, see Index State Management.\nFor instructions on how to reindex data, see Reindex data.",
    "ancestors": [
      "Managing Indexes"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/api/",
    "title": "ISM API",
    "content": "Use the index state management operations to programmatically work with policies and managed indexes.\nTable of contents Create policy Add policy Update policy Get policy Remove policy from index Update managed index policy Retry failed index Explain index Delete policy Error prevention validation Create policy\nIntroduced 1.0\nCreates a policy.\nExample request PUT _plugins/_ism/policies/policy_ 1 { \"policy\": { \"description\": \"ingesting logs\", \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } }], \"transitions\": [ { \"state_name\": \"search\" }] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} }], \"transitions\": [] }] } } Example response { \"_id\": \"policy_1\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 7, \"policy\": { \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990761311, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } }], \"transitions\": [ { \"state_name\": \"search\" }] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} }], \"transitions\": [] }] } } } Add policy\nIntroduced 1.0\nAdds a policy to an index. This operation does not change the policy if the index already has one.\nExample request POST _plugins/_ism/add/index_ 1 { \"policy_id\": \"policy_1\" } Example response { \"updated_indices\": 1, \"failures\": false, \"failed_indices\": [] } If you use a wildcard * while adding a policy to an index, the ISM plugin interprets * as all indexes, including system indexes like.opendistro-security, which stores users, roles, and tenants. A delete action in your policy might accidentally delete all user roles and tenants in your cluster.\nDon’t use the broad * wildcard, and instead add a prefix, such as my-logs*, when specifying indexes with the _ism/add API.\nUpdate policy\nIntroduced 1.0\nUpdates a policy. Use the seq_no and primary_term parameters to update an existing policy. If these numbers don’t match the existing policy or the policy doesn’t exist, ISM throws an error.\nIt’s possible that the policy currently applied to your index isn’t the most up-to-date policy available. To see what policy is currently applied to your index, see Explain index. To get the most up-to-date version of a policy, see Get policy.\nExample request PUT _plugins/_ism/policies/policy_ 1?if_seq_no= 7 &amp;if_primary_term= 1 { \"policy\": { \"description\": \"ingesting logs\", \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } }], \"transitions\": [ { \"state_name\": \"search\" }] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} }], \"transitions\": [] }] } } Example response { \"_id\": \"policy_1\", \"_version\": 2, \"_primary_term\": 1, \"_seq_no\": 10, \"policy\": { \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990934044, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } }], \"transitions\": [ { \"state_name\": \"search\" }] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} }], \"transitions\": [] }] } } } Get policy\nIntroduced 1.0\nGets the policy by policy_id.\nExample request GET _plugins/_ism/policies/policy_ 1 Example response { \"_id\": \"policy_1\", \"_version\": 2, \"_seq_no\": 10, \"_primary_term\": 1, \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990934044, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } }], \"transitions\": [ { \"state_name\": \"search\" }] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} }], \"transitions\": [] }] } } Remove policy from index\nIntroduced 1.0\nRemoves any ISM policy from the index.\nExample request POST _plugins/_ism/remove/index_ 1 Example response { \"updated_indices\": 1, \"failures\": false, \"failed_indices\": [] } Update managed index policy\nIntroduced 1.0\nUpdates the managed index policy to a new policy (or to a new version of the policy). You can use an index pattern to update multiple indexes at once. When updating multiple indexes, you might want to include a state filter to only affect certain managed indexes. The change policy filters out all the existing managed indexes and only applies the change to the ones in the state that you specify. You can also explicitly specify the state that the managed index transitions to after the change policy takes effect.\nA policy change is an asynchronous background process. The changes are queued and are not executed immediately by the background process. This delay in execution protects the currently running managed indexes from being put into a broken state. If the policy you are changing to has only some small configuration changes, then the change takes place immediately. For example, if the policy changes the min_index_age parameter in a rollover condition from 1000d to 100d, this change takes place immediately in its next execution. If the change modifies the state, actions, or the order of actions of the current state the index is in, then the change happens at the end of its current state before transitioning to a new state.\nIn this example, the policy applied on the index_1 index is changed to policy_1, which could either be a completely new policy or an updated version of its existing policy. The process only applies the change if the index is currently in the searches state. After this change in policy takes place, index_1 transitions to the delete state.\nExample request POST _plugins/_ism/change_policy/index_ 1 { \"policy_id\": \"policy_1\", \"state\": \"delete\", \"include\": [ { \"state\": \"searches\" }] } Example response { \"updated_indices\": 0, \"failures\": false, \"failed_indices\": [] } Retry failed index\nIntroduced 1.0\nRetries the failed action for an index. For the retry call to succeed, ISM must manage the index, and the index must be in a failed state. You can use index patterns ( *) to retry multiple failed indexes.\nExample request POST _plugins/_ism/retry/index_ 1 { \"state\": \"delete\" } Example response { \"updated_indices\": 0, \"failures\": false, \"failed_indices\": [] } Explain index\nIntroduced 1.0\nGets the current state of the index. You can use index patterns to get the status of multiple indexes.\nExample request GET _plugins/_ism/explain/index_ 1 Example response { \"index_1\": { \"index.plugins.index_state_management.policy_id\": \"policy_1\" } } Optionally, you can add the show_policy parameter to your request’s path to get the policy that is currently applied to your index, which is useful for seeing whether the policy applied to your index is the latest one. To get the most up-to-date policy, see Get Policy API.\nExample request GET _plugins/_ism/explain/index_ 1?show_policy= true Example response { \"index_1\": { \"index.plugins.index_state_management.policy_id\": \"sample-policy\", \"index.opendistro.index_state_management.policy_id\": \"sample-policy\", \"index\": \"index_1\", \"index_uuid\": \"gCFlS_zcTdih8xyxf3jQ-A\", \"policy_id\": \"sample-policy\", \"enabled\": true, \"policy\": { \"policy_id\": \"sample-policy\", \"description\": \"ingesting logs\", \"last_updated_time\": 1647284980148, \"schema_version\": 13, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [...], \"ism_template\": null } }, \"total_managed_indices\": 1 } The plugins.index_state_management.policy_id setting is deprecated starting from ODFE version 1.13.0. We retain this field in the response API for consistency.\nDelete policy\nIntroduced 1.0\nDeletes the policy by policy_id.\nExample request DELETE _plugins/_ism/policies/policy_ 1 Example response { \"_index\": \".opendistro-ism-config\", \"_id\": \"policy_1\", \"_version\": 3, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 15, \"_primary_term\": 1 } Error prevention validation\nIntroduced 2.4\nISM allows you to run an action automatically. However, running an action can fail for a variety of reasons. You can use error prevention validation to test an action in order to rule out failures.\nTo enable error prevention validation, set the plugins.index_state_management.validation_service.enabled setting to true: PUT _cluster/settings { \"persistent\": { \"plugins.index_state_management.validation_action.enabled\": true } } Example response { \"acknowledged\": true, \"persistent\": { \"plugins\": { \"index_state_management\": { \"validation_action\": { \"enabled\": \"true\" } } } }, \"transient\": { } } To check an error prevention validation status and message, pass validate_action=true to the _plugins/_ism/explain endpoint: GET _plugins/_ism/explain/test-000001?validate_action = true Example response\nThe response contains an additional validate object with a validation message and status: { \"test-000001\": { \"index.plugins.index_state_management.policy_id\": \"test_rollover\", \"index.opendistro.index_state_management.policy_id\": \"test_rollover\", \"index\": \"test-000001\", \"index_uuid\": \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\": \"test_rollover\", \"policy_seq_no\": -2, \"policy_primary_term\": 0, \"rolled_over\": false, \"index_creation_date\": 1667410460649, \"state\": { \"name\": \"rollover\", \"start_time\": 1667410766045 }, \"action\": { \"name\": \"rollover\", \"start_time\": 1667411127803, \"index\": 0, \"failed\": false, \"consumed_retries\": 0, \"last_retry_time\": 0 }, \"step\": { \"name\": \"attempt_rollover\", \"start_time\": 1667411127803, \"step_status\": \"starting\" }, \"retry_info\": { \"failed\": true, \"consumed_retries\": 0 }, \"info\": { \"message\": \"Previous action was not able to update IndexMetaData.\" }, \"enabled\": false, \"validate\": { \"validation_message\": \"Missing rollover_alias index setting [index=test-000001]\", \"validation_status\": \"re_validating\" } }, \"total_managed_indices\": 1 } If you pass validate_action=false or do not pass a validate_action value to the _plugins/_ism/explain endpoint, the response will not contain an error prevention validation status and message: GET _plugins/_ism/explain/test-000001?validate_action = false Or: GET _plugins/_ism/explain/test-000001 Example response { \"test-000001\": { \"index.plugins.index_state_management.policy_id\": \"test_rollover\", \"index.opendistro.index_state_management.policy_id\": \"test_rollover\", \"index\": \"test-000001\", \"index_uuid\": \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\": \"test_rollover\", \"policy_seq_no\": -2, \"policy_primary_term\": 0, \"rolled_over\": false, \"index_creation_date\": 1667410460649, \"state\": { \"name\": \"rollover\", \"start_time\": 1667410766045 }, \"action\": { \"name\": \"rollover\", \"start_time\": 1667411127803, \"index\": 0, \"failed\": false, \"consumed_retries\": 0, \"last_retry_time\": 0 }, \"step\": { \"name\": \"attempt_rollover\", \"start_time\": 1667411127803, \"step_status\": \"starting\" }, \"retry_info\": { \"failed\": true, \"consumed_retries\": 0 }, \"info\": { \"message\": \"Previous action was not able to update IndexMetaData.\" }, \"enabled\": false }, \"total_managed_indices\": 1 }",
    "ancestors": [
      "Managing Indexes",
      "Index State Management"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/error-prevention/api/",
    "title": "ISM Error Prevention API",
    "content": "The ISM Error Prevention API allows you to enable Index State Management (ISM) error prevention and check the validation status and message.\nEnable error prevention validation\nYou can configure error prevention validation by setting the plugins.index_state_management.validation_service.enabled parameter.\nExample request PUT _cluster/settings { \"persistent\": { \"plugins.index_state_management.validation_action.enabled\": true } } Example response { \"acknowledged\": true, \"persistent\": { \"plugins\": { \"index_state_management\": { \"validation_action\": { \"enabled\": \"true\" } } } }, \"transient\": { } } Check validation status and message via the Explain API\nPass the validate_action=true path parameter in the Explain API URI to see the validation status and message.\nExample request GET _plugins/_ism/explain/test-000001?validate_action = true Example response { \"test-000001\": { \"index.plugins.index_state_management.policy_id\": \"test_rollover\", \"index.opendistro.index_state_management.policy_id\": \"test_rollover\", \"index\": \"test-000001\", \"index_uuid\": \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\": \"test_rollover\", \"policy_seq_no\": -2, \"policy_primary_term\": 0, \"rolled_over\": false, \"index_creation_date\": 1667410460649, \"state\": { \"name\": \"rollover\", \"start_time\": 1667410766045 }, \"action\": { \"name\": \"rollover\", \"start_time\": 1667411127803, \"index\": 0, \"failed\": false, \"consumed_retries\": 0, \"last_retry_time\": 0 }, \"step\": { \"name\": \"attempt_rollover\", \"start_time\": 1667411127803, \"step_status\": \"starting\" }, \"retry_info\": { \"failed\": true, \"consumed_retries\": 0 }, \"info\": { \"message\": \"Previous action was not able to update IndexMetaData.\" }, \"enabled\": false, \"validate\": { \"validation_message\": \"Missing rollover_alias index setting [index=test-000001]\", \"validation_status\": \"re_validating\" } }, \"total_managed_indices\": 1 } If you pass the parameter without a value or false, then it doesn’t return the validation status and message. Only if you pass validate_action=true will the response will return the validation status and message.\nExample request GET _plugins/_ism/explain/test-000001?validate_action = false --- OR --- GET _plugins/_ism/explain/test-000001 Example response { \"test-000001\": { \"index.plugins.index_state_management.policy_id\": \"test_rollover\", \"index.opendistro.index_state_management.policy_id\": \"test_rollover\", \"index\": \"test-000001\", \"index_uuid\": \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\": \"test_rollover\", \"policy_seq_no\": -2, \"policy_primary_term\": 0, \"rolled_over\": false, \"index_creation_date\": 1667410460649, \"state\": { \"name\": \"rollover\", \"start_time\": 1667410766045 }, \"action\": { \"name\": \"rollover\", \"start_time\": 1667411127803, \"index\": 0, \"failed\": false, \"consumed_retries\": 0, \"last_retry_time\": 0 }, \"step\": { \"name\": \"attempt_rollover\", \"start_time\": 1667411127803, \"step_status\": \"starting\" }, \"retry_info\": { \"failed\": true, \"consumed_retries\": 0 }, \"info\": { \"message\": \"Previous action was not able to update IndexMetaData.\" }, \"enabled\": false }, \"total_managed_indices\": 1 }",
    "ancestors": [
      "Managing Indexes",
      "Index State Management",
      "ISM Error Prevention"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/error-prevention/index/",
    "title": "ISM Error Prevention",
    "content": "Error prevention validates Index State Management (ISM) actions before they are performed in order to prevent actions from failing. It also outputs additional information from the action validation results in the response of the Index Explain API. Validation rules and troubleshooting of each action are listed in the following sections.\nTable of contents rollover delete force_merge replica_count open read_only read_write close index_priority snapshot transition rollover\nISM does not perform a rollover action for an index under any of these conditions: The index is not the write index. The index does not have an alias. The rollover policy does not contain a rollover_alias index setting. Skipping of a rollover action has occured. The index has already been rolled over using the alias successfully.\ndelete\nISM does not perform a delete action for an index under any of these conditions:\nThe index does not exist.\nThe index name is invalid.\nThe index is the write index for a data stream.\nforce_merge\nISM does not perform a force_merge action for an index if its dataset is too large and exceeds the threshold.\nreplica_count\nISM does not perform a replica_count action for an index under any of these conditions:\nThe amount of data exceeds the threshold.\nThe number of shards exceeds the maximum.\nopen\nISM does not perform an open action for an index under any of these conditions:\nThe index is blocked.\nThe number of shards exceeds the maximum.\nread_only\nISM does not perform a read_only action for an index under any of these conditions:\nThe index is blocked.\nThe amount of data exceeds the threshold.\nread_write\nISM does not perform a read_write action for an index if the index is blocked.\nclose\nISM does not perform a close action for an index under any of these conditions:\nThe index does not exist.\nThe index name is invalid.\nindex_priority\nISM does not perform an index_priority action for an index that does not have read-only-allow-delete permission.\nsnapshot\nISM does not perform a snapshot action for an index under any of these conditions:\nThe index does not exist.\nThe index name is invalid.\ntransition\nISM does not perform a transition action for an index under any of these conditions:\nThe index does not exist.\nThe index name is invalid.",
    "ancestors": [
      "Managing Indexes"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/error-prevention/resolutions/",
    "title": "ISM Error Prevention resolutions",
    "content": "Resolutions of errors for each validation rule action are listed in the following sections.\nTable of contents The index is not the write index The index does not have an alias Skipping rollover action is true This index has already been rolled over successfully The rollover policy misses rollover_alias index setting Data too large and exceeding the threshold Maximum shards exceeded The index is a write index for some data stream The index is blocked The index is not the write index\nTo confirm that the index is a write index, run the following request: GET &lt;index&gt;/_alias?pretty If the response does not contain \"is_write_index\": true, the index is not a write index. The following example confirms that the index is a write index: { \"&lt;index&gt;\": { \"aliases\": { \"&lt;index_alias&gt;\": { \"is_write_index\": true } } } } To set the index as a write index, run the following request: PUT &lt;index&gt; { \"aliases\": { \"&lt;index_alias&gt;\": { \"is_write_index\": true } } } The index does not have an alias\nIf the index does not have an alias, you can add one by running the following request: POST _aliases { \"actions\": [ { \"add\": { \"index\": \"&lt;target_index&gt;\", \"alias\": \"&lt;index_alias&gt;\" } }] } Skipping rollover action is true\nIn the event that skipping a rollover action occurs, run the following request: GET &lt;target_index&gt;/_settings?pretty If you receive the response in the first example, you can reset it by running the request in the second example: { \"index\": { \"opendistro.index_state_management.rollover_skip\": true } } PUT &lt;target_index&gt;/_settings { \"index\": { \"index_state_management.rollover_skip\": false } } This index has already been rolled over successfully\nRemove the rollover policy from the index to prevent this error from reoccurring.\nThe rollover policy misses rollover_alias index setting\nAdd a rollover_alias index setting to the rollover policy to resolve this issue. Run the following request: PUT _index_template/ism_rollover { \"index_patterns\": [ \"&lt;index_patterns_in_rollover_policy&gt;\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"&lt;rollover_alias&gt;\" } } } Data too large and exceeding the threshold\nCheck the JVM information and increase the heap memory.\nMaximum shards exceeded\nThe shard limit per node, or per index, causes this issue to occur. Check whether there is a total_shards_per_node limit by running the following request: GET /_cluster/settings If the response contains total_shards_per_node, increase its value temporarily by running the following request: PUT _cluster/settings { \"transient\": { \"cluster.routing.allocation.total_shards_per_node\":100 } } To check whether there is a shard limit for an index, run the following request: GET &lt;index&gt;/_settings/index.routing- If the response contains the setting in the first example, increase its value or set it to -1 for unlimited shards, as shown in the second example: \"index\": { \"routing\": { \"allocation\": { \"total_shards_per_node\": \"10\" } } } PUT &lt;index&gt;/_settings { \"index.routing.allocation.total_shards_per_node\":-1 } The index is a write index for some data stream\nIf you still want to delete the index, check your data stream settings and change the write index.\nThe index is blocked\nGenerally, the index is blocked because disk usage has exceeded the flood-stage watermark and the index has a read-only-allow-delete block. To resolve this issue, you can:\nRemove the -index.blocks.read_only_allow_delete- parameter.\nTemporarily increase the disk watermarks.\nTemporarily disable the disk allocation threshold.\nTo prevent the issue from reoccurring, it is better to reduce the usage of the disk by increasing disk space, adding new nodes, or removing data or indexes that are no longer needed.\nRemove -index.blocks.read_only_allow_delete- by running the following request: PUT &lt;index&gt;/_settings { \"index.blocks.read_only_allow_delete\": null } Increase the low disk watermarks by running the following request: PUT _cluster/settings { \"transient\": { \"cluster\": { \"routing\": { \"allocation\": { \"disk\": { \"watermark\": { \"low\": \"25.0gb\" } } } } } } } Disable the disk allocation threshold by running the following request: PUT _cluster/settings { \"transient\": { \"cluster\": { \"routing\": { \"allocation\": { \"disk\": { \"threshold_enabled\": false } } } } } }",
    "ancestors": [
      "Managing Indexes",
      "Index State Management",
      "ISM Error Prevention"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/index/",
    "title": "Index State Management",
    "content": "OpenSearch Dashboards\nIf you analyze time-series data, you likely prioritize new data over old data. You might periodically perform certain operations on older indexes, such as reducing replica count or deleting them.\nIndex State Management (ISM) is a plugin that lets you automate these periodic, administrative operations by triggering them based on changes in the index age, index size, or number of documents. Using the ISM plugin, you can define policies that automatically handle index rollovers or deletions to fit your use case.\nFor example, you can define a policy that moves your index into a read_only state after 30 days and then deletes it after a set period of 90 days. You can also set up the policy to send you a notification message when the index is deleted.\nYou might want to perform an index rollover after a certain amount of time or run a force_merge operation on an index during off-peak hours to improve search performance during peak hours.\nTo use the ISM plugin, your user role needs to be mapped to the all_access role that gives you full access to the cluster. To learn more, see Users and roles.\nGet started with ISM\nTo get started, choose Index Management in OpenSearch Dashboards.\nStep 1: Set up policies\nA policy is a set of rules that describes how an index should be managed. For information about creating a policy, see Policies.\nYou can use the visual editor or JSON editor to create policies. Compared to the JSON editor, the visual editor offers a more structured way of defining policies by separating the process into creating error notifications, defining ISM templates, and adding states. We recommend using the visual editor if you want to see pre-defined fields, such as which actions you can assign to a state or under what conditions a state can transition into a destination state.\nVisual editor\nChoose the Index Policies tab.\nChoose Create policy.\nChoose Visual editor.\nIn the Policy info section, enter a policy ID and an optional description.\nIn the Error notification section, set up an optional error notification that gets sent whenever a policy execution fails. For more information, see Error notifications. If you’re using auto rollovers in your policy, we recommend setting up error notifications, which notify you of unexpectedly large indexes if rollovers fail.\nIn ISM templates, enter any ISM template patterns to automatically apply this policy to future indexes. For example, if you specify a template of sample-index*, the ISM plugin automatically applies this policy to any indexes whose names start with sample-index. Your pattern cannot contain any of the following characters::, \", +, /, \\, |,?, #, &gt;, and &lt;.\nIn States, add any states you want to include in the policy. Each state has actions the plugin executes when the index enters a certain state, and transitions, which have conditions that, when met, transition the index into a destination state. The first state you create in a policy is automatically set as the initial state. Each policy must have at least one state, but actions and transitions are optional.\nChoose Create.\nJSON editor\nChoose the Index Policies tab.\nChoose Create policy.\nChoose JSON editor.\nIn the Name policy section, enter a policy ID.\nIn the Define policy section, enter your policy.\nChoose Create.\nAfter you create a policy, your next step is to attach it to an index or indexes.\nYou can set up an ism_template in the policy so when an index that matches the ISM template pattern is created, the plugin automatically attaches the policy to the index.\nThe following example demonstrates how to create a policy that automatically gets attached to all indexes whose names start with index_name-. PUT _plugins/_ism/policies/policy_id { \"policy\": { \"description\": \"Example policy.\", \"default_state\": \"...\", \"states\": [...], \"ism_template\": { \"index_patterns\": [ \"index_name-*\"], \"priority\": 100 } } } If you have more than one template that matches an index pattern, ISM uses the priority value to determine which template to apply.\nFor an example ISM template policy, see Sample policy with ISM template for auto rollover.\nOlder versions of the plugin include the policy_id in an index template, so when an index is created that matches the index template pattern, the index will have the policy attached to it: PUT _index_template/&lt;template_name&gt; { \"index_patterns\": [ \"index_name-*\"], \"template\": { \"settings\": { \"opendistro.index_state_management.policy_id\": \"policy_id\" } } } The opendistro.index_state_management.policy_id setting is deprecated. You can continue to automatically manage newly created indexes with the ISM template field.\nStep 2: Attach policies to indexes\nChoose indexes.\nChoose the index or indexes that you want to attach your policy to.\nChoose Apply policy.\nFrom the Policy ID menu, choose the policy that you created.\nYou can see a preview of your policy.\nIf your policy includes a rollover operation, specify a rollover alias.\nMake sure that the alias that you enter already exists. For more information about the rollover operation, see rollover.\nChoose Apply.\nAfter you attach a policy to an index, ISM creates a job that runs every 5 minutes by default to perform policy actions, check conditions, and transition the index into different states. To change the default time interval for this job, see Settings.\nISM does not run jobs if the cluster state is red.\nStep 3: Manage indexes\nChoose Managed indexes.\nTo change your policy, see Change Policy.\nTo attach a rollover alias to your index, select your policy and choose Add rollover alias.\nMake sure that the alias that you enter already exists. For more information about the rollover operation, see rollover.\nTo remove a policy, choose your policy, and then choose Remove policy.\nTo retry a policy, choose your policy, and then choose Retry policy.\nFor information about managing your policies, see Managed indexes.",
    "ancestors": [
      "Managing Indexes"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/managedindexes/",
    "title": "Managed Indices",
    "content": "You can change or update a policy using the managed index operations.\nThis table lists the fields of managed index operations. Parameter Description Type Required Read Only name The name of the managed index policy. string Yes\nNo index The name of the managed index that this policy is managing. string Yes\nNo index_uuid The uuid of the index. string Yes\nNo enabled When true, the managed index is scheduled and run by the scheduler. boolean Yes\nNo enabled_time The time the managed index was last enabled. If the managed index process is disabled, then this is null. timestamp Yes\nYes last_updated_time The time the managed index was last updated. timestamp Yes\nYes schedule The schedule of the managed index job. object Yes\nNo policy_id The name of the policy used by this managed index. string Yes\nNo policy_seq_no The sequence number of the policy used by this managed index. number Yes\nNo policy_primary_term The primary term of the policy used by this managed index. number Yes\nNo policy_version The version of the policy used by this managed index. number Yes\nYes policy The cached JSON of the policy for the policy_version that’s used during runs. If the policy is null, it means that this is the first execution of the job and the latest policy document is read in/saved. object No\nNo change_policy The information regarding what policy and state to change to. object No\nNo policy_name The name of the policy to update to. To update to the latest version, set this to be the same as the current policy_name. string No\nYes state The state of the managed index after it finishes updating. If no state is specified, it’s assumed that the policy structure did not change. string No\nYes The following example shows a managed index policy: { \"managed_index\": { \"name\": \"my_index\", \"index\": \"my_index\", \"index_uuid\": \"sOKSOfkdsoSKeofjIS\", \"enabled\": true, \"enabled_time\": 1553112384, \"last_updated_time\": 1553112384, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"MINUTES\", \"start_time\": 1553112384 } }, \"policy_id\": \"log_rotation\", \"policy_version\": 1, \"policy\": {... }, \"change_policy\": null } } Change policy\nYou can change any managed index policy, but ISM has a few constraints in place to make sure that policy changes don’t break indices.\nIf an index is stuck in its current state, never proceeding, and you want to update its policy immediately, make sure that the new policy includes the same state—same name, same actions, same order—as the old policy. In this case, even if the policy is in the middle of executing an action, ISM applies the new policy.\nIf you update the policy without including an identical state, ISM updates the policy only after all actions in the current state finish executing. Alternately, you can choose a specific state in your old policy after which you want the new policy to take effect.\nTo change a policy using OpenSearch Dashboards, do the following:\nUnder Managed indices, choose the indices that you want to attach the new policy to.\nTo attach the new policy to indices in specific states, choose Choose state filters, and then choose those states.\nUnder Choose New Policy, choose the new policy.\nTo start the new policy for indices in the current state, choose Keep indices in their current state after the policy takes effect.\nTo start the new policy in a specific state, choose Start from a chosen state after changing policies, and then choose the default start state in your new policy.",
    "ancestors": [
      "Managing Indexes",
      "Index State Management"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/policies/",
    "title": "Policies",
    "content": "Policies are JSON documents that define the following:\nThe states that an index can be in, including the default state for new indexes. For example, you might name your states “hot,” “warm,” “delete,” and so on. For more information, see States.\nAny actions that you want the plugin to take when an index enters a state, such as performing a rollover. For more information, see Actions.\nThe conditions that must be met for an index to move into a new state, known as transitions. For example, if an index is more than eight weeks old, you might want to move it to the “delete” state. For more information, see Transitions.\nIn other words, a policy defines the states that an index can be in, the actions to perform when in a state, and the conditions that must be met to transition between states.\nYou have complete flexibility in the way you can design your policies. You can create any state, transition to any other state, and specify any number of actions in each state.\nThis table lists the relevant fields of a policy. Field Description Type Required Read Only policy_id The name of the policy. string Yes\nYes description A human-readable description of the policy. string Yes\nNo ism_template Specify an ISM template pattern that matches the index to apply the policy. nested list of objects No\nNo last_updated_time The time the policy was last updated. timestamp Yes\nYes error_notification The destination and message template for error notifications. The destination could be Amazon Chime, Slack, or a webhook URL. object No\nNo default_state The default starting state for each index that uses this policy. string Yes\nNo states The states that you define in the policy. nested list of objects Yes\nNo Table of contents States Actions ISM supported operations force_merge read_only read_write replica_count shrink close open delete rollover notification snapshot index_priority allocation rollup Transitions Error notifications Sample policy with ISM template for auto rollover Example policy with ISM templates for the alias action Example policy States\nA state is the description of the status that the managed index is currently in. A managed index can be in only one state at a time. Each state has associated actions that are executed sequentially on entering a state and transitions that are checked after all the actions have been completed.\nThis table lists the parameters that you can define for a state. Field Description Type Required name The name of the state. string Yes actions The actions to execute after entering a state. For more information, see Actions. nested list of objects Yes transitions The next states and the conditions required to transition to those states. If no transitions exist, the policy assumes that it’s complete and can now stop managing the index. For more information, see Transitions. nested list of objects Yes Actions\nActions are the steps that the policy sequentially executes on entering a specific state.\nISM executes actions in the order in which they are defined. For example, if you define actions [A,B,C,D], ISM executes action A, and then goes into a sleep period based on the cluster setting plugins.index_state_management.job_interval. Once the sleep period ends, ISM continues to execute the remaining actions. However, if ISM cannot successfully execute action A, the operation ends, and actions B, C, and D do not get executed.\nOptionally, you can define an action’s timeout period, which, if exceeded, forcibly fails the action. For example, if timeout is set to 1d, and ISM has not completed the action within one day, even after retries, the action fails.\nThis table lists the parameters that you can define for an action. Parameter Description Type Required Default timeout The timeout period for the action. Accepts time units for minutes, hours, and days. time unit No\n- retry The retry configuration for the action. object No\nSpecific to action The retry operation has the following parameters: Parameter Description Type Required Default count The number of retry counts. number Yes\n- backoff The backoff policy type to use when retrying. Valid values are Exponential, Constant, and Linear. string No\nExponential delay The time to wait between retries. Accepts time units for minutes, hours, and days. time unit No\n1 minute The following example action has a timeout period of one hour. The policy retries this action three times with an exponential backoff policy, with a delay of 10 minutes between each retry: \"actions\": { \"timeout\": \"1h\", \"retry\": { \"count\": 3, \"backoff\": \"exponential\", \"delay\": \"10m\" } } For a list of available unit types, see Supported units.\nISM supported operations\nISM supports the following operations: force_merge read_only read_write replica_count shrink close open delete rollover notification snapshot index_priority allocation rollup force_merge\nReduces the number of Lucene segments by merging the segments of individual shards. This operation attempts to set the index to a read-only state before starting the merging process. Parameter Description Type Required max_num_segments The number of segments to reduce the shard to. number Yes\nwait_for_completion\nBoolean\nWhen set to false, the request returns immediately instead of after the operation is finished. To monitor the operation status, use the Tasks API with the task ID returned by the request. Default is true.\n \ntask_execution_timeout\nTime\nThe explicit task execution timeout. Only useful when wait_for_completion is set to false. Default is 1h.\nNo { \"force_merge\": { \"max_num_segments\": 1 } } read_only\nSets a managed index to be read only. { \"read_only\": {} } Set the index setting index.blocks.write to true for a managed index. *Note: this block does not prevent the index from refreshing.\nread_write\nSets a managed index to be writeable. { \"read_write\": {} } replica_count\nSets the number of replicas to assign to an index. Parameter Description Type Required number_of_replicas Defines the number of replicas to assign to an index. number Yes { \"replica_count\": { \"number_of_replicas\": 2 } } For information about setting replicas, see Primary and replica shards.\nshrink\nAllows you to reduce the number of primary shards in your indexes. With this action, you can specify:\nThe number of primary shards that the target index should contain.\nA max shard size for the primary shards in the target index.\nSpecify a percentage to shrink the number of primary shards in the target index. \"shrink\": { \"num_new_shards\": 1, \"target_index_name_template\": { \"source\": \"_shrunken\" }, \"aliases\": [ { \"my-alias\": {} }], \"force_unsafe\": false } Parameter Description Type Example Required num_new_shards The maximum number of primary shards in the shrunken index.\ninteger 5 Yes, however it cannot be used with max_shard_size or percentage_of_source_shards max_shard_size The maximum size in bytes of a shard for the target index.\nkeyword 5gb Yes, however it cannot be used with num_new_shards or percentage_of_source_shards percentage_of_source_shards Percentage of the number of original primary shards to shrink. This parameter indicates the minimum percentage to use when shrinking the number of primary shards. Must be between 0.0 and 1.0, exclusive.\nPercentage 0.5 Yes, however it cannot be used with max_shard_size or num_new_shards target_index_name_template The name of the shrunken index. Accepts strings and the Mustache variables and. string or Mustache template {\"source\": \"_shrunken\"} No aliases Aliases to add to the new index.\nobject myalias No, but must be an array of alias objects force_unsafe If true, executes the shrink action even if there are no replicas.\nboolean false No If you want to add aliases to the action, the parameter must include an array of alias objects. For example, \"aliases\": [ { \"my-alias\": {} }, { \"my-second-alias\": { \"is_write_index\": false, \"filter\": { \"multi_match\": { \"query\": \"QUEEN\", \"fields\": [ \"speaker\", \"text_entry\"] } }, \"index_routing\": \"1\", \"search_routing\": \"1\" } },] close\nCloses the managed index. { \"close\": {} } Closed indexes remain on disk, but consume no CPU or memory. You can’t read from, write to, or search closed indexes.\nClosing an index is a good option if you need to retain data for longer than you need to actively search it and have sufficient disk space on your data nodes. If you need to search the data again, reopening a closed index is simpler than restoring an index from a snapshot.\nopen\nOpens a managed index. { \"open\": {} } delete\nDeletes a managed index. { \"delete\": {} } rollover\nRolls an alias over to a new index when the managed index meets one of the rollover conditions.\nISM checks the conditions for operations on every execution of the policy based on the set interval, not continuously. The rollover will be performed if the value has reached or has exceeded the configured limit when the check is performed. For example with min_size configured to a value of 100GiB, ISM might check the index at 99 GiB and not perform the rollover. However, if the index has grown past the limit (e.g., 105GiB) by the next check, the operation is performed.\nIf you need to skip the rollover action, you can set the index setting index.plugins.index_state_management.rollover_skip to true. For example, if you receive the error message “Missing alias or not the write index…”, you can set the index.plugins.index_state_management.rollover_skip parameter to true and retry to skip rollover action.\nThe index format must match the pattern: ^.*-\\d+$. For example, (logs-000001).\nSet index.plugins.index_state_management.rollover_alias as the alias to rollover. Parameter Description Type Example Required min_size The minimum size of the total primary shard storage (not counting replicas) required to roll over the index. For example, if you set min_size to 100 GiB and your index has 5 primary shards and 5 replica shards of 20 GiB each, the total size of all primary shards is 100 GiB, so the rollover occurs. See Important note above. string 20gb or 5mb No min_primary_shard_size The minimum storage size of a single primary shard required to roll over the index. For example, if you set min_primary_shard_size to 30 GiB and one of the primary shards in the index has a size greater than the condition, the rollover occurs. See Important note above. string 20gb or 5mb No min_doc_count The minimum number of documents required to roll over the index. See Important note above. number 2000000 No min_index_age The minimum age required to roll over the index. Index age is the time between its creation and the present. Supported units are d (days), h (hours), m (minutes), s (seconds), ms (milliseconds), and micros (microseconds). See Important note above. string 5d or 7h No { \"rollover\": { \"min_size\": \"50gb\" } } { \"rollover\": { \"min_primary_shard_size\": \"30gb\" } } { \"rollover\": { \"min_doc_count\": 100000000 } } { \"rollover\": { \"min_index_age\": \"30d\" } } notification\nSends you a notification. Parameter Description Type Required destination The destination URL. Slack, Amazon Chime, or webhook URL Yes message_template The text of the message. You can add variables to your messages using Mustache templates. object Yes The destination system must return a response otherwise the notification operation throws an error.\nExample 1: Chime notification { \"notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;url&gt;\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } Example 2: Custom webhook notification { \"notification\": { \"destination\": { \"custom_webhook\": { \"url\": \"https://&lt;your_webhook&gt;\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } Example 3: Slack notification { \"notification\": { \"destination\": { \"slack\": { \"url\": \"https://hooks.slack.com/services/xxx/xxxxxx\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } You can use ctx variables in your message to represent a number of policy parameters based on the past executions of your policy. For example, if your policy has a rollover action, you can use {{ctx.action.name}} in your message to represent the name of the rollover.\nThe following ctx variable options are available for every policy:\nGuaranteed variables Parameter Description Type index The name of the index. string index_uuid The uuid of the index. string policy_id The name of the policy. string snapshot\nBack up your cluster’s indexes and state. For more information about snapshots, see Take and restore snapshots.\nThe snapshot operation has the following parameters: Parameter Description Type Required Default repository The repository name that you register through the native snapshot API operations. string Yes\n- snapshot The name of the snapshot. Accepts strings and the Mustache variables and. If the Mustache variables are invalid, then the snapshot name defaults to the index’s name. string or Mustache template\nYes\n- { \"snapshot\": { \"repository\": \"my_backup\", \"snapshot\": \"\" } } index_priority\nSet the priority for the index in a specific state. Unallocated shards of indexes are recovered in the order of their priority, whenever possible. The indexes with higher priority values are recovered first followed by the indexes with lower priority values.\nThe index_priority operation has the following parameter: Parameter Description Type Required Default priority The priority for the index as soon as it enters a state. number Yes\n1 \"actions\": [ { \"index_priority\": { \"priority\": 50 } }] allocation\nAllocate the index to a node with a specific attribute set like this.\nFor example, setting require to warm moves your data only to “warm” nodes.\nThe allocation operation has the following parameters: Parameter Description Type Required require Allocate the index to a node with a specified attribute. string Yes include Allocate the index to a node with any of the specified attributes. string Yes exclude Don’t allocate the index to a node with any of the specified attributes. string Yes wait_for Wait for the policy to execute before allocating the index to a node with a specified attribute. string Yes \"actions\": [ { \"allocation\": { \"require\": { \"temp\": \"warm\" } } }] rollup Index rollup lets you periodically reduce data granularity by rolling up old data into summarized indexes.\nRollup jobs can be continuous or non-continuous. A rollup job created using an ISM policy can only be non-continuous.\nPath and HTTP methods PUT _plugins/_rollup/jobs/&lt;rollup_id&gt;\nGET _plugins/_rollup/jobs/&lt;rollup_id&gt;\nDELETE _plugins/_rollup/jobs/&lt;rollup_id&gt;\nPOST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_start\nPOST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_stop\nGET _plugins/_rollup/jobs/&lt;rollup_id&gt;/_explain Sample ISM rollup policy { \"policy\": { \"description\": \"Sample rollup\", \"default_state\": \"rollup\", \"states\": [ { \"name\": \"rollup\", \"actions\": [ { \"rollup\": { \"ism_rollup\": { \"description\": \"Creating rollup through ISM\", \"target_index\": \"target\", \"page_size\": 1000, \"dimensions\": [ { \"date_histogram\": { \"fixed_interval\": \"60m\", \"source_field\": \"order_date\", \"target_field\": \"order_date\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"customer_gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day_of_week\" } }], \"metrics\": [ { \"source_field\": \"taxless_total_price\", \"metrics\": [ { \"sum\": {} }] }, { \"source_field\": \"total_quantity\", \"metrics\": [ { \"avg\": {} }, { \"max\": {} }] }] } } }], \"transitions\": [] }] } } Request fields\nRequest fields are required when creating an ISM policy. You can reference the Index rollups API page for request field options.\nAdding a rollup policy in Dashboards\nTo add a rollup policy in Dashboards, follow the steps below.\nSelect the menu button on the top-left of the Dashboards user interface.\nIn the Dashboards menu, select Index Management.\nOn the next screen select Rollup jobs.\nSelect the Create rollup button.\nFollow the steps in the Create rollup job wizard.\nAdd a name for the policy in the Name box.\nYou can reference the Index rollups API page to configure the rollup policy.\nFinally, select the Create button on the bottom-right of the Dashboards user interface.\nTransitions\nTransitions define the conditions that need to be met for a state to change. After all actions in the current state are completed, the policy starts checking the conditions for transitions.\nISM evaluates transitions in the order in which they are defined. For example, if you define transitions: [A,B,C,D], ISM iterates through this list of transitions until it finds a transition that evaluates to true, it then stops and sets the next state to the one defined in that transition. On its next execution, ISM dismisses the rest of the transitions and starts in that new state.\nIf you don’t specify any conditions in a transition and leave it empty, then it’s assumed to be the equivalent of always true. This means that the policy transitions the index to this state the moment it checks.\nThis table lists the parameters you can define for transitions. Parameter Description Type Required state_name The name of the state to transition to if the conditions are met. string Yes conditions List the conditions for the transition. list Yes The conditions object has the following parameters: Parameter Description Type Required min_index_age The minimum age of the index required to transition. string No min_rollover_age The minimum age required after a rollover has occurred to transition to the next state. string No min_doc_count The minimum document count of the index required to transition. number No min_size The minimum size of the total primary shard storage (not counting replicas) required to transition. For example, if you set min_size to 100 GiB and your index has 5 primary shards and 5 replica shards of 20 GiB each, the total size of all primary shards is 100 GiB, so your index is transitioned to the next state. string No cron The cron job that triggers the transition if no other transition happens first. object No cron.cron.expression The cron expression that triggers the transition. string Yes cron.cron.timezone The timezone that triggers the transition. string Yes The following example transitions the index to a cold state after a period of 30 days: \"transitions\": [ { \"state_name\": \"cold\", \"conditions\": { \"min_index_age\": \"30d\" } }] ISM checks the conditions on every execution of the policy based on the set interval.\nThis example uses the cron condition to transition indexes every Saturday at 5:00 PT: \"transitions\": [ { \"state_name\": \"cold\", \"conditions\": { \"cron\": { \"cron\": { \"expression\": \"* 17 * * SAT\", \"timezone\": \"America/Los_Angeles\" } } } }] Note that this condition does not execute at exactly 5:00 PM; the job still executes based off the job_interval setting. Due to this variance in start time and the amount of time that it can take for actions to complete prior to checking transition conditions, we recommend against overly narrow cron expressions. For example, don’t use 15 17 * * SAT (5:15 PM on Saturday).\nA window of an hour, which this example uses, is generally sufficient, but you might increase it to 2–3 hours to avoid missing the window and having to wait a week for the transition to occur. Alternately, you could use a broader expression such as * * * * SAT,SUN to have the transition occur at any time during the weekend.\nFor information on writing cron expressions, see Cron expression reference.\nError notifications\nThe error_notification operation sends you a notification if your managed index fails.\nIt notifies a single destination or notification channel with a custom message.\nSet up error notifications at the policy level: { \"policy\": { \"description\": \"hot warm delete workflow\", \"default_state\": \"hot\", \"schema_version\": 1, \"error_notification\": { }, \"states\": [] } } Parameter Description Type Required destination The destination URL. Slack, Amazon Chime, or webhook URL Yes if channel isn’t specified channel A notification channel’s ID string Yes if destination isn’t specified message_template The text of the message. You can add variables to your messages using Mustache templates. object Yes The destination system must return a response otherwise the error_notification operation throws an error.\nExample 1: Chime notification { \"error_notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;url&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } Example 2: Custom webhook notification { \"error_notification\": { \"destination\": { \"custom_webhook\": { \"url\": \"https://&lt;your_webhook&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } Example 3: Slack notification { \"error_notification\": { \"destination\": { \"slack\": { \"url\": \"https://hooks.slack.com/services/xxx/xxxxxx\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } Example 4: Using a notification channel { \"error_notification\": { \"channel\": { \"id\": \"some-channel-config-id\" }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } You can use the same options for ctx variables as the notification operation.\nSample policy with ISM template for auto rollover\nThe following sample template policy is for a rollover use case.\nIf you want to skip rollovers for an index, set index.plugins.index_state_management.rollover_skip to true in the settings of that index.\nCreate a policy with an ism_template field: PUT _plugins/_ism/policies/rollover_policy { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } }], \"transitions\": [] }], \"ism_template\": { \"index_patterns\": [ \"log*\"], \"priority\": 100 } } } You need to specify the index_patterns field. If you don’t specify a value for priority, it defaults to 0.\nSet up a template with the rollover_alias as log: PUT _index_template/ism_rollover { \"index_patterns\": [ \"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } Create an index with the log alias: PUT log -000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } Index a document to trigger the rollover condition: POST log/_doc { \"message\": \"dummy\" } Verify if the policy is attached to the log-000001 index: GET _plugins/_ism/explain/log -000001?pretty Example policy with ISM templates for the alias action\nThe following example policy is for an alias action use case.\nIn the following example, the first job will trigger the rollover action, and a new index will be created. Next, another document is added to the two indexes. The new job will then cause the second index to point to the log alias, and the older index will be removed due to the alias action.\nFirst, create an ISM policy: PUT /_plugins/_ism/policies/rollover_policy?pretty { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } }], \"transitions\": [{ \"state_name\": \"alias\", \"conditions\": { \"min_doc_count\": \"2\" } }] }, { \"name\": \"alias\", \"actions\": [ { \"alias\": { \"actions\": [ { \"remove\": { \"alias\": \"log\" } }] } }] }], \"ism_template\": { \"index_patterns\": [ \"log*\"], \"priority\": 100 } } } Next, create an index template on which to enable the policy: PUT /_index_template/ism_rollover? { \"index_patterns\": [ \"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } copy Next, change the cluster settings to trigger jobs every minute: PUT /_cluster/settings?pretty= true { \"persistent\": { \"plugins.index_state_management.job_interval\": 1 } } copy Next, create a new index: PUT /log -000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } copy Finally, add a document to the index to trigger the job: POST /log -000001 /_doc { \"message\": \"dummy\" } copy You can verify these steps using the Alias and Index API: GET /_cat/indices?pretty copy GET /_cat/aliases?pretty copy Note: The index and remove_index parameters are not allowed with alias action policies. Only the add and remove alias action parameters are allowed.\nExample policy\nThe following example policy implements a hot, warm, and delete workflow. You can use this policy as a template to prioritize resources to your indexes based on their levels of activity.\nIn this case, an index is initially in a hot state. After a day, it changes to a warm state, where the number of replicas increases to 5 to improve the read performance.\nAfter 30 days, the policy moves this index into a delete state. The service sends a notification to a Chime room that the index is being deleted, and then permanently deletes it. { \"policy\": { \"description\": \"hot warm delete workflow\", \"default_state\": \"hot\", \"schema_version\": 1, \"states\": [ { \"name\": \"hot\", \"actions\": [ { \"rollover\": { \"min_index_age\": \"1d\", \"min_primary_shard_size\": \"30gb\" } }], \"transitions\": [ { \"state_name\": \"warm\" }] }, { \"name\": \"warm\", \"actions\": [ { \"replica_count\": { \"number_of_replicas\": 5 } }], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"30d\" } }] }, { \"name\": \"delete\", \"actions\": [ { \"notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;URL&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} is being deleted\" } } }, { \"delete\": {} }] }], \"ism_template\": { \"index_patterns\": [ \"log*\"], \"priority\": 100 } } } This diagram shows the states, transitions, and actions of the above policy as a finite-state machine. For more information about finite-state machines, see Wikipedia.",
    "ancestors": [
      "Managing Indexes",
      "Index State Management"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/ism/settings/",
    "title": "Settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases.\nIndex State Management (ISM) stores its configuration in the.opendistro-ism-config index. Don’t modify this index without using the ISM API operations.\nAll settings are available using the OpenSearch _cluster/settings operation. None require a restart, and all can be marked persistent or transient. Setting Default Description plugins.index_state_management.enabled True\nSpecifies whether ISM is enabled or not. plugins.index_state_management.job_interval 5 minutes\nThe interval at which the managed index jobs are run. plugins.index_state_management.jitter 0.6\nA randomized delay that is added to a job’s base run time to prevent a surge of activity from all indices at the same time. A value of 0.6 means a delay of 0-60% of a job interval is added to the base interval. For example, if you have a base interval time of 30 minutes, a value of 0.6 means an amount anywhere between 0 to 18 minutes gets added to your job interval. Maximum is 1, which means an additional interval time of 100%. This maximum cannot exceed plugins.jobscheduler.jitter_limit, which also has a default of 0.6. For example, if plugins.index_state_management.jitter is set to 0.8, ISM uses plugins.jobscheduler.jitter_limit of 0.6 instead. plugins.index_state_management.coordinator.sweep_period 10 minutes\nHow often the routine background sweep is run. plugins.index_state_management.coordinator.backoff_millis 50 milliseconds\nThe backoff time between retries for failures in the ManagedIndexCoordinator (such as when we update managed indices). plugins.index_state_management.coordinator.backoff_count 2\nThe count of retries for failures in the ManagedIndexCoordinator. plugins.index_state_management.history.enabled True\nSpecifies whether audit history is enabled or not. The logs from ISM are automatically indexed to a logs document. plugins.index_state_management.history.max_docs 2,500,000\nThe maximum number of documents before rolling over the audit history index. plugins.index_state_management.history.max_age 24 hours\nThe maximum age before rolling over the audit history index. plugins.index_state_management.history.rollover_check_period 8 hours\nThe time between rollover checks for the audit history index. plugins.index_state_management.history.rollover_retention_period 30 days\nHow long audit history indices are kept. plugins.index_state_management.allow_list All actions\nList of actions that you can use.",
    "ancestors": [
      "Managing Indexes",
      "Index State Management"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/reindex-data/",
    "title": "Reindex data",
    "content": "After creating an index, you might need to make an extensive change such as adding a new field to every document or combining multiple indexes to form a new one. Rather than deleting your index, making the change offline, and then indexing your data again, you can use the reindex operation.\nWith the reindex operation, you can copy all or a subset of documents that you select through a query to another index. Reindex is a POST operation. In its most basic form, you specify a source index and a destination index.\nReindexing can be an expensive operation depending on the size of your source index. We recommend you disable replicas in your destination index by setting number_of_replicas to 0 and re-enable them once the reindex process is complete.\nTable of contents Reindex all documents Reindex from a remote cluster Reindex a subset of documents Combine one or more indexes Reindex only unique documents Transform documents during reindexing Update documents in the current index Source index options Destination index options Reindex all documents\nYou can copy all documents from one index to another.\nYou first need to create a destination index with your desired field mappings and settings or you can copy the ones from your source index: PUT destination { \"mappings\":{ \"Add in your desired mappings\" }, \"settings\":{ \"Add in your desired settings\" } } This reindex command copies all the documents from a source index to a destination index: POST _reindex { \"source\":{ \"index\": \"source\" }, \"dest\":{ \"index\": \"destination\" } } If the destination index is not already created, the reindex operation creates a new destination index with default configurations.\nReindex from a remote cluster\nYou can copy documents from an index in a remote cluster. Use the remote option to specify the remote hostname and the required login credentials.\nThis command reaches out to a remote cluster, logs in with the username and password, and copies all the documents from the source index in that remote cluster to the destination index in your local cluster: POST _reindex { \"source\":{ \"remote\":{ \"host\": \"https://&lt;REST_endpoint_of_remote_cluster&gt;:9200\", \"username\": \"YOUR_USERNAME\", \"password\": \"YOUR_PASSWORD\" }, \"index\": \"source\" }, \"dest\":{ \"index\": \"destination\" } } You can specify the following options: Options Valid values Description Required host String\nThe REST endpoint of the remote cluster.\nYes username String\nThe username to log into the remote cluster.\nNo password String\nThe password to log into the remote cluster.\nNo socket_timeout Time Unit\nThe wait time for socket reads (default 30s).\nNo connect_timeout Time Unit\nThe wait time for remote connection timeouts (default 30s).\nNo Reindex a subset of documents\nYou can copy a specific set of documents that match a search query.\nThis command copies only a subset of documents matched by a query operation to the destination index: POST _reindex { \"source\":{ \"index\": \"source\", \"query\": { \"match\": { \"field_name\": \"text\" } } }, \"dest\":{ \"index\": \"destination\" } } For a list of all query operations, see Full-text queries.\nCombine one or more indexes\nYou can combine documents from one or more indexes by adding the source indexes as a list.\nThis command copies all documents from two source indexes to one destination index: POST _reindex { \"source\":{ \"index\":[ \"source_1\", \"source_2\"] }, \"dest\":{ \"index\": \"destination\" } } Make sure the number of shards for your source and destination indexes is the same.\nReindex only unique documents\nYou can copy only documents missing from a destination index by setting the op_type option to create.\nIn this case, if a document with the same ID already exists, the operation ignores the one from the source index.\nTo ignore all version conflicts of documents, set the conflicts option to proceed. POST _reindex { \"conflicts\": \"proceed\", \"source\":{ \"index\": \"source\" }, \"dest\":{ \"index\": \"destination\", \"op_type\": \"create\" } } Transform documents during reindexing\nYou can transform your data during the reindexing process using the script option.\nWe recommend Painless for scripting in OpenSearch.\nThis command runs the source index through a Painless script that increments a number field inside an account object before copying it to the destination index: POST _reindex { \"source\":{ \"index\": \"source\" }, \"dest\":{ \"index\": \"destination\" }, \"script\":{ \"lang\": \"painless\", \"source\": \"ctx._account.number++\" } } You can also specify an ingest pipeline to transform your data during the reindexing process.\nYou would first have to create a pipeline with processors defined. You have a number of different processors available to use in your ingest pipeline.\nHere’s a sample ingest pipeline that defines a split processor that splits a text field based on a space separator and stores it in a new word field. The script processor is a Painless script that finds the length of the word field and stores it in a new word_count field. The remove processor removes the test field. PUT _ingest/pipeline/pipeline-test { \"description\": \"Splits the text field into a list. Computes the length of the 'word' field and stores it in a new 'word_count' field. Removes the 'test' field.\", \"processors\": [ { \"split\": { \"field\": \"text\", \"separator\": \" \\\\ s+\", \"target_field\": \"word\" }, } { \"script\": { \"lang\": \"painless\", \"source\": \"ctx.word_count = ctx.word.length\" } }, { \"remove\": { \"field\": \"test\" } }] } After creating a pipeline, you can use the reindex operation: POST _reindex { \"source\": { \"index\": \"source\", }, \"dest\": { \"index\": \"destination\", \"pipeline\": \"pipeline-test\" } } Update documents in the current index\nTo update the data in your current index itself without copying it to a different index, use the update_by_query operation.\nThe update_by_query operation is POST operation that you can perform on a single index at a time. POST &lt;index_name&gt;/_update_by_query If you run this command with no parameters, it increments the version number for all documents in the index.\nSource index options\nYou can specify the following options for your source index: Option Valid values Description Required index String\nThe name of the source index. You can provide multiple source indexes as a list.\nYes max_docs Integer\nThe maximum number of documents to reindex.\nNo query Object\nThe search query to use for the reindex operation.\nNo size Integer\nThe number of documents to reindex.\nNo slice String\nSpecify manual or automatic slicing to parallelize reindexing.\nNo Destination index options\nYou can specify the following options for your destination index: Option Valid values Description Required index String\nThe name of the destination index.\nYes version_type Enum\nThe version type for the indexing operation. Valid values: internal, external, external_gt, external_gte.\nNo",
    "ancestors": [
      "Managing Indexes"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/im-plugin/security/",
    "title": "Index management security",
    "content": "Using the Security plugin with index management lets you limit non-admin users to certain actions. For example, you might want to set up your security such that a group of users can only read ISM policies, while others can create, delete, or change policies.\nAll index management data are protected as system indices, and only a super admin or an admin with a Transport Layer Security (TLS) certificate can access system indices. For more information, see System indices.\nBasic permissions\nThe Security plugin comes with one role that offers full access to index management: index_management_full_access. For a description of the role’s permissions, see Predefined roles.\nWith security enabled, users not only need the correct index management permissions, but they also need permissions to execute actions to involved indices. For example, if a user wants to use the REST API to attach a policy that executes a rollup job to an index named system-logs, they would need the permissions to attach a policy and execute a rollup job, as well as access to system-logs.\nFinally, with the exceptions of Create Policy, Get Policy, and Delete Policy, users also need the indices:admin/opensearch/ism/managedindex permission to execute ISM APIs.\n(Advanced) Limit access by backend role\nYou can use backend roles to configure fine-grained access to index management policies and actions. For example, users of different departments in an organization might view different policies depending on what roles and permissions they are assigned.\nFirst, ensure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually.\nUse the REST API to enable the following setting: PUT _cluster/settings { \"transient\": { \"plugins.index_management.filter_by_backend_roles\": \"true\" } } With security enabled, only users who share at least one backend role can see and execute the policies and actions relevant to their roles.\nFor example, consider a scenario with three users: John and Jill, who have the backend role helpdesk_staff, and Jane, who has the backend role phone_operator. John wants to create a policy that performs a rollup job on an index named airline_data, so John would need a backend role that has permissions to access that index, create relevant policies, and execute relevant actions, and Jill would be able to access the same index, policy, and job. However, Jane cannot access or edit those resources or actions.",
    "ancestors": [
      "Managing Indexes"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/branding/",
    "title": "Customizing your branding",
    "content": "Introduced 1.2\nBy default, OpenSearch Dashboards uses the OpenSearch logo, but if you want to use custom branding elements such as the favicon or main Dashboards logo, you can do so by editing opensearch_dashboards.yml or by including a custom opensearch_dashboards.yml file when you start your OpenSearch cluster.\nFor example, if you’re using Docker to start your OpenSearch cluster, include the following lines in the opensearch-dashboards section of your docker-compose.yml file: volumes:\n-./opensearch_dashboards.yml:/usr/share/opensearch-dashboards/config/opensearch_dashboards.yml Doing so replaces the Docker image’s default opensearch_dashboards.yml with your custom opensearch_dashboards.yml file, so be sure to include your desired settings as well. For example, if you want to configure TLS for OpenSearch Dashboards, see Configure TLS for OpenSearch Dashboards.\nRe-launch OpenSearch Dashboards, and OpenSearch Dashboards now uses your custom elements.\nBranding elements\nThe following elements in OpenSearch Dashboards are customizable: Setting Corresponding branding element logo\nHeader logo. See #1 in the image.\nmark\nOpenSearch Dashboards mark. See #2 in the image.\nloadingLogo\nLoading logo used when OpenSearch Dashboards is starting. See #3 in the image.\nfaviconUrl\nWebsite icon. Loads next to the application title. See #4 in the image.\napplicationTitle\nThe application’s title. See #5 in the image. To consolidate navigation controls and reduce the space the header takes up on the page, see Condensed header.\nTo start using your own branding elements in OpenSearch Dashboards, first uncomment this section of opensearch_dashboards.yml: # opensearchDashboards.branding: # logo: # defaultUrl: \"\" # darkModeUrl: \"\" # mark: # defaultUrl: \"\" # darkModeUrl: \"\" # loadingLogo: # defaultUrl: \"\" # darkModeUrl: \"\" # faviconUrl: \"\" # applicationTitle: \"\" Add the URLs you want to use as branding elements to the appropriate setting. Valid image types are SVG, PNG, and GIF.\nCustomization of dark mode Dashboards is also available, but you first must supply a valid link to defaultUrl, and then link to your preferred image with darkModeUrl. If you don’t provide a darkModeUrl link, then Dashboards uses the provided defaultUrl element for dark mode. You are not required to customize all branding elements, so if you wanted to, it’s perfectly valid to change just the logo or any other element. Leave unchanged elements as commented.\nThe following example demonstrates how to use SVG files as logos but leaves the other elements as defaults. logo: defaultUrl: \" https://example.com/validUrl.svg\" darkModeUrl: \" https://example.com/validDarkModeUrl.svg\" # mark: # defaultUrl: \"\" # darkModeUrl: \"\" # loadingLogo: # defaultUrl: \"\" # darkModeUrl: \"\" # faviconUrl: \"\" applicationTitle: \" My custom application\" We recommend linking to images that are hosted on a web server, but if you really want to use locally hosted images, save your images inside assets, and then configure opensearch_dashboards.yml to use the correct paths. You can access locally stored images through the ui/assets folder.\nThe following example assumes the default port of 5601 that Dashboards uses and demonstrates how to link to locally stored images. logo: defaultUrl: \" https://localhost:5601/ui/assets/my-own-image.svg\" darkModeUrl: \" https://localhost:5601/ui/assets/dark-mode-my-own-image.svg\" mark: defaultUrl: \" https://localhost:5601/ui/assets/my-own-image2.svg\" darkModeUrl: \" https://localhost:5601/ui/assets/dark-mode-my-own-image2.svg\" # loadingLogo: # defaultUrl: \"\" # darkModeUrl: \"\" # faviconUrl: \"\" applicationTitle: \" My custom application\" Condensed header\nThe condensed header view reduces the footprint of the header and frees up space on the page by combining navigational elements into a single header bar.\nThe current default view remains close in appearance to the two-bar header offered in the previous version of Dashboards, with minor differences. To specify the condensed header, add the configuration property useExpandedHeader to the opensearch_dashboards.yml file and set the value to false, as the following example illustrates. # opensearchDashboards.branding: # logo: defaultUrl: \" https://example.com/sample.svg\" darkModeUrl: \" https://example.com/dark-mode-sample.svg\" # mark: # defaultUrl: \"\" # darkModeUrl: \"\" # loadingLogo: # defaultUrl: \"\" # darkModeUrl: \"\" # faviconUrl: \"\" applicationTitle: \" my custom application\" useExpandedHeader: false In a future release, default behavior will become useExpandedHeader: false. If you want to retain the default view in subsequent releases, you can explicitly set the property to true in advance. Alternatively, you can also do this when upgrading.\nThe condensed view header appears as in the example below. Header element Description OpenSearch logo\nSee #1. Functions as the home button.\nHeader bar\nSee #2. A single header bar used for all navigation controls. The default view remains close to the traditional view, with minor changes. Header element Description Home button\nSee #1. Returns to the home page and provides an indication when a page is loading.\nHeader label\nSee #2. The label also functions as a home button.\nNavigation controls\nSee #3. Additional navigation controls on right-side insertion points. Preserving nagivation elements in the default view\nYou can continue using the top header bar in the default view for custom navigation links (such as menu items and plugins). Follow the steps below to keep these elements in the top header in the default view.\nReplace the property coreStart.chrome.navControls.registerRight(...) with coreStart.chrome.navControls.registerExpandedRight(...) and then replace the property coreStart.chrome.navControls.registerCenter(...) with coreStart.chrome.navControls.registerExpandedCenter(...) Make sure the configuration property useExpandedHeader is explicitly set to true.\nSample configuration\nThe following configuration enables the Security plugin and SSL within OpenSearch Dashboards and uses custom branding elements to replace the OpenSearch logo and application title. server.host: \" 0\" opensearch.hosts: [ \" https://localhost:9200\"] opensearch.ssl.verificationMode: none opensearch.username: \" kibanaserver\" opensearch.password: \" kibanaserver\" opensearch.requestHeadersAllowlist: [ authorization, securitytenant] #server.ssl.enabled: true #server.ssl.certificate: /path/to/your/server/certificate #server.ssl.key: /path/to/your/server/key opensearch_security.multitenancy.enabled: true opensearch_security.multitenancy.tenants.preferred: [ \" Private\", \" Global\"] opensearch_security.readonly_mode.roles: [ \" kibana_read_only\"] # Use this setting if you are running opensearch-dashboards without https opensearch_security.cookie.secure: false opensearchDashboards.branding: logo: defaultUrl: \" https://example.com/sample.svg\" darkModeUrl: \" https://example.com/dark-mode-sample.svg\" # mark: # defaultUrl: \"\" # darkModeUrl: \"\" # loadingLogo: # defaultUrl: \"\" # darkModeUrl: \"\" # faviconUrl: \"\" applicationTitle: \" Just some testing\"",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/dashboard/index/",
    "title": "Creating dashboards",
    "content": "The Dashboard application in OpenSearch Dashboards lets you visually represent your analytical, operational, and strategic data to help you quickly understand the trends in your data, giving you a high-level view of key metrics, simplifying data exploration, and delivering insights when and where you need them.\nIn this tutorial you’ll learn the basics of creating a dashboard using the Dashboard application and OpenSearch sample data. The sample dataset has existing sample visualizations, and you can use those visualizations or create new visualizations for the dashboard. In this tutorial, you’ll do both. Once you’ve completed this tutorial, you’ll have learned the foundations of creating a new dashboard with multiple panels in OpenSearch Dashboards.\nThis OpenSearch Playground dashboard example shows you what’s possible with OpenSearch Dashboards.\nGetting familiar with the UI\nBefore getting started, let’s get familiar with the Dashboard UI. The UI comprises the following main components: The navigation panel (A) on the left contains the OpenSearch Dashboards applications.\nThe search bar (B) lets you search for documents and other objects and add filters.\nThe filter (C) lets you narrow a dashboard’s results.\nThe toolbar (D) contains frequently used commands and shortcuts.\nThe time filter (E) lets you customize the time and date.\nThe panel (F) allows you to add existing visualizations to the dashboard or create new ones for the dashboard.\nDefining terminology\nThe following is some useful terminology for working with OpenSearch Dashboards and the Dashboard application: Dashboards is the abbreviated name for OpenSearch Dashboards. OpenSearch Dashboards is an open-source visualization tool designed to work with OpenSearch. Dashboard is the OpenSearch Dashboards application used to track, analyze, and display data. dashboard or dashboards are common names for a tool used to visually display data. Panel is a term used to refer to a visualization displayed on a dashboard. The terms panel and visualization may be used interchangeably throughout this and other Dashboards documentation.\nThe following tutorial assumes you’re either using your existing installation of OpenSearch Dashboards or using the OpenSearch Playground. Depending on which one you use, certain capabilities may not be available. For example, sample datasets may not be included in your existing installation, and saving a dashboard isn’t an option in the OpenSearch Playground.\nCreating a dashboard and adding an existing visualization\nTo create a dashboard and add a sample visualization:\nConnect to https://localhost:5601. The username and password are admin. Alternatively, go to the OpenSearch Playground.\nOn the top menu, go to OpenSearch Dashboards &gt; Dashboard.\nFrom the Dashboards panel, choose Create Dashboard.\nChoose the calendar icon and set the time filter to Last 30 days.\nFrom the panel, choose Add an existing.\nFrom the Add panels window, choose [eCommerce] Promotion Tracking, and then choose x to close the panel.\nYou’ve now created the following basic dashboard with a single panel, which you’ll continue using throughout this tutorial. Creating visualizations\nContinuing with the dashboard you created in the preceding steps, you’ll create a new visualization and save it to the dashboard:\nFrom the dashboard toolbar, choose Create new.\nFrom the New Visualization window, choose Gauge and then select the index pattern opensearch_dashboards_sample_data_ecommerce.\nFrom the toolbar, choose Save.\nIn the Save visualization window, enter a title for the visualization. For example, the title for the gauge chart panel is [eCommerce] Orders.\nChoose Save and return.\nThe gauge chart visualization is now saved and you are taken back to the dashboard. You’ll see two visualizations on the dashboard, like the following. Adding subsequent panels\nContinuing with the dashboard you created in the preceding steps, you’ll add an existing visualization to the dashboard:\nFrom the dashboard toolbar, choose Add.\nFrom the Add panels window, choose [eCommerce] Sales by Category.\nChoose x to close the Add panels window.\nYou’ll see an area chart visualization display on the dashboard, as shown in the following image. Saving dashboards\nWhen you’ve finalized your dashboard, save it. If you’re saving a new dashboard:\nIn the toolbar, choose Save.\nIn the Save dashboard window, enter the Title. The Description is optional.\nTo save the time filter to the dashboard, select Store time with dashboard.\nChoose Save.\nCustomizing the look of a panel\nTo customize the panels, you’ll need to be in edit mode:\nChoose Edit at the top right of the toolbar.\nIf you see Create new at the top right of the toolbar, you’re already in edit mode.\nDisplaying a legend can give readers more information, while hiding a legend can give the panel a cleaner look. If you want to display or hide the panel legend:\nChoose the list icon in the panel’s lower left corner.\nIf you want to change the color of the panel legend:\nFrom the visualization legend, select a category and then select a color from the flyout. The area chart updates with your change.\nThis color change is only saved for the current panel and dashboard and doesn’t affect the saved visualization.\nIf you want to change the color of the panel legend in the visualization:\nChoose the gear icon on the area chart panel.\nFrom the Options window, select Edit visualization.\nFrom the visualization legend, select a category and then select a color from the flyout. The area chart updates with your change.\nChoose Save and return.\nThis color change affects the saved visualization and any dashboard that links to the visualization.\nIf you want to display, hide, or customize the panel title:\nChoose the gear icon on the panel.\nFrom the Options window, select Edit panel title.\nFrom the Customize panel, enter a title under Panel title or toggle the Show panel title to hide the title.\nChoose Save.\nChanging panel titles only affects the particular panel on the particular dashboard and won’t affect any other panel containing that same visualization or any other dashboard.\nArranging panels\nTo organize panels, arrange them side by side, or resize them, you can use these options:\nTo move a panel, select and hold the panel title or the top of the panel and drag to the new location.\nTo resize a panel, choose the resize icon in the panel’s lower-right corner and drag to the new dimensions.\nTo view a panel in full screen mode, choose the gear icon (edit mode) or vertical ellipsis (⋮) at the top right of the panel and select Maximize panel. To minimize the full screen mode, choose the gear icon or vertical ellipsis and select Minimize.\nThe following is an example of a customized dashboard created by using this tutorial.",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/dashboard/plugins-dashboards/",
    "title": "Integrating plugins into a dashboard",
    "content": "Observability is a collection of plugins and applications that let you visualize data-driven events by using Piped Processing Language to explore, discover, and query data stored in OpenSearch. Observability provides a unified experience for collecting and monitoring metrics, logs, and traces from common data sources. With data collection and monitoring in one place, you have full-stack, end-to-end observability of your entire infrastructure.\nAs of OpenSearch 2.7, you can manage your observability plugins with Observability Dashboards or Dashboard instead of the plugins page. This feature provides you: Instant access to installed plugins: The dashboard displays all installed plugins in one place. Improved efficiency: With a list of plugins readily available from a dashboard, you can enable, disable, update, or remove plugins in the OpenSearch Dashboards UI. Better troubleshooting: Viewing a list of plugins from a dashboard can help you quickly identify which plugins may be causing a problem. Enhanced security: With a list of plugins readily available from a dashboard, you can easily see if any outdated or vulnerable plugins are present and then quickly remove or update them, minimizing or avoiding security risks. Improved website performance: Viewing a list of plugins from a dashboard can help you identify any plugins that may be slowing down your website or causing performance issues.\nGet familiar with the basics of managing plugins from the Dashboard app in less than 20 seconds in the following video. Viewing a list of installed plugins\nTo view a list of installed plugins from the Dashboard app, follow these steps:\nFrom the OpenSearch Dashboards main menu, select Dashboard.\nView the list of items and select your plugin. Plugins are categorized automatically as the Observability Dashboard data type, which you can filter in order to concentrate on just what you want to see.\nAdding and removing plugins\nTo add a plugin from the Dashboard app, follow these steps:\nFrom the OpenSearch Dashboards main menu, select Dashboard.\nIn the Dashboards window, select Create &gt; Dashboard.\nIn the Create operational panel window, enter a name in the Name field and then select Create. The plugin is added to both the Observability app and the Dashboard app.\nYou can remove a plugin from the Dashboard app by selecting the edit icon under the Actions column and then selecting Delete.\nStaying updated about OpenSearch Dashboards plugins\nThe OpenSearch plugins repository on GitHub is a great way to keep track of and contribute to tasks, features, enhancements, and bugs. The OpenSearch Project team welcomes your input.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/dev-tools/index-dev/",
    "title": "Dev Tools",
    "content": "Dev Tools allows you to set up your OpenSearch Dashboards environment, identify and fix bugs, and customize your dashboards’ appearance and behavior.\nTo access the Dev Tools console, select Dev Tools in the menu on the OpenSearch Dashboards home page. You’ll see an interface like the one shown in the following image.",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/dev-tools/run-queries/",
    "title": "Running queries in the Dev Tools console",
    "content": "You can use the OpenSearch Dev Tools Console to send queries to OpenSearch.\nNavigating to the console\nTo open the console, select Dev Tools on the main OpenSearch Dashboards page: You can open the console from any other page by navigating to the main menu and selecting Management &gt; Dev Tools. Writing queries\nWrite your queries in the editor pane on the left side of the console: You can collapse and expand parts of your query by selecting the small triangles next to the line numbers.\nTo learn more about writing queries in OpenSearch domain-specific language (DSL), see Query DSL.\nComments\nUse # at the beginning of a line to write single-line comments.\nAutocomplete\nOpenSearch provides autocomplete suggestions for fields, indexes and their aliases, and templates. To configure autocomplete preferences, update them in Console Settings.\nSending the request\nTo send a query to OpenSearch, select the query by placing the cursor anywhere in the query text. Then choose the play icon () on the upper right of the request or press Ctrl/Cmd+Enter: OpenSearch displays the response in the response pane on the right side of the console: Working in the cURL and console formats\nThe console uses an easier syntax to format REST requests than the curl command.\nFor example, the following curl command runs a search query: curl -XGET http://localhost:9200/shakespeare/_search?pretty -H 'Content-Type: application/json' -d '\n{\n\"query\": {\n\"match\": {\n\"text_entry\": \"To be, or not to be\"\n}\n}\n}' The same query has a simpler syntax in the console format: GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"To be, or not to be\" } } } If you paste a curl command directly into the console, the command is automatically converted into the format the console uses.\nTo import a query in cURL format, select the query, select the wrench icon (), and choose Copy as cURL: Viewing documentation\nTo view the OpenSearch documentation, select the wrench icon () and choose Open documentation.\nAuto indenting\nTo use auto indent, select the queries that you want to format, select the wrench icon (), and choose Auto indent.\nAuto indenting a collapsed query expands it.\nAuto indenting a well-formatted query puts the request body on a single line. This is useful for working with bulk APIs.\nViewing your request history\nYou can view up to the 500 most recent requests that OpenSearch ran successfully. To view request history, select History from the top menu. If you select the request you want to view from the left pane, the query is shown in the right pane.\nTo copy the query into the editor pane, select the query text and then select Apply.\nTo clear the history, select Clear.\nUpdating the console settings\nTo update your preferences, select Settings from the top menu: Using keyboard shortcuts\nTo view all available keyboard shortcuts, select Help from the top menu.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Dev Tools"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/discover/dql/",
    "title": "Using Dashboards Query Language",
    "content": "Dashboards Query Language (DQL) is a simple text-based query language for filtering data in OpenSearch Dashboards. Similar to Query DSL, DQL uses an HTTP request body. For example, to display your site visitor data for a host in the United States, you would enter geo.dest:US in the search field, as shown in the following image. Before you can search data in Dashboards, you must index it. In OpenSearch, the basic unit of data is a JSON document. Within an index, OpenSearch identifies each document using a unique ID. To learn more about indexing in OpenSearch, see Index data.\nSearching with terms queries\nThe most basic query specifies the search term, for example: host:www.example.com To access an object’s nested field, list the complete path to the field separated by periods. For example, use the following path to retrieve the lat field in the coordinates object: coordinates.lat:43.7102 DQL supports leading and trailing wildcards, so you can search for any terms that match your pattern, for example: host.keyword:*.example.com/* To check whether a field exists or has any data, use a wildcard to see whether Dashboards returns any results,for example: host.keyword:* Searching with Boolean queries\nTo mix and match or combine multiple queries for more refined results, you can use the Boolean operators and, or, and not. DQL is not case sensitive, so AND and and are the same, for example: host.keyword:www.example.com and response.keyword:200 You also can use multiple Boolean operators in one query, for example: geo.dest:US or response.keyword:200 and host.keyword:www.example.com Remember that Boolean operators follow the logical precedence order of not, and, and or, so if you have an expression like the one in the preceding example, response.keyword:200 and host.keyword:www.example.com is evaluated first.\nTo avoid confusion, use parentheses to dictate the order in which you want to evaluate operands. If you want to evaluate geo.dest:US or response.keyword:200 first, you can use an expression like the following: (geo.dest:US or response.keyword:200) and host.keyword:www.example.com Querying dates and ranges\nDQL supports numeric inequalities, for example, bytes &gt;= 15 and memory &lt; 15.\nYou can use the same method to find a date before or after the date specified in the query. &gt; indicates a search for a date after the specified date, and &lt; returns dates before the specified date, for example, @timestamp &gt; \"2020-12-14T09:35:33.\nQuerying nested fields\nSearching a document with nested fields requires you to specify the full path of the field to be retrieved. In the following example document, the superheroes field has nested objects: { \"superheroes\":[ { \"hero-name\": \"Superman\", \"real-identity\": \"Clark Kent\", \"age\": 28 }, { \"hero-name\": \"Batman\", \"real-identity\": \"Bruce Wayne\", \"age\": 26 }, { \"hero-name\": \"Flash\", \"real-identity\": \"Barry Allen\", \"age\": 28 }, { \"hero-name\": \"Robin\", \"real-identity\": \"Dick Grayson\", \"age\": 15 }] } copy To retrieve documents that match a specific field using DQL, specify the field, for example: superheroes: {hero-name: Superman} copy To retrieve documents that match multiple fields, specify all the fields, for example: superheroes: {hero-name: Superman} and superheroes: {hero-name: Batman} copy You can combine multiple Boolean and range queries to create a more refined query, for example: superheroes: {hero-name: Superman and age &lt; 50} copy Querying doubly nested objects\nIf a document has doubly nested objects (objects nested inside other objects), retrieve a field value by specifying the full path to the field. In the following example document, the superheroes object is nested inside the justice-league object: { \"justice-league\": [ { \"superheroes\":[ { \"hero-name\": \"Superman\", \"real-identity\": \"Clark Kent\", \"age\": 28 }, { \"hero-name\": \"Batman\", \"real-identity\": \"Bruce Wayne\", \"age\": 26 }, { \"hero-name\": \"Flash\", \"real-identity\": \"Barry Allen\", \"age\": 28 }, { \"hero-name\": \"Robin\", \"real-identity\": \"Dick Grayson\", \"age\": 15 }] }] } copy The following image shows the query result using the example notation justice-league.superheroes: {hero-name:Superman}.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Exploring data"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/discover/index-discover/",
    "title": "Exploring data",
    "content": "Discover in OpenSearch Dashboards helps you extract insights and get value out of data assets across your organization. Discover enables you to: Explore data. You can explore, customize, and filter data as well as search data using Dashboards Query Language (DQL). Analyze data. You can analyze data, view individual documents, and create tables summarizing data contents. Visualize data. You can display findings from your saved searches in a single dashboard that combines different data visualization types.\nTry it: Exploring sample data with Discover\nThis tutorial shows you how to use Discover to analyze and understand a sample dataset. At the end of this tutorial, you should be ready to use Discover with your own data.\nBefore starting this tutorial, make sure you’ve added the Sample flight data. See Quickstart guide for OpenSearch Dashboards for information about how to get started.\nSetting up data\nWatch the following short video or start with the tutorial steps to learn how to set up a sample dataset in Discover. Verify access to OpenSearch Dashboards by connecting to http://localhost:5601 from a browser. The default username and password are admin.\nOn the Home page, choose Discover in the navigation pane.\nOn the index pattern toolbar, select the opensearch_dashboards_sample_data_flights dataset.\nOn the time filter toolbar, choose the calendar icon and then change the time range to Last 7 days.\nExploring the data fields\nIn the Discover panel, you’ll see a table that shows all the documents that match your search. The table includes a list of data fields that are available in the document table, as shown in the following image. Follow these steps to explore the data fields:\nView the list of Available fields.\nChoose Cancelled to view the values ( true and false).\nChoose the plus (+) sign to add the field to the document table. The field will be automatically added to Selected fields and the document table.\nSelect FlightDelay from the Available fields list, and then choose the plus (+) sign to add the field to the document table.\nOptional: Rearrange the table columns by selecting the table header and then choosing Move left or Move right.\nSearching data\nYou can use the search toolbar or enter a DQL query in the DevTools console to search data in Dashboards, as shown in the following image. The search toolbar is best for basic queries, such as searching by a field name. DQL is best for complex queries, such as searching data using a term, string, Boolean, date, range, or nested query. Follow these steps to search data:\nIn the search toolbar, enter the Boolean query. For example, enter FlightDelay:true AND FlightDelayMin &gt;= 60 to search the data for flights delayed by 60 minutes or more.\nChoose Update.\nOptional: Choose the arrow ( &gt;) in a table row to expand the row and view the document table details.\nFiltering data\nFilters allow you to refine sets of documents to subsets of those documents. For example, you can filter data to include or exclude certain fields, as shown in the following image. Follow these steps to filter data:\nIn the filter bar, choose Add filter.\nSelect options from the Field, Operator, and Value dropdown lists. For example, Cancelled, is, and true.\nChoose Save.\nTo remove the filter, choose the close icon (x) next to the filter name.\nOptional: Add more filters to further explore the data.\nAnalyzing data in the document table\nYou can view the document table fields to better understand the data and gather insights for more informed decision-making:\nChoose the arrow icon (&gt;) to expand a table row.\nView the fields and details.\nSwitch between the Table and JSON tabs to view the different formats, as shown in the following image. Saving the search\nSaving a search saves the query text, filters, and current data view. To save your search to use it later, generate a report, or build visualizations and dashboards:\nChoose the save icon in the toolbar.\nGive the search a title, and then choose Save.\nChoose the save icon to access the saved search, as shown in the following image. Visualizing the search\nYou can quickly visualize an aggregated field from Discover:\nFrom the Available fields list, select FlightDelayType and then choose Visualize, as shown in the following image. Dashboards creates a visualization for this field, which in this case is a basic bar chart, as shown in the following image.",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/discover/multi-data-sources/",
    "title": "Adding multiple data sources",
    "content": "OpenSearch Dashboards allows you to dynamically manage data sources, create index patterns based on those data sources, run queries against a specific data source, and combine visualizations in one dashboard.\nIn this tutorial we provide the steps for enabling the data_source setting in Dashboards; adding credentials, data source connections, and index patterns; and combining visualizations in a single dashboard.\nEnabling the multiple data sources feature\nThis tutorial uses a preconfigured data source and index pattern, and you aren’t required to configure settings. However, you’ll need to enable the data_source setting in the configuration file before before getting started with exploring this feature.\nModifying the YAML file settings for multiple data sources\nDashboards is configured in the cluster settings, and the multiple data sources feature is disabled by default. To enable it, you need to edit the configuration in opensearch_dashboards.yml and then restart the cluster.\nTo enable the feature:\nNavigate to your Dashboards home directory; for example, in Docker, /usr/share/opensearch-dashboards.\nOpen your local copy of the Dashboards configuration file, opensearch_dashboards.yml. If you don’t have a copy, get one from GitHub: opensearch_dashboards.yml.\nSet data_source.enabled: false to data_source.enabled: true and save the configuration.\nRestart the Dashboards container.\nVerify the feature configuration settings were created and configured properly by connecting to Dashboards through http://localhost:5601 and viewing the Stack Management console. Data Sources Experimental will appear in the sidebar. Alternatively, you can open on http://localhost:5601/app/management/opensearch-dashboards/dataSources.\nCreating a data source connection\nA data source connection specifies the parameters needed to connect to a data source. These parameters form a connection string for the data source. In Dashboards, you can add new data source connections or edit existing connections.\nTo create a new data source connection:\nOpen Dashboards. If you’re not running the Security plugin, go to http://localhost:5601. If you’re running the Security plugin, go to https://localhost:5601 and log in with the username admin and password admin.\nUnder Management in the OpenSearch Dashboards main menu, choose Stack Management, Data Sources Experimental, Data Sources, and then choose Create data source connection, as shown in the following image. Add information to each field to configure Connection Details, Endpoint URL, and Authentication Method, as shown in the following image. For this tutorial, the Endpoint URL is http://localhost:5601/app/management/opensearch-dashboards/dataSources. In Connection Details, enter a title for the connection and the endpoint URL used to connect to the data source. A description of the connection is optional.\nFor Authentication Method, first select the type of authentication: No authentication: No authentication is used to connect to the data source. Username &amp; Password: A basic username and password are used to connect to the data source. AWS SigV4: An AWS Signature Version 4 authenticating request is used to connect to the data source. AWS SigV4 requires an access key ID and a secret access key. First specify the Region, and then enter the Access Key and Secret Key for authorization. For information on available Regions for AWS accounts, see Available Regions. For more on SigV4 authentication requests, see Authenticating Requests (AWS Signature Version 4).\nWhen you select the authentication method, the applicable fields appear for the selected method. Enter the required details.\nAfter you have entered the appropriate details in all of the required fields, the Test connection and Create data source connection buttons become active. You can choose Test connection to confirm that the connection is valid.\nChoose Create data source connection to save your settings. The connection is created. The active window returns to the Data Sources main page, and the new connection appears in the list of data sources. You can also delete the data source connection from this page by selecting the check box to the left of the title and then choosing Delete 1 connection to the right of the search bar. Selecting multiple check boxes for multiple connections is also supported.\nEditing and updating a data source connection\nTo make changes to the data source connection, select a connection in the list on the Data Sources main page. The connection details window opens, as shown in the following image. To make changes to Connection Details, edit one or both of the Title and Description fields and choose Save changes in the lower-right corner of the screen. You can also cancel changes here. To change the Authentication Method, choose a different authentication method, enter your credentials if applicable, and then choose Save changes in the lower-right corner of the screen. The changes are saved.\nWhen Username &amp; Password is the selected authentication method, you can update the password by choosing Update stored password next to the Password field. In the pop-up window, enter a a new password in the first field and then enter it again in the second field to confirm. Choose Update stored password in the pop-up window. The new password is saved. Choose Test connection in the upper-right corner of the screen to confirm that the connection is valid.\nWhen AWS SigV4 is the selected authentication method, you can update the credentials by choosing Update stored AWS credential. In the pop-up window, enter a new access key in the first field and a new secret key in the second field. Choose Update stored AWS credential in the pop-up window. The new credentials are saved. Choose Test connection in the upper-right corner of the screen to confirm that the connection is valid.\nTo delete the data source connection, choose the red trash can icon in the upper-right corner of the screen.\nCreating an index pattern\nIndex patterns allow you to access the OpenSearch data that you want to explore. An index pattern selects the data to use and allows you to define the field properties. Learn how to load your own data and create an index pattern following these steps. This tutorial uses the preconfigured index pattern opensearch_dashboards_sample_data_ecommerce Default.\nIn the Dashboards console, choose Index Patterns &gt; Create index pattern, as shown in the following image. Choose Use external data source connection.\nStart typing in the Search data sources field to search for the data source you created earlier and then select the data source and Next step, as shown in the following image. Add an Index pattern name to define the index pattern and then choose Next step, as shown in the following image. Select an option for the Time field and then choose Create index pattern, as shown in the following image. Searching data\nBefore you start searching for data, set up the time filter. The sample index pattern used for this tutorial contains time-based data. You can set a time filter that displays only the data within a specified time range, and you can choose the time filter to change the time range or select a specific time range in the histogram.\nAdjusting the time filter\nTo adjust the time filter:\nIn the Dashboards console, choose Discover and confirm that the index pattern being used is opensearch_dashboards_sample_data_ecommerce.\nChoose the calendar icon to change the time field. The default is Last 15 minutes.\nChange the time field to Last 7 days and choose Refresh, as shown in the following image. To set the start and end times, choose the bar next to the time filter. In the popup, select Absolute, Relative, or Now and then specify the required options, as shown in the following image. Selecting a time range from the histogram\nTo select a time range for the histogram, you can do one of the following:\nSelect the bar that represents the time range you want to zoom in on.\nSelect the bar and drag to view a specific time range. You must start the selection with the cursor over the background of the chart (the cursor changes to a plus sign when you hover over a valid start point).\nSelect the dropdown and then select an interval.\nThe following image shows a date histogram with an interval dropdown list. Selecting multiple data sources in the Dev Tools console\nSelecting multiple data sources in the Dev Tools console allows you to work with a broader range of data and gain deeper insights into your code and applications. You can follow these steps:\nOpen opensearch_dashboards.yml in the editor of your choice.\nSet data_source.enabled: to true, as shown in the following example: # Set the value of this setting to true to enable the experimental multiple data source\n# support feature. Use with caution.\ndata_source.enabled: true Connect to OpenSearch Dashboards and select Dev Tools in the menu.\nEnter the following query in the editor pane of the Console and then select the play button: GET /_cat/indices From the DataSource dropdown menu, select a data source and then query the source.\nRepeat the preceding steps for each data source you want to select.\nFor an overview of this process, watch the following 15-second video. Creating data visualizations for a dashboard\nFollow these steps to learn how to create data visualizations for a dashboard:\nIn the Dashboards console, choose Visualize &gt; Create visualization.\nSelect the visualization type. For this tutorial, choose Line.\nSelect a source. For this tutorial, choose the index pattern opensearch_dashboards_sample_data_ecommerce.\nUnder Buckets, choose Add &gt; X-axis.\nIn the Aggregation field, choose Date Histogram and then choose Update.\nOptional: Choose Save and add the file name. This tutorial uses preconfigured data visualizations, so you can’t save the file for this tutorial.\nConnecting visualizations in a single dashboard\nFollow these steps to connect your visualizations in a single dashboard:\nIn the Dashboards console, choose Dashboard &gt; Create dashboard.\nChoose Add an existing and then select the data you want to add.\nChoose Save and add the dashboard name in the Title field. This tutorial uses preconfigured dashboards, so you won’t be able to save your dashboard.\nClick on the white space left of Add panels to view the visualizations in a single dashboard.\nYour dashboard might look like the one in the following image. You have now explored the data sources experimental feature. To provide feedback on how this feature can be improved ahead of its release for production use, comment in the OpenSearch forum.\nUnderstanding feature limitations\nThe following limitations apply to this experimental feature:\nThe multiple data sources feature is supported for index-pattern-based visualizations only.\nThe visualization types Time Series Visual Builder (TSVB), Vega and Vega-Lite, and timeline are not supported.\nExternal plugins, such as Gantt chart, and non-visualization plugins, such as the developer console, are not supported.\nRelated topics OpenSearch Forum",
    "ancestors": [
      "OpenSearch Dashboards",
      "Exploring data"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/discover/time-filter/",
    "title": "Setting the time filter",
    "content": "You can change the time range to display dashboard data over minutes, hours, days, weeks, months, or years.\nThe default time range is Last 15 minutes. You can change the time range at the dashboard level or under Stack Management &gt; Advanced Settings &gt; Time filter defaults.\nTo change the time range at the dashboard level, perform the following steps:\nFrom an OpenSearch Dashboards application (Discover, Dashboard, or Visualize), select the time clock or calendar icon.\nSelect one of the time filter options, as shown in the following image: Quick select: Choose a time based on the last or next number of seconds, minutes, hours, days, or another time unit. Commonly used: Choose a common time range like Today, Last 7 days, or Last 30 days. Recently used date ranges: Select a previously used time range. Refresh every: Set an automatic refresh period. Choose Show dates to set start and end times, and then select anywhere inside the toolbar to access the time filter pop-up window, as shown in the following image. Select Absolute, Relative, or Now and specify ranges.\nChoose Update to apply changes, as shown in the following image.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Exploring data"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/im-dashboards/component-templates/",
    "title": "Component templates",
    "content": "Introduced 2.7\nComponent templates allow you to create a single index pattern that matches multiple indexes. This pattern can include wildcards or regular expressions, enabling you to apply the same setting or mapping to multiple indexes simultaneously.\nUsing them with index templates can provide a powerful tool for managing large volumes of data. You can create an index template that defines the basic structure and settings of your indexes and then use the component templates to apply the settings to all indexes that match a specific pattern or set of criteria.\nYou can create component templates using the Index Management UI. The UI maximizes ease of use for common indexing and data stream administrative operations such as create, read, update, delete (CRUD) and mapping indexes; CRUD and mapping aliases; reindexing; and open/close, shrink, and split indexes, along with the monitoring of actions and logging of audit records.\nThe following GIF demonstrates creating a component template. Prerequisites\nThis tutorial is intended for admin users who manage OpenSearch clusters and are familiar with index management in OpenSearch Dashboards.\nKey terms\nIt’s helpful to understand the following terms before starting this tutorial: Component template refers to a reusable building block with settings, mappings, and aliases that can be attached to an index template. Index template refers to a predefined structure used to organize and store data in a database or search index.\nCreating component templates using the Index Management UI\nYou can use predefined OpenSearch Dashboards component templates or customize your own, either by creating original templates or by modifying existing templates. Predefined component templates include preconfigured charts, tables, and graphs and are a good starting point for users who are new to OpenSearch Dashboards. Alternatively, customized template components provide you with options for tailoring reports and visualizations that meet your specific requirements and preferences.\nTo create template components using the UI, follow these steps:\nOn the OpenSearch Dashboards main page, select Index Management in the navigation menu.\nIn the Index Management window, select Templates &gt; Component templates.\nSelect Create and then define the component template settings.\nTo configure aliases, settings, and mappings, toggle Use configuration, as shown in the following image. Enter details in the aliases, settings, and mappings fields.\nSelect Create component template.\nWhen you create component templates, those templates apply only to new index templates that you create and not to existing index templates.\nAssociating component templates with index templates\nTo associate a component template with an index template, follow these steps:\nIn the Index Management navigation menu, select Templates.\nIn the Templates window, select Create template.\nSelect Component template as the method for defining your template.\nIn the Component template pane, select Associate component template, as shown in the following image. In the Associate component template pop-up window, select the component templates that you want to associate with your index template.\nSelect Associate.\nSelect Preview template to view the template settings.\nSelect Create template.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Index management in Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/im-dashboards/datastream/",
    "title": "Data streams",
    "content": "Introduced 2.6\nIn OpenSearch Dashboards, the Index Management application allows you to view and manage data streams as shown in the following image. Viewing a data stream\nTo view a data stream and its health status, choose Data streams under Index management as shown in the following image. The following are the three data stream health statuses:\nGreen: All primary and replica shards are assigned.\nYellow: At least one replica shard is not assigned.\nRed: At least one primary shard is not assigned.\nCreating a data stream\nTo create a data stream, perform the following steps:\nUnder Index Management, choose Data streams.\nChoose Create data stream.\nEnter a name for the data stream under Data stream name.\nEnsure that you have a matching index template. This will be populated under Matching index template, as shown in the following image. The Inherited settings from template and Index alias sections are read-only, and display the backing indexes that are contained in the data stream.\nThe number of primary shards, number of replicas, and the refresh interval are inherited from the template, as shown in the following image. Choose Create data stream.\nDeleting a data stream\nTo delete a data stream, perform the following steps:\nUnder Index Management, choose Data streams.\nSelect the data stream that you want to delete.\nChoose Actions, and then choose Delete.\nRolling over a data stream\nTo perform a rollover operation on a data stream, perform the following steps:\nUnder Index Management, choose Data streams.\nChoose Actions, and then choose Roll over, as shown in the following image. Under Configure source, select the source data stream on which you want to perform the rollover operation.\nChoose Roll over, as shown in the following image. Force merging data streams\nTo perform a force merge operation on two or more indexes, perform the following steps:\nUnder Index Management, choose Data streams.\nSelect the data streams on which you want to perform the force merge operation.\nChoose Actions, and then choose Force merge.\nUnder Configure source index, specify the data streams you want to force merge.\nOptionally, under Advanced settings you can to choose to Flush indices or Only expunge delete and then specify the Max number of segments to merge to as shown in the following image.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Index management in Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/im-dashboards/forcemerge/",
    "title": "Force merge",
    "content": "Introduced 2.6\nOpenSearch Dashboards allows you to perform a force merge operation on two or more indexes with Index Management.\nForce merging indexes\nTo perform a force merge operation on two or more indexes, perform the following steps:\nUnder Index Management, choose Indices.\nSelect the indexes you want to force merge.\nChoose Actions, and then choose Force merge, as shown in the following image. Under Configure source index, specify the indexes you want to force merge.\nOptionally, under Advanced settings you can to choose to Flush indices or Only expunge delete and then specify the Max number of segments to merge to as shown in the following image. Force merging data streams\nTo perform a force merge operation on two or more indexes, perform the following steps:\nUnder Index Management, choose Data streams.\nSelect the data streams you want to force merge.\nChoose Actions, and then choose Force merge.\nUnder Configure source index, specify the data streams you want to force merge.\nOptionally, under Advanced settings you can to choose to Flush indices or Only expunge delete and then specify the Max number of segments to merge to as shown in the following image.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Index management in Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/im-dashboards/index-management/",
    "title": "Indexes",
    "content": "Introduced 2.5\nIn the Index Management section, you can perform the operations available in the Index API.\nIndex policies Policies are configurations that define the possible states of an index, the actions to perform when an index enters a given state, and the conditions that must be met to transition between states: States: The possible states of an index, including the default state for new indexes. For example, you might name your states hot, warm, or delete. For more information, see States. Actions: Any actions that you want the plugin to take when an index enters a given state, such as performing a rollover. For more information, see Actions. Transitions: The conditions that must be met for an index to move into a new state. For example, if an index is more than 8 weeks old, you might want to move it to the delete state. For more information, see Transitions.\nYou can also upload a JSON document to specify an index policy.\nYou have complete flexibility in designing your policies. You can create any state, transition to any other state, and specify any number of actions in each state.\nTo attach policies to indexes, perform the following steps:\nUnder Index Management, choose Index policies.\nSelect the index or indexes to which you want to attach your policy.\nChoose the Apply policy button.\nFrom the Policy ID menu, select the policy that you created.\nView the preview of your policy.\n(Optional): Specify a rollover alias if your policy includes a rollover operation. Make sure that the alias already exists. For more information about the rollover operation, see rollover.\nChoose the Apply button.\nAfter you attach a policy to an index, Index State Management (ISM) creates a job that runs every 5 minutes by default to perform policy actions, check conditions, and transition the index into different states. To change the default time interval for this job, see Settings.\nPolicy jobs don’t run if the cluster state is red.\nManaged indexes\nTo attach policies to indexes, perform the following steps:\nUnder Index Management, choose Manage Indices.\nSelect the index or indexes to which you want to attach your policy.\nChoose the Change policy button.\nChoose the Apply policy button.\nIndexes\nThe Indices section displays a list of indexes in your OpenSearch cluster. For each index, you can see its health status ( green, yellow, or red), policy (if the index is managed by a policy), status, total size, primary sizes, total documents, deleted documents, primaries, and replicas.\nThe following are the three index health statuses:\nGreen: All primary and replica shards are assigned.\nYellow: At least one replica shard is not assigned.\nRed: At least one primary shard is not assigned.\nCreating an index\nWhile you can create an index by using a document as a base, you can also create an empty index for later use.\nTo create an index, select the Create Index button located under the Indices section of Index Management. Then define the index by setting the following parameters:\nIndex name\nNumber of primary shards\nNumber of replicas\nRefresh interval\nYou can also add fields and objects using either the visual editor or the JSON editor.\nThe Advanced settings allow you to upload a JSON configuration.\nApplying a policy\nIf you analyze time-series data, you likely want to prioritize new data over old data. You might periodically perform certain operations on older indexes, such as reducing replica count or deleting them. ISM is a plugin that lets you automate these periodic administrative operations by triggering them based on changes in the index age, index size, or number of documents. You can define policies that automatically handle index rollovers or deletions to fit your use case.\nFor example, you can define a policy that moves your index into a read_only state after 30 days and then deletes it after a set period of 90 days. You can also set up the policy to send you a notification message when the index is deleted.\nYou might want to perform an index rollover after a certain amount of time or run a force_merge operation on an index during off-peak hours to improve search performance during peak hours.\nTo apply a policy, select the index to which you want to apply the policy from the Indices list under Index Management. Then select the Actions button and select Apply policy from the dropdown list as shown in the following image. Closing an index\nThe close index operation closes an index. Once an index is closed, you cannot add data to it or search for any data within the index.\nTo close an index, select the index you want to close from the Indices list under Index Management. Then select the Actions button and select Close from the dropdown list.\nOpening an index\nThe open index operation opens a closed index, letting you add data to it or search for data within the index.\nTo open an index, select the index you want to open from the Indices list under Index Management. Then select the Actions button and select Open from the dropdown list.\nReindexing an index\nThe reindex operation lets you copy all of your data or a subset of data from a source index into a destination index.\nTo reindex an index, select the index from the Indices list under Index Management. Then select the Actions button and select Reindex from the dropdown list as shown in the following image. Shrinking an index\nThe shrink index operation copies all of the data in an existing index into a new index with fewer primary shards.\nTo shrink an index, select the index you want to shrink from the Indices list under Index Management. Then choose the Actions button and choose Shrink from the dropdown list as shown in the following image. Splitting an index\nThe split index operation splits an existing read-only index into a new index, splitting each primary shard into a number of primary shards in the new index.\nTo split an index, select the index you want to split from the Indices list under Index Management. Then choose the Actions button and choose Split from the dropdown list as shown in the following image. Deleting an index\nIf you no longer need an index, you can use the delete index operation to delete it.\nTo delete an index, select the index you want to delete from the Indices list under Index Management. Then select the Actions button and select Delete from the dropdown list.\nTemplates Index templates let you initialize new indexes with predefined mappings and settings. For example, if you continuously index log data, you can define an index template so that all of the indexes have the same number of shards and replicas as shown in the following image. Creating a template\nTo create a template, choose the Create template button on the Templates page under Index Management.\nNext, define the template:\nEnter the template name.\nSelect the template type.\nSpecify any index patterns you would like to use.\nSet the priority of the template.\nSelect an index alias.\nSet the number of primary shards.\nSet the number of replicas.\nSet the refresh intervals.\nAdd fields and objects for your index mapping using either the visual editor or the JSON editor.\nUnder Advanced Settings you can specify advanced index settings with a comma-delimited list as shown in the following image. Editing a template\nTo edit a template, select the template you want to edit from the list of templates. Next, select the Actions dropdown list and select the Edit option.\nDeleting a template\nTo delete a template, select the template you want to delete from the list of templates. Next, select the Actions dropdown list and select the Delete option.\nAliases\nAn alias is a virtual index name that can point to one or more indexes. If your data is spread across multiple indexes, rather than keeping track of which indexes to query, you can create an alias and query it instead as shown in the following image. To create an alias, perform the following steps:\nChoose the Create Alias button on the Aliases page under Index Management.\nSpecify the alias name.\nEnter the index, or index patterns, to be included in the alias.\nChoose Create alias as shown in the following image. To edit an alias, perform the following steps:\nSelect the alias you want to edit.\nChoose the Actions button.\nChoose Edit from the dropdown list.\nTo delete an alias, perform the following steps:\nSelect the alias you want to edit.\nChoose the Actions button.\nChoose Delete from the dropdown list.\nRollup jobs\nThe Rollup Jobs section under Index Management allows you to create or update index rollup jobs.\nTo create a rollup job, perform the following steps:\nChoose the Create rollup job button on the Rollup Jobs page under Index Management.\nSet the name, source index, and target index.\nChoose Next.\nSet the timestamp field and interval type.\nOptionally, set additional aggregations and metrics.\nChoose Next.\nUnder Schedule, check or uncheck Enable job by default.\nSet the Continuous, Execution frequency, Rollup interval, and Pages per execution settings.\nAdditionally, you can set an execution delay.\nChoose Next.\nReview the settings for the rollup job and choose Create.\nYou can also enable and disable rollup jobs by choosing the corresponding buttons on the Rollup Jobs page.\nTransform jobs\nYou can create, start, stop, and complete operations with transform jobs.\nTo create a transform job, perform the following steps:\nChoose the Create transform job button on the Transform Jobs page under Index Management.\nSet the name, source index, and target index.\nChoose Next.\nSelect the fields to transform. From the table, select a field you want to transform by choosing + next to the field name.\nChoose Next.\nCheck or uncheck Job enabled by default.\nSet the transform execution interval and whether the schedule is continuous.\nOptionally, set pages per execution under the Advanced dropdown list.\nChoose Next.\nReview the settings for the rollup job and choose Create.\nYou can also enable and disable rollup jobs by choosing the corresponding buttons on the Transform Jobs page.\nLong-running operation status check\nCertain index operations take additional time to complete (usually more than 30 seconds, but up to tens of minutes or hours). This is tracked in the index status column on the Indices page.\nYou can check the status of the reindex, shrink, and split operations because they are one-time, non-recursive operations.\nSecurity integration\nPermission control is managed with existing permissions or action groups that are enforced at the API level. There is currently no UI-level permission control. Users with permission to access the ISM plugin are able to view new pages. They can also make changes if they have permission to run the related APIs.\nError handling\nSimilar to API calls, if the operation fails immediately, you will be notified with an error message. However, if it is a long-running operation, you will be notified of the failure at the time of failure, or you can check the index status on the Indices page.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Index management in Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/im-dashboards/index/",
    "title": "Index management in Dashboards",
    "content": "Introduced 2.5\nPreviously, users relied on REST APIs or YAML configurations for basic administrative operations and interventions. This release takes the first step toward a unified administration panel in OpenSearch Dashboards with the launch of several index management UI enhancements. The new interface provides a more user-friendly way to run common indexing and data stream operations. Now you can perform create, read, update, and delete (CRUD) and mapping operations for indexes, index templates, and aliases through the UI. Additionally, you can open, close, reindex, shrink, and split indexes. The UI runs index status and data validation before submitting requests and lets you compare changes with previously saved settings before making updates.",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/im-dashboards/rollover/",
    "title": "Rollover",
    "content": "Introduced 2.6\nOpenSearch Dashboards allows you to perform an index rollover operation with Index Management.\nData streams\nTo perform a rollover operation on a data stream, perform the following steps:\nUnder Index Management, choose Data streams.\nChoose Actions, and then choose Roll over, as shown in the following image. Under Configure source, select the source data stream on which you want to perform the rollover operation.\nChoose Roll over, as shown in the following image. Aliases\nTo perform a rollover operation on an alias, perform the following steps:\nUnder Index Management, choose Aliases.\nChoose Actions, and then choose Roll over, as shown in the following image. Under Configure source, select the source alias on which you want to perform the rollover operation.\nIf the alias does not contain a write index, you are prompted to assign a write index, as shown in the following image. Under Configure a new rollover index and on the Define index pane, specify an index name and an optional index alias.\nUnder Index settings, specify the number of primary shards, the number of replicas, and the refresh interval, as shown in the following image. Choose Roll over.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Index management in Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/quickstart-dashboards/",
    "title": "Quickstart guide for OpenSearch Dashboards",
    "content": "This quickstart guide covers the core concepts that you need to understand to get started with OpenSearch Dashboards. You’ll learn how to:\nAdd sample data.\nExplore and inspect data.\nVisualize data.\nBefore you get started, make sure you’ve installed OpenSearch and OpenSearch Dashboards. For information on installation and configuration, see Install and configure OpenSearch and Install and configure OpenSearch Dashboards.\nSample datasets come with visualizations, dashboards, and other tools to help you explore Dashboards before you add your own data. To add sample data, perform the following steps:\nVerify access to OpenSearch Dashboards by connecting to http://localhost:5601 from a browser. The default username and password are admin.\nOn the OpenSearch Dashboards Home page, choose Add sample data.\nChoose Add data to add the datasets, as shown in the following image. In Discover, you can:\nChoose data to explore, set a time range for that data, search it using Dashboards Query Language (DQL), and filter the results.\nExplore the data, view individual documents, and create tables summarizing the data’s contents.\nVisualize your findings.\nTry it: Getting familiar with Discover\nOn the OpenSearch Dashboards Home page, choose Discover.\nChange the time filter to Last 7 days, as shown in the following image. Search using the DQL query FlightDelay:true AND DestCountry: US AND FlightDelayMin &gt;= 60 and then choose Update. You should see results for US-bound flights delayed by 60 minutes or more, as shown in the following image. To filter data, choose Add filter and then select an Available field. For example, select FlightDelayType, is, and Weather delay from the Field, Operator, and Value dropdown lists, as shown in the following image. Raw data can be difficult to comprehend and use. Data visualizations help you prepare and present data in a visual form. In Dashboard you can:\nDisplay data in a single view.\nBuild dynamic dashboards.\nCreate and share reports.\nEmbed analytics to differentiate your applications.\nTry it: Getting familiar with Dashboard\nOn the OpenSearch Dashboards Home page, choose Dashboard.\nChoose [Flights] Global Flight Data in the Dashboards window, as shown in the following image. To add panels to the dashboard, choose Edit and then Add from the toolbar.\nIn the Add panels window, choose the existing panel [Flights] Delay Buckets. You’ll see a pop-up window on the lower right confirming that you’ve added the panel.\nSelect x to close the Add panels window.\nView the added panel [Flights] Delay Buckets, which is added as the last panel on the dashboard, as shown in the following image. Try it: Creating a visualization panel\nContinuing with the preceding dashboard, you’ll create a bar chart comparing the number of canceled flights and delayed flights to delay type and then add the panel to the dashboard:\nChange the default time range from 24 hours to Last 7 days.\nIn the toolbar, choose Edit, then Create new.\nSelect VisBuilder in the New Visualizations window.\nIn the Data Source dropdown list, choose opensearch_dashboards_sample_data_flights.\nDrag the fields Cancelled and FlightDelay to the y-axis column.\nDrag the field FlightDelayType to the x-axis column.\nChoose Save and name the visualization in the Title field.\nChoose Save and return. The following bar chart is added as the last panel on the dashboard, as shown in the following image. Interactive dashboards allow you analyze data in more depth and filter it in several ways. In Dashboards, you can interact directly with data on a dashboard by using dashboard-level filters. For example, continuing with the preceding dashboard, you can filter to show delays and cancellations for a specific airline.\nTry it: Interacting with the sample flight data\nOn the [Flights] Airline Carrier panel, choose OpenSearch-Air. The dashboard updates automatically.\nChoose Save to save the customized dashboard.\nAlternatively, you can apply filters using the dashboard toolbar:\nIn the dashboard toolbar, choose Add filter.\nFrom the Field, Operator, and Value dropdown lists, choose Carrier, is, and OpenSearch-Air, respectively, as shown in the following image. Choose Save. The dashboard updates automatically, and the result is the dashboard shown in the following image. Visualize data. To learn more about data visualizations in OpenSearch Dashboards, see Building data visualizations. Create dashboards. To learn more about creating dashboards in OpenSearch Dashboards, see Creating dashboards. Explore data. To learn more about exploring data in OpenSearch Dashboards, see Exploring data.",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/reporting-cli/rep-cli-create/",
    "title": "Creating and requesting a visualization report",
    "content": "First, you need to get the URL for the visualization that you want to download as an image file or PDF.\nTo generate a visualization report, you need to specify the Dashboards URL.\nOpen the visualization for which you want to generate a report, and select Share &gt; Permalinks &gt; Generate link as Shapshot &gt; Short URL &gt; Copy link, as shown in the following image. You will need to add the URL with the -u argument when you request the report in the CLI.\nExample: Requesting a PNG file\nThe following command requests a report in PNG format with basic authentication and sends the report to an email address using Amazon SES: opensearch-reporting-cli -u https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d -a basic -c admin:Test@1234 -e ses -s &lt;email address&gt; -r &lt;email address&gt; -f png Example: Requesting a PDF file\nThe following command requests a PDF file and specifies the recipient’s email address: opensearch-reporting-cli -u https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d -a basic -c admin:Test@1234 -e ses -s &lt;email address&gt; -r &lt;email address&gt; -f pdf Upon success, the file will be sent to the specified email address. The following image shows an example PDF report. Example: Requesting a CSV file\nThe following command generates a report that contains all table content in CSV format and sends the report to an email address using Amazon SES transport: opensearch-reporting-cli -u https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d -f csv -a basic -c admin:Test@1234 -e ses -s &lt;email address&gt; -r &lt;email address&gt; Upon success, the email will be sent to the specified email address with the CSV file attached.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Creating reports with the Reporting CLI"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/reporting-cli/rep-cli-cron/",
    "title": "Scheduling reports with the cron utility",
    "content": "You can use the cron command-line utility to initiate a report request with the Reporting CLI that runs periodically at any date or time interval. Follow the cron expression syntax to specify the date and time that precedes the command that you want to initiate.\nTo learn about the cron expression syntax, see Cron expression reference. To get help with cron, open the man page by running the following command: man cron Prerequisites\nYou need a machine with cron installed.\nYou need to install the Reporting CLI. See Downloading and installing the Reporting CLI tool Specifying the report details\nOpen the crontab editor by running the following command: crontab -e In the crontab editor, enter the report request. The following example shows a cron report that runs every day at 8:00 AM: 0 8 * * * opensearch-reporting-cli -u https://playground.opensearch.org/app/dashboards#/view/084aed50-6f48-11ed-a3d5-1ddbf0afc873 -e ses -s &lt;sender_email&gt; -r &lt;recipient_email&gt;",
    "ancestors": [
      "OpenSearch Dashboards",
      "Creating reports with the Reporting CLI"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/reporting-cli/rep-cli-env-var/",
    "title": "Using environment variables with the Reporting CLI",
    "content": "Instead of explicitly providing values in the command line, you can save them as environment variables. The Reporting CLI reads environment variables from the current directory inside the project.\nTo set the environment variables in Linux, use the following command: export NAME=VALUE Each line should use the format NAME=VALUE.\nEach line that starts with a hashtag (#) is considered to be a comment.\nQuotation marks (“) don’t get any special handling.\nValues from the command line argument have higher priority than the environment file. For example, if you add the file name as test in the.env file and also add the --filename report command option, the generated report’s name will be report.\nExample: Requesting a PNG report with environment variables set\nThe following command requests a report with basic authentication in PNG format: opensearch-reporting-cli --url https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d --format png --auth basic --credentials admin:admin Upon success, the report will download to the current directory.\nUsing Amazon SES to request an email with a report attachment\nTo use Amazon SES as the email transport mechanism, the following prerequisites apply:\nThe sender’s email address must be verified by Amazon SES. The AWS Command Line Interface (AWS CLI) is required to interact with Amazon SES. To configure basic settings used by the AWS CLI, see Quick configuration with aws configure in the AWS Command Line Interface user guide.\nAmazon SES transport requires the ses:SendRawEmail role: { \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ses:SendRawEmail\", \"Resource\": \"*\" }] } The following command requests an email with the report attached: opensearch-reporting-cli --url https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d --transport ses --from &lt;sender_email_id&gt; --to &lt;recipient_email_id&gt; The following command uses default values for all other options. You can also set OPENSEARCH_FROM, OPENSEARCH_TO, and OPENSEARCH_TRANSPORT in your.env file and use the following command: opensearch-reporting-cli --url https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d To modify the body of your email, you can edit the index.hbs file.\nExample: Sending a report to an email address with SMTP\nTo send a report to an email address with SMTP transport, you need to set the options OPENSEARCH_SMTP_HOST, OPENSEARCH_SMTP_PORT, OPENSEARCH_SMTP_USER, OPENSEARCH_SMTP_PASSWORD, and OPENSEARCH_SMTP_SECURE in your.env file.\nOnce the transport options are set in your.env file, you can send the email using the following command: opensearch-reporting-cli --url https://localhost:5601/app/dashboards#/view/7adfa750-4c81-11e8-b3d7-01146121b73d --transport smtp --from &lt;sender_email_id&gt; --to &lt;recipient_email_id&gt; You can choose to set options using either your.env file or the command line argument values in any combination. Make sure to specify all required values to avoid errors.\nTo modify the body of your email, you can edit the index.hbs file.\nLimitations\nThe following limitations apply to environment variable usage with the Reporting CLI:\nSupported platforms are Windows x86, Windows x64, Mac Intel, Mac ARM, Linux x86, and Linux x64.\nFor any other platform, users can take advantage of the CHROMIUM_PATH environment variable to use custom Chromium.\nIf a URL contains an exclamation point (!), then the history expansion needs to be disabled temporarily. Depending on which shell you are using, you can disable history expansion using one of the following commands:\nFor bash, use set +H.\nFor zsh, use setopt nobanghist.\nAlternatively, you can add a URL value as an environment variable using this format: URL=\"&lt;url-with-!&gt;\".\nAll command options only accept lowercase letters.\nTroubleshooting\nTo resolve MessageRejected: Email address is not verified, see Why am I getting a 400 “message rejected” error with the message “Email address is not verified” from Amazon SES? in the AWS Knowledge Center.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Creating reports with the Reporting CLI"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/reporting-cli/rep-cli-index/",
    "title": "Creating reports with the Reporting CLI",
    "content": "You can programmatically create dashboard reports in PDF or PNG format with the Reporting CLI without using OpenSearch Dashboards or the Reporting plugin. This allows you to create reports automatically within your email workflows.\nIf you want to download a CSV file, you need to have the Reporting plugin installed.\nFor any dashboard view, you can request a report in PNG or PDF format to be sent to an email address. This can be useful for sending reports to multiple email recipients with an email alias. The only dashboard application that supports creating a CSV report is Discover.\nWith the Reporting CLI, you can specify options for your report in the command line. The report is sent to an email address as a PDF attachment by default. You can also request a PNG image or a CSV file with the --formats argument.\nYou can download the report to the directory in which you are running the Reporting CLI, or you can email the report by specifying Amazon Simple Email Service (Amazon SES) or SMTP for the email transport option.\nYou can connect to OpenSearch with any of the following authentication types: Basic – Basic HTTP authentication. Use -a basic. Cognito – Authentication through Amazon Cognito. Use -a cognito. SAML – Authentication between an identity provider and a service provider. Use -a saml. Okta provides the SAML third-party authentication. No auth – No authentication. Use -a none. Authentication defaults to No auth if the -a flag is not specified.\nTo learn more about Amazon Cognito, see What is Amazon Cognito?.\n<!--\n### Bypass authentication option\nThe Reporting CLI tool allows you to integrate it into your own workflow or environment so that you can bypass authentication or potential security issues. For example, if you use the Reporting CLI tool within an AWS Lambda instance, no security issues would occur as long as you run the Reporting plugin in OpenSearch Dashboards. In this case, you would use \"No auth\" to bypass the authentication process. To specify \"No Auth\" use `--auth none` in your request. Lambda users should test to make sure they can bypass access to Dashboards without credentials using No Auth.\nTo get a list of all options, see [Reporting CLI options](#reporting-cli-options).\n-->",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/reporting-cli/rep-cli-install/",
    "title": "Downloading and installing the Reporting CLI tool",
    "content": "You can download and install the Reporting CLI tool from either the npm software registry or the OpenSearch.org Artifacts hub. Refer to the following sections for instructions.\nTo learn more about the npm software registry, see the npm documentation.\nDownloading and installing the Reporting CLI from npm\nTo download and install the Reporting CLI from npm, run the following command to initiate installation: npm i @opensearch-project/reporting-cli Downloading and installing the Reporting CLI from OpenSearch.org\nYou can download the opensearch-reporting-cli tool from the OpenSearch.org Artifacts hub.\nNext, run the following command to install the.tar archive: npm install -g opensearch-reporting-cli-1.0.0.tgz To provide better security for artifacts, we recommend that you verify signatures by downloading the Reporting CLI signature file.\nTo learn more about verifying signatures, see How to verify signatures for downloadable artifacts.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Creating reports with the Reporting CLI"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/reporting-cli/rep-cli-lambda/",
    "title": "Scheduling reports with AWS Lambda",
    "content": "You can use AWS Lambda with the Reporting CLI tool to specify an AWS Lambda function to trigger the report generation.\nThis requires that you use an AMD64 system and Docker.\nPrerequisites\nTo use the Reporting CLI with AWS Lambda, you need to do the following preliminary steps.\nGet an AWS account. For instructions, see Creating an AWS account in the AWS Account Management reference guide.\nSet up an Amazon Elastic Container Registry (ECR). For instructions, see Getting started with Amazon ECR using the AWS Management Console.\nStep 1: Create a container image with a Dockerfile\nYou need to assemble the container image by running a Dockerfile. When you run the Dockerfile, it downloads the OpenSearch artifact required to use the Reporting CLI. To learn more about Dockerfiles, see Dockerfile reference.\nCopy the following sample configurations into a Dockerfile: # Define function directory ARG FUNCTION_DIR=\"/function\" # Base image of the docker container FROM node:lts-slim as build-image # Include global arg in this stage of the build ARG FUNCTION_DIR # AWS Lambda runtime dependencies RUN apt-get update &amp;&amp; \\ apt-get install -y \\ g++ \\ make \\ unzip \\ libcurl4-openssl-dev \\ autoconf \\ automake \\ libtool \\ cmake \\ python3 \\ libkrb5-dev \\ curl # Copy function code WORKDIR ${FUNCTION_DIR} RUN npm install @opensearch-project/reporting-cli &amp;&amp; npm install aws-lambda-ric # Build Stage 2: Copy Build Stage 1 files in to Stage 2. Install chrome, then remove chrome to keep the dependencies. FROM node:lts-slim # Include global arg in this stage of the build ARG FUNCTION_DIR # Set working directory to function root directory WORKDIR ${FUNCTION_DIR} # Copy in the build image dependencies COPY --from=build-image ${FUNCTION_DIR} ${FUNCTION_DIR} # Install latest chrome dev package and fonts to support major char sets (Chinese, Japanese, Arabic, Hebrew, Thai and a few others) # Note: this installs the necessary libs to make the bundled version of Chromium that Puppeteer installs, work. RUN apt-get update \\ &amp;&amp; apt-get install -y wget gnupg \\ &amp;&amp; wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \\ &amp;&amp; sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" &gt;&gt; /etc/apt/sources.list.d/google.list' \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y google-chrome-stable fonts-ipafont-gothic fonts-wqy-zenhei fonts-thai-tlwg fonts-kacst fonts-freefont-ttf libxss1 \\ --no-install-recommends \\ &amp;&amp; apt-get remove -y google-chrome-stable \\ &amp;&amp; rm -rf /var/lib/apt/lists/ * ENTRYPOINT [\"/usr/local/bin/npx\", \"aws-lambda-ric\"] ENV HOME=\"/tmp\" CMD [ \"/function/node_modules/@opensearch-project/reporting-cli/src/index.handler\"] Next, run the following build command within the same directory that contains the Dockerfile: docker build -t opensearch-reporting-cli. Step 2: Create a private repository with Amazon ECR\nYou need to follow the instructions to create an image repository, see Getting started with Amazon ECR using the AWS Management Console.\nGive your repository the name opensearch-reporting-cli.\nIn addition to the Amazon ECR instructions, you need to make several adjustments for the Reporting CLI to function properly as described in the following steps in this procedure.\nStep 3: Push the image to the private repository\nYou need to get several commands from the AWS ECR Console to run within the Dockerfile directory.\nAfter you create your repository, select it from Private repositories.\nChoose view push commands.\nCopy and run each command shown in Push commands for opensearch-reporting-cli sequentially in the Dockerfile directory.\nFor more details about Docker push commands, see Pushing a Docker image in the Amazon ECR user guide.\nStep 4: Create a Lambda function with the container image\nNow that you have a container image created for the Reporting CLI, you need to create a function defined as the container image.\nOpen the AWS Lambda console and choose Functions.\nChoose Create function, then choose Container image and fill in a name for the function.\nIn Container image URI, choose Browse images and select opensearch-reporting-cli for the image repository.\nIn Images select the image, and choose Select image.\nIn Architecture, choose x86_64.\nChoose Create function.\nGo to Lambda &gt; functions and choose the function you created.\nChoose Configuration &gt; General configuration &gt; Edit timeout and set the timeout in lambda to 5 minutes to allow the Reporting CLI to generate the report.\nChange the Ephemeral storage setting to at least 1024MB. The default setting is not a sufficient storage amount to support report generation.\nNext, test the function either by providing values JSON format or by providing AWS Lambda environment variables.\nIf the function contains fixed values, such as email address you do not need a JSON file. You can specify an environment variable in AWS Lambda.\nIf the function takes a variable key-value pair, then you need to specify the values in the JSON with the same naming convention as command options, for example the --credentials option requires the username and password.\nThe following example shows fixed values provided for the sender and recipient email addresses: { \"url\": \"https://playground.opensearch.org/app/dashboards#/view/084aed50-6f48-11ed-a3d5-1ddbf0afc873\", \"transport\": \"ses\", \"from\": \"sender@amazon.com\", \"to\": \"recipient@amazon.com\", \"subject\": \"Test lambda docker image\" } To learn more about AWS Lambda functions, see Deploying Lambda functions as container images in the AWS Lambda documentation.\nStep 5: Add the trigger to start the AWS Lambda function\nSet the trigger to start running the report. AWS Lambda can use any AWS service as a trigger, such as SNS, S3, or an AWS CloudWatch EventBridge.\nIn the Triggers section, choose Add trigger.\nSelect a trigger from the list. For example, you can set an AWS CloudWatch Event. To learn more about Amazon ECR events you can schedule, see Sample events from Amazon ECR.\nChoose Test to initiate the function.\n(Optional) Step 6: Add the role permission for Amazon SES\nIf you want to use Amazon SES for the email transport, you need to set up permissions.\nSelect Configuration and choose Execution role.\nIn Summary, choose Permissions.\nSelect {}JSON to open the JSON policy editor.\nAdd the permissions for the Amazon SES resource that you want to use.\nThe following example provides the resource ARN for the send email action: { \"Effect\": \"Allow\", \"Action\": [ \"ses:SendEmail\", \"ses:SendRawEmail\"], \"Resource\": \"arn:aws:ses:us-west-2:555555511111:identity/username@amazon.com\" } To learn more about setting role permissions, see Permissions in the AWS Lambda user guide.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Creating reports with the Reporting CLI"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/reporting-cli/rep-cli-options/",
    "title": "Reporting CLI options",
    "content": "You can use any of the following arguments with the opensearch-reporting-cli tool. Argument Description Acceptable values and usage Environment variable -u, --url The URL for the visualization.\nObtain from OpenSearch Dashboards &gt; Visualize &gt; Share &gt; Permalinks &gt; Copy link.\nOPENSEARCH_URL -a, --auth The authentication type for the report.\nYou can specify either Basic basic, Cognito cognito, SAML saml, or No Auth none. If no value is specified, the Reporting CLI tool defaults to no authentication, type none. Basic, Cognito, and SAML require credentials with the -c flag.\nN/A -c, --credentials The OpenSearch login credentials.\nEnter your username and password separated by a colon. For example, username:password. Required for Basic, Cognito, and SAML authentication types.\nOPENSEARCH_USERNAME and OPENSEARCH_PASSWORD -t, --tenant The tenants in OpenSearch Dashboards.\nThe default tenant is private.\nN/A -f, --format The file format for the report.\nCan be either pdf, png, or csv. The default is pdf.\nN/A -w, --width The window width in pixels for the report.\nDefault is 1680.\nN/A -l, --height The minimum window height in pixels for the report.\nDefault is 600.\nN/A -n, --filename The file name of the report.\nDefault is reporting.\nopensearch-report-YYY-MM-DDTHH-mm-ss.sssZ -e, --transport The transport mechanism for sending the email.\nFor Amazon SES, specify ses. Amazon SES requires an AWS configuration on your system to store the credentials. For SMTP, use smtp and also specify the login credentials with --smtpusername and --smtppassword.\nOPENSEARCH_TRANSPORT -s, --from The email address of the sender.\nFor example, user@amazon.com.\nOPENSEARCH_FROM -r, --to The email address of the recipient.\nFor example, user@amazon.com.\nOPENSEARCH_TO --smtphost The hostname of the SMTP server.\nFor example, SMTP_HOST.\nOPENSEARCH_SMTP_HOST --smtpport The port for the SMTP connection.\nFor example, SMTP_PORT.\nOPENSEARCH_SMTP_PORT --smtpsecure Specifies to use TLS when connecting to the server.\nFor example, SMTP_SECURE.\nOPENSEARCH_SMTP_SECURE --smtpusername The SMTP username.\nFor example, SMTP_USERNAME.\nOPENSEARCH_SMTP_USERNAME --smtppassword The SMTP password.\nFor example, SMTP_PASSWORD.\nOPENSEARCH_SMTP_PASSWORD --subject The email subject text encased in quotes.\nCan be any string. The default is “This is an email containing your dashboard report”.\nOPENSEARCH_EMAIL_SUBJECT --note The email body, either a string or a path to a text file.\nThe default note is “Hi,\\nHere is the latest report!”\nOPENSEARCH_EMAIL_NOTE -h, --help Specifies to display the list of optional arguments from the command line.\nN/A\n  Getting help\nTo get a list of all available CLI arguments, run the following command: $ opensearch-reporting-cli -h",
    "ancestors": [
      "OpenSearch Dashboards",
      "Creating reports with the Reporting CLI"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/reporting/",
    "title": "Creating reports with the Dashboards interface",
    "content": "You can use OpenSearch Dashboards to create PNG, PDF, and CSV reports. To create reports, you must have the correct permissions. For a summary of the predefined roles and the permissions they grant, see the Security plugin.\nCSV reports have a non-configurable 10,000 row limit. They have no explicit size limit (for example, MB), but extremely large documents could cause report generation to fail with an out of memory error from the V8 JavaScript engine.\nGenerating reports with the interface\nTo generate a report from the interface:\nFrom the navigation panel, choose Reporting.\nFor dashboards, visualizations, or notebooks, choose Download PDF or Download PNG. If you’re creating a report from the Discover page, choose Generate CSV.\nReports generate asynchronously in the background and might take a few minutes, depending on the size of the report. A notification appears when your report is ready to download.\nTo create a schedule-based report, choose Create report definition. Then proceed to Create reports using a definition. This option pre-fills many of the fields for you based on the visualization, dashboard, or data you were viewing.\nCreating reports using a definition\nDefinitions let you generate reports on a periodic schedule.\nFrom the navigation panel, choose Reporting.\nChoose Create.\nUnder Report settings, enter a name and optional description for your report.\nChoose the Report source (i.e. the page from which the report is generated). You can generate reports from the Dashboard, Visualize, Discover (saved search), or Notebooks pages.\nSelect your dashboard, visualization, saved search, or notebook. Then choose a time range for the report.\nChoose an appropriate file format for the report.\n(Optional) Add a header or footer to the report. Headers and footers are only available for dashboard, visualization, and notebook reports.\nUnder Report trigger, choose either On demand or Schedule.\nFor scheduled reports, select either Recurring or Cron based. You can receive reports daily or at some other time interval, and Cron expressions give you more flexibility. See Cron expression reference for more information.\nChoose Create.\nTroubleshooting\nChromium fails to launch with OpenSearch Dashboards\nWhile creating a report for dashboards or visualizations, you might see a the following error: This problem can occur for two reasons:\nYou don’t have the correct version of headless-chrome to match the operating system on which OpenSearch Dashboards is running. Download the correct version.\nYou’re missing additional dependencies. Install the required dependencies for your operating system from the additional libraries section.\nCharacters not loading in reports\nYou might encounter an issue where UTF-8 encoded characters look fine in your browser, but they don’t load in your generated reports because you’re missing the required font dependencies. Install the font dependencies, and then generate your reports again.",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/search-telemetry/",
    "title": "Managing search telemetry settings",
    "content": "You can use search telemetry to analyze search request performance by success or failure in OpenSearch Dashboards. OpenSearch stores telemetry data in the.kibana_1 index.\nBecause there are thousands of concurrent search requests from OpenSearch Dashboards, the heavy traffic can cause significant load in an OpenSearch cluster.\nOpenSearch clusters perform better with search telemetry turned off.\nTurning on search telemetry\nSearch usage telemetry is turned off by default. To turn it on, you need to set data.search.usageTelemetry.enabled to true in the opensearch_dashboards.yml file.\nYou can find the OpenSearch Dashboards YAML file in the opensearch-project repository on GitHub.\nTurning on telemetry in the opensearch_dashboards.yml file overrides the default search telemetry setting of false in the Data plugin configuration file.\nTurning search telemetry on or off\nThe following table shows the data.search.usageTelemetry.enabled values you can set in opensearch_dashboards.yml to turn search telemetry on or off. OpenSearch Dashboards YAML value Search telemetry status: on or off true On false Off none Off Sample opensearch_dashboards.yml with telemetry enabled\nThis OpenSearch Dashboards YAML file excerpt shows the telemetry setting set to true to turn on search telemetry: # Set the value of this setting to false to suppress # search usage telemetry to reduce the load of the OpenSearch cluster. data.search.usageTelemetry.enabled: true",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/sm-dashboards/",
    "title": "Snapshot management in Dashboards",
    "content": "You can set up Snapshot Management (SM) in OpenSearch Dashboards. Snapshots are backups of a cluster’s indexes and state. The state includes cluster settings, node information, index metadata (mappings, settings, templates), and shard allocation.\nSnapshots have two main uses:\nRecovering from failure\nFor example, if cluster health goes red, you might restore the red indexes from a snapshot.\nMigrating from one cluster to another\nFor example, if you’re moving from a proof of concept to a production cluster, you might take a snapshot of the former and restore it on the latter.\nYou can take and restore snapshots using snapshot management in OpenSearch Dashboards.\nIf you need to automate snapshots creation, you can use a snapshot policy.\nCreating a repository\nBefore you create an SM policy, you need to set up a repository for snapshots.\nOn the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management.\nIn the left panel, under Snapshot Management, select Repositories.\nChoose the Create Repository button.\nEnter the repository name, type, and location.\n(Optional) Select Advanced Settings and enter additional settings for this repository as a JSON object. Example: { \"chunk_size\": null, \"compress\": false, \"max_restore_bytes_per_sec\": \"40m\", \"max_snapshot_bytes_per_sec\": \"40m\", \"readonly\": false } Choose the Add button.\nDeleting a repository\nTo delete a snapshot repository configuration, select the repository from the Repositories list and then choose the Delete button.\nCreating an SM policy\nCreate an SM policy to set up automatic snapshots. An SM policy defines an automated snapshot creation schedule and an optional automated deletion schedule.\nOn the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management.\nIn the left panel, under Snapshot Management, select Snapshot Policies.\nSelect the Create Policy button.\nIn the Policy settings section:\nEnter the policy name.\n(Optional) Enter the policy description.\nIn the Source and destination section:\nSelect or enter source indexes either as a list or as an index pattern.\nSelect a repository for snapshots. To create a new repository, select the Create button.\nIn the Snapshot schedule section:\nSelect the desired snapshot frequency or enter a custom cron expression for snapshot frequency.\nSelect the start time and time zone.\nIn the Retention period section:\nChoose to retain all snapshots or specify retention conditions (the maximum age of retained snapshots).\n(Optional) In Additional settings, select the minimum and maximum number of retained snapshots, deletion frequency, and deletion start time.\nIn the Notifications section, select the snapshot activities you want to be notified about.\n(Optional) In the Advanced settings section, select the desired options: Include cluster state in snapshots Ignore unavailable indices Allow partial snapshots Select the Create button.\nView, edit, or delete an SM policy\nYou can view, edit, or delete an SM policy on the policy details page.\nOn the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management.\nIn the left panel, under Snapshot Management, select Snapshot Policies.\nClick on the Policy name of the policy you want to view, edit, or delete. The policy settings, snapshot schedule, snapshot retention period, notifications, and last creation and deletion are displayed in the policy details page. If a snapshot creation or deletion fails, you can view information about the failure in the Last Creation/Deletion section. To view the failure message, click on the cause in the Info column.\nTo edit or delete the SM policy, select the Edit or Delete button.\nEnable, disable, or delete SM policies\nOn the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management.\nIn the left panel, under Snapshot Management, select Snapshot Policies.\nSelect one or more policies in the list.\nTo enable or disable selected SM policies, select the Enable or Disable button. To delete selected SM policies, in the Actions list, select the Delete option.\nView snapshots\nOn the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management.\nIn the left panel, under Snapshot Management, select Snapshots.\nAll automatically or manually taken snapshots appear in the list.\nTo view a snapshot, click on its Name.\nTake a snapshot\nUse the steps below to take a snapshot manually:\nOn the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management.\nIn the left panel, under Snapshot Management, select Snapshots.\nSelect the Take snapshot button.\nEnter the snapshot name.\nSelect or enter source indexes either as a list or as an index pattern.\nSelect a repository for the snapshot.\n(Optional) In the Advanced options section, select the desired options: Include cluster state in snapshots Ignore unavailable indices Allow partial snapshots Choose the Add button.\nDeleting a snapshot\nThe Delete button deletes a snapshot from a repository.\nTo view a list of your repositories, choose Repositories under the Snapshot Management section.\nTo view a list of your snapshots, choose Snapshots under the Snapshot Management section.\nRestoring a snapshot\nOn the top menu bar, go to OpenSearch Plugins &gt; Snapshot Management.\nIn the left panel, under Snapshot Management, select Snapshots. The Snapshots tab is selected by default.\nSelect the checkbox next to the snapshot you want to restore, as shown in the following image: You can only restore snapshots with the status of Success or Partial. The status of the snapshot is displayed in the Snapshot status column.\nIn the Restore snapshot flyout, select the options for restoring the snapshot.\nThe Restore snapshot flyout lists the snapshot name and status. To view the list of indexes in the snapshot, select the number under Indices (for example, 27 in the following image). This number represents the number of indexes in the snapshot. For more information about the options in the Restore snapshot flyout, see Restore snapshots. Ignoring missing indexes If you specify which indexes you want to restore from the snapshot and select the Ignore unavailable indices option, the restore operation ignores the indexes that are missing from the snapshot. For example, if you want to restore the log1 and log2 indexes, but log2 is not in the snapshot, log1 is restored and log2 is ignored. If you don’t select Ignore unavailable indices, the entire restore operation fails if an index to be restored is missing from a snapshot. Custom index settings You can choose to customize some settings for the indexes restored from a snapshot:  • Select the Customize index settings checkbox to provide new values for the specified index settings. All newly restored indexes will use these values instead of the ones in the snapshot.  • Select the Ignore index settings checkbox to specify the settings in the snapshot to ignore. All newly restored indexes will use the cluster defaults for these settings.\nThe examples in the following image set index.number_of_replicas to 0, index.auto_expand_replicas to true, and index.refresh_interval and index.max_script_fields to the cluster default values for all newly restored indexes. For more information about index settings, see Index settings.\nFor a list of settings that you cannot change or ignore, see Restore snapshots.\nAfter choosing the options, select the Restore snapshot button.\n(Optional) To monitor the restore progress, select View restore activities in the confirmation dialog. You can also monitor the restore progress at any time by selecting the Restore activities in progress tab, as shown in the following image. You can view the percentage of the job that has been completed in the Status column. Once the snapshot restore is complete, the Status changes to Completed (100%).\nThe Restore activities in progress panel is not persistent. It displays only the progress of the current restore operation. If multiple restore operations are running, the panel displays the most recent one.\nTo view the status of each index being restored, select the link in the Indices being restored column (in the preceding image, the 27 Indices link). The Indices being restored flyout (shown in the following image) displays each index and its restore status. After the restore operation is complete, the restored indexes are listed in the Indices panel. To view the indexes, in the left panel, under Index Management, choose Indices.",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/visualize/area/",
    "title": "Using area charts",
    "content": "An area chart is a line chart with the area between the line and the axis shaded with a color, and is a primary visualization type used to display time series data. You can create area charts in Dashboards using the Area visualization type or using the Time Series Visual Builder (TSVB), Vega, or VisBuilder visualization tools. For this tutorial, you’ll use the Area visualization type. In this tutorial you’ll create a simple area chart using sample data and aggregations in OpenSearch Dashboards by connecting to http://localhost:5601 from a browser.\nYou have several aggregation options in Dashboards, and the choice influences your analysis. The use cases for aggregations vary from analyzing data in real time to using Dashboards to create a visualization dashboard. If you need an overview of aggregations in OpenSearch, see Aggregations before starting this tutorial.\nMake sure you have installed the latest version of Dashboards and added the sample data before continuing with this tutorial. This tutorial uses Dashboards version 2.4.1.\nSet up the area chart\nAccess Dashboards by connecting to http://localhost:5601 from a browser.\nSelect Visualize from the menu and then select Create visualization.\nSelect Area from the window.\nSelect opensearch_dashboards_sample_data_flights in the New Area/Choose a source window.\nSelect the calendar icon and set the time filter to Last 7 days.\nSelect Update.\nAdd aggregations to the area chart\nContinuing with the area chart created in the preceding steps, you’ll create a visualization that displays the top five logs for flights delayed for every three hours over the last seven days:\nAdd a Metrics aggregation.\nUnder Metrics, select the Aggregation dropdown list and choose Average and then select the Field dropdown list and choose FlightDelayMin.\nUnder Metrics, select Add to add another Y-axis aggregation.\nSelect the Aggregation dropdown list and choose Max and then select the Field dropdown list and choose FlightDelayMin.\nAdd a Buckets aggregation.\nSelect Add to open the Add Bucket window and then select X-axis.\nFrom the Aggregation dropdown list, select Date Histogram.\nFrom the Field dropdown list, select timestamp.\nSelect Update.\nAdd a sub-aggregation.\nSelect Add to open the Add Sub-Buckets window and then select Split series.\nFrom the Sub aggregation dropdown list, select Terms.\nFrom the Field dropdown list, select FlightDelay.\nSelect Update to reflect these parameters in the graph.\nYou’ve now created the following aggregation-based area chart. Visualize Visualization types in OpenSearch Dashboards Install and configure OpenSearch Dashboards Aggregations",
    "ancestors": [
      "OpenSearch Dashboards",
      "Building data visualizations"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/visualize/gantt/",
    "title": "Using Gantt charts",
    "content": "OpenSearch Dashboards includes a Gantt chart visualization. Gantt charts show the start, end, and duration of unique events in a sequence. Gantt charts are useful in trace analytics, telemetry, and anomaly detection use cases, where you want to understand interactions and dependencies between various events in a schedule.\nFor example, consider an index of log data. The fields in a typical set of log data, especially audit logs, contain a specific operation or event with a start time and duration.\nTo create a Gantt chart, perform the following steps:\nIn the visualizations menu, choose Create visualization and Gantt Chart.\nChoose a source for the chart (e.g. some log data).\nUnder Metrics, choose Event. For log data, each log is an event.\nSelect the Start Time and Duration fields from your data set. The start time is the timestamp for the beginning of an event. The duration is the amount of time to add to the start time.\nUnder Results, choose the number of events to display on the chart. Gantt charts sequence events from earliest to latest based on start time.\nChoose Panel settings to adjust axis labels, time format, and colors.\nChoose Update. This Gantt chart displays the ID of each log on the y-axis. Each bar is a unique event that spans some amount of time. Hover over a bar to see the duration of that event.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Building data visualizations"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/visualize/geojson-regionmaps/",
    "title": "Using coordinate and region maps",
    "content": "OpenSearch has a standard set of GeoJSON files that provide a vector map with each region map. OpenSearch Dashboards also provides basic map tiles with a standard vector map to create region maps. You can configure the base map tiles using Web Map Service (WMS). For more information, see Configuring WMS in OpenSearch Dashboards.\nFor air gapped environments, OpenSearch Dashboards provides a self-host maps server. For more information, see Using the self-host maps server While you can’t configure a server to support user-defined vector map layers, you can configure your own GeoJSON file and upload it for this purpose.\nCustomizing vector maps with GeoJSON\nIf you have a specific locale that is not provided by OpenSearch Dashboards vector maps, such as a US county or US ZIP Code, you can create your own custom vector map with a GeoJSON file. To create a custom region map you would define a geographic shape such as a polygon with multiple coordinates. To learn more about the various geographic shapes that support a custom region map location, see Geoshape field type.\nGeoJSON format allows you to encode geographic data structures. To learn more about the GeoJSON specification, go to geojson.org.\nYou can use geojson.io to extract GeoJSON files. PREREQUISITE To use a custom vector map with GeoJSON, install these two required plugins:\nOpenSearch Dashboards Maps dashboards-maps front-end plugin\nOpenSearch geospatial backend plugin\nStep 1: Creating a region map visualization\nTo create your own custom vector map, upload a JSON file that contains GEO data for your customized regional maps. The JSON file contains vector layers for visualization.\nPrepare a JSON file to upload. Make sure the file has either a.geojson or.json extension.\nOn the top menu bar, go to OpenSearch Dashboards &gt; Visualize.\nSelect the Create Visualization button.\nSelect Region Map.\nChoose a source. For example, [Flights] Flight Log.\nIn the right panel, select Import Vector Map.\nIn Upload map, select or drag and drop your JSON file and then enter Map name prefix (for example, usa-counties). Your map will have the prefix that you defined followed by the -map suffix (for example, usa-counties-map), as shown in the following image: Select the Import file button and then select the Refresh button in the pop-up window confirming successful upload, as shown in the following image. Step 2: Viewing the custom region map in OpenSearch Dashboards\nAfter you upload a custom GeoJSON file, you need to set the vector map layer to custom, and select your vector map:\nFrom Layer Options &gt; Layer settings, select Custom vector map.\nUnder Vector map, select the name of the vector map that you just uploaded.\nOptional: Under Style settings, increase Border thickness to see the borders more clearly.\nSelect the Update button.\nView your region map in the Dashboards. For example, the following image shows the Los Angeles and San Diego county regions: Example GeoJSON file\nThe following example GeoJSON file provides coordinates for two US counties. { \"type\": \"FeatureCollection\", \"name\": \"usa counties\", \"features\": [ { \"type\": \"Feature\", \"properties\": { \"iso2\": \"US\", \"iso3\": \"LA-CA\", \"name\": \"Los Angeles County\", \"country\": \"US\", \"county\": \"LA\" }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\":[[[ -118.71826171875, 34.07086232376631],[ -118.69628906249999, 34.03445260967645],[ -118.56994628906249, 34.02990029603907],[ -118.487548828125, 33.957030069982316],[ -118.37219238281249, 33.86129311351553],[ -118.45458984375, 33.75631505992707],[ -118.33923339843749, 33.715201644740844],[ -118.22937011718749, 33.75631505992707],[ -118.1414794921875, 33.678639851675555],[ -117.9107666015625, 33.578014746143985],[ -117.75146484375, 33.4955977448657],[ -117.55920410156249, 33.55512901742288],[ -117.3065185546875, 33.5963189611327],[ -117.0703125, 33.67406853374198],[ -116.69677734375, 34.06176136129718],[ -116.9439697265625, 34.28445325435288],[ -117.18017578125, 34.42956713470528],[ -117.3779296875, 34.542762387234845],[ -117.62512207031251, 34.56990638085636],[ -118.048095703125, 34.615126683462194],[ -118.44909667968749, 34.542762387234845],[ -118.61938476562499, 34.38877925439021],[ -118.740234375, 34.21180215769026],[ -118.71826171875, 34.07086232376631]]] } }, { \"type\": \"Feature\", \"properties\": { \"iso2\": \"US\", \"iso3\": \"SD-CA\", \"name\": \"San Diego County\", \"country\": \"US\", \"county\": \"SD\" }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\":[[[ -117.23510742187501, 32.861132322810946],[ -117.2406005859375, 32.75494243654723],[ -117.1636962890625, 32.68099643258195],[ -117.14172363281251, 32.58384932565662],[ -117.09228515624999, 32.46342595776104],[ -117.0538330078125, 32.29177633471201],[ -116.96044921875, 32.194208672875384],[ -116.85607910156249, 32.16631295696736],[ -116.6748046875, 32.20350534542368],[ -116.3671875, 32.319633552035214],[ -116.1474609375, 32.55144352864431],[ -116.1639404296875, 32.80574473290688],[ -116.4111328125, 33.073130945006625],[ -116.72973632812499, 33.08233672856376],[ -117.09228515624999, 32.99484290420988],[ -117.2515869140625, 32.96258644191747], [ -117.23510742187501, 32.861132322810946]]] } }] }",
    "ancestors": [
      "OpenSearch Dashboards",
      "Building data visualizations"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/visualize/maps-stats-api/",
    "title": "Maps Stats API",
    "content": "Introduced 2.7\nWhen you create and save a map in OpenSearch Dashboards, the map becomes a saved object of type map. The Maps Stats API provides information about such saved objects in OpenSearch Dashboards.\nExample request\nYou can access the Maps Stats API by providing its URL address in the following format: &lt;opensearch-dashboards-endpoint-address&gt;/api/maps-dashboards/stats The OpenSearch Dashboards endpoint address may contain a port number if it is specified in the OpenSearch configuration file. The specific URL format depends on the type of OpenSearch deployment and the network environment in which it is hosted.\nYou can query the endpoint in two ways:\nBy accessing the endpoint address (for example, http://localhost:5601/api/maps-dashboards/stats) in a browser\nBy using the curl command in the terminal: curl -X GET http://localhost:5601/api/maps-dashboards/stats copy Example response\nThe following is the response for the preceding request: { \"maps_total\": 4, \"layers_filters_total\": 4, \"layers_total\":{ \"opensearch_vector_tile_map\": 2, \"documents\": 7, \"wms\": 1, \"tms\": 2 }, \"maps_list\":[ { \"id\": \"88a24e6c-0216-4f76-8bc7-c8db6c8705da\", \"layers_filters_total\": 4, \"layers_total\":{ \"opensearch_vector_tile_map\": 1, \"documents\": 3, \"wms\": 0, \"tms\": 0 } }, { \"id\": \"4ce3fe50-d309-11ed-a958-770756e00bcd\", \"layers_filters_total\": 0, \"layers_total\":{ \"opensearch_vector_tile_map\": 0, \"documents\": 2, \"wms\": 0, \"tms\": 1 } }, { \"id\": \"af5d3b90-d30a-11ed-a605-f7ad7bc98642\", \"layers_filters_total\": 0, \"layers_total\":{ \"opensearch_vector_tile_map\": 1, \"documents\": 1, \"wms\": 0, \"tms\": 1 } }, { \"id\": \"5ca1ec10-d30b-11ed-a042-93d8ff0f09ee\", \"layers_filters_total\": 0, \"layers_total\":{ \"opensearch_vector_tile_map\": 0, \"documents\": 1, \"wms\": 1, \"tms\": 0 } }] } Response fields\nThe response contains statistics for the following layer types:\nBasemaps: Either a default OpenSearch map or custom base layer maps.\nWMS layers: Custom WMS base layer maps.\nTMS layers: Custom TMS base layer maps.\nDocument layers: The map’s data layers.\nFor more information about the layer types, see Adding layers.\nThe following table lists all response fields. Field Data type Description maps_total Integer\nThe total number of maps registered as saved objects with the Maps plugin. layers_filters_total Integer\nThe total number of filters for all layers in all maps. This includes layer-level filters but excludes global filters like shape filters. layers_total Object\nTotals statistics for all layers in all maps. layers_total.opensearch_vector_tile_map Integer\nThe total number of OpenSearch basemaps in all maps. layers_total.documents Integer\nThe total number of document layers in all maps. layers_total.wms Integer\nThe total number of WMS layers in all maps. layers_total.tms Integer\nThe total number of TMS layers in all maps. maps_list Array\nA list of all maps saved in OpenSearch Dashboards. Each map in the map_list contains the following fields. Field Data type Description id String\nThe map’s saved object ID. layers_filters_total Integer\nThe total number of filters for all layers in the map. This includes layer-level filters but excludes global filters like shape filters. layers_total Object\nTotals statistics for all layers in the map. layers_total.opensearch_vector_tile_map Integer\nThe total number of OpenSearch basemaps in the map. layers_total.documents Integer\nThe total number of document layers in the map. layers_total.wms Integer\nThe total number of WMS layers in the map. layers_total.tms Integer\nThe total number of TMS layers in the map. The saved object ID helps you navigate to a particular map because the ID is the last part of the map’s URL. For example, in OpenSearch Playground, the address of the [Flights] Flights Status on Maps Destination Location map is https://playground.opensearch.org/app/maps-dashboards/88a24e6c-0216-4f76-8bc7-c8db6c8705da, where 88a24e6c-0216-4f76-8bc7-c8db6c8705da is the saved object ID for this map.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Building data visualizations",
      "Using coordinate and region maps"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/visualize/maps/",
    "title": "Using maps",
    "content": "With OpenSearch Dashboards, you can create maps to visualize your geographical data. OpenSearch lets you construct map visualizations with multiple layers, combining data across different indexes. You can build each layer from a different index pattern. Additionally, you can configure maps to show specific data at different zoom levels. OpenSearch maps are powered by the OpenSearch maps service, which uses vector tiles to render maps.\nCreating a new map\nYou can create a new map from the Maps or Visualize workflows by performing the following steps:\nTo create a new map from the Maps workflow, perform the following steps:\nOn the top menu bar, go to OpenSearch Plugins &gt; Maps.\nChoose the Create map button.\nTo create a new map from the Visualize workflow, perform the following steps:\nOn the top menu bar, go to OpenSearch Dashboards &gt; Visualize.\nChoose the Create visualization button.\nIn the New Visualization dialog, choose Maps.\nYou can now see the default OpenSearch basemap.\nTo examine the Default map layer configuration, in the Layers panel on the upper left of the map, select Default map, as shown in the following image. To hide the Layers panel, select the collapse (arrow) icon in the panel’s upper-right corner.\nLayer settings\nTo change the default map settings, select Default map in the Layers panel. Under Layer settings, you can change the layer name and description and configure zoom levels and opacity for your layer: Zoom levels: By default, a layer is visible at all zoom levels. If you want to make a layer visible only for a certain range of zoom levels, you can specify the zoom levels either by entering them in the text boxes or by sliding the range slider to the desired values. Opacity: If your map contains multiple layers, one layer can obscure another one. In this case, you may want to reduce the opacity of the top layer so you can see both layers at the same time.\nAdding layers\nTo add a layer to the map, in the Layers panel, select the Add layer button. The Add layer dialog is shown in the following image. You can add base layers or data layers to the map:\nA base layer serves as a basemap. To use your own or a third-party map as a base layer, add it as a Custom map. Data layers let you visualize data from various data sources.\nAdding a custom map\nOpenSearch supports Web Map Service (WMS) or Tile Map Service (TMS) custom maps. To add a TMS custom map, perform the following steps:\nIn the Layers panel, select the Add layer button.\nFrom the Add layer dialog, select Base layer &gt; Custom map.\nFollow the next steps in the New layer dialog, which is shown in the following image. In the Custom type dropdown list, select Tile Map Service (TMS).\nEnter the TMS URL.\n(Optional) In TMS attribution, enter a TMS attribution for the basemap. For example, if you’re using a custom basemap, enter the custom map name. This name will be displayed in the lower-right corner of the map.\nSelect the Settings tab to edit the layer settings.\nEnter the layer name in Name.\n(Optional) Enter the layer description in Description.\n(Optional) Select the zoom levels and opacity for this layer.\nSelect the Update button.\nAdding a document layer\nAdding document layers lets you visualize your data. You can add one index pattern per document layer. To view multiple index patterns, create multiple layers.\nDocument layers can display geopoint and geoshape document fields.\nThe following example assumes that you have the opensearch_dashboards_sample_data_flights dataset installed. If you don’t have this dataset installed, perform the following steps:\nOn the top left, select the home icon.\nSelect Add sample data.\nIn the Sample flight data panel, select the Add data button.\nAdd a document layer as follows:\nIn the Layers panel, select the Add layer button.\nFrom the Add layer dialog, select Data layer &gt; Documents.\nIn Data source, select opensearch_dashboards_sample_data_flights. Alternatively, you can enter another index pattern to visualize.\nIn Geospatial field, select a geospatial field (geopoint or geoshape) to be displayed in the visualization. In this example, select DestLocation.\n(Optional) Select the Style tab to change the fill color, border color, border thickness, or marker size.\nSelect the Settings tab to edit layer settings.\nEnter Flight destination in Name.\nSelect the Update button.\nTo see more data, in the upper-right corner select the calendar icon dropdown list, then under Quick select, choose Last 15 days and select the Apply button.\nYou should see the flight destination data, as in the following image. Filtering data\nTo show a subset of the data in the index, filter the data. You can either filter data at the layer level or draw shapes on the map to filter all layer data globally.\nFiltering data at the layer level\nTo filter data at the layer level, select the layer and add a filter to it.\nThe following example shows how to filter the flight destination data to display only United States destinations:\nIn the Layers panel, select Flight destination.\nSelect Filters.\nSelect Add filter.\nIn Edit filter, select DestCountry in Field.\nIn Operator, select is.\nIn Value, select US.\nSelect the Save button.\nSelect the Update button.\nFor large datasets, you may want to avoid loading data for the full map. To load data only for a specific geographic area, select Only request data around map extent.\nDrawing shapes to filter data\nYou can filter your data globally by drawing shapes on the map. To draw a rectangle or polygon on the map, perform the following steps:\nSelect the Rectangle or Polygon icon on the right side of the map.\nIn the Filter label field, enter a name for the filter.\nChoose a spatial relation type. By default, Intersects is selected. See Spatial relations for more information about spatial relationship types.\nSelect the Draw Rectangle or Draw Polygon button.\nDraw the shape over the map area that you want to select:\nFor a rectangle, select any starting point on the map (this point becomes a rectangle vertex). Then hover (do not drag) to another point on the map and select it (this point becomes the opposite vertex).\nFor a polygon, select any starting point on the map (this point becomes a polygon vertex) and hover (do not drag) to each subsequent vertex and select that point. Finally, make sure to select the starting point again to close the polygon, as shown in the following image. Disabling the shape filter for a map layer\nBy default, the shape filter is applied globally to all layers on the map. If you want to disable your shape filter for a map layer, perform the following steps:\nSelect the layer from the Layers panel.\nIn the Filters section, deselect Apply global filters.\nSelect the Update button.\nModifying an existing shape filter\nTo modify an existing shape filter, select your filter on the top left above the map. You can perform the following operations on an existing filter: Edit filter: Change the filter name or modify the shape’s coordinates. Exclude results: Negate the filter, that is, show all data points except those to which the filter applies. Temporarily disable: Disable the filter until you select Re-enable. Delete: Remove your filter completely.\nUsing tooltips to visualize additional data\nDocument layers show geopoint and geoshape document fields as locations on the map. To add more information to the locations, you can use tooltips. For example, you may want to show flight delay, destination weather, and destination country information in the Flight destination layer. Perform the following steps to configure tooltips to show additional data:\nIn the Layers panel, select Flight destination.\nSelect Tooltips.\nSelect the Show tooltips checkbox.\nIn the Tooltip fields dropdown list, select the fields that you’d like to display. In this example, select FlightDelay, DestWeather, and DestCountry.\nSelect the Update button.\nTo view tooltips, hover over the geographical point you’re interested in. One tooltip can display many data points. For example, in the Flight destination layer there are multiple flights for a single destination city. To paginate over the flights, select the city you’re interested in and use the arrows in the tooltip, as shown in the following image. If a point on the map contains data from multiple layers, one tooltip can display data from multiple layers. To see all layers, select All layers. To choose a particular layer, select the layer name in the tooltip layer selection panel, as shown in the following image. Adding labels to layers\nAdding a label to a layer lets you visualize additional data on the map. For example, you may want to see the origin weather in the Flight destination layer. Perform the following steps to add a label to the Flight destination layer:\nIn the Layers panel, select Flight destination.\nIn the Style tab, select the Add label checkbox.\nYou can choose to add a label based on fixed text to all data points in the layer or to use a field value as the label text.\nTo add a fixed-text label, under Label text, select Fixed and enter your desired label text.\nTo add a label based on a field value, under Label text, select Field value and select the field name. In this example, select OriginWeather.\n(Optional) Change the label size, color, border color, or border width.\nSelect the Update button.\nThe label with the origin weather is visible on the map and also added to the tooltips, as shown in the following image. Reordering, hiding, and deleting layers\nThe Layers panel lets you reorder, hide, and delete layers:\nLayers on a map are stacked on top of each other. To reorder layers, use the handlebar (two horizontal lines) icon next to the layer name to drag the layer to the desired position.\nIf you’d like to hide a layer, select the show/hide (eye) icon next to the layer name. Toggle the show/hide icon to show the layer again.\nTo delete a layer, select the delete (trash can) icon next to the layer name.\nRefreshing data for a real-time dataset\nIf you want to visualize a real-time dataset, after adding layers to the map, perform the following steps to set the refresh interval:\nSelect the calendar icon in the upper-right corner.\nUnder Refresh every, select or enter the refresh interval (for example, 1 second).\nSelect the Start button. Saving a map\nTo save a map with all the layers that you set up, perform the following steps:\nSelect the Save button in the upper-right corner.\nIn the Save map dialog, enter the map name in the Title text box.\n(Optional) In the Description text box, enter the map description.\nSelect the Save button.\nTo open your saved map, choose Maps in the upper-left corner. The list of saved maps is displayed.\nAdding a map to a dashboard\nYou can add a new or existing map to a new or existing dashboard by performing the following steps:\nTo add a map to a new dashboard, first create the dashboard as follows:\nOn the top menu bar, go to OpenSearch Dashboards &gt; Dashboard.\nChoose the Create dashboard button.\nChoose the Create new button.\nTo add a map to an existing dashboard, first open the dashboard as follows:\nOn the top menu bar, go to OpenSearch Dashboards &gt; Dashboard.\nSelect the dashboard you want to open from the list.\nIn the upper-right corner, choose Edit.\nOnce you’ve opened a dashboard, you can add a new or existing map to it.\nAdding an existing map\nFrom the top menu, choose Add.\nIn the Types dropdown list, select Maps.\nSelect the map you want to add from the list.\nAdding a new map\nFrom the top menu, choose the Create new button.\nIn the New Visualization dialog, choose Maps.\nEdit the default map by adding a basemap, layers, or tooltips.\nIn the upper-right corner, choose the Save button.\nIn the Save map dialog, enter the Title and optional Description of the map.\nSelect Add to Dashboard after saving (this option is selected by default).\nChoose the Save and return button.\nEditing a map from a dashboard\nIn the dashboard, choose the gear icon in the upper-right corner of the map you want to edit.\nChoose Edit maps.\nEdit the map.\nIn the upper-right corner, choose the Save button.\nIn the Save map dialog, choose the Save and return button.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Building data visualizations",
      "Using coordinate and region maps"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/visualize/maptiles/",
    "title": "Configuring a Web Map Service (WMS)",
    "content": "The Open Geospatial Consortium (OGC) Web Map Service (WMS) specification is an international specification for requesting dynamic maps on the web. OpenSearch Dashboards includes default map tiles. For specialized maps, you can configure a WMS on OpenSearch Dashboards following these steps:\nLog in to OpenSearch Dashboards at https://&lt;host&gt;:&lt;port&gt;. For example, you can connect to OpenSearch Dashboards by connecting to https://localhost:5601. The default username and password are admin.\nChoose Management &gt; Advanced Settings.\nLocate visualization:tileMap:WMSdefaults.\nChange enabled to true and add the URL of a valid WMS server, as shown in the following example: { \"enabled\": true, \"url\": \"&lt;wms-map-server-url&gt;\", \"options\": { \"format\": \"image/png\", \"transparent\": true } } Web map services may have licensing fees or restrictions, and you are responsible for complying with any such fees or restrictions.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Building data visualizations",
      "Using coordinate and region maps"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/visualize/selfhost-maps-server/",
    "title": "Using the self-host maps server",
    "content": "The self-host maps server for OpenSearch Dashboards allows users to access the default maps service in air-gapped environments. OpenSearch-compatible map URLs include a map manifest with map tiles and vectors, the map tiles, and the map vectors.\nThe following sections provide steps for setting up and using the self-host maps server with OpenSearch Dashboards.\nYou can access the maps-server image via the official OpenSearch Docker Hub repository.\nPulling the Docker image\nOpen your terminal and run the following command: docker pull opensearch/opensearch-maps-server Setting up the server\nYou must set up the map tiles before running the server. You have two setup options: Use the OpenSearch-provided maps service tiles set, or generate the raster tiles set.\nOption 1: Use the OpenSearch-provided maps service tiles set\nCreate a Docker volume to hold the tiles set: docker volume create tiles-data Download the tiles set from the OpenSearch maps service. Two planet tiles sets are available based on the desired zoom level:\nZoom Level 8 (https://maps.opensearch.org/offline/planet-osm-default-z0-z8.tar.gz)\nZoom level 10 (https://maps.opensearch.org/offline/planet-osm-default-z0-z10.tar.gz)\nThe planet tiles set for zoom level 10 (2 GB compressed/6.8 GB uncompressed) is approximately 10 times larger than the set for zoom level 8 (225 MB compressed/519 MB uncompressed). docker run \\\n-e DOWNLOAD_TILES=https://maps.opensearch.org/offline/planet-osm-default-z0-z8.tar.gz \\\n-v tiles-data:/usr/src/app/public/tiles/data/ \\\nopensearch/opensearch-maps-server \\\nimport Option 2: Generate the raster tiles set\nTo generate the raster tiles set, use the raster tile generation pipeline and then use the tiles set absolute path to create a volume to start the server.\nStarting the server\nUse the following command to start the server using the Docker volume tiles-data. The following command is an example using host URL “localhost” and port “8080”: docker run \\\n-v tiles-data:/usr/src/app/public/tiles/data/ \\\n-e HOST_URL='http://localhost' \\\n-p 8080:8080 \\\nopensearch/opensearch-maps-server \\\nrun Or, if you generated the raster tiles set, run the server using that tiles set: docker run \\\n-v /absolute/path/to/tiles/:/usr/src/app/dist/public/tiles/data/ \\\n-p 8080:8080 \\\nopensearch/opensearch-maps-server \\\nrun To access the tiles set, open the URLs in a browser on the host or use the curl command curl http://localhost:8080/manifest.json.\nConfirm the server is running by opening each of the following links in a browser on your host or with a curl command (for example, curl http://localhost:8080/manifest.json).\nMap manifest URL: http://localhost:8080/manifest.json Map tiles URL: http://localhost:8080/tiles/data/{z}/{x}/{y}.png Map tiles demo URL: http://localhost:8080/ Using the self-host maps server with OpenSearch Dashboards\nYou can use the self-host maps server with OpenSearch Dashboards by either adding the parameter to opensearch_dashboards.yml or configuring the default WMS properties in OpenSearch Dashboards.\nOption 1: Configure opensearch_dashboards.yml\nConfigure the manifest URL in opensearch_dashboards.yml: map.opensearchManifestServiceUrl: \"http://localhost:8080/manifest.json\" Option 2: Configure Default WMS properties in OpenSearch Dashboards\nOn the OpenSearch Dashboards console, select Stack Management &gt; Advanced Settings.\nLocate visualization:tileMap:WMSdefaults under Default WMS properties.\nChange \"enabled\": false to \"enabled\": true and add the URL for the valid map server.\nLicenses\nTiles are generated per Terms of Use for Natural Earth vector map data and Copyright and License for OpenStreetMap.\nRelated articles Configuring a Web Map Service (WMS) Using coordinate and region maps",
    "ancestors": [
      "OpenSearch Dashboards",
      "Building data visualizations",
      "Using coordinate and region maps"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/visualize/visbuilder/",
    "title": "Using VisBuilder",
    "content": "VisBuilder is an experimental feature and shouldn’t be used in a production environment. For updates on its progress, or if you want to leave feedback that helps improve the feature, see the GitHub issue.\nYou can use the VisBuilder visualization type in OpenSearch Dashboards to create data visualizations by using a drag-and-drop gesture. With VisBuilder you have:\nAn immediate view of your data without the need to preselect the visualization output.\nThe flexibility to change visualization types and index patterns quickly.\nThe ability to easily navigate between multiple screens. Try VisBuilder in the OpenSearch Dashboards playground\nIf you’d like to try out VisBuilder without installing OpenSearch locally, you can do so in the Dashboards playground. VisBuilder is enabled by default.\nTry VisBuilder locally\nVisBuilder is enabled by default. If you want to disable it, set the feature flag vis_builder.enabled: to false in the opensearch_dashboards.yml file as follows: # Set the value of this setting to false to disable VisBuilder\n# functionality in Visualization.\nvis_builder.enabled: false Follow these steps to create a new visualization using VisBuilder in your environment:\nOpen Dashboards:\nIf you’re not running the Security plugin, go to http://localhost:5601.\nIf you’re running the Security plugin, go to https://localhost:5601 and log in with your username and password (default is admin/admin).\nConfirm that the Enable experimental visualizations option is turned on.\nFrom the top menu, select Management &gt; Stack Management &gt; Advanced Settings.\nSelect Visualization and verify that the option is turned on. From the top menu, select Visualize &gt; Create visualization &gt; VisBuilder. Drag and drop field names from the left column into the Configuration panel to generate a visualization.\nHere’s an example visualization. Your visualization will look different depending on your data and the fields you select.",
    "ancestors": [
      "OpenSearch Dashboards",
      "Building data visualizations"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/dashboards/visualize/viz-index/",
    "title": "Building data visualizations",
    "content": "By visualizing your data, you translate complex, high-volume, or numerical data into a visual representation that is easier to process. OpenSearch Dashboards gives you data visualization tools to improve and automate the visual communication process. By using visual elements like charts, graphs, or maps to represent data, you can advance business intelligence and support data-driven decision-making and strategic planning.\nUnderstanding the visualization types in OpenSearch Dashboards\nDashboards has several visualization types to support your data analysis needs. The following sections provide an overview of the visualization types in Dashboards and their common use cases.\nArea charts\nArea charts depict changes over time, and they are commonly used to show trends. Area charts more efficiently identify patterns in log data, such as sales data for a time range and trends over that time. See Using area charts to learn more about how to create and use them in Dashboards. Bar charts\nBar charts (vertical or horizontal) compare categorical data and depict changes of a variable over a period of time. Vertical bar chart Horizontal bar chart Controls\nControls is a panel, instead of a visualization type, added to a dashboard to filter data. Controls gives users the capability to add interactive inputs to a dashboard. You can create two types of controls in Dashboards: Options list and Range slider. Options list is a dropdown options list that allows filtering of data by a terms aggregation, such as machine.os.keyword. Range slider allows filtering within specified value ranges, such as hour_of_day. Data tables\nData tables, or tables, show your raw data in tabular form. Gantt charts\nGantt charts show the start, end, and duration of unique events in a sequence. Gantt charts are useful in trace analytics, telemetry, and anomaly detection use cases where you want to understand interactions and dependencies between various events in a schedule. Gantt chart is currently a plugin, instead of built-in, visualization type in Dashboards. See Gantt charts to learn how to create and use them in Dashboards. Gauge charts\nGauge charts look similar to an analog speedometer that reads left to right from zero. They display how much there is of the thing you are measuring, and this measurement can exist alone or in relation to another measurement, such as tracking performance against benchmarks or goals. Heat maps\nA heat map is a view of a histogram (a graphical representation of the distribution of numerical data) over time. Instead of using bar height as a representation of frequency, as with a histogram, heat maps display data in a tabular form using colors to differentiate where values fall in a range. Line charts\nLine charts compare changes in measured values over a period of time, such as gross sales by month or gross sales and net sales by month. Maps\nYou can create two types of maps in Dashboards: Coordinate maps and Region maps. Coordinate maps show the difference between data values for each location by size. Region maps show the difference between data values for each location by varying shades of color. See Using maps to learn more about maps capabilities in Dashboards.\nCoordinate maps\nCoordinate maps show location-based data on a map. Use coordinate maps to visualize GPS data (latitude and longitude coordinates) on a map. For information about OpenSearch-supported coordinate field types, see Geographic field types and Cartesian field types. Region maps\nRegion maps show patterns and trends across geographic locations. A region map is one of the basemaps in Dashboards. For information about creating custom vector maps in Dashboards, see Using coordinate and region maps to learn how to create and use maps in Dashboards. Markdown\nMarkdown is a the markup language used in Dashboards to provide context to your data visualizations. Using Markdown, you can display information and instructions along with the visualization. Metric values\nMetric values, or number charts, compare values in different measures. For example, you can create a metrics visualization to compare two values, such as actual sales compared to sales goals. Pie charts\nPie charts compare values for items in a dimension, such as a percentage of a total amount. TSVB\nThe time-series visual builder (TSVB) is a data visualization tool in Dashboards used to create detailed time-series visualizations. For example, you can use TSVB to build visualizations that show data over time, such as flights by status over time or flight delays by delay type over time. Currently, TSVB can be used to create the following Dashboards visualization types: Area, Line, Metric, Gauge, Markdown, and Data Table. Tag cloud\nTag (or word) clouds are a way to display how often a word is used in relation to other words in a dataset. The best use for this type of visual is to show word or phrase frequency. Timeline\nTimeline is a data visualization tool in Dashboards that you can use to create time-series visualizations. Currently, Timeline can be used to create the following Dashboards visualization types: Area and Line. VisBuilder\nVisBuilder is a drag-and-drop data visualization tool in Dashboards. It gives you an immediate view of your data without the need to preselect the data source or visualization type output. Currently, VisBuilder can be used to create the following Dashboards visualization types: Area, Bar, Line, Metric, and Data Table. See VisBuilder to learn how to create and use drag-and-drop visualizations in Dashboards. Vega Vega and Vega-Lite are open-source, declarative language visualization grammars for creating, sharing, and saving interactive data visualizations. Vega visualizations give you the flexibility to visualize multidimensional data using a layered approach in order to build and manipulate visualizations in a structured manner. Vega can be used to create customized visualizations using any Dashboards visualization type.",
    "ancestors": [
      "OpenSearch Dashboards"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/index/",
    "title": "Availability and Recovery",
    "content": "The following OpenSearch features help ensure consistent uptime so that your cluster can complete and scale based on your use case, as well as creating snapshots.",
    "ancestors": [
      "Tuning your cluster"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/remote/",
    "title": "Remote-backed storage",
    "content": "Remote-backed storage is an experimental feature. Therefore, we do not recommend the use of remote-backed storage in a production environment. For updates on the progress of remote-backed storage, or if you want leave feedback that could help improve the feature, refer to the issue on GitHub.\nRemote-backed storage offers OpenSearch users a new way to protect against data loss by automatically creating backups of all index transactions and sending them to remote storage. In order to expose this feature, segment replication must also be enabled. See Segment replication for additional information.\nTranslog\nAny index changes, such as indexing or deleting documents, are written to disk during a Lucene commit. However, Lucene commits are expensive operations, so they cannot be performed after every change to the index. Instead, each shard records every indexing operation in a transaction log called translog. When a document is indexed, it is added to the memory buffer and recorded in the translog. Frequent refresh operations write the documents in the memory buffer to a segment and then clear the memory buffer. Periodically, a flush performs a Lucene commit, which includes writing the segments to disk using fsync, purging the old translog, and starting a new translog. Thus, a translog contains all operations that have not yet been flushed.\nSegment replication and remote-backed storage\nWhen neither segment replication nor remote-backed storage is enabled, OpenSearch uses document replication. In document replication, when a write request lands on the primary shard, the request is indexed to Lucene and stored in the translog. After this, the request is sent to the replicas, where, in turn, it is indexed to Lucene and stored in the translog for durability.\nWith segment replication, segments are created on the primary shard only and then copied to all replicas. The replicas do not index requests to Lucene, but they do create and maintain a translog.\nWith remote-backed storage, when a write request lands on the primary shard, the request is indexed to Lucene on the primary shard only. The corresponding translog is then uploaded to remote store. OpenSearch does not send the write request to the replicas, but rather performs a primary term validation to confirm that the request originator shard is still the primary shard. Primary term validation ensures that the acting primary shard fails if it becomes isolated and is unaware of the cluster manager electing a new primary.\nThe index.translog.durability translog setting\nWithout remote-backed storage, indexing operations are only persisted to disk when the translog is fsynced. Therefore, any data that has not been written to disk can potentially be lost.\nThe index.translog.durability setting controls how frequently OpenSearch fsyncs the translog to disk:\nBy default, index.translog.durability is set to request. This means that fsync happens after every request, and all acknowledged write requests persist in case of failure.\nIf you set index.translog.durability to async, fsync happens periodically at the specified sync_interval (5 seconds by default). The fsync operation is asynchronous, so acknowledge is sent without waiting for fsync. Consequently, all acknowledged writes since the last commit are lost in case of failure.\nWith remote-backed storage, the translog is uploaded to a remote store for durability. index.translog.durability is a dynamic setting. To update it, use the following query: PUT my_index/_settings { \"index\": { \"translog.durability\": \"request\" } } Refresh-level and request-level durability\nThe remote store feature supports two levels of durability:\nRefresh-level durability: Segment files are uploaded to remote store after every refresh. Set the remote_store flag to true to achieve refresh-level durability. Commit-level durability is inherent, and uploads are asynchronous.\nIf you need to refresh an index manually, you can use the _refresh API. For example, to refresh the my_index index, use the following request: POST my_index/_refresh Request-level durability: Translogs are uploaded before acknowledging the request. Set the translog flag to true to achieve request-level durability. In this scenario, we recommend to batch as many requests as possible in a bulk request. Batching requests will improve indexing throughput and latency compared to sending individual write requests.\nEnable the feature flag\nThere are several methods for enabling remote store feature, depending on the install type. You will also need to enable remote_store property when creating the index.\nSegment replication must also be enabled to use remote-backed storage.\nEnable on a node using a tarball install\nThe flag is toggled using a new jvm parameter that is set either in OPENSEARCH_JAVA_OPTS or in config/jvm.options.\nOption 1: Modify jvm.options\nAdd the following lines to config/jvm.options before starting the OpenSearch process to enable the feature and its dependency: -Dopensearch.experimental.feature.replication_type.enabled=true\n-Dopensearch.experimental.feature.remote_store.enabled=true Run OpenSearch./bin/opensearch Option 2: Enable from an environment variable\nAs an alternative to directly modifying config/jvm.options, you can define the properties by using an environment variable. This can be done in a single command when you start OpenSearch or by defining the variable with export.\nTo add these flags in-line when starting OpenSearch: OPENSEARCH_JAVA_OPTS = \"-Dopensearch.experimental.feature.replication_type.enabled=true -Dopensearch.experimental.feature.remote_store.enabled=true\"./opensearch-2.7.0/bin/opensearch If you want to define the environment variable separately, prior to running OpenSearch: export OPENSEARCH_JAVA_OPTS = \"-Dopensearch.experimental.feature.replication_type.enabled=true -Dopensearch.experimental.feature.remote_store.enabled=true\"./bin/opensearch Enable with Docker containers\nIf you’re running Docker, add the following line to docker-compose.yml underneath the opensearch-node and environment section: OPENSEARCH_JAVA_OPTS= \"-Dopensearch.experimental.feature.replication_type.enabled=true -Dopensearch.experimental.feature.remote_store.enabled=true\" Enable for OpenSearch development\nTo create new indexes with remote-backed storage enabled, you must first enable these features by adding the correct properties to run.gradle before building OpenSearch. See the developer guide for information about to use how Gradle to build OpenSearch.\nAdd the following properties to run.gradle to enable the feature: testClusters { runTask { testDistribution = 'archive' if ( numZones &gt; 1) numberOfZones = numZones if ( numNodes &gt; 1) numberOfNodes = numNodes\nsystemProperty 'opensearch.experimental.feature.replication_type.enabled', 'true' systemProperty 'opensearch.experimental.feature.remote_store.enabled', 'true' } } Register a remote repository\nNow that your deployment is running with the feature flags enabled, the next step is to register a remote repository where backups will be stored. See Register repository for more information.\nCreate an index\nRemote-backed storage is enabled for an index when it is created. This feature cannot be enabled for indexes that already exist.\nFor refresh-level durability, include the remote_store property to enable the feature and specify a segment repository: curl -X PUT \"https://localhost:9200/my-index?pretty\" -ku admin:admin -H 'Content-Type: application/json' -d '\n{\n\"settings\": {\n\"index\": {\n\"number_of_shards\": 1,\n\"number_of_replicas\": 0,\n\"replication\": {\n\"type\": \"SEGMENT\"\n},\n\"remote_store\": {\n\"enabled\": true,\n\"repository\": \"segment-repo\"\n}\n}\n}\n}\n' For request-level durability, in addition to the remote_store and segment repository, include the translog property and specify a translog repository: curl -X PUT \"https://localhost:9200/my-index?pretty\" -ku admin:admin -H 'Content-Type: application/json' -d '\n{\n\"settings\": {\n\"index\": {\n\"number_of_shards\": 1,\n\"number_of_replicas\": 1,\n\"replication\": {\n\"type\": \"SEGMENT\"\n},\n\"remote_store\": {\n\"enabled\": true,\n\"repository\": \"segment-repo\",\n\"translog\": {\n\"enabled\": true,\n\"repository\": \"translog-repo\",\n\"buffer_interval\": \"300ms\"\n}\n}\n}\n}\n}\n' You can have the same repository serve as both the segment repository and translog repository.\nAs data is added to the index, it also will be continuously uploaded to remote storage in the form of segment and translog files because of refreshes, flushes, and translog fsyncs to disk. Along with data, other metadata files will be uploaded.\nThe buffer_interval setting specifies the time interval during which translog operations are buffered. Instead of uploading individual translog files, OpenSearch creates a single translog file with all the write operations received during the configured interval. Bundling translog files leads to higher throughput but also increases latency. The default buffer_interval value is 100 ms.\nSetting translog.enabled to true is currently an irreversible operation.\nRestoring from a backup\nTo restore an index from a remote backup, such as in the event of a node failure, you must first close the index: curl -X POST \"https://localhost:9200/my-index/_close\" -ku admin:admin Restore the index from the backup stored on the remote repository: curl -X POST \"https://localhost:9200/_remotestore/_restore\" -ku admin:admin -H 'Content-Type: application/json' -d '\n{\n\"indices\": [\"my-index\"]\n}\n' If the Security plugin is enabled, a user must have the cluster:admin/remotestore/restore permission. See Access control for information about configuring user permissions.\nPotential use cases\nYou can use remote-backed storage for the following purposes:\nTo restore red clusters or indexes\nTo recover all data up to the last acknowledged write, regardless of replica count, if index.translog.durability is set to request Known limitations\nThe following are known limitations of the remote-backed storage feature:\nWriting data to a remote store can be a high-latency operation when compared to writing data on the local file system. This may impact the indexing throughput and latency. For performance benchmarking results, see issue #6376.",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/search-backpressure/",
    "title": "Search backpressure",
    "content": "Search backpressure is a mechanism used to identify resource-intensive search requests and cancel them when the node is under duress. If a search request on a node or shard has breached the resource limits and does not recover within a certain threshold, it is rejected. These thresholds are dynamic and configurable through cluster settings.\nMeasuring resource consumption\nTo decide whether to apply search backpressure, OpenSearch periodically measures the following resource consumption statistics for each search request:\nCPU usage\nHeap usage\nElapsed time\nAn observer thread periodically measures the resource usage of the node. If OpenSearch determines that the node is under duress, OpenSearch examines the resource usage of each search task and search shard task and compares it against configurable thresholds. OpenSearch considers CPU usage, heap usage, and elapsed time and assigns each task a cancellation score that is then used to cancel the most resource-intensive tasks.\nOpenSearch limits the number of cancellations to a fraction of successful task completions. Additionally, it limits the number of cancellations per unit time. OpenSearch continues to monitor and cancel tasks until the node is no longer under duress.\nCanceled queries\nIf a query is canceled, OpenSearch may return partial results if some shards failed. If all shards failed, OpenSearch returns an error from the server similar to the following error: { \"error\": { \"root_cause\": [ { \"type\": \"task_cancelled_exception\", \"reason\": \"cancelled task with reason: cpu usage exceeded [17.9ms &gt;= 15ms], elapsed time exceeded [1.1s &gt;= 300ms]\" }, { \"type\": \"task_cancelled_exception\", \"reason\": \"cancelled task with reason: elapsed time exceeded [1.1s &gt;= 300ms]\" }], \"type\": \"search_phase_execution_exception\", \"reason\": \"all shards failed\", \"phase\": \"query\", \"grouped\": true, \"failed_shards\": [ { \"shard\": 0, \"index\": \"foobar\", \"node\": \"7yIqOeMfRyWW1rHs2S4byw\", \"reason\": { \"type\": \"task_cancelled_exception\", \"reason\": \"cancelled task with reason: cpu usage exceeded [17.9ms &gt;= 15ms], elapsed time exceeded [1.1s &gt;= 300ms]\" } }, { \"shard\": 1, \"index\": \"foobar\", \"node\": \"7yIqOeMfRyWW1rHs2S4byw\", \"reason\": { \"type\": \"task_cancelled_exception\", \"reason\": \"cancelled task with reason: elapsed time exceeded [1.1s &gt;= 300ms]\" } }] }, \"status\": 500 } Search backpressure modes\nSearch backpressure runs in monitor_only (default), enforced, or disabled mode. In the enforced mode, the server rejects search requests. In the monitor_only mode, the server does not actually cancel search requests but tracks statistics about them. You can specify the mode in the search_backpressure.mode parameter.\nSearch backpressure settings\nSearch backpressure adds several settings to the standard OpenSearch cluster settings. These settings are dynamic, so you can change the default behavior of this feature without restarting your cluster. Setting Default Description search_backpressure.mode monitor_only The search backpressure mode. Valid values are monitor_only, enforced, or disabled.\nsearch_backpressure.cancellation_ratio Deprecated in 2.6. Replaced by search_backpressure.search_shard_task.cancellation_ratio 10%\nThe maximum number of tasks to cancel, as a percentage of successful task completions.\nsearch_backpressure.cancellation_rate Deprecated in 2.6. Replaced by search_backpressure.search_shard_task.cancellation_rate 0.003\nThe maximum number of tasks to cancel per millisecond of elapsed time.\nsearch_backpressure.cancellation_burst Deprecated in 2.6. Replaced by search_backpressure.search_shard_task.cancellation_burst 10\nThe maximum number of search shard tasks to cancel in a single iteration of the observer thread.\nsearch_backpressure.node_duress.num_successive_breaches\n3\nThe number of successive limit breaches after which the node is considered to be under duress.\nsearch_backpressure.node_duress.cpu_threshold\n90%\nThe CPU usage threshold (as a percentage) required for a node to be considered to be under duress.\nsearch_backpressure.node_duress.heap_threshold\n70%\nThe heap usage threshold (as a percentage) required for a node to be considered to be under duress.\nsearch_backpressure.search_task.elapsed_time_millis_threshold\n45,000\nThe elapsed time threshold (in milliseconds) required for an individual parent task before it is considered for cancellation.\nsearch_backpressure.search_task.cancellation_ratio\n0.1\nThe maximum number of search tasks to cancel, as a percentage of successful search task completions.\nsearch_backpressure.search_task.cancellation_rate\n0.003\nThe maximum number of search tasks to cancel per millisecond of elapsed time.\nsearch_backpressure.search_task.cancellation_burst\n5\nThe maximum number of search tasks to cancel in a single iteration of the observer thread.\nsearch_backpressure.search_task.heap_percent_threshold\n2%\nThe heap usage threshold (as a percentage) required for an individual parent task before it is considered for cancellation.\nsearch_backpressure.search_task.total_heap_percent_threshold\n5%\nThe heap usage threshold (as a percentage) required for the sum of heap usages of all search tasks before cancellation is applied.\nsearch_backpressure.search_task.heap_variance\n2.0\nThe heap usage variance required for an individual parent task before it is considered for cancellation. A task is considered for cancellation when taskHeapUsage is greater than or equal to heapUsageMovingAverage * variance.\nsearch_backpressure.search_task.heap_moving_average_window_size\n10\nThe window size used to calculate the rolling average of the heap usage for the completed parent tasks.\nsearch_backpressure.search_task.cpu_time_millis_threshold\n30,000\nThe CPU usage threshold (in milliseconds) required for an individual parent task before it is considered for cancellation.\nsearch_backpressure.search_shard_task.elapsed_time_millis_threshold\n30,000\nThe elapsed time threshold (in milliseconds) required for a single search shard task before it is considered for cancellation.\nsearch_backpressure.search_shard_task.cancellation_ratio\n0.1\nThe maximum number of search shard tasks to cancel, as a percentage of successful search shard task completions.\nsearch_backpressure.search_shard_task.cancellation_rate\n0.003\nThe maximum number of search shard tasks to cancel per millisecond of elapsed time.\nsearch_backpressure.search_shard_task.cancellation_burst\n10\nThe maximum number of search shard tasks to cancel in a single iteration of the observer thread.\nsearch_backpressure.search_shard_task.heap_percent_threshold\n0.5%\nThe heap usage threshold (as a percentage) required for a single search shard task before it is considered for cancellation.\nsearch_backpressure.search_shard_task.total_heap_percent_threshold\n5%\nThe heap usage threshold (as a percentage) required for the sum of heap usages of all search shard tasks before cancellation is applied.\nsearch_backpressure.search_shard_task.heap_variance\n2.0\nThe minimum variance required for a single search shard task’s heap usage compared to the rolling average of previously completed tasks before it is considered for cancellation.\nsearch_backpressure.search_shard_task.heap_moving_average_window_size\n100\nThe number of previously completed search shard tasks to consider when calculating the rolling average of heap usage.\nsearch_backpressure.search_shard_task.cpu_time_millis_threshold\n15,000\nThe CPU usage threshold (in milliseconds) required for a single search shard task before it is considered for cancellation. Search Backpressure Stats API\nIntroduced 2.4\nYou can use the nodes stats API operation to monitor server-side request cancellations.\nExample request\nTo retrieve the statistics, use the following request: GET _nodes/stats/search_backpressure Example response\nThe response contains server-side request cancellation statistics: { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"T7aqO6zaQX-lt8XBWBYLsA\": { \"timestamp\": 1667409521070, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [], \"attributes\": { \"testattr\": \"test\", \"shard_indexing_pressure_enabled\": \"true\" }, \"search_backpressure\": { \"search_task\": { \"resource_tracker_stats\": { \"heap_usage_tracker\": { \"cancellation_count\": 57, \"current_max_bytes\": 5739204, \"current_avg_bytes\": 962465, \"rolling_avg_bytes\": 4009239 }, \"elapsed_time_tracker\": { \"cancellation_count\": 97, \"current_max_millis\": 15902, \"current_avg_millis\": 9705 }, \"cpu_usage_tracker\": { \"cancellation_count\": 64, \"current_max_millis\": 8483, \"current_avg_millis\": 7843 } }, \"cancellation_stats\": { \"cancellation_count\": 102, \"cancellation_limit_reached_count\": 25 } }, \"search_shard_task\": { \"resource_tracker_stats\": { \"heap_usage_tracker\": { \"cancellation_count\": 34, \"current_max_bytes\": 1203272, \"current_avg_bytes\": 700267, \"rolling_avg_bytes\": 1156270 }, \"cpu_usage_tracker\": { \"cancellation_count\": 318, \"current_max_millis\": 731, \"current_avg_millis\": 303 }, \"elapsed_time_tracker\": { \"cancellation_count\": 310, \"current_max_millis\": 1305, \"current_avg_millis\": 649 } }, \"cancellation_stats\": { \"cancellation_count\": 318, \"cancellation_limit_reached_count\": 97 } }, \"mode\": \"enforced\" } } } } Response fields\nThe response contains the following fields. Field Name Data type Description search_backpressure\nObject\nStatistics about search backpressure.\nsearch_backpressure.search_task\nObject\nStatistics specific to the search task.\nsearch_backpressure.search_task. resource_tracker_stats Object\nStatistics about the current search tasks.\nsearch_backpressure.search_task. cancellation_stats Object\nStatistics about the search tasks canceled since the node last restarted.\nsearch_backpressure.search_shard_task\nObject\nStatistics specific to the search shard task.\nsearch_backpressure.search_shard_task. resource_tracker_stats Object\nStatistics about the current search shard tasks.\nsearch_backpressure.search_shard_task. cancellation_stats Object\nStatistics about the search shard tasks canceled since the node last restarted.\nsearch_backpressure.mode\nString\nThe mode for search backpressure. resource_tracker_stats The resource_tracker_stats object contains the statistics for each resource tracker: elapsed_time_tracker, heap_usage_tracker, and cpu_usage_tracker. elapsed_time_tracker The elapsed_time_tracker object contains the following statistics related to the elapsed time. Field Name Data type Description cancellation_count\nInteger\nThe number of tasks marked for cancellation because of excessive elapsed time since the node last restarted.\ncurrent_max_millis\nInteger\nThe maximum elapsed time for all tasks currently running on the node, in milliseconds.\ncurrent_avg_millis\nInteger\nThe average elapsed time for all tasks currently running on the node, in milliseconds. heap_usage_tracker The heap_usage_tracker object contains the following statistics related to the heap usage. Field Name Data type Description cancellation_count\nInteger\nThe number of tasks marked for cancellation because of excessive heap usage since the node last restarted.\ncurrent_max_bytes\nInteger\nThe maximum heap usage for all tasks currently running on the node, in bytes.\ncurrent_avg_bytes\nInteger\nThe average heap usage for all tasks currently running on the node, in bytes.\nrolling_avg_bytes\nInteger\nThe rolling average heap usage for n most recent tasks, in bytes. n is configurable and defined by the search_backpressure.search_shard_task.heap_moving_average_window_size setting. The default value for this setting is 100. cpu_usage_tracker The cpu_usage_tracker object contains the following statistics related to the CPU usage. Field Name Data type Description cancellation_count\nInteger\nThe number of tasks marked for cancellation because of excessive CPU usage since the node last restarted.\ncurrent_max_millis\nInteger\nThe maximum CPU time for all tasks currently running on the node, in milliseconds.\ncurrent_avg_millis\nInteger\nThe average CPU time for all tasks currently running on the node, in milliseconds. cancellation_stats The cancellation_stats object contains the following statistics for the tasks that are marked for cancellation. Field Name Data type Description cancellation_count\nInteger\nThe total number of tasks marked for cancellation since the node last restarted.\ncancellation_limit_reached_count\nInteger\nThe number of times when the number of tasks eligible for cancellation exceeded the set cancellation threshold.",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/segment-replication/backpressure/",
    "title": "Segment replication back-pressure",
    "content": "Segment replication backpressure\nSegment replication backpressure is a shard-level rejection mechanism that dynamically rejects indexing requests as replica shards in your cluster fall behind primary shards. With segment replication backpressure, indexing requests are rejected when the percentage of stale shards in the replication group exceeds MAX_ALLOWED_STALE_SHARDS (50% by default). A replica is considered stale if it is behind the primary shard by the number of checkpoints that exceeds the MAX_INDEXING_CHECKPOINTS setting and its current replication lag is greater than the defined MAX_REPLICATION_TIME_SETTING field.\nReplica shards are also monitored to determine whether the shards are stuck or lagging for an extended period of time. When replica shards are stuck or lagging for more than double the amount of time defined by the MAX_REPLICATION_TIME_SETTING field, the shards are removed and replaced with new replica shards.\nRequest fields\nSegment replication backpressure is disabled by default. To enable it, set SEGMENT_REPLICATION_INDEXING_PRESSURE_ENABLED to true. You can update the following dynamic cluster settings using the cluster settings API endpoint. Field Data type Description SEGMENT_REPLICATION_INDEXING_PRESSURE_ENABLED\nBoolean\nEnables the segment replication backpressure mechanism. Default is false.\nMAX_REPLICATION_TIME_SETTING\nTime unit\nThe maximum amount of time that a replica shard can take to copy from the primary shard. Once MAX_REPLICATION_TIME_SETTING is breached along with MAX_INDEXING_CHECKPOINTS, the segment replication backpressure mechanism is initiated. Default is 5 minutes.\nMAX_INDEXING_CHECKPOINTS\nInteger\nThe maximum number of indexing checkpoints that a replica shard can fall behind when copying from primary. Once MAX_INDEXING_CHECKPOINTS is breached along with MAX_REPLICATION_TIME_SETTING, the segment replication backpressure mechanism is initiated. Default is 4 checkpoints.\nMAX_ALLOWED_STALE_SHARDS\nFloating point\nThe maximum number of stale replica shards that can exist in a replication group. Once MAX_ALLOWED_STALE_SHARDS is breached, the segment replication backpressure mechanism is initiated. Default is.5, which is 50% of a replication group. Path and HTTP methods\nYou can use the segment replication API endpoint to retrieve segment replication backpressure metrics as follows: GET _cat/segment_replication copy Example response shardId target_node target_host checkpoints_behind bytes_behind current_lag last_completed_lag rejected_requests [ index -1][ 0] runTask -1 127.0. 0.1 0 0 b 0 s 7 ms 0 The checkpoints_behind and current_lag metrics are taken into consideration when initiating segment replication backpressure. They are checked against MAX_INDEXING_CHECKPOINTS and MAX_REPLICATION_TIME_SETTING, respectively.",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery",
      "Segment replication"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/segment-replication/index/",
    "title": "Segment replication",
    "content": "With segment replication, segment files are copied across shards instead of documents being indexed on each shard copy. This improves indexing throughput and lowers resource utilization at the expense of increased network utilization.\nWhen the primary shard sends a checkpoint to replica shards on a refresh, a new segment replication event is triggered on replica shards. This happens:\nWhen a new replica shard is added to a cluster.\nWhen there are segment file changes on a primary shard refresh.\nDuring peer recovery, such as replica shard recovery and shard relocation (explicit allocation using the move allocation command or automatic shard rebalancing).\nSegment replication is the first feature in a series of features designed to decouple reads and writes in order to lower compute costs.\nUse cases\nUsers who have high write loads but do not have high search requirements and are comfortable with longer refresh times.\nUsers with very high loads who want to add new nodes, as you do not need to index all nodes when adding a new node to the cluster.\nOpenSearch cluster deployments with low replica counts, such as those used for log analytics.\nSegment replication configuration\nTo set segment replication as the replication strategy, create an index with replication.type set to SEGMENT: PUT /my-index 1 { \"settings\": { \"index\": { \"replication.type\": \"SEGMENT\" } } } copy In segment replication, the primary shard is usually generating more network traffic than the replicas because it copies segment files to the replicas. Thus, it’s beneficial to distribute primary shards equally between the nodes. To ensure balanced primary shard distribution, set the dynamic cluster.routing.allocation.balance.prefer_primary setting to true. For more information, see Cluster settings.\nSegment replication currently does not support the wait_for value in the refresh query parameter.\nFor the best performance, we recommend enabling both of the following settings: Segment replication backpressure.\nBalanced primary shard allocation: curl -X PUT \"$host/_cluster/settings?pretty\" -H 'Content-Type: application/json' -d' { \"persistent\": { \"cluster.routing.allocation.balance.prefer_primary\": true, \"segrep.pressure.enabled\": true } } copy Considerations\nWhen using segment replication, consider the following:\nEnabling segment replication for an existing index requires reindexing.\nRolling upgrades are not currently supported. Full cluster restarts are required when upgrading indexes using segment replication. See Issue 3881. Cross-cluster replication does not currently use segment replication to copy between clusters.\nSegment replication leads to increased network congestion on primary shards. See Issue - Optimize network bandwidth on primary shards.\nIntegration with remote-backed storage as the source of replication is currently not supported.\nRead-after-write guarantees: The wait_until refresh policy is not compatible with segment replication. If you use the wait_until refresh policy while ingesting documents, you’ll get a response only after the primary node has refreshed and made those documents searchable. Replica shards will respond only after having written to their local translog. We are exploring other mechanisms for providing read-after-write guarantees. For more information, see the corresponding GitHub issue.\nSystem indexes will continue to use document replication internally until read-after-write guarantees are available. In this case, document replication does not hinder the overall performance because there are few system indexes.\nBenchmarks\nDuring initial benchmarks, segment replication users reported 40% higher throughput than when using document replication with the same cluster setup.\nThe following benchmarks were collected with OpenSearch-benchmark using the stackoverflow and nyc_taxi datasets.\nThe benchmarks demonstrate the effect of the following configurations on segment replication: The workload size The number of primary shards The number of replicas Your results may vary based on the cluster topology, hardware used, shard count, and merge settings.\nIncreasing the workload size\nThe following table lists benchmarking results for the nyc_taxi dataset with the following configuration:\n10 m5.xlarge data nodes\n40 primary shards, 1 replica each (80 shards total)\n4 primary shards and 4 replica shards per node 40 GB primary shard, 80 GB total 240 GB primary shard, 480 GB total Document Replication\nSegment Replication\nPercent difference\nDocument Replication\nSegment Replication\nPercent difference\nStore size\n85.2781\n91.2268\nN/A\n515.726\n558.039\nN/A\nIndex throughput (number of requests per second)\nMinimum\n148,134\n185,092\n24.95%\n100,140\n168,335\n68.10%\nMedian\n160,110\n189,799\n18.54%\n106,642\n170,573\n59.95%\nMaximum\n175,196\n190,757\n8.88%\n108,583\n172,507\n58.87%\nError rate\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nAs the size of the workload increases, the benefits of segment replication are amplified because the replicas are not required to index the larger dataset. In general, segment replication leads to higher throughput at lower resource costs than document replication in all cluster configurations, not accounting for replication lag.\nIncreasing the number of primary shards\nThe following table lists benchmarking results for the nyc_taxi dataset for 40 and 100 primary shards. 40 primary shards, 1 replica 100 primary shards, 1 replica Document Replication\nSegment Replication\nPercent difference\nDocument Replication\nSegment Replication\nPercent difference\nIndex throughput (number of requests per second)\nMinimum\n148,134\n185,092\n24.95%\n151,404\n167,391\n9.55%\nMedian\n160,110\n189,799\n18.54%\n154,796\n172,995\n10.52%\nMaximum\n175,196\n190,757\n8.88%\n166,173\n174,655\n4.86%\nError rate\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nAs the number of primary shards increases, the benefits of segment replication over document replication decrease. While segment replication is still beneficial with a larger number of primary shards, the difference in performance becomes less pronounced because there are more primary shards per node that must copy segment files across the cluster.\nIncreasing the number of replicas\nThe following table lists benchmarking results for the stackoverflow dataset for 1 and 9 replicas. 10 primary shards, 1 replica 10 primary shards, 9 replicas Document Replication\nSegment Replication\nPercent difference\nDocument Replication\nSegment Replication\nPercent difference\nIndex throughput (number of requests per second)\nMedian\n72,598.10\n90,776.10\n25.04%\n16,537.00\n14,429.80\n&minus;12.74%\nMaximum\n86,130.80\n96,471.00\n12.01%\n21,472.40\n38,235.00\n78.07%\nCPU usage (%)\np50\n17\n18.857\n10.92%\n69.857\n8.833\n&minus;87.36%\np90\n76\n82.133\n8.07%\n99\n86.4\n&minus;12.73%\np99\n100\n100\n0%\n100\n100\n0%\np100\n100\n100\n0%\n100\n100\n0%\nMemory usage (%)\np50\n35\n23\n&minus;34.29%\n42\n40\n&minus;4.76%\np90\n59\n57\n&minus;3.39%\n59\n63\n6.78%\np99\n69\n61\n&minus;11.59%\n66\n70\n6.06%\np100\n72\n62\n&minus;13.89%\n69\n72\n4.35%\nError rate\n0.00%\n0.00%\n0.00%\n0.00%\n2.30%\n2.30%\nAs the number of replicas increases, the amount of time required for primary shards to keep replicas up to date (known as the replication lag) also increases. This is because segment replication copies the segment files directly from primary shards to replicas.\nThe benchmarking results show a non-zero error rate as the number of replicas increases. The error rate indicates that the segment replication backpressure mechanism is initiated when replicas cannot keep up with the primary shard. However, the error rate is offset by the significant CPU and memory gains that segment replication provides.\nNext steps\nTrack future enhancements to segment replication.\nRead this blog post about segment replication.",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/shard-indexing-backpressure/",
    "title": "Shard indexing backpressure",
    "content": "Shard indexing backpressure is a smart rejection mechanism at a per-shard level that dynamically rejects indexing requests when your cluster is under strain. It propagates a backpressure that transfers requests from an overwhelmed node or shard to other nodes or shards that are still healthy.\nWith shard indexing backpressure, you can prevent nodes in your cluster from running into cascading failures due to performance degradation caused by slow nodes, stuck tasks, resource-intensive requests, traffic surges, skewed shard allocations, and so on.\nShard indexing backpressure comes into effect only when one primary and one secondary parameter is breached.\nPrimary parameters\nPrimary parameters are early indicators that a cluster is under strain:\nShard memory limit breach: If the memory usage of a shard exceeds 95% of its allocated memory, this limit is breached.\nNode memory limit breach: If the memory usage of a node exceeds 70% of its allocated memory, this limit is breached.\nThe breach of primary parameters doesn’t cause any actual request rejections, it just triggers an evaluation of the secondary parameters.\nSecondary parameters\nSecondary parameters check the performance at the shard level to confirm that the cluster is under strain:\nThroughput: If the throughput at the shard level decreases significantly in its historic view, this limit is breached.\nSuccessful Request: If the number of pending requests increases significantly in its historic view, this limit is breached.",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/shard-indexing-settings/",
    "title": "Settings",
    "content": "Shard indexing backpressure adds several settings to the standard OpenSearch cluster settings. They are dynamic, so you can change the default behavior of this feature without restarting your cluster.\nHigh-level controls\nThe high-level controls allow you to turn the shard indexing backpressure feature on or off. Setting Default Description shard_indexing_pressure.enabled False\nChange to true to enable shard indexing backpressure. shard_indexing_pressure.enforced False\nRun shard indexing backpressure in shadow mode or enforced mode. In shadow mode (value set as false), shard indexing backpressure tracks all granular-level metrics, but it doesn’t actually reject any indexing requests. In enforced mode (value set as true), shard indexing backpressure rejects any requests to the cluster that might cause a dip in its performance. Node-level limits\nNode-level limits allow you to control memory usage on a node. Setting Default Description shard_indexing_pressure.primary_parameter.node.soft_limit 70%\nDefine the percentage of the node-level memory threshold that acts as a soft indicator for strain on a node. Shard-level limits\nShard-level limits allow you to control memory usage on a shard. Setting Default Description shard_indexing_pressure.primary_parameter.shard.min_limit 0.001d\nSpecify the minimum assigned quota for a new shard in any role (coordinator, primary, or replica). Shard indexing backpressure increases or decreases this allocated quota based on the inflow of traffic for the shard. shard_indexing_pressure.operating_factor.lower 75%\nSpecify the lower occupancy limit of the allocated quota of memory for the shard. If the total memory usage of a shard is below this limit, shard indexing backpressure decreases the current allocated memory for that shard. shard_indexing_pressure.operating_factor.optimal 85%\nSpecify the optimal occupancy of the allocated quota of memory for the shard. If the total memory usage of a shard is at this level, shard indexing backpressure doesn’t change the current allocated memory for that shard. shard_indexing_pressure.operating_factor.upper 95%\nSpecify the upper occupancy limit of the allocated quota of memory for the shard. If the total memory usage of a shard is above this limit, shard indexing backpressure increases the current allocated memory for that shard. Performance degradation factors\nThe performance degradation factors allow you to control the dynamic performance thresholds for a shard. Setting Default Description shard_indexing_pressure.secondary_parameter.throughput.request_size_window 2,000\nThe number of requests in the sampling window size on a shard. Shard indexing backpressure compares the overall performance of requests with the requests in the sample window to detect any performance degradation. shard_indexing_pressure.secondary_parameter.throughput.degradation_factor 5x\nThe degradation factor per unit byte for a request. This parameter determines the threshold for any latency spikes. The default value is 5x, which implies that if the latency shoots up 5 times in the historic view, shard indexing backpressure marks it as a performance degradation. shard_indexing_pressure.secondary_parameter.successful_request.elapsed_timeout 300000 ms\nThe amount of time a request is pending in a cluster. This parameter helps identify any stuck-request scenarios. shard_indexing_pressure.secondary_parameter.successful_request.max_outstanding_requests 100\nThe maximum number of pending requests in a cluster.",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery",
      "Shard indexing backpressure"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/snapshots/index/",
    "title": "Snapshots",
    "content": "Snapshots are backups of a cluster’s indexes and state. State includes cluster settings, node information, index metadata (mappings, settings, templates, etc.), and shard allocation.\nSnapshots have two main uses: Recovering from failure For example, if cluster health goes red, you might restore the red indexes from a snapshot. Migrating from one cluster to another For example, if you’re moving from a proof-of-concept to a production cluster, you might take a snapshot of the former and restore it on the latter.\nYou can take and restore snapshots using the snapshot API.\nIf you need to automate taking snapshots, you can use the snapshot management feature.",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/snapshots/searchable_snapshot/",
    "title": "Searchable snapshots",
    "content": "A searchable snapshot index reads data from a snapshot repository on demand in real time (at search time) rather than downloading all index data to cluster storage at restore time. Because the index data remains in the snapshot format in the repository, searchable snapshot indexes are inherently read-only. Any attempt to write to a searchable snapshot index results in an error.\nThe searchable snapshot feature incorporates techniques like caching frequently used data segments in cluster nodes and removing the least used data segment from the cluster nodes to make space for frequently used data segments. The data segments downloaded from snapshots on block storage reside alongside the general indexes of the cluster nodes. As such, the computing capacity of cluster nodes is shared between indexing, local search, and data segments on a snapshot residing on lower-cost object storage like Amazon Simple Storage Service (Amazon S3). While cluster node resources are utilized much more efficiently, the high number of tasks results in slower and longer snapshot searches. The local storage of the node is also used for caching the snapshot data.\nConfiguring a node to use searchable snapshots\nTo configure the searchable snapshots feature, create a node in your opensearch.yml file and define the node role as search: ```bash\nnode.name: snapshots-node\nnode.roles: [ search]\n``` If you’re running Docker, you can create a node with the search node role by adding the line - node.roles: [ search] to your docker-compose.yml file: version: '3' services:\nopensearch-node1:\nimage: opensearchproject/opensearch:2.7.0\ncontainer_name: opensearch-node1\nenvironment:\n- cluster.name = opensearch-cluster\n- node.name = opensearch-node1\n- node.roles: [ search] Create a searchable snapshot index\nA searchable snapshot index is created by specifying the remote_snapshot storage type using the restore snapshots API. Request Field Description storage_type local indicates that all snapshot metadata and index data will be downloaded to local storage. remote_snapshot indicates that snapshot metadata will be downloaded to the cluster, but the remote repository will remain the authoritative store of the index data. Data will be downloaded and cached as necessary to service queries. At least one node in the cluster must be configured with the search node role in order to restore a snapshot using the remote_snapshot type. Defaults to local. Listing indexes\nTo determine whether an index is a searchable snapshot index, look for a store type with the value of remote_snapshot: GET /my-index/_settings?pretty { \"my-index\": { \"settings\": { \"index\": { \"store\": { \"type\": \"remote_snapshot\" } } } } } Potential use cases\nThe following are potential use cases for the searchable snapshots feature:\nThe ability to offload indexes from cluster-based storage but retain the ability to search them.\nThe ability to have a large number of searchable indexes in lower-cost media.\nKnown limitations\nThe following are known limitations of the searchable snapshots feature:\nAccessing data from a remote repository is slower than local disk reads, so higher latencies on search queries are expected.\nMany remote object stores charge on a per-request basis for retrieval, so users should closely monitor any costs incurred.\nSearching remote data can impact the performance of other queries running on the same node. We recommend that users provision dedicated nodes with the search role for performance-critical applications.",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery",
      "Snapshots"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/snapshots/sm-api/",
    "title": "Snapshot management API",
    "content": "Use the snapshot management (SM) API to automate taking snapshots.\nTable of contents Create or update a policy Example Response Parameters Get policies Example Response Explain Example Response Start a policy Example Response Stop a policy Example Response Delete a policy Example Response Create or update a policy\nIntroduced 2.1\nCreates or updates an SM policy.\nRequest\nCreate: POST _plugins/_sm/policies/&lt;policy_name&gt; Update: PUT _plugins/_sm/policies/&lt;policy_name&gt;?if_seq_no= 0 &amp;if_primary_term= 1 You must provide the seq_no and primary_term parameters for an update request.\nExample POST _plugins/_sm/policies/daily-policy { \"description\": \"Daily snapshot policy\", \"creation\": { \"schedule\": { \"cron\": { \"expression\": \"0 8 * * *\", \"timezone\": \"UTC\" } }, \"time_limit\": \"1h\" }, \"deletion\": { \"schedule\": { \"cron\": { \"expression\": \"0 1 * * *\", \"timezone\": \"America/Los_Angeles\" } }, \"condition\": { \"max_age\": \"7d\", \"max_count\": 21, \"min_count\": 7 }, \"time_limit\": \"1h\" }, \"snapshot_config\": { \"date_format\": \"yyyy-MM-dd-HH:mm\", \"timezone\": \"America/Los_Angeles\", \"indices\": \"*\", \"repository\": \"s3-repo\", \"ignore_unavailable\": \"true\", \"include_global_state\": \"false\", \"partial\": \"true\", \"metadata\": { \"any_key\": \"any_value\" } }, \"notification\": { \"channel\": { \"id\": \"NC3OpoEBzEoHMX183R3f\" }, \"conditions\": { \"creation\": true, \"deletion\": false, \"failure\": false, \"time_limit_exceeded\": false } } } Response { \"_id\": \"daily-policy-sm-policy\", \"_version\": 5, \"_seq_no\": 54983, \"_primary_term\": 21, \"sm_policy\": { \"name\": \"daily-policy\", \"description\": \"Daily snapshot policy\", \"schema_version\": 15, \"creation\": { \"schedule\": { \"cron\": { \"expression\": \"0 8 * * *\", \"timezone\": \"UTC\" } }, \"time_limit\": \"1h\" }, \"deletion\": { \"schedule\": { \"cron\": { \"expression\": \"0 1 * * *\", \"timezone\": \"America/Los_Angeles\" } }, \"condition\": { \"max_age\": \"7d\", \"min_count\": 7, \"max_count\": 21 }, \"time_limit\": \"1h\" }, \"snapshot_config\": { \"indices\": \"*\", \"metadata\": { \"any_key\": \"any_value\" }, \"ignore_unavailable\": \"true\", \"timezone\": \"America/Los_Angeles\", \"include_global_state\": \"false\", \"date_format\": \"yyyy-MM-dd-HH:mm\", \"repository\": \"s3-repo\", \"partial\": \"true\" }, \"schedule\": { \"interval\": { \"start_time\": 1656425122909, \"period\": 1, \"unit\": \"Minutes\" } }, \"enabled\": true, \"last_updated_time\": 1656425122909, \"enabled_time\": 1656425122909, \"notification\": { \"channel\": { \"id\": \"NC3OpoEBzEoHMX183R3f\" }, \"conditions\": { \"creation\": true, \"deletion\": false, \"failure\": false, \"time_limit_exceeded\": false } } } } Parameters\nYou can specify the following parameters to create/update an SM policy. Parameter Type Description description String\nThe description of the SM policy. Optional. enabled Boolean\nShould this SM policy be enabled at creation? Optional. snapshot_config Object\nThe configuration options for snapshot creation. Required. snapshot_config.date_format String\nSnapshot names have the format &lt;policy_name&gt;-&lt;date&gt;-&lt;random number&gt;. date_format specifies the format for the date in the snapshot name. Supports all date formats supported by OpenSearch. Optional. Default is “yyyy-MM-dd’T’HH:mm:ss”. snapshot_config.date_format_timezone String\nSnapshot names have the format &lt;policy_name&gt;-&lt;date&gt;-&lt;random number&gt;. date_format_timezone specifies the time zone for the date in the snapshot name. Optional. Default is UTC. snapshot_config.indices String\nThe names of the indexes in the snapshot. Multiple index names are separated by,. Supports wildcards ( *). Optional. Default is * (all indexes). snapshot_config.repository String\nThe repository in which to store snapshots. Required. snapshot_config.ignore_unavailable Boolean\nDo you want to ignore unavailable indexes? Optional. Default is false. snapshot_config.include_global_state Boolean\nDo you want to include cluster state? Optional. Default is true because of Security plugin considerations. snapshot_config.partial Boolean\nDo you want to allow partial snapshots? Optional. Default is false. snapshot_config.metadata Object\nMetadata in the form of key/value pairs. Optional. creation Object\nConfiguration for snapshot creation. Required. creation.schedule String\nThe cron schedule used to create snapshots. Required. creation.time_limit String\nSets the maximum time to wait for snapshot creation to finish. If time_limit is longer than the scheduled time interval for taking snapshots, no scheduled snapshots are taken until time_limit elapses. For example, if time_limit is set to 35 minutes and snapshots are taken every 30 minutes starting at midnight, the snapshots at 00:00 and 01:00 are taken, but the snapshot at 00:30 is skipped. Optional. deletion Object\nConfiguration for snapshot deletion. Optional. Default is to retain all snapshots. deletion.schedule String\nThe cron schedule used to delete snapshots. Optional. Default is to use creation.schedule, which is required. deletion.time_limit String\nSets the maximum time to wait for snapshot deletion to finish. Optional. deletion.delete_condition Object\nConditions for snapshot deletion. Optional. deletion.delete_condition.max_count Integer\nThe maximum number of snapshots to be retained. Optional. deletion.delete_condition.max_age String\nThe maximum time a snapshot is retained. Optional. deletion.delete_condition.min_count Integer\nThe minimum number of snapshots to be retained. Optional. Default is one. notification Object\nDefines notifications for SM events. Optional. notification.channel Object\nDefines a channel for notifications. You must create and configure a notification channel before setting up SM notifications. Required. notification.channel.id String\nThe channel ID of the channel used for notifications. To get the channel IDs of all created channels, use GET _plugins/_notifications/configs. Required. notification.conditions Object\nSM events you want to be notified about. Set the ones you are interested in to true. notification.conditions.creation Boolean\nDo you want notifications about snapshot creation? Optional. Default is true. notification.conditions.deletion Boolean\nDo you want notifications about snapshot deletion? Optional. Default is false. notification.conditions.failure Boolean\nDo you want notifications about creation or deletion failure? Optional. Default is false. notification.conditions.time_limit_exceeded Boolean\nDo you want notifications when snapshot operations take longer than time_limit? Optional. Default is false. Get policies\nIntroduced 2.1\nGets SM policies.\nRequest\nGet all SM policies: GET _plugins/_sm/policies You can use a query string and specify pagination, the field to be sorted by, and sort order: GET _plugins/_sm/policies?from= 0 &amp;size= 20 &amp;sortField=sm_policy.name&amp;sortOrder=desc&amp;queryString=* Get a specific SM policy: GET _plugins/_sm/policies/&lt;policy_name&gt; Example GET _plugins/_sm/policies/daily-policy Response { \"_id\": \"daily-policy-sm-policy\", \"_version\": 6, \"_seq_no\": 44696, \"_primary_term\": 19, \"sm_policy\": { \"name\": \"daily-policy\", \"description\": \"Daily snapshot policy\", \"schema_version\": 15, \"creation\": { \"schedule\": { \"cron\": { \"expression\": \"0 8 * * *\", \"timezone\": \"UTC\" } }, \"time_limit\": \"1h\" }, \"deletion\": { \"schedule\": { \"cron\": { \"expression\": \"0 1 * * *\", \"timezone\": \"America/Los_Angeles\" } }, \"condition\": { \"max_age\": \"7d\", \"min_count\": 7, \"max_count\": 21 }, \"time_limit\": \"1h\" }, \"snapshot_config\": { \"metadata\": { \"any_key\": \"any_value\" }, \"ignore_unavailable\": \"true\", \"include_global_state\": \"false\", \"date_format\": \"yyyy-MM-dd-HH:mm\", \"repository\": \"s3-repo\", \"partial\": \"true\" }, \"schedule\": { \"interval\": { \"start_time\": 1656341042874, \"period\": 1, \"unit\": \"Minutes\" } }, \"enabled\": true, \"last_updated_time\": 1656341042874, \"enabled_time\": 1656341042874 } } Explain\nIntroduced 2.1\nProvides the enabled/disabled status and the metadata for all policies specified. Multiple policy names are separated with,. You can also specify desired policies with a wildcard pattern. SM uses a state machine for snapshot creation and deletion. The image on the left shows one execution period of the creation workflow, from the CREATION_START state to the CREATION_FINISHED state. Deletion workflow follows the same pattern as creation workflow.\nThe creation workflow starts in the CREATION_START state and continuously checks if the conditions in the creation cron schedule are met. After the conditions are met, the creation workflow switches to the CREATION_CONDITION_MET state and continues to the CREATING state. The CREATING state calls the create snapshot API asynchronously and then waits for snapshot creation to end in the CREATION_FINISHED state. Once snapshot creation ends, the creation workflow goes back to the CREATION_START state, and the cycle continues. The current_state field of metadata.creation and metadata.deletion returns the current state of the state machine.\nRequest GET _plugins/_sm/policies/&lt;policy_names&gt;/_explain Example GET _plugins/_sm/policies/daily*/_explain Response { \"policies\": [ { \"name\": \"daily-policy\", \"creation\": { \"current_state\": \"CREATION_START\", \"trigger\": { \"time\": 1656403200000 } }, \"deletion\": { \"current_state\": \"DELETION_START\", \"trigger\": { \"time\": 1656403200000 } }, \"policy_seq_no\": 44696, \"policy_primary_term\": 19, \"enabled\": true }] } The following table lists all fields for each policy in the response. Field Description name The name of the SM policy. creation Information about the latest creation operation. See subfields below. deletion Information about the latest deletion operation. See subfields below. policy_seq_no policy_primary_term The version of the SM policy. enabled Is the policy running? The following table lists all fields in the creation and deletion objects of each policy. Field Description current_state The current state of the state machine that runs snapshot creation/deletion as described above. trigger.time The next creation/deletion execution time in milliseconds since the epoch. latest_execution Describes the latest creation/deletion execution. latest_execution.status The execution status of the latest creation/deletion. Possible values are: IN_PROGRESS: Snapshot creation/deletion has started. SUCCESS: Snapshot creation/deletion has finished successfully. RETRYING: The creation/deletion attempt has failed. It will be retried three times. FAILED: The creation/deletion attempt failed after three retries. End the current execution period and go to the next execution period. TIME_LIMIT_EXCEEDED: The creation/deletion time exceeded the time_limit set in the policy. End the current execution period and go to the next execution period. latest_execution.start_time The start time of the latest execution in milliseconds since the epoch. latest_execution.end_time The end time of the latest execution in milliseconds since the epoch. latest_execution.info.message A user-friendly message describing the status of the latest execution. latest_execution.info.cause Contains the failure reason if the latest execution fails. retry.count The number of remaining execution retry attempts. Start a policy\nIntroduced 2.1\nStarts the policy by setting its enabled flag to true.\nRequest POST _plugins/_sm/policies/&lt;policy_name&gt;/_start Example POST _plugins/_sm/policies/daily-policy/_start Response { \"acknowledged\": true } Stop a policy\nIntroduced 2.1\nSets the enabled flag to false for an SM policy. The policy will not run until you start it.\nRequest POST _plugins/_sm/policies/&lt;policy_name&gt;/_stop Example POST _plugins/_sm/policies/daily-policy/_stop Response { \"acknowledged\": true } Delete a policy\nIntroduced 2.1\nDeletes the specified SM policy.\nRequest DELETE _plugins/_sm/policies/&lt;policy_name&gt; Example DELETE _plugins/_sm/policies/daily-policy Response { \"_index\": \".opendistro-ism-config\", \"_id\": \"daily-policy-sm-policy\", \"_version\": 8, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 45366, \"_primary_term\": 20 }",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery",
      "Snapshots"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-management/",
    "title": "Snapshot management",
    "content": "Snapshot management (SM) lets you automate taking snapshots. To use this feature, you need to install the Index Management (IM) Plugin. Snapshots store only incremental changes since the last snapshot. Thus, while taking an initial snapshot may be a heavy operation, subsequent snapshots have minimal overhead. To set up automatic snapshots, you have to create an SM policy with a desired SM schedule and configuration.\nWhen you create an SM policy, its document ID is given the name &lt;policy_name&gt;-sm-policy. Because of this, SM policies have to obey the following rules:\nSM policies must have unique names.\nYou cannot update the policy name after its creation.\nSM-created snapshots have names in the format &lt;policy_name&gt;-&lt;date&gt;-&lt;random number&gt;. Two snapshots created by different policies at the same time always have different names because of the &lt;policy_name&gt; prefix. To avoid name collisions within the same policy, each snapshot’s name contains a random string suffix.\nEach policy has associated metadata that stores the policy status. Snapshot management saves SM policies and metadata in the system index and reads them from the system index. Thus, Snapshot Management depends on the OpenSearch cluster’s indexing and searching functions. The policy’s metadata keeps information about the latest creation and deletion only. The metadata is read before running every scheduled job so that SM can continue execution from the previous job’s state. You can view the metadata using the explain API.\nAn SM schedule is a custom cron expression. It consists of two parts: a creation schedule and a deletion schedule. You must set up a creation schedule that specifies the frequency and timing of snapshot creation. Optionally, you can set up a separate schedule for deleting snapshots.\nAn SM configuration includes the indexes and repository for the snapshots and supports all parameters you can define when creating a snapshot using the API. Additionally, you can specify the format and time zone for the date used in the snapshot’s name.\nPerformance\nOne snapshot can contain as many indexes as there are in the cluster. We expect at most dozens of SM policies in one cluster, but a snapshot repository can safely scale to thousands of snapshots. However, to manage its metadata, a large repository requires more memory on the cluster manager node.\nSnapshot Management depends on the Job Scheduler plugin to schedule a job that is run periodically. Each SM policy corresponds to one SM-scheduled job. The scheduled job is lightweight, so the burden of SM depends on the snapshot creation frequency and the burden of running the snapshot operation itself.\nConcurrency\nAn SM policy does not support concurrent snapshot operations, since too many such operations may degrade the cluster. Snapshot operations (creation or deletion) are performed asynchronously. SM does not start a new operation until the previous asynchronous operation finishes.\nWe don’t recommend creating several SM policies with the same schedule and overlapping indexes in one cluster because it leads to concurrent snapshot creation on the same indexes and hinders performance.\nWe don’t recommend setting up the same repository for multiple SM policies with same schedule in different clusters, since it may cause a sudden spike of burden in this repository.\nFailure management\nIf a snapshot operation fails, it is retried a maximum of three times. The failure message is saved in metadata.latest_execution and is overwritten when a subsequent snapshot operation starts. You can view the failure message using the explain API. When using OpenSearch Dashboards, you can view the failure message on the policy details page. Possible reasons for failure include red index status and shard reallocation.\nSecurity\nThe Security plugin has two built-in roles for Snapshot Management actions: snapshot_management_full_access and snapshot_management_read_access. For descriptions of each, see Predefined roles.\nThe following table lists the required permissions for each Snapshot Management API. Function API Permission Get policy\nGET _plugins/_sm/policies GET _plugins/_sm/policies/ policy_name cluster:admin/opensearch/snapshot_management/policy/get cluster:admin/opensearch/snapshot_management/policy/search\nCreate/update policy\nPOST _plugins/_sm/policies/ policy_name PUT _plugins/_sm/policies/ policy_name?if_seq_no=1&amp;if_primary_term=1\ncluster:admin/opensearch/snapshot_management/policy/write\nDelete policy\nDELETE _plugins/_sm/policies/ policy_name cluster:admin/opensearch/snapshot_management/policy/delete\nExplain\nGET _plugins/_sm/policies/ policy_names /_explain\ncluster:admin/opensearch/snapshot_management/policy/explain\nStart\nPOST _plugins/_sm/policies/ policy_name /_start\ncluster:admin/opensearch/snapshot_management/policy/start\nStop\nPOST _plugins/_sm/policies/ policy_name /_stop\ncluster:admin/opensearch/snapshot_management/policy/stop API\nThe following table lists all Snapshot Management API functions. Function API Description Create policy POST _plugins/_sm/policies/ policy_name Creates an SM policy. Update policy PUT _plugins/_sm/policies/ policy_name?if_seq_no= sequence_number &amp;if_primary_term= primary_term Modifies the policy_name policy. Get all policies GET _plugins/_sm/policies\nReturns all SM policies. Get the policy policy_name GET _plugins/_sm/policies/ policy_name Returns the policy_name SM policy. Delete policy DELETE _plugins/_sm/policies/ policy_name Deletes the policy_name policy. Explain GET _plugins/_sm/policies/ policy_names /_explain\nProvides the enabled/disabled status and the metadata for all policies specified by policy_names. Start policy POST _plugins/_sm/policies/ policy_name /_start\nStarts the policy_name policy. Stop policy POST _plugins/_sm/policies/ policy_name /_stop\nStops the policy_name policy.",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery",
      "Snapshots"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/snapshots/snapshot-restore/",
    "title": "Take and restore snapshots",
    "content": "Snapshots aren’t instantaneous. They take time to complete and do not represent perfect point-in-time views of the cluster. While a snapshot is in progress, you can still index documents and send other requests to the cluster, but new documents and updates to existing documents generally aren’t included in the snapshot. The snapshot includes primary shards as they existed when OpenSearch initiated the snapshot. Depending on the size of your snapshot thread pool, different shards might be included in the snapshot at slightly different times.\nOpenSearch snapshots are incremental, meaning that they only store data that has changed since the last successful snapshot. The difference in disk usage between frequent and infrequent snapshots is often minimal.\nIn other words, taking hourly snapshots for a week (for a total of 168 snapshots) might not use much more disk space than taking a single snapshot at the end of the week. Also, the more frequently you take snapshots, the less time they take to complete. Some OpenSearch users take snapshots as often as every 30 minutes.\nIf you need to delete a snapshot, be sure to use the OpenSearch API rather than navigating to the storage location and purging files. Incremental snapshots from a cluster often share a lot of the same data; when you use the API, OpenSearch only removes data that no other snapshot is using.\nTable of contents Register repository Shared file system Amazon S3 Take snapshots Restore snapshots Conflicts and compatibility Security considerations Register repository\nBefore you can take a snapshot, you have to “register” a snapshot repository. A snapshot repository is just a storage location: a shared file system, Amazon S3, Hadoop Distributed File System (HDFS), Azure Storage, etc.\nShared file system\nTo use a shared file system as a snapshot repository, add it to opensearch.yml: path.repo: [ \" /mnt/snapshots\"] On the RPM and Debian installs, you can then mount the file system. If you’re using the Docker install, add the file system to each node in docker-compose.yml before starting the cluster: volumes: - /Users/jdoe/snapshots:/mnt/snapshots Then register the repository using the REST API: PUT /_snapshot/my-fs-repository { \"type\": \"fs\", \"settings\": { \"location\": \"/mnt/snapshots\" } } copy You will most likely not need to specify any parameters except for location. For allowed request parameters, see Register or update snapshot repository API.\nAmazon S3\nTo use an Amazon S3 bucket as a snapshot repository, install the repository-s3 plugin on all nodes: sudo./bin/opensearch-plugin install repository-s3 If you’re using the Docker installation, see Working with plugins. Your Dockerfile should look something like this: FROM opensearchproject/opensearch:2.7.0\nENV AWS_ACCESS_KEY_ID &lt;access-key&gt;\nENV AWS_SECRET_ACCESS_KEY &lt;secret-key&gt;\n# Optional\nENV AWS_SESSION_TOKEN &lt;optional-session-token&gt;\nRUN /usr/share/opensearch/bin/opensearch-plugin install --batch repository-s3\nRUN /usr/share/opensearch/bin/opensearch-keystore create\nRUN echo $AWS_ACCESS_KEY_ID | /usr/share/opensearch/bin/opensearch-keystore add --stdin s3.client.default.access_key\nRUN echo $AWS_SECRET_ACCESS_KEY | /usr/share/opensearch/bin/opensearch-keystore add --stdin s3.client.default.secret_key\n# Optional\nRUN echo $AWS_SESSION_TOKEN | /usr/share/opensearch/bin/opensearch-keystore add --stdin s3.client.default.session_token After the Docker cluster starts, skip to step 7.\nAdd your AWS access and secret keys to the OpenSearch keystore: sudo./bin/opensearch-keystore add s3.client.default.access_key sudo./bin/opensearch-keystore add s3.client.default.secret_key (Optional) If you’re using temporary credentials, add your session token: sudo./bin/opensearch-keystore add s3.client.default.session_token (Optional) If you connect to the internet through a proxy, add those credentials: sudo./bin/opensearch-keystore add s3.client.default.proxy.username sudo./bin/opensearch-keystore add s3.client.default.proxy.password (Optional) Add other settings to opensearch.yml: s3.client.default.disable_chunked_encoding: false # Disables chunked encoding for compatibility with some storage services, but you probably don't need to change this value. s3.client.default.endpoint: s3.amazonaws.com # S3 has alternate endpoints, but you probably don't need to change this value. s3.client.default.max_retries: 3 # number of retries if a request fails s3.client.default.path_style_access: false # whether to use the deprecated path-style bucket URLs. # You probably don't need to change this value, but for more information, see https://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html#path-style-access. s3.client.default.protocol: https # http or https s3.client.default.proxy.host: my-proxy-host # the hostname for your proxy server s3.client.default.proxy.port: 8080 # port for your proxy server s3.client.default.read_timeout: 50s # the S3 connection timeout s3.client.default.use_throttle_retries: true # whether the client should wait a progressively longer amount of time (exponential backoff) between each successive retry s3.client.default.region: us-east-2 # AWS region to use (Optional) If you don’t want to use AWS access and secret keys, you could configure the S3 plugin to use AWS Identity and Access Management (IAM) roles for service accounts: sudo./bin/opensearch-keystore add s3.client.default.role_arn sudo./bin/opensearch-keystore add s3.client.default.role_session_name If you don’t want to configure AWS access and secret keys, modify the following opensearch.yml setting. Make sure the file is accessible by the repository-s3 plugin: s3.client.default.identity_token_file: /usr/share/opensearch/plugins/repository-s3/token If copying is not an option, you can create a symlink to the web identity token file in the ${OPENSEARCH_PATH_CONFIG} folder: ln -s $AWS_WEB_IDENTITY_TOKEN_FILE \"${OPENSEARCH_PATH_CONFIG}/aws-web-identity-token-file\" You can reference the web identity token file in the following opensearch.yml setting by specifying the relative path that is resolved against ${OPENSEARCH_PATH_CONFIG}: s3.client.default.identity_token_file: aws-web-identity-token-file IAM roles require at least one of the above settings. Other settings will be taken from environment variables (if available): AWS_ROLE_ARN, AWS_WEB_IDENTITY_TOKEN_FILE, AWS_ROLE_SESSION_NAME.\nIf you changed opensearch.yml, you must restart each node in the cluster. Otherwise, you only need to reload secure cluster settings: POST /_nodes/reload_secure_settings copy Create an S3 bucket if you don’t already have one. To take snapshots, you need permissions to access the bucket. The following IAM policy is an example of those permissions: { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Action\": [ \"s3:*\"], \"Effect\": \"Allow\", \"Resource\": [ \"arn:aws:s3:::your-bucket\", \"arn:aws:s3:::your-bucket/*\"] }] } Register the repository using the REST API: PUT /_snapshot/my-s 3 -repository { \"type\": \"s3\", \"settings\": { \"bucket\": \"my-s3-bucket\", \"base_path\": \"my/snapshot/directory\" } } copy You will most likely not need to specify any parameters except for bucket and base_path. For allowed request parameters, see Register or update snapshot repository API.\nTake snapshots\nYou specify two pieces of information when you create a snapshot:\nName of your snapshot repository\nName for the snapshot\nThe following snapshot includes all indices and the cluster state: PUT /_snapshot/my-repository/ 1 copy You can also add a request body to include or exclude certain indices or specify other settings: PUT /_snapshot/my-repository/ 2 { \"indices\": \"opensearch-dashboards*,my-index*,-my-index-2016\", \"ignore_unavailable\": true, \"include_global_state\": false, \"partial\": false } copy Request fields Description indices The indices you want to include in the snapshot. You can use, to create a list of indices, * to specify an index pattern, and - to exclude certain indices. Don’t put spaces between items. Default is all indices. ignore_unavailable If an index from the indices list doesn’t exist, whether to ignore it rather than fail the snapshot. Default is false. include_global_state Whether to include cluster state in the snapshot. Default is true. partial Whether to allow partial snapshots. Default is false, which fails the entire snapshot if one or more shards fails to store. If you request the snapshot immediately after taking it, you might see something like this: GET /_snapshot/my-repository/ 2 { \"snapshots\": [{ \"snapshot\": \"2\", \"version\": \"6.5.4\", \"indices\": [ \"opensearch_dashboards_sample_data_ecommerce\", \"my-index\", \"opensearch_dashboards_sample_data_logs\", \"opensearch_dashboards_sample_data_flights\"], \"include_global_state\": true, \"state\": \"IN_PROGRESS\",... }] } copy Note that the snapshot is still in progress. If you want to wait for the snapshot to finish before continuing, add the wait_for_completion parameter to your request. Snapshots can take a while to complete, so consider whether or not this option fits your use case: PUT _snapshot/my-repository/3?wait_for_completion=true copy Snapshots have the following states: State Description SUCCESS\nThe snapshot successfully stored all shards.\nIN_PROGRESS\nThe snapshot is currently running.\nPARTIAL\nAt least one shard failed to store successfully. Can only occur if you set partial to true when taking the snapshot.\nFAILED\nThe snapshot encountered an error and stored no data.\nINCOMPATIBLE\nThe snapshot is incompatible with the version of OpenSearch running on this cluster. See Conflicts and compatibility. You can’t take a snapshot if one is currently in progress. To check the status: GET /_snapshot/_status copy Restore snapshots\nThe first step in restoring a snapshot is retrieving existing snapshots. To see all snapshot repositories: GET /_snapshot/_all copy To see all snapshots in a repository: GET /_snapshot/my-repository/_all copy Then restore a snapshot: POST /_snapshot/my-repository/2/_restore copy Just like when taking a snapshot, you can add a request body to include or exclude certain indices or specify some other settings: POST /_snapshot/my-repository/ 2 /_restore { \"indices\": \"opensearch-dashboards*,my-index*\", \"ignore_unavailable\": true, \"include_global_state\": false, \"include_aliases\": false, \"partial\": false, \"rename_pattern\": \"opensearch-dashboards(.+)\", \"rename_replacement\": \"restored-opensearch-dashboards$1\", \"index_settings\": { \"index.blocks.read_only\": false }, \"ignore_index_settings\": [ \"index.refresh_interval\"] } copy Request parameters Description indices The indices you want to restore. You can use, to create a list of indices, * to specify an index pattern, and - to exclude certain indices. Don’t put spaces between items. Default is all indices. ignore_unavailable If an index from the indices list doesn’t exist, whether to ignore it rather than fail the restore operation. Default is false. include_global_state Whether to restore the cluster state. Default is false. include_aliases Whether to restore aliases alongside their associated indices. Default is true. partial Whether to allow the restoration of partial snapshots. Default is false. rename_pattern If you want to rename indices as you restore them, use this option to specify a regular expression that matches all indices you want to restore. Use capture groups ( ()) to reuse portions of the index name. rename_replacement If you want to rename indices as you restore them, use this option to specify the replacement pattern. Use $0 to include the entire matching index name, $1 to include the content of the first capture group, etc. index_settings If you want to change index settings applied during restore, specify them here. You cannot change index.number_of_shards. ignore_index_settings Rather than explicitly specifying new settings with index_settings, you can ignore certain index settings in the snapshot and use the cluster defaults applied during restore. You cannot ignore index.number_of_shards, index.number_of_replicas, or index.auto_expand_replicas. storage_type local indicates that all snapshot metadata and index data will be downloaded to local storage. remote_snapshot indicates that snapshot metadata will be downloaded to the cluster, but the remote repository will remain the authoritative store of the index data. Data will be downloaded and cached as necessary to service queries. At least one node in the cluster must be configured with the search role in order to restore a snapshot using the type remote_snapshot. Defaults to local. Conflicts and compatibility\nOne way to avoid naming conflicts when restoring indices is to use the rename_pattern and rename_replacement options. Then, if necessary, you can use the _reindex API to combine the two. The simpler way is to delete existing indices prior to restoring from a snapshot.\nYou can use the _close API to close existing indices prior to restoring from a snapshot, but the index in the snapshot has to have the same number of shards as the existing index.\nWe recommend ceasing write requests to a cluster before restoring from a snapshot, which helps avoid scenarios such as:\nYou delete an index, which also deletes its alias.\nA write request to the now-deleted alias creates a new index with the same name as the alias.\nThe alias from the snapshot fails to restore due to a naming conflict with the new index.\nSnapshots are only forward-compatible by one major version. If you have an old snapshot, you can sometimes restore it into an intermediate cluster, reindex all indices, take a new snapshot, and repeat until you arrive at your desired version, but you might find it easier to just manually index your data on the new cluster.\nSecurity considerations\nIf you’re using the Security plugin, snapshots have some additional restrictions:\nTo perform snapshot and restore operations, users must have the built-in manage_snapshots role.\nYou can’t restore snapshots that contain global state or the.opendistro_security index.\nIf a snapshot contains global state, you must exclude it when performing the restore. If your snapshot also contains the.opendistro_security index, either exclude it or list all the other indices you want to include: POST /_snapshot/my-repository/ 3 /_restore { \"indices\": \"-.opendistro_security\", \"include_global_state\": false } copy The.opendistro_security index contains sensitive data, so we recommend excluding it when you take a snapshot. If you do need to restore the index from a snapshot, you must include an admin certificate in the request: curl -k --cert./kirk.pem --key./kirk-key.pem -XPOST 'https://localhost:9200/_snapshot/my-repository/3/_restore?pretty' copy We strongly recommend against restoring.opendistro_security using an admin certificate because doing so can alter the security posture of the entire cluster. See A word of caution for a recommended process to back up and restore your Security plugin configuration.",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery",
      "Snapshots"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/availability-and-recovery/stats-api/",
    "title": "Stats API",
    "content": "Use the stats operation to monitor shard indexing backpressure.\nStats\nIntroduced 1.2\nReturns node-level and shard-level stats for indexing request rejections.\nRequest GET _nodes/_local/stats/shard_indexing_pressure If enforced is true:\nExample response { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072111162, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\"], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": { \"[index_name][0]\": { \"memory\": { \"current\": { \"coordinating_in_bytes\": 0, \"primary_in_bytes\": 0, \"replica_in_bytes\": 0 }, \"total\": { \"coordinating_in_bytes\": 299, \"primary_in_bytes\": 299, \"replica_in_bytes\": 0 } }, \"rejection\": { \"coordinating\": { \"coordinating_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"primary\": { \"primary_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"replica\": { \"replica_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } } }, \"last_successful_timestamp\": { \"coordinating_last_successful_request_timestamp_in_millis\": 1613072107990, \"primary_last_successful_request_timestamp_in_millis\": 0, \"replica_last_successful_request_timestamp_in_millis\": 0 }, \"indexing\": { \"coordinating_time_in_millis\": 96, \"coordinating_count\": 1, \"primary_time_in_millis\": 0, \"primary_count\": 0, \"replica_time_in_millis\": 0, \"replica_count\": 0 }, \"memory_allocation\": { \"current\": { \"current_coordinating_and_primary_bytes\": 0, \"current_replica_bytes\": 0 }, \"limit\": { \"current_coordinating_and_primary_limits_in_bytes\": 51897, \"current_replica_limits_in_bytes\": 77845 } } } }, \"total_rejections_breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\": true } } } } If enforced is false:\nExample response { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072111162, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\"], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": { \"[index_name][0]\": { \"memory\": { \"current\": { \"coordinating_in_bytes\": 0, \"primary_in_bytes\": 0, \"replica_in_bytes\": 0 }, \"total\": { \"coordinating_in_bytes\": 299, \"primary_in_bytes\": 299, \"replica_in_bytes\": 0 } }, \"rejection\": { \"coordinating\": { \"coordinating_rejections\": 0, \"breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"primary\": { \"primary_rejections\": 0, \"breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"replica\": { \"replica_rejections\": 0, \"breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } } }, \"last_successful_timestamp\": { \"coordinating_last_successful_request_timestamp_in_millis\": 1613072107990, \"primary_last_successful_request_timestamp_in_millis\": 0, \"replica_last_successful_request_timestamp_in_millis\": 0 }, \"indexing\": { \"coordinating_time_in_millis\": 96, \"coordinating_count\": 1, \"primary_time_in_millis\": 0, \"primary_count\": 0, \"replica_time_in_millis\": 0, \"replica_count\": 0 }, \"memory_allocation\": { \"current\": { \"current_coordinating_and_primary_bytes\": 0, \"current_replica_bytes\": 0 }, \"limit\": { \"current_coordinating_and_primary_limits_in_bytes\": 51897, \"current_replica_limits_in_bytes\": 77845 } } } }, \"total_rejections_breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\": false } } } } To include all the shards with both active and previous write operations performed on them, specify the include_all parameter:\nRequest GET _nodes/_local/stats/shard_indexing_pressure?include_all Example response { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072198171, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\"], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": { \"[index_name][0]\": { \"memory\": { \"current\": { \"coordinating_in_bytes\": 0, \"primary_in_bytes\": 0, \"replica_in_bytes\": 0 }, \"total\": { \"coordinating_in_bytes\": 604, \"primary_in_bytes\": 604, \"replica_in_bytes\": 0 } }, \"rejection\": { \"coordinating\": { \"coordinating_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"primary\": { \"primary_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } }, \"replica\": { \"replica_rejections\": 0, \"breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 } } }, \"last_successful_timestamp\": { \"coordinating_last_successful_request_timestamp_in_millis\": 1613072194656, \"primary_last_successful_request_timestamp_in_millis\": 0, \"replica_last_successful_request_timestamp_in_millis\": 0 }, \"indexing\": { \"coordinating_time_in_millis\": 145, \"coordinating_count\": 2, \"primary_time_in_millis\": 0, \"primary_count\": 0, \"replica_time_in_millis\": 0, \"replica_count\": 0 }, \"memory_allocation\": { \"current\": { \"current_coordinating_and_primary_bytes\": 0, \"current_replica_bytes\": 0 }, \"limit\": { \"current_coordinating_and_primary_limits_in_bytes\": 51897, \"current_replica_limits_in_bytes\": 77845 } } } }, \"total_rejections_breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\": true } } } } To get only all the top-level aggregated stats, specify the top parameter (skips the per-shard stats).\nRequest GET _nodes/_local/stats/shard_indexing_pressure?top If enforced is true:\nExample response { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072382719, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\"], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": {}, \"total_rejections_breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\": true } } } } If enforced is false:\nExample response { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072382719, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\"], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": {}, \"total_rejections_breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\": false } } } } To get the shard-level breakup of rejections for every node (only includes shards with active write operations):\nRequest GET _nodes/stats/shard_indexing_pressure Example response { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"runTask\", \"nodes\": { \"q3e1dQjFSqyPSLAgpyQlfw\": { \"timestamp\": 1613072382719, \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"data\", \"ingest\", \"cluster_manager\", \"remote_cluster_client\"], \"attributes\": { \"testattr\": \"test\" }, \"shard_indexing_pressure\": { \"stats\": {}, \"total_rejections_breakup\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": true, \"enforced\": true } } } }",
    "ancestors": [
      "Tuning your cluster",
      "Availability and Recovery",
      "Shard indexing backpressure"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/cluster-manager-task-throttling/",
    "title": "Cluster manager task throttling",
    "content": "For many cluster state updates, such as defining a mapping or creating an index, nodes submit tasks to the cluster manager. The cluster manager maintains a pending task queue for these tasks and runs them in a single-threaded environment. When nodes send tens of thousands of resource-intensive tasks, like put-mapping or snapshot tasks, these tasks can pile up in the queue and flood the cluster manager. This affects the cluster manager’s performance and may in turn affect the availability of the whole cluster.\nThe first line of defense is to implement mechanisms in the caller nodes to avoid task overload on the cluster manager. However, even with those mechanisms in place, the cluster manager needs a built-in way to protect itself: cluster manager task throttling.\nTo turn on cluster manager task throttling, you need to set throttling limits. The cluster manager uses the throttling limits to determine whether to reject a task.\nThe cluster manager rejects a task based on its type. For any incoming task, the cluster manager evaluates the total number of tasks of the same type in the pending task queue. If this number exceeds the threshold for this task type, the cluster manager rejects the incoming task. Rejecting a task does not affect tasks of a different type. For example, if the cluster manager rejects a put-mapping task, it can still accept a subsequent create-index task.\nWhen the cluster manager rejects a task, the node performs retries with exponential backoff to resubmit the task to the cluster manager. If retries are unsuccessful within the timeout period, OpenSearch returns a cluster timeout error.\nSetting throttling limits\nYou can set throttling limits by specifying them in the cluster_manager.throttling.thresholds object and updating the OpenSearch cluster settings. The setting is dynamic, so you can change the behavior of this feature without restarting your cluster.\nBy default, throttling is disabled for all task types.\nThe request has the following format: PUT _cluster/settings { \"persistent\": { \"cluster_manager.throttling.thresholds\": { \"&lt;task-type&gt;\": { \"value\": &lt;threshold limit&gt; } } } } The following table describes the cluster_manager.throttling.thresholds object. Field Name Description task-type\nThe task type. See supported task types for a list of valid values.\nvalue\nThe maximum number of tasks of the task-type type in the cluster manager’s pending task queue. Default is -1 (no task throttling). Supported task types\nThe following task types are supported: create-index update-settings cluster-update-settings auto-create delete-index delete-dangling-index create-data-stream remove-data-stream rollover-index index-aliases put-mapping create-index-template remove-index-template create-component-template remove-component-template create-index-template-v2 remove-index-template-v2 put-pipeline delete-pipeline create-persistent-task finish-persistent-task remove-persistent-task update-task-state put-script delete-script put-repository delete-repository create-snapshot delete-snapshot update-snapshot-state restore-snapshot cluster-reroute-api Example request\nThe following request sets the throttling threshold for the put-mapping task type to 100: PUT _cluster/settings { \"persistent\": { \"cluster_manager.throttling.thresholds\": { \"put-mapping\": { \"value\": 100 } } } } Set the threshold to -1 to disable throttling for a task type.",
    "ancestors": [
      "Tuning your cluster"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/cluster/",
    "title": "Creating a cluster",
    "content": "Before diving into OpenSearch and searching and aggregating data, you first need to create an OpenSearch cluster.\nOpenSearch can operate as a single-node or multi-node cluster. The steps to configure both are, in general, quite similar. This page demonstrates how to create and configure a multi-node cluster, but with only a few minor adjustments, you can follow the same steps to create a single-node cluster.\nTo create and deploy an OpenSearch cluster according to your requirements, it’s important to understand how node discovery and cluster formation work and what settings govern them.\nThere are many ways to design a cluster. The following illustration shows a basic architecture that includes a four-node cluster that has one dedicated cluster manager node, one dedicated coordinating node, and two data nodes that are cluster manager eligible and also used for ingesting data.\nThe nomenclature recently changed for the master node; it is now called the cluster manager node. Nodes\nThe following table provides brief descriptions of the node types: Node type Description Best practices for production Cluster manager\nManages the overall operation of a cluster and keeps track of the cluster state. This includes creating and deleting indexes, keeping track of the nodes that join and leave the cluster, checking the health of each node in the cluster (by running ping requests), and allocating shards to nodes.\nThree dedicated cluster manager nodes in three different zones is the right approach for almost all production use cases. This configuration ensures your cluster never loses quorum. Two nodes will be idle for most of the time except when one node goes down or needs some maintenance.\nCluster manager eligible\nElects one node among them as the cluster manager node through a voting process.\nFor production clusters, make sure you have dedicated cluster manager nodes. The way to achieve a dedicated node type is to mark all other node types as false. In this case, you have to mark all the other nodes as not cluster manager eligible.\nData\nStores and searches data. Performs all data-related operations (indexing, searching, aggregating) on local shards. These are the worker nodes of your cluster and need more disk space than any other node type.\nAs you add data nodes, keep them balanced between zones. For example, if you have three zones, add data nodes in multiples of three, one for each zone. We recommend using storage and RAM-heavy nodes.\nIngest\nPre-processes data before storing it in the cluster. Runs an ingest pipeline that transforms your data before adding it to an index.\nIf you plan to ingest a lot of data and run complex ingest pipelines, we recommend you use dedicated ingest nodes. You can also optionally offload your indexing from the data nodes so that your data nodes are used exclusively for searching and aggregating.\nCoordinating\nDelegates client requests to the shards on the data nodes, collects and aggregates the results into one final result, and sends this result back to the client.\nA couple of dedicated coordinating-only nodes is appropriate to prevent bottlenecks for search-heavy workloads. We recommend using CPUs with as many cores as you can.\nDynamic\nDelegates a specific node for custom work, such as machine learning (ML) tasks, preventing the consumption of resources from data nodes and therefore not affecting any OpenSearch functionality.\n  By default, each node is a cluster-manager-eligible, data, ingest, and coordinating node. Deciding on the number of nodes, assigning node types, and choosing the hardware for each node type depends on your use case. You must take into account factors like the amount of time you want to hold on to your data, the average size of your documents, your typical workload (indexing, searches, aggregations), your expected price-performance ratio, your risk tolerance, and so on.\nAfter you assess all these requirements, we recommend you use a benchmark testing tool like OpenSearch Benchmark to provision a small sample cluster and run tests with varying workloads and configurations. Compare and analyze the system and query metrics for these tests to design an optimum architecture.\nThis page demonstrates how to work with the different node types. It assumes that you have a four-node cluster similar to the preceding illustration.\nPrerequisites\nBefore you get started, you must install and configure OpenSearch on all of your nodes. For information about the available options, see Install and configure OpenSearch.\nAfter you’re done, use SSH to connect to each node, then open the config/opensearch.yml file. You can set all configurations for your cluster in this file.\nStep 1: Name a cluster\nSpecify a unique name for the cluster. If you don’t specify a cluster name, it’s set to opensearch by default. Setting a descriptive cluster name is important, especially if you want to run multiple clusters inside a single network.\nTo specify the cluster name, change the following line: #cluster.name: my-application to cluster.name: opensearch-cluster Make the same change on all the nodes to make sure that they’ll join to form a cluster.\nStep 2: Set node attributes for each node in a cluster\nAfter you name the cluster, set node attributes for each node in your cluster.\nCluster manager node\nGive your cluster manager node a name. If you don’t specify a name, OpenSearch assigns a machine-generated name that makes the node difficult to monitor and troubleshoot. node.name: opensearch-cluster_manager You can also explicitly specify that this node is a cluster manager node, even though it is already set to true by default. Set the node role to cluster_manager to make it easier to identify the cluster manager node. node.roles: [ cluster_manager] Data nodes\nChange the name of two nodes to opensearch-d1 and opensearch-d2, respectively: node.name: opensearch-d1 node.name: opensearch-d2 You can make them cluster-manager-eligible data nodes that will also be used for ingesting data: node.roles: [ data, ingest] You can also specify any other attributes that you’d like to set for the data nodes.\nCoordinating node\nChange the name of the coordinating node to opensearch-c1: node.name: opensearch-c1 Every node is a coordinating node by default, so to make this node a dedicated coordinating node, set node.roles to an empty list: node.roles: [] Step 3: Bind a cluster to specific IP addresses network_host defines the IP address used to bind the node. By default, OpenSearch listens on a local host, which limits the cluster to a single node. You can also use _local_ and _site_ to bind to any loopback or site-local address, whether IPv4 or IPv6: network.host: [ _local_, _site_] To form a multi-node cluster, specify the IP address of the node: network.host: &lt;IP address of the node&gt; Make sure to configure these settings on all of your nodes.\nStep 4: Configure discovery hosts for a cluster\nNow that you’ve configured the network hosts, you need to configure the discovery hosts.\nZen Discovery is the built-in, default mechanism that uses unicast to find other nodes in the cluster.\nYou can generally just add all of your cluster-manager-eligible nodes to the discovery.seed_hosts array. When a node starts up, it finds the other cluster-manager-eligible nodes, determines which one is the cluster manager, and asks to join the cluster.\nFor example, for opensearch-cluster_manager the line looks something like this: discovery.seed_hosts: [ \" &lt;private IP of opensearch-d1&gt;\", \" &lt;private IP of opensearch-d2&gt;\", \" &lt;private IP of opensearch-c1&gt;\"] Step 5: Start the cluster\nAfter you set the configurations, start OpenSearch on all nodes: sudo systemctl start opensearch.service Installing OpenSearch from a tar archive will not automatically create a service with systemd. See Run OpenSearch as a service with systemd for instructions on how to create and start the service if you receive an error like Failed to start opensearch.service: Unit not found. Then go to the logs file to see the formation of the cluster: less /var/log/opensearch/opensearch-cluster.log Perform the following _cat query on any node to see all the nodes formed as a cluster: curl -XGET https://&lt;private-ip&gt;:9200/_cat/nodes?v -u 'admin:admin' --insecure ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role cluster_manager name\nx.x.x.x 13 61 0 0.02 0.04 0.05 mi * opensearch-cluster_manager\nx.x.x.x 16 60 0 0.06 0.05 0.05 md - opensearch-d1\nx.x.x.x 34 38 0 0.12 0.07 0.06 md - opensearch-d2\nx.x.x.x 23 38 0 0.12 0.07 0.06 md - opensearch-c1 To better understand and monitor your cluster, use the CAT API.\n(Advanced) Step 6: Configure shard allocation awareness or forced awareness\nShard allocation awareness\nIf your nodes are spread across several geographical zones, you can configure shard allocation awareness to allocate all replica shards to a zone that’s different from their primary shard.\nWith shard allocation awareness, if the nodes in one of your zones fail, you can be assured that your replica shards are spread across your other zones. It adds a layer of fault tolerance to ensure your data survives a zone failure beyond just individual node failures.\nTo configure shard allocation awareness, add zone attributes to opensearch-d1 and opensearch-d2, respectively: node.attr.zone: zoneA node.attr.zone: zoneB Update the cluster settings: PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.awareness.attributes\": \"zone\" } } You can either use persistent or transient settings. We recommend the persistent setting because it persists through a cluster reboot. Transient settings don’t persist through a cluster reboot.\nShard allocation awareness attempts to separate primary and replica shards across multiple zones. However, if only one zone is available (such as after a zone failure), OpenSearch allocates replica shards to the only remaining zone.\nForced awareness\nAnother option is to require that primary and replica shards are never allocated to the same zone. This is called forced awareness.\nTo configure forced awareness, specify all the possible values for your zone attributes: PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.awareness.attributes\": \"zone\", \"cluster.routing.allocation.awareness.force.zone.values\":[ \"zoneA\", \"zoneB\"] } } Now, if a data node fails, forced awareness doesn’t allocate the replicas to a node in the same zone. Instead, the cluster enters a yellow state and only allocates the replicas when nodes in another zone come online.\nIn our two-zone architecture, we can use allocation awareness if opensearch-d1 and opensearch-d2 are less than 50% utilized, so that each of them have the storage capacity to allocate replicas in the same zone.\nIf that is not the case, and opensearch-d1 and opensearch-d2 do not have the capacity to contain all primary and replica shards, we can use forced awareness. This approach helps to make sure that, in the event of a failure, OpenSearch doesn’t overload your last remaining zone and lock up your cluster due to lack of storage.\nChoosing allocation awareness or forced awareness depends on how much space you might need in each zone to balance your primary and replica shards.\nReplica count enforcement\nTo enforce an even distribution of shards across all zones and avoid hotspots, you can set the routing.allocation.awareness.balance attribute to true. This setting can be configured in the opensearch.yml file and dynamically updated using the cluster update settings API: PUT _cluster/settings { \"persistent\": { \"cluster\": { \"routing.allocation.awareness.balance\": \"true\" } } } The routing.allocation.awareness.balance setting is false by default. When it is set to true, the total number of shards for the index must be a multiple of the highest count for any awareness attribute. For example, consider a configuration with two awareness attributes—zones and rack IDs. Let’s say there are two zones and three rack IDs. The highest count of either the number of zones or the number of rack IDs is three. Therefore, the number of shards must be a multiple of three. If it is not, OpenSearch throws a validation exception. routing.allocation.awareness.balance takes effect only if cluster.routing.allocation.awareness.attributes and cluster.routing.allocation.awareness.force.zone.values are set. routing.allocation.awareness.balance applies to all operations that create or update indices. For example, let’s say you’re running a cluster with three nodes and three zones in a zone-aware setting. If you try to create an index with one replica or update an index’s settings to one replica, the attempt will fail with a validation exception because the number of shards must be a multiple of three. Similarly, if you try to create an index template with one shard and no replicas, the attempt will fail for the same reason. However, in all of those operations, if you set the number of shards to one and the number of replicas to two, the total number of shards is three and the attempt will succeed.\n(Advanced) Step 7: Set up a hot-warm architecture\nYou can design a hot-warm architecture where you first index your data to hot nodes—fast and expensive—and after a certain period of time move them to warm nodes—slow and cheap.\nIf you analyze time series data that you rarely update and want the older data to go onto cheaper storage, this architecture can be a good fit.\nThis architecture helps save money on storage costs. Rather than increasing the number of hot nodes and using fast, expensive storage, you can add warm nodes for data that you don’t access as frequently.\nTo configure a hot-warm storage architecture, add temp attributes to opensearch-d1 and opensearch-d2, respectively: node.attr.temp: hot node.attr.temp: warm You can set the attribute name and value to whatever you want as long as it’s consistent for all your hot and warm nodes.\nTo add an index newindex to the hot node: PUT newindex { \"settings\": { \"index.routing.allocation.require.temp\": \"hot\" } } Take a look at the following shard allocation for newindex: GET _cat/shards/newindex?v index shard prirep state docs store ip node new_index 2 p STARTED 0 230 b 10.0. 0.225 opensearch-d 1 new_index 2 r UNASSIGNED new_index 3 p STARTED 0 230 b 10.0. 0.225 opensearch-d 1 new_index 3 r UNASSIGNED new_index 4 p STARTED 0 230 b 10.0. 0.225 opensearch-d 1 new_index 4 r UNASSIGNED new_index 1 p STARTED 0 230 b 10.0. 0.225 opensearch-d 1 new_index 1 r UNASSIGNED new_index 0 p STARTED 0 230 b 10.0. 0.225 opensearch-d 1 new_index 0 r UNASSIGNED In this example, all primary shards are allocated to opensearch-d1, which is our hot node. All replica shards are unassigned, because we’re forcing this index to allocate only to hot nodes.\nTo add an index oldindex to the warm node: PUT oldindex { \"settings\": { \"index.routing.allocation.require.temp\": \"warm\" } } The shard allocation for oldindex: GET _cat/shards/oldindex?v index shard prirep state docs store ip node old_index 2 p STARTED 0 230 b 10.0. 0.74 opensearch-d 2 old_index 2 r UNASSIGNED old_index 3 p STARTED 0 230 b 10.0. 0.74 opensearch-d 2 old_index 3 r UNASSIGNED old_index 4 p STARTED 0 230 b 10.0. 0.74 opensearch-d 2 old_index 4 r UNASSIGNED old_index 1 p STARTED 0 230 b 10.0. 0.74 opensearch-d 2 old_index 1 r UNASSIGNED old_index 0 p STARTED 0 230 b 10.0. 0.74 opensearch-d 2 old_index 0 r UNASSIGNED In this case, all primary shards are allocated to opensearch-d2. Again, all replica shards are unassigned because we only have one warm node.\nA popular approach is to configure your index templates to set the index.routing.allocation.require.temp value to hot. This way, OpenSearch stores your most recent data on your hot nodes.\nYou can then use the Index State Management (ISM) plugin to periodically check the age of an index and specify actions to take on it. For example, when the index reaches a specific age, change the index.routing.allocation.require.temp setting to warm to automatically move your data from hot nodes to warm nodes.\nNext steps\nIf you are using the Security plugin, the previous request to _cat/nodes?v might have failed with an initialization error. For full guidance around using the Security plugin, see Security configuration.",
    "ancestors": [
      "Tuning your cluster"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/replication-plugin/api/",
    "title": "API",
    "content": "Use these replication operations to programmatically manage cross-cluster replication.\nTable of contents Start replication Stop replication Pause replication Resume replication Get replication status Get leader cluster stats Get follower cluster stats Get auto-follow stats Update settings Create replication rule Delete replication rule Start replication\nIntroduced 1.1\nInitiate replication of an index from the leader cluster to the follower cluster. Send this request to the follower cluster.\nRequest PUT /_plugins/_replication/&lt;follower-index&gt;/_start { \"leader_alias\": \"&lt;connection-alias-name&gt;\", \"leader_index\": \"&lt;index-name&gt;\", \"use_roles\":{ \"leader_cluster_role\": \"&lt;role-name&gt;\", \"follower_cluster_role\": \"&lt;role-name&gt;\" } } Specify the following options: Options Description Type Required leader_alias The name of the cross-cluster connection. You define this alias when you set up a cross-cluster connection. string Yes leader_index The index on the leader cluster that you want to replicate. string Yes use_roles The roles to use for all subsequent backend replication tasks between the indexes. Specify a leader_cluster_role and follower_cluster_role. See Map the leader and follower cluster roles. string If Security plugin is enabled Example response { \"acknowledged\": true } Stop replication\nIntroduced 1.1\nTerminates replication and converts the follower index to a standard index. Send this request to the follower cluster.\nRequest POST /_plugins/_replication/&lt;follower-index&gt;/_stop {} Example response { \"acknowledged\": true } Pause replication\nIntroduced 1.1\nPauses replication of the leader index. Send this request to the follower cluster.\nRequest POST /_plugins/_replication/&lt;follower-index&gt;/_pause {} You can’t resume replication after it’s been paused for more than 12 hours. You must stop replication, delete the follower index, and restart replication of the leader.\nExample response { \"acknowledged\": true } Resume replication\nIntroduced 1.1\nResumes replication of the leader index. Send this request to the follower cluster.\nRequest POST /_plugins/_replication/&lt;follower-index&gt;/_resume {} Example response { \"acknowledged\": true } Get replication status\nIntroduced 1.1\nGets the status of index replication. Possible statuses are SYNCING, BOOTSTRAPING, PAUSED, and REPLICATION NOT IN PROGRESS. Use the syncing details to measure replication lag. Send this request to the follower cluster.\nRequest GET /_plugins/_replication/&lt;follower-index&gt;/_status Example response { \"status\": \"SYNCING\", \"reason\": \"User initiated\", \"leader_alias\": \"my-connection-name\", \"leader_index\": \"leader-01\", \"follower_index\": \"follower-01\", \"syncing_details\": { \"leader_checkpoint\": 19, \"follower_checkpoint\": 19, \"seq_no\": 0 } } To include shard replication details in the response, add the &amp;verbose=true parameter.\nThe leader and follower checkpoint values begin as negative integers and reflect the shard count (-1 for one shard, -5 for five shards, and so on). The values increment toward positive integers with each change that you make. For example, when you make a change on the leader index, the leader_checkpoint becomes 0. The follower_checkpoint is initially still -1 until the follower index pulls the change from the leader, at which point it increments to 0. If the values are the same, it means the indexes are fully synced.\nGet leader cluster stats\nIntroduced 1.1\nGets information about replicated leader indexes on a specified cluster.\nRequest GET /_plugins/_replication/leader_stats Example response { \"num_replicated_indices\": 2, \"operations_read\": 15, \"translog_size_bytes\": 1355, \"operations_read_lucene\": 0, \"operations_read_translog\": 15, \"total_read_time_lucene_millis\": 0, \"total_read_time_translog_millis\": 659, \"bytes_read\": 1000, \"index_stats\":{ \"leader-index-1\":{ \"operations_read\": 7, \"translog_size_bytes\": 639, \"operations_read_lucene\": 0, \"operations_read_translog\": 7, \"total_read_time_lucene_millis\": 0, \"total_read_time_translog_millis\": 353, \"bytes_read\": 466 }, \"leader-index-2\":{ \"operations_read\": 8, \"translog_size_bytes\": 716, \"operations_read_lucene\": 0, \"operations_read_translog\": 8, \"total_read_time_lucene_millis\": 0, \"total_read_time_translog_millis\": 306, \"bytes_read\": 534 } } } Get follower cluster stats\nIntroduced 1.1\nGets information about follower (syncing) indexes on a specified cluster.\nRequest GET /_plugins/_replication/follower_stats Example response { \"num_syncing_indices\": 2, \"num_bootstrapping_indices\": 0, \"num_paused_indices\": 0, \"num_failed_indices\": 0, \"num_shard_tasks\": 2, \"num_index_tasks\": 2, \"operations_written\": 3, \"operations_read\": 3, \"failed_read_requests\": 0, \"throttled_read_requests\": 0, \"failed_write_requests\": 0, \"throttled_write_requests\": 0, \"follower_checkpoint\": 1, \"leader_checkpoint\": 1, \"total_write_time_millis\": 2290, \"index_stats\":{ \"follower-index-1\":{ \"operations_written\": 2, \"operations_read\": 2, \"failed_read_requests\": 0, \"throttled_read_requests\": 0, \"failed_write_requests\": 0, \"throttled_write_requests\": 0, \"follower_checkpoint\": 1, \"leader_checkpoint\": 1, \"total_write_time_millis\": 1355 }, \"follower-index-2\":{ \"operations_written\": 1, \"operations_read\": 1, \"failed_read_requests\": 0, \"throttled_read_requests\": 0, \"failed_write_requests\": 0, \"throttled_write_requests\": 0, \"follower_checkpoint\": 0, \"leader_checkpoint\": 0, \"total_write_time_millis\": 935 } } } Get auto-follow stats\nIntroduced 1.1\nGets information about auto-follow activity and any replication rules configured on the specified cluster.\nRequest GET /_plugins/_replication/autofollow_stats Example response { \"num_success_start_replication\": 2, \"num_failed_start_replication\": 0, \"num_failed_leader_calls\": 0, \"failed_indices\":[], \"autofollow_stats\":[ { \"name\": \"my-replication-rule\", \"pattern\": \"movies*\", \"num_success_start_replication\": 2, \"num_failed_start_replication\": 0, \"num_failed_leader_calls\": 0, \"failed_indices\":[] }] } Update settings\nIntroduced 1.1\nUpdates settings on the follower index.\nRequest PUT /_plugins/_replication/&lt;follower-index&gt;/_update { \"settings\":{ \"index.number_of_shards\": 4, \"index.number_of_replicas\": 2 } } Example response { \"acknowledged\": true } Create replication rule\nIntroduced 1.1\nAutomatically starts replication on indexes matching a specified pattern. If a new index on the leader cluster matches the pattern, OpenSearch automatically creates a follower index and begins replication. You can also use this API to update existing replication rules.\nSend this request to the follower cluster.\nMake sure to note the names of all auto-follow patterns after you create them. The replication plugin currently does not include an API operation to retrieve a list of existing patterns.\nRequest POST /_plugins/_replication/_autofollow { \"leader_alias\": \"&lt;connection-alias-name&gt;\", \"name\": \"&lt;auto-follow-pattern-name&gt;\", \"pattern\": \"&lt;pattern&gt;\", \"use_roles\":{ \"leader_cluster_role\": \"&lt;role-name&gt;\", \"follower_cluster_role\": \"&lt;role-name&gt;\" } } Specify the following options: Options Description Type Required leader_alias The name of the cross-cluster connection. You define this alias when you set up a cross-cluster connection. string Yes name A name for the auto-follow pattern. string Yes pattern An array of index patterns to match against indexes in the specified leader cluster. Supports wildcard characters. For example, leader-*. string Yes use_roles The roles to use for all subsequent backend replication tasks between the indexes. Specify a leader_cluster_role and follower_cluster_role. See Map the leader and follower cluster roles. string If Security plugin is enabled Example response { \"acknowledged\": true } Delete replication rule\nIntroduced 1.1\nDeletes the specified replication rule. This operation prevents any new indexes from being replicated but does not stop existing replication that the rule has already initiated. Replicated indexes remain read-only until you stop replication.\nSend this request to the follower cluster.\nRequest DELETE /_plugins/_replication/_autofollow { \"leader_alias\": \"&lt;connection-alias-name&gt;\", \"name\": \"&lt;auto-follow-pattern-name&gt;\", } Specify the following options: Options Description Type Required leader_alias The name of the cross-cluster connection. You define this alias when you set up a cross-cluster connection. string Yes name The name of the pattern. string Yes Example response { \"acknowledged\": true }",
    "ancestors": [
      "Tuning your cluster",
      "Cross-cluster replication"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/replication-plugin/auto-follow/",
    "title": "Auto-follow",
    "content": "Auto-follow lets you automatically replicate indexes created on the leader cluster based on matching patterns. When you create an index on the leader cluster with a name that matches a specified pattern (for example, index-01*), a corresponding follower index is automatically created on the follower cluster.\nYou can configure multiple replication rules for a single cluster. The patterns currently only support wildcard matching.\nPrerequisites\nYou need to set up a cross-cluster connection between two clusters before you can enable auto-follow.\nPermissions\nIf the Security plugin is enabled, make sure that non-admin users are mapped to the appropriate permissions so they can perform replication actions. For index and cluster-level permissions requirements, see Cross-cluster replication permissions.\nGet started with auto-follow\nReplication rules are a collection of patterns that you create against a single follower cluster. When you create a replication rule, it starts by automatically replicating any existing indexes that match the pattern. It will then continue to replicate any new indexes that you create that match the pattern.\nCreate a replication rule on the follower cluster: curl -XPOST -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/_autofollow?pretty' -d '\n{\n\"leader_alias\": \"my-connection-alias\",\n\"name\": \"my-replication-rule\",\n\"pattern\": \"movies*\",\n\"use_roles\":{\n\"leader_cluster_role\": \"all_access\",\n\"follower_cluster_role\": \"all_access\"\n}\n}' If the Security plugin is disabled, you can leave out the use_roles parameter. If it’s enabled, however, you need to specify the leader and follower cluster roles that OpenSearch uses to authenticate requests. This example uses all_access for simplicity, but we recommend creating a replication user on each cluster and mapping it accordingly.\nTo test the rule, create a matching index on the leader cluster: curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9201/movies-0001?pretty' And confirm its replica shows up on the follower cluster: curl -XGET -u 'admin:admin' -k 'https://localhost:9200/_cat/indices?v' It might take several seconds for the index to appear. health status index uuid pri rep docs.count docs.deleted store.size pri.store.size\nyellow open movies-0001 kHOxYYHxRMeszLjTD9rvSQ 1 1 0 0 208b 208b Retrieve replication rules\nTo retrieve a list of existing replication rules that are configured on a cluster, send the following request: curl -XGET -u 'admin:admin' -k 'https://localhost:9200/_plugins/_replication/autofollow_stats' { \"num_success_start_replication\": 1, \"num_failed_start_replication\": 0, \"num_failed_leader_calls\": 0, \"failed_indices\":[], \"autofollow_stats\":[ { \"name\": \"my-replication-rule\", \"pattern\": \"movies*\", \"num_success_start_replication\": 1, \"num_failed_start_replication\": 0, \"num_failed_leader_calls\": 0, \"failed_indices\":[] }] } Delete a replication rule\nTo delete a replication rule, send the following request to the follower cluster: curl -XDELETE -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/_autofollow?pretty' -d '\n{\n\"leader_alias\": \"my-conection-alias\",\n\"name\": \"my-replication-rule\"\n}' When you delete a replication rule, OpenSearch stops replicating new indexes that match the pattern, but existing indexes that the rule previously created remain read-only and continue to replicate. If you need to stop existing replication activity and open the indexes up for writes, use the stop replication API operation.",
    "ancestors": [
      "Tuning your cluster",
      "Cross-cluster replication"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/replication-plugin/getting-started/",
    "title": "Getting started",
    "content": "With cross-cluster replication, you index data to a leader index, and OpenSearch replicates that data to one or more read-only follower indexes. All subsequent operations on the leader are replicated on the follower, such as creating, updating, or deleting documents.\nPrerequisites\nCross-cluster replication has the following prerequisites:\nBoth the leader and follower cluster must have the replication plugin installed.\nIf you’ve overridden node.roles in opensearch.yml on the follower cluster, make sure it also includes the remote_cluster_client role: node.roles: [ &lt;other_roles&gt;, remote_cluster_client] Permissions\nMake sure the Security plugin is either enabled on both clusters or disabled on both clusters. If you disabled the Security plugin, you can skip this section. However, we strongly recommend enabling the Security plugin in production scenarios.\nIf the Security plugin is enabled, make sure that non-admin users are mapped to the appropriate permissions so they can perform replication actions. For index and cluster-level permissions requirements, see Cross-cluster replication permissions.\nIn addition, verify and add the distinguished names (DNs) of each follower cluster node on the leader cluster to allow connections from the followers to the leader.\nFirst, get the node’s DN from each follower cluster: curl -XGET -k -u 'admin:admin' 'https://localhost:9200/_opendistro/_security/api/ssl/certs?pretty' { \"transport_certificates_list\": [ { \"issuer_dn\": \"CN=Test,OU=Server CA 1B,O=Test,C=US\", \"subject_dn\": \"CN=follower.test.com\", # To be added under leader's nodes_dn configuration \"not_before\": \"2021-11-12T00:00:00Z\", \"not_after\": \"2022-12-11T23:59:59Z\" }] } Then verify that it’s part of the leader cluster configuration in opensearch.yml. Otherwise, add it under the following setting: plugins.security.nodes_dn: - \" CN=*.leader.com, OU=SSL, O=Test, L=Test, C=DE\" # Already part of the configuration - \" CN=follower.test.com\" # From the above response from follower Example setup\nTo start two single-node clusters on the same network, save this sample file as docker-compose.yml and run docker-compose up: version: ' 3' services: replication-node1: image: opensearchproject/opensearch:2.7.0 container_name: replication-node1 environment: - cluster.name=leader-cluster - discovery.type=single-node - bootstrap.memory_lock=true - \" OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - opensearch-data2:/usr/share/opensearch/data ports: - 9201:9200 - 9700:9600 # required for Performance Analyzer networks: - opensearch-net replication-node2: image: opensearchproject/opensearch:2.7.0 container_name: replication-node2 environment: - cluster.name=follower-cluster - discovery.type=single-node - bootstrap.memory_lock=true - \" OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - opensearch-data1:/usr/share/opensearch/data ports: - 9200:9200 - 9600:9600 # required for Performance Analyzer networks: - opensearch-net volumes: opensearch-data1: opensearch-data2: networks: opensearch-net: After the clusters start, verify the names of each: curl -XGET -u 'admin:admin' -k 'https://localhost:9201' { \"cluster_name\": \"leader-cluster\",... } curl -XGET -u 'admin:admin' -k 'https://localhost:9200' { \"cluster_name\": \"follower-cluster\",... } For this example, use port 9201 ( replication-node1) as the leader and port 9200 ( replication-node2) as the follower cluster.\nTo get the IP address for the leader cluster, first identify its container ID: docker ps\nCONTAINER ID IMAGE PORTS NAMES\n3b8cdc698be5 opensearchproject/opensearch:2.7.0 0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9300/tcp replication-node2\n731f5e8b0f4b opensearchproject/opensearch:2.7.0 9300/tcp, 0.0.0.0:9201-&gt;9200/tcp, 0.0.0.0:9700-&gt;9600/tcp replication-node1 Then get that container’s IP address: docker inspect --format = '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' 731f5e8b0f4b\n172.22.0.3 Set up a cross-cluster connection\nCross-cluster replication follows a “pull” model, so most changes occur on the follower cluster, not the leader cluster.\nOn the follower cluster, add the IP address (with port 9300) for each seed node. Because this is a single-node cluster, you only have one seed node. Provide a descriptive name for the connection, which you’ll use in the request to start replication: curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_cluster/settings?pretty' -d '\n{\n\"persistent\": {\n\"cluster\": {\n\"remote\": {\n\"my-connection-alias\": {\n\"seeds\": [\"172.22.0.3:9300\"]\n}\n}\n}\n}\n}' Start replication\nTo get started, create an index called leader-01 on the leader cluster: curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9201/leader-01?pretty' Then start replication from the follower cluster. In the request body, provide the connection name and leader index that you want to replicate, along with the security roles you want to use: curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_start?pretty' -d '\n{\n\"leader_alias\": \"my-connection-alias\",\n\"leader_index\": \"leader-01\",\n\"use_roles\":{\n\"leader_cluster_role\": \"all_access\",\n\"follower_cluster_role\": \"all_access\"\n}\n}' If the Security plugin is disabled, omit the use_roles parameter. If it’s enabled, however, you must specify the leader and follower cluster roles that OpenSearch will use to authenticate the request. This example uses all_access for simplicity, but we recommend creating a replication user on each cluster and mapping it accordingly.\nThis command creates an identical read-only index named follower-01 on the follower cluster that continuously stays updated with changes to the leader-01 index on the leader cluster. Starting replication creates a follower index from scratch – you can’t convert an existing index to a follower index.\nConfirm replication\nAfter replication starts, get the status: curl -XGET -k -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_status?pretty' { \"status\": \"SYNCING\", \"reason\": \"User initiated\", \"leader_alias\": \"my-connection-alias\", \"leader_index\": \"leader-01\", \"follower_index\": \"follower-01\", \"syncing_details\": { \"leader_checkpoint\": -1, \"follower_checkpoint\": -1, \"seq_no\": 0 } } Possible statuses are SYNCING, BOOTSTRAPPING, PAUSED, and REPLICATION NOT IN PROGRESS.\nThe leader and follower checkpoint values begin as negative numbers and reflect the shard count (-1 for one shard, -5 for five shards, and so on). The values increment with each change and illustrate how many updates the follower is behind the leader. If the indexes are fully synced, the values are the same.\nTo confirm that replication is actually happening, add a document to the leader index: curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9201/leader-01/_doc/1?pretty' -d '{\"The Shining\": \"Stephen King\"}' Then validate the replicated content on the follower index: curl -XGET -k -u 'admin:admin' 'https://localhost:9200/follower-01/_search?pretty' {... \"hits\": [{ \"_index\": \"follower-01\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"The Shining\": \"Stephen King\" } }] } Pause and resume replication\nYou can temporarily pause replication of an index if you need to remediate issues or reduce load on the leader cluster: curl -XPOST -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_pause?pretty' -d '{}' To confirm that replication is paused, get the status: curl -XGET -k -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_status?pretty' { \"status\": \"PAUSED\", \"reason\": \"User initiated\", \"leader_alias\": \"my-connection-alias\", \"leader_index\": \"leader-01\", \"follower_index\": \"follower-01\" } When you’re done making changes, resume replication: curl -XPOST -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_resume?pretty' -d '{}' When replication resumes, the follower index picks up any changes that were made to the leader index while replication was paused.\nNote that you can’t resume replication after it’s been paused for more than 12 hours. You must stop replication, delete the follower index, and restart replication of the leader.\nStop replication\nWhen you no longer need to replicate an index, terminate replication from the follower cluster: curl -XPOST -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_stop?pretty' -d '{}' When you stop replication, the follower index un-follows the leader and becomes a standard index that you can write to. You can’t restart replication after stopping it.\nGet the status to confirm that the index is no longer being replicated: curl -XGET -k -u 'admin:admin' 'https://localhost:9200/_plugins/_replication/follower-01/_status?pretty' { \"status\": \"REPLICATION NOT IN PROGRESS\" } You can further confirm that replication is stopped by making modifications to the leader index and confirming they don’t show up on the follower index.",
    "ancestors": [
      "Tuning your cluster",
      "Cross-cluster replication"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/replication-plugin/index/",
    "title": "Cross-cluster replication",
    "content": "The cross-cluster replication plugin lets you replicate indexes, mappings, and metadata from one OpenSearch cluster to another. Cross-cluster replication has the following benefits:\nBy replicating your indexes, you ensure that you can continue to handle search requests if there’s an outage.\nReplicating data across geographically distant data centers minimizes the distance between the data and the application server. This reduces expensive latencies.\nYou can replicate data from multiple smaller clusters to a centralized reporting cluster, which is useful when it’s inefficient to query across a large network.\nReplication follows an active-passive model where the follower index (where the data is replicated) pulls data from the leader (remote) index.\nThe replication plugin supports replication of indexes using wildcard pattern matching and provides commands to pause, resume, and stop replication. Once replication starts on an index, it initiates persistent background tasks on all primary shards on the follower cluster, which continuously poll corresponding shards from the leader cluster for updates.\nYou can use the replication plugin with the Security plugin to encrypt cross-cluster traffic with node-to-node encryption and control access to replication activities.\nTo start, see Get started with cross-cluster replication.",
    "ancestors": [
      "Tuning your cluster"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/replication-plugin/permissions/",
    "title": "Replication security",
    "content": "You can use the Security plugin with cross-cluster replication to limit users to certain actions. For example, you might want certain users to only perform replication activity on the leader or follower cluster.\nBecause cross-cluster replication involves multiple clusters, it’s possible that clusters might have different security configurations. The following configurations are supported:\nSecurity plugin fully enabled on both clusters\nSecurity plugin enabled only for TLS on both clusters ( plugins.security.ssl_only)\nSecurity plugin absent or disabled on both clusters (not recommended)\nEnable node-to-node encryption on both the leader and the follower cluster to ensure that replication traffic between the clusters is encrypted.\nBasic permissions\nIn order for non-admin users to perform replication activities, they must be mapped to the appropriate permissions.\nThe Security plugin has two built-in roles that cover most replication use cases: cross_cluster_replication_leader_full_access, which provides replication permissions on the leader cluster, and cross_cluster_replication_follower_full_access, which provides replication permissions on the follower cluster. For descriptions of each, see Predefined roles.\nIf you don’t want to use the default roles, you can combine individual replication permissions to meet your needs. Most permissions correspond to specific REST API operations. For example, the indices:admin/plugins/replication/index/pause permission lets you pause replication.\nMap the leader and follower cluster roles\nThe start replication and create replication rule operations are special cases. They involve background processes on the leader and follower clusters that must be associated with roles. When you perform one of these actions, you must explicitly pass the leader_cluster_role and follower_cluster_role in the request, which OpenSearch then uses in all backend replication tasks.\nTo enable non-admins to start replication and create replication rules, create an identical user on each cluster (for example, replication_user) and map them to the cross_cluster_replication_leader_full_access role on the remote cluster and cross_cluster_replication_follower_full_access on the follower cluster. For instructions, see Map users to roles.\nThen add those roles to the request, and sign it with the appropriate credentials: curl -XPUT -k -H 'Content-Type: application/json' -u 'replication_user:password' 'https://localhost:9200/_plugins/_replication/follower-01/_start?pretty' -d '\n{\n\"leader_alias\": \"leader-cluster\",\n\"leader_index\": \"leader-01\",\n\"use_roles\":{\n\"leader_cluster_role\": \"cross_cluster_replication_leader_full_access\",\n\"follower_cluster_role\": \"cross_cluster_replication_follower_full_access\"\n}\n}' You can create your own, custom leader and follower cluster roles using individual permissions, but we recommend using the default roles, which are a good fit for most use cases.\nReplication permissions\nThe following sections list the available index and cluster-level permissions for cross-cluster replication.\nFollower cluster\nThe Security plugin supports these permissions for the follower cluster: indices:admin/plugins/replication/index/setup/validate\nindices:admin/plugins/replication/index/start\nindices:admin/plugins/replication/index/pause\nindices:admin/plugins/replication/index/resume\nindices:admin/plugins/replication/index/stop\nindices:admin/plugins/replication/index/update\nindices:admin/plugins/replication/index/status_check\nindices:data/write/plugins/replication/changes\ncluster:admin/plugins/replication/autofollow/update Leader cluster\nThe Security plugin supports these permissions for the leader cluster: indices:admin/plugins/replication/validate\nindices:data/read/plugins/replication/file_chunk\nindices:data/read/plugins/replication/changes",
    "ancestors": [
      "Tuning your cluster",
      "Cross-cluster replication"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tuning-your-cluster/replication-plugin/settings/",
    "title": "Replication settings",
    "content": "The replication plugin adds several settings to the standard OpenSearch cluster settings.\nThe settings are dynamic, so you can change the default behavior of the plugin without restarting your cluster.\nYou can mark settings as persistent or transient.\nFor example, to update how often the follower cluster polls the leader cluster for updates: PUT _cluster/settings { \"persistent\": { \"plugins.replication.follower.metadata_sync_interval\": \"30s\" } } These settings manage the resources consumed by remote recoveries. We don’t recommend changing these settings; the defaults should work well for most use cases. Setting Default Description plugins.replication.follower.index.recovery.chunk_size 10 MB\nThe chunk size requested by the follower cluster during file transfer. Specify the chunk size as a value and unit, for example, 10 MB, 5 KB. See Supported units. plugins.replication.follower.index.recovery.max_concurrent_file_chunks 4\nThe number of file chunk requests that can be sent in parallel for each recovery. plugins.replication.follower.index.ops_batch_size 5000\nThe number of operations that can be fetched at a time during the syncing phase of replication. plugins.replication.follower.concurrent_readers_per_shard 2\nThe number of concurrent requests from the follower cluster per shard during the syncing phase of replication. plugins.replication.autofollow.fetch_poll_interval 30s\nHow often auto-follow tasks poll the leader cluster for new matching indexes. plugins.replication.follower.metadata_sync_interval 60s\nHow often the follower cluster polls the leader cluster for updated index metadata. plugins.replication.translog.retention_lease.pruning.enabled true\nIf enabled, prunes the translog based on retention leases on the leader index. plugins.replication.translog.retention_size 512 MB\nControls the size of the translog on the leader index.",
    "ancestors": [
      "Tuning your cluster",
      "Cross-cluster replication"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/access-control/api/",
    "title": "API",
    "content": "The Security plugin REST API lets you programmatically create and manage users, roles, role mappings, action groups, and tenants.\nTable of contents Access control for the API Reserved and hidden resources Account Get account details Change password Action groups Get action group Get action groups Delete action group Create action group Patch action group Patch action groups Users Get user Get users Delete user Create user Patch user Patch users Roles Get role Get roles Delete role Create role Patch role Patch roles Role mappings Get role mapping Get role mappings Delete role mapping Create role mapping Patch role mapping Patch role mappings Tenants Get tenant Get tenants Delete tenant Create tenant Patch tenant Patch tenants Configuration Get configuration Update configuration Patch configuration Distinguished names Get distinguished names Update distinguished names Delete distinguished names Certificates Get certificates Reload certificates Cache Flush cache Health Health check Audit logs Enable Audit Logs Access control for the API\nJust like OpenSearch permissions, you control access to the Security plugin REST API using roles. Specify roles in opensearch.yml: plugins.security.restapi.roles_enabled: [ \" &lt;role&gt;\",...] copy These roles can now access all APIs. To prevent access to certain APIs: plugins.security.restapi.endpoints_disabled.&lt;role&gt;.&lt;endpoint&gt;: [ \" &lt;method&gt;\",...] copy Possible values for endpoint are:\nACTIONGROUPS\nROLES\nROLESMAPPING\nINTERNALUSERS\nCONFIG\nCACHE\nSYSTEMINFO\nPossible values for method are:\nGET\nPUT\nPOST\nDELETE\nPATCH\nFor example, the following configuration grants three roles access to the REST API, but then prevents test-role from making PUT, POST, DELETE, or PATCH requests to _plugins/_security/api/roles or _plugins/_security/api/internalusers: plugins.security.restapi.roles_enabled: [ \" all_access\", \" security_rest_api_access\", \" test-role\"] plugins.security.restapi.endpoints_disabled.test-role.ROLES: [ \" PUT\", \" POST\", \" DELETE\", \" PATCH\"] plugins.security.restapi.endpoints_disabled.test-role.INTERNALUSERS: [ \" PUT\", \" POST\", \" DELETE\", \" PATCH\"] copy To use the PUT and PATCH methods for the configuration APIs, add the following line to opensearch.yml: plugins.security.unsupported.restapi.allow_securityconfig_modification: true copy Reserved and hidden resources\nYou can mark users, role, role mappings, and action groups as reserved. Resources that have this flag set to true can’t be changed using the REST API or OpenSearch Dashboards.\nTo mark a resource as reserved, add the following flag: kibana_user: reserved: true copy Likewise, you can mark users, role, role mappings, and action groups as hidden. Resources that have this flag set to true are not returned by the REST API and not visible in OpenSearch Dashboards: kibana_user: hidden: true copy Hidden resources are automatically reserved.\nTo add or remove these flags, modify config/opensearch-security/internal_users.yml and run plugins/opensearch-security/tools/securityadmin.sh.\nAccount\nGet account details\nIntroduced 1.0\nReturns account details for the current user. For example, if you sign the request as the admin user, the response includes details for that user.\nRequest GET _plugins/_security/api/account copy Example response { \"user_name\": \"admin\", \"is_reserved\": true, \"is_hidden\": false, \"is_internal_user\": true, \"user_requested_tenant\": null, \"backend_roles\": [ \"admin\"], \"custom_attribute_names\": [], \"tenants\": { \"global_tenant\": true, \"admin_tenant\": true, \"admin\": true }, \"roles\": [ \"all_access\", \"own_index\"] } Change password\nIntroduced 1.0\nChanges the password for the current user.\nRequest PUT _plugins/_security/api/account { \"current_password\": \"old-password\", \"password\": \"new-password\" } copy Example response { \"status\": \"OK\", \"message\": \"'test-user' updated.\" } Action groups\nGet action group\nIntroduced 1.0\nRetrieves one action group. GET _plugins/_security/api/actiongroups/&lt;action-group&gt; copy Request GET _plugins/_security/api/actiongroups/custom_action_group copy Example response { \"custom_action_group\": { \"reserved\": false, \"hidden\": false, \"allowed_actions\": [ \"kibana_all_read\", \"indices:admin/aliases/get\", \"indices:admin/aliases/exists\"], \"description\": \"My custom action group\", \"static\": false } } Get action groups\nIntroduced 1.0\nRetrieves all action groups.\nRequest GET _plugins/_security/api/actiongroups/ copy Example response { \"read\": { \"reserved\": true, \"hidden\": false, \"allowed_actions\": [ \"indices:data/read*\", \"indices:admin/mappings/fields/get*\", \"indices:admin/resolve/index\"], \"type\": \"index\", \"description\": \"Allow all read operations\", \"static\": true }, \"cluster_all\": { \"reserved\": true, \"hidden\": false, \"allowed_actions\": [ \"cluster:*\"], \"type\": \"cluster\", \"description\": \"Allow everything on cluster level\", \"static\": true },... } Delete action group\nIntroduced 1.0\nRequest DELETE _plugins/_security/api/actiongroups/&lt;action-group&gt; copy Example response { \"status\": \"OK\", \"message\": \"actiongroup SEARCH deleted.\" } Create action group\nIntroduced 1.0\nCreates or replaces the specified action group.\nRequest PUT _plugins/_security/api/actiongroups/&lt;action-group&gt; { \"allowed_actions\": [ \"indices:data/write/index*\", \"indices:data/write/update*\", \"indices:admin/mapping/put\", \"indices:data/write/bulk*\", \"read\", \"write\"] } copy Example response { \"status\": \"CREATED\", \"message\": \"'my-action-group' created.\" } Patch action group\nIntroduced 1.0\nUpdates individual attributes of an action group.\nRequest PATCH _plugins/_security/api/actiongroups/&lt;action-group&gt; [ { \"op\": \"replace\", \"path\": \"/allowed_actions\", \"value\": [ \"indices:admin/create\", \"indices:admin/mapping/put\"] }] copy Example response { \"status\": \"OK\", \"message\": \"actiongroup SEARCH deleted.\" } Patch action groups\nIntroduced 1.0\nCreates, updates, or deletes multiple action groups in a single call.\nRequest PATCH _plugins/_security/api/actiongroups [ { \"op\": \"add\", \"path\": \"/CREATE_INDEX\", \"value\": { \"allowed_actions\": [ \"indices:admin/create\", \"indices:admin/mapping/put\"] } }, { \"op\": \"remove\", \"path\": \"/CRUD\" }] copy Example response { \"status\": \"OK\", \"message\": \"actiongroup SEARCH deleted.\" } Users\nThese calls let you create, update, and delete internal users. If you use an external authentication backend, you probably don’t need to worry about internal users.\nGet user\nIntroduced 1.0\nRequest GET _plugins/_security/api/internalusers/&lt;username&gt; copy Example response { \"kirk\": { \"hash\": \"\", \"roles\": [ \"captains\", \"starfleet\"], \"attributes\": { \"attribute1\": \"value1\", \"attribute2\": \"value2\", } } } Get users\nIntroduced 1.0\nRequest GET _plugins/_security/api/internalusers/ copy Example response { \"kirk\": { \"hash\": \"\", \"roles\": [ \"captains\", \"starfleet\"], \"attributes\": { \"attribute1\": \"value1\", \"attribute2\": \"value2\", } } } Delete user\nIntroduced 1.0\nRequest DELETE _plugins/_security/api/internalusers/&lt;username&gt; copy Example response { \"status\": \"OK\", \"message\": \"user kirk deleted.\" } Create user\nIntroduced 1.0\nCreates or replaces the specified user. You must specify either password (plain text) or hash (the hashed user password). If you specify password, the Security plugin automatically hashes the password before storing it.\nNote that any role you supply in the opendistro_security_roles array must already exist for the Security plugin to map the user to that role. To see predefined roles, refer to the list of predefined roles. For instructions on how to create a role, refer to creating a role.\nRequest PUT _plugins/_security/api/internalusers/&lt;username&gt; { \"password\": \"kirkpass\", \"opendistro_security_roles\": [ \"maintenance_staff\", \"weapons\"], \"backend_roles\": [ \"role 1\", \"role 2\"], \"attributes\": { \"attribute1\": \"value1\", \"attribute2\": \"value2\" } } copy Example response { \"status\": \"CREATED\", \"message\": \"User kirk created\" } Patch user\nIntroduced 1.0\nUpdates individual attributes of an internal user.\nRequest PATCH _plugins/_security/api/internalusers/&lt;username&gt; [ { \"op\": \"replace\", \"path\": \"/backend_roles\", \"value\": [ \"klingons\"] }, { \"op\": \"replace\", \"path\": \"/opendistro_security_roles\", \"value\": [ \"ship_manager\"] }, { \"op\": \"replace\", \"path\": \"/attributes\", \"value\": { \"newattribute\": \"newvalue\" } }] copy Example response { \"status\": \"OK\", \"message\": \"'kirk' updated.\" } Patch users\nIntroduced 1.0\nCreates, updates, or deletes multiple internal users in a single call.\nRequest PATCH _plugins/_security/api/internalusers [ { \"op\": \"add\", \"path\": \"/spock\", \"value\": { \"password\": \"testpassword1\", \"backend_roles\": [ \"testrole1\"] } }, { \"op\": \"add\", \"path\": \"/worf\", \"value\": { \"password\": \"testpassword2\", \"backend_roles\": [ \"testrole2\"] } }, { \"op\": \"remove\", \"path\": \"/riker\" }] copy Example response { \"status\": \"OK\", \"message\": \"Resource updated.\" } Roles\nGet role\nIntroduced 1.0\nRetrieves one role.\nRequest GET _plugins/_security/api/roles/&lt;role&gt; copy Example response { \"test-role\": { \"reserved\": false, \"hidden\": false, \"cluster_permissions\": [ \"cluster_composite_ops\", \"indices_monitor\"], \"index_permissions\": [{ \"index_patterns\": [ \"movies*\"], \"dls\": \"\", \"fls\": [], \"masked_fields\": [], \"allowed_actions\": [ \"read\"] }], \"tenant_permissions\": [{ \"tenant_patterns\": [ \"human_resources\"], \"allowed_actions\": [ \"kibana_all_read\"] }], \"static\": false } } Get roles\nIntroduced 1.0\nRetrieves all roles.\nRequest GET _plugins/_security/api/roles/ copy Example response { \"manage_snapshots\": { \"reserved\": true, \"hidden\": false, \"description\": \"Provide the minimum permissions for managing snapshots\", \"cluster_permissions\": [ \"manage_snapshots\"], \"index_permissions\": [{ \"index_patterns\": [ \"*\"], \"fls\": [], \"masked_fields\": [], \"allowed_actions\": [ \"indices:data/write/index\", \"indices:admin/create\"] }], \"tenant_permissions\": [], \"static\": true },... } Delete role\nIntroduced 1.0\nRequest DELETE _plugins/_security/api/roles/&lt;role&gt; copy Example response { \"status\": \"OK\", \"message\": \"role test-role deleted.\" } Create role\nIntroduced 1.0\nCreates or replaces the specified role.\nRequest PUT _plugins/_security/api/roles/&lt;role&gt; { \"cluster_permissions\": [ \"cluster_composite_ops\", \"indices_monitor\"], \"index_permissions\": [{ \"index_patterns\": [ \"movies*\"], \"dls\": \"\", \"fls\": [], \"masked_fields\": [], \"allowed_actions\": [ \"read\"] }], \"tenant_permissions\": [{ \"tenant_patterns\": [ \"human_resources\"], \"allowed_actions\": [ \"kibana_all_read\"] }] } copy Example response { \"status\": \"OK\", \"message\": \"'test-role' updated.\" } Due to word boundaries associated with Unicode special characters, the Unicode standard analyzer cannot index a text field type value as a whole value when it includes one of these special characters. As a result, a text field value that includes a special character is parsed by the standard analyzer as multiple values separated by the special character, effectively tokenizing the different elements on either side of it.\nFor example, since the values in the fields \"user.id\": \"User-1\" and \"user.id\": \"User-2\" contain the hyphen/minus sign, this special character will prevent the analyzer from distinguishing between the two different users for user.id and interpret them as one and the same. This can lead to unintentional filtering of documents and potentially compromise control over their access.\nTo avoid this circumstance, you can use a custom analyzer or map the field as keyword, which performs an exact-match search. See Keyword field type for the latter option.\nFor a list of characters that should be avoided when field type is text, see Word Boundaries.\nPatch role\nIntroduced 1.0\nUpdates individual attributes of a role.\nRequest PATCH _plugins/_security/api/roles/&lt;role&gt; [ { \"op\": \"replace\", \"path\": \"/index_permissions/0/fls\", \"value\": [ \"myfield1\", \"myfield2\"] }, { \"op\": \"remove\", \"path\": \"/index_permissions/0/dls\" }] copy Example response { \"status\": \"OK\", \"message\": \"'&lt;role&gt;' updated.\" } Patch roles\nIntroduced 1.0\nCreates, updates, or deletes multiple roles in a single call.\nRequest PATCH _plugins/_security/api/roles [ { \"op\": \"replace\", \"path\": \"/role1/index_permissions/0/fls\", \"value\": [ \"test1\", \"test2\"] }, { \"op\": \"remove\", \"path\": \"/role1/index_permissions/0/dls\" }, { \"op\": \"add\", \"path\": \"/role2/cluster_permissions\", \"value\": [ \"manage_snapshots\"] }] copy Example response { \"status\": \"OK\", \"message\": \"Resource updated.\" } Role mappings\nGet role mapping\nIntroduced 1.0\nRetrieves one role mapping.\nRequest GET _plugins/_security/api/rolesmapping/&lt;role&gt; copy Example response { \"role_starfleet\": { \"backend_roles\": [ \"starfleet\", \"captains\", \"defectors\", \"cn=ldaprole,ou=groups,dc=example,dc=com\"], \"hosts\": [ \"*.starfleetintranet.com\"], \"users\": [ \"worf\"] } } Get role mappings\nIntroduced 1.0\nRetrieves all role mappings.\nRequest GET _plugins/_security/api/rolesmapping copy Example response { \"role_starfleet\": { \"backend_roles\": [ \"starfleet\", \"captains\", \"defectors\", \"cn=ldaprole,ou=groups,dc=example,dc=com\"], \"hosts\": [ \"*.starfleetintranet.com\"], \"users\": [ \"worf\"] } } Delete role mapping\nIntroduced 1.0\nDeletes the specified role mapping.\nRequest DELETE _plugins/_security/api/rolesmapping/&lt;role&gt; copy Example response { \"status\": \"OK\", \"message\": \"'my-role' deleted.\" } Create role mapping\nIntroduced 1.0\nCreates or replaces the specified role mapping.\nRequest PUT _plugins/_security/api/rolesmapping/&lt;role&gt; { \"backend_roles\": [ \"starfleet\", \"captains\", \"defectors\", \"cn=ldaprole,ou=groups,dc=example,dc=com\"], \"hosts\": [ \"*.starfleetintranet.com\"], \"users\": [ \"worf\"] } copy Example response { \"status\": \"CREATED\", \"message\": \"'my-role' created.\" } Patch role mapping\nIntroduced 1.0\nUpdates individual attributes of a role mapping.\nRequest PATCH _plugins/_security/api/rolesmapping/&lt;role&gt; [ { \"op\": \"replace\", \"path\": \"/users\", \"value\": [ \"myuser\"] }, { \"op\": \"replace\", \"path\": \"/backend_roles\", \"value\": [ \"mybackendrole\"] }] copy Example response { \"status\": \"OK\", \"message\": \"'my-role' updated.\" } Patch role mappings\nIntroduced 1.0\nCreates or updates multiple role mappings in a single call.\nRequest PATCH _plugins/_security/api/rolesmapping [ { \"op\": \"add\", \"path\": \"/human_resources\", \"value\": { \"users\": [ \"user1\"], \"backend_roles\": [ \"backendrole2\"] } }, { \"op\": \"add\", \"path\": \"/finance\", \"value\": { \"users\": [ \"user2\"], \"backend_roles\": [ \"backendrole2\"] } }] copy Example response { \"status\": \"OK\", \"message\": \"Resource updated.\" } Tenants\nGet tenant\nIntroduced 1.0\nRetrieves one tenant.\nRequest GET _plugins/_security/api/tenants/&lt;tenant&gt; copy Example response { \"human_resources\": { \"reserved\": false, \"hidden\": false, \"description\": \"A tenant for the human resources team.\", \"static\": false } } Get tenants\nIntroduced 1.0\nRetrieves all tenants.\nRequest GET _plugins/_security/api/tenants/ copy Example response { \"global_tenant\": { \"reserved\": true, \"hidden\": false, \"description\": \"Global tenant\", \"static\": true }, \"human_resources\": { \"reserved\": false, \"hidden\": false, \"description\": \"A tenant for the human resources team.\", \"static\": false } } Delete tenant\nIntroduced 1.0\nDeletes the specified tenant.\nRequest DELETE _plugins/_security/api/tenants/&lt;tenant&gt; copy Example response { \"status\": \"OK\", \"message\": \"tenant human_resources deleted.\" } Create tenant\nIntroduced 1.0\nCreates or replaces the specified tenant.\nRequest PUT _plugins/_security/api/tenants/&lt;tenant&gt; { \"description\": \"A tenant for the human resources team.\" } copy Example response { \"status\": \"CREATED\", \"message\": \"tenant human_resources created\" } Patch tenant\nIntroduced 1.0\nAdd, delete, or modify a single tenant.\nRequest PATCH _plugins/_security/api/tenants/&lt;tenant&gt; [ { \"op\": \"replace\", \"path\": \"/description\", \"value\": \"An updated description\" }] copy Example response { \"status\": \"OK\", \"message\": \"Resource updated.\" } Patch tenants\nIntroduced 1.0\nAdd, delete, or modify multiple tenants in a single call.\nRequest PATCH _plugins/_security/api/tenants/ [ { \"op\": \"replace\", \"path\": \"/human_resources/description\", \"value\": \"An updated description\" }, { \"op\": \"add\", \"path\": \"/another_tenant\", \"value\": { \"description\": \"Another description.\" } }] copy Example response { \"status\": \"OK\", \"message\": \"Resource updated.\" } Configuration\nGet configuration\nIntroduced 1.0\nRetrieves the current Security plugin configuration in JSON format.\nRequest GET _plugins/_security/api/securityconfig copy Update configuration\nIntroduced 1.0\nCreates or updates the existing configuration using the REST API. This operation can easily break your existing configuration, so we recommend using securityadmin.sh instead, which is far safer. See Access control for the API for how to enable this operation.\nRequest PUT _plugins/_security/api/securityconfig/config { \"dynamic\": { \"filtered_alias_mode\": \"warn\", \"disable_rest_auth\": false, \"disable_intertransport_auth\": false, \"respect_request_indices_options\": false, \"opensearch-dashboards\": { \"multitenancy_enabled\": true, \"server_username\": \"kibanaserver\", \"index\": \".opensearch-dashboards\" }, \"http\": { \"anonymous_auth_enabled\": false }, \"authc\": { \"basic_internal_auth_domain\": { \"http_enabled\": true, \"transport_enabled\": true, \"order\": 0, \"http_authenticator\": { \"challenge\": true, \"type\": \"basic\", \"config\": {} }, \"authentication_backend\": { \"type\": \"intern\", \"config\": {} }, \"description\": \"Authenticate via HTTP Basic against internal users database\" } }, \"auth_failure_listeners\": {}, \"do_not_fail_on_forbidden\": false, \"multi_rolespan_enabled\": true, \"hosts_resolver_mode\": \"ip-only\", \"do_not_fail_on_forbidden_empty\": false } } copy Example response { \"status\": \"OK\", \"message\": \"'config' updated.\" } Patch configuration\nIntroduced 1.0\nUpdates the existing configuration using the REST API. This operation can easily break your existing configuration, so we recommend using securityadmin.sh instead, which is far safer. See Access control for the API for how to enable this operation.\nBefore you can execute the operation, you must first add the following line to opensearch.yml: plugins.security.unsupported.restapi.allow_securityconfig_modification: true copy Request PATCH _plugins/_security/api/securityconfig [ { \"op\": \"replace\", \"path\": \"/config/dynamic/authc/basic_internal_auth_domain/transport_enabled\", \"value\": \"true\" }] copy Example response { \"status\": \"OK\", \"message\": \"Resource updated.\" } Distinguished names\nThese REST APIs let a super admin add, retrieve, update, or delete any distinguished names from an allow list to enable communication between clusters and/or nodes.\nBefore you can use the REST API to configure the allow list, you must first add the following line to opensearch.yml: plugins.security.nodes_dn_dynamic_config_enabled: true copy Get distinguished names\nRetrieves all distinguished names in the allow list.\nRequest GET _plugins/_security/api/nodesdn copy Example response { \"cluster1\": { \"nodes_dn\": [ \"CN=cluster1.example.com\"] } } To get the distinguished names from a specific cluster’s or node’s allow list, include the cluster’s name in the request path.\nRequest GET _plugins/_security/api/nodesdn/&lt;cluster-name&gt; copy Example response { \"cluster3\": { \"nodes_dn\": [ \"CN=cluster3.example.com\"] } } Update distinguished names\nAdds or updates the specified distinguished names in the cluster’s or node’s allow list.\nRequest PUT _plugins/_security/api/nodesdn/&lt;cluster-name&gt; { \"nodes_dn\": [ \"CN=cluster3.example.com\"] } copy Example response { \"status\": \"CREATED\", \"message\": \"'cluster3' created.\" } Delete distinguished names\nDeletes all distinguished names in the specified cluster’s or node’s allow list.\nRequest DELETE _plugins/_security/api/nodesdn/&lt;cluster-name&gt; copy Example response { \"status\": \"OK\", \"message\": \"'cluster3' deleted.\" } Certificates\nGet certificates\nIntroduced 1.0\nRetrieves the cluster’s security certificates.\nRequest GET _plugins/_security/api/ssl/certs copy Example response { \"http_certificates_list\": [ { \"issuer_dn\": \"CN=Example Com Inc. Root CA,OU=Example Com Inc. Root CA,O=Example Com Inc.,DC=example,DC=com\", \"subject_dn\": \"CN=node-0.example.com,OU=node,O=node,L=test,DC=de\", \"san\": \"[[8, 1.2.3.4.5.5], [2, node-0.example.com]\", \"not_before\": \"2018-04-22T03:43:47Z\", \"not_after\": \"2028-04-19T03:43:47Z\" }], \"transport_certificates_list\": [ { \"issuer_dn\": \"CN=Example Com Inc. Root CA,OU=Example Com Inc. Root CA,O=Example Com Inc.,DC=example,DC=com\", \"subject_dn\": \"CN=node-0.example.com,OU=node,O=node,L=test,DC=de\", \"san\": \"[[8, 1.2.3.4.5.5], [2, node-0.example.com]\", \"not_before\": \"2018-04-22T03:43:47Z\", \"not_after\": \"2028-04-19T03:43:47Z\" }] } Reload certificates\nIntroduced 1.0\nReloads SSL certificates that are about to expire without restarting the OpenSearch node.\nThis call assumes that new certificates are in the same location specified by the security configurations in opensearch.yml. To keep sensitive certificate reloads secure, this call only allows hot reload with certificates issued by the same issuer and subject DN and SAN with expiry dates after the current certificate.\nRequest PUT _opendistro/_security/api/ssl/transport/reloadcerts copy PUT _opendistro/_security/api/ssl/http/reloadcerts copy Example response { \"message\": \"updated http certs\" } Cache\nFlush cache\nIntroduced 1.0\nFlushes the Security plugin user, authentication, and authorization cache.\nRequest DELETE _plugins/_security/api/cache copy Example response { \"status\": \"OK\", \"message\": \"Cache flushed successfully.\" } Health\nHealth check\nIntroduced 1.0\nChecks to see if the Security plugin is up and running. If you operate your cluster behind a load balancer, this operation is useful for determining node health and doesn’t require a signed request.\nRequest GET _plugins/_security/health copy Example response { \"message\": null, \"mode\": \"strict\", \"status\": \"UP\" } Audit logs\nThe following API is available for audit logging in the Security plugin.\nEnable Audit Logs\nThis API allows you to enable or disable audit logging, define the configuration for audit logging and compliance, and make updates to settings.\nFor details on using audit logging to track access to OpenSearch clusters, as well as information on further configurations, see Audit logs.\nYou can do an initial configuration of audit logging in the audit.yml file, found in the opensearch-project/security/config directory. Thereafter, you can use the REST API or Dashboards for further changes to the configuration.\nRequest fields Field Data Type Description enabled Boolean\nEnables or disables audit logging. Default is true. audit Object\nContains fields for audit logging configuration. audit.ignore_users Array\nUsers to be excluded from auditing. Wildcard patterns are supported Example: ignore_users: [\"test-user\", employee-*\"] audit.ignore_requests Array\nRequests to be excluded from auditing. Wildcard patterns are supported. Example: ignore_requests: [\"indices:data/read/*\", \"SearchRequest\"] audit.disabled_rest_categories Array\nCategories to exclude from REST API auditing. Default categories are AUTHENTICATED, GRANTED_PRIVILEGES. audit.disabled_transport_categories Array\nCategories to exclude from Transport API auditing. Default categories are AUTHENTICATED, GRANTED_PRIVILEGES. audit.log_request_body Boolean\nIncludes the body of the request (if available) for both REST and the transport layer. Default is true. audit.resolve_indices Boolean\nLogs all indexes affected by a request. Resolves aliases and wildcards/date patterns. Default is true. audit.resolve_bulk_requests Boolean\nLogs individual operations in a bulk request. Default is false. audit.exclude_sensitive_headers Boolean\nExcludes sensitive headers from being included in the logs. Default is true. audit.enable_transport Boolean\nEnables/disables Transport API auditing. Default is true. audit.enable_rest Boolean\nEnables/disables REST API auditing. Default is true. compliance Object\nContains fields for compliance configuration. compliance.enabled Boolean\nEnables or disables compliance. Default is true. compliance.write_log_diffs Boolean\nLogs only diffs for document updates. Default is false. compliance.read_watched_fields Object\nMap of indexes and fields to monitor for read events. Wildcard patterns are supported for both index names and fields. compliance.read_ignore_users Array\nList of users to ignore for read events. Wildcard patterns are supported. Example: read_ignore_users: [\"test-user\", \"employee-*\"] compliance.write_watched_indices Array\nList of indexes to watch for write events. Wildcard patterns are supported. Example: write_watched_indices: [\"twitter\", \"logs-*\"] compliance.write_ignore_users Array\nList of users to ignore for write events. Wildcard patterns are supported. Example: write_ignore_users: [\"test-user\", \"employee-*\"] compliance.read_metadata_only Boolean\nLogs only metadata of the document for read events. Default is true. compliance.write_metadata_only Boolean\nLog only metadata of the document for write events. Default is true. compliance.external_config Boolean\nLogs external configuration files for the node. Default is false. compliance.internal_config Boolean\nLogs updates to internal security changes. Default is true. Changes to the _readonly property result in a 409 error, as indicated in the response below. { \"status\": \"error\", \"reason\": \"Invalid configuration\", \"invalid_keys\": { \"keys\": \"_readonly,config\" } } Example request GET A GET call retrieves the audit configuration. GET /_opendistro/_security/api/audit copy PUT A PUT call updates the audit configuration. PUT /_opendistro/_security/api/audit/config { \"enabled\": true, \"audit\": { \"ignore_users\": [], \"ignore_requests\": [], \"disabled_rest_categories\": [ \"AUTHENTICATED\", \"GRANTED_PRIVILEGES\"], \"disabled_transport_categories\": [ \"AUTHENTICATED\", \"GRANTED_PRIVILEGES\"], \"log_request_body\": false, \"resolve_indices\": false, \"resolve_bulk_requests\": false, \"exclude_sensitive_headers\": true, \"enable_transport\": false, \"enable_rest\": true }, \"compliance\": { \"enabled\": true, \"write_log_diffs\": false, \"read_watched_fields\": {}, \"read_ignore_users\": [], \"write_watched_indices\": [], \"write_ignore_users\": [], \"read_metadata_only\": true, \"write_metadata_only\": true, \"external_config\": false, \"internal_config\": true } } copy PATCH A PATCH call is used to update specified fields in the audit configuration. The PATCH method requires an operation, a path, and a value to complete a valid request. For details on using the PATCH method, see the following Patching resources description at Wikipedia.\nUsing the PATCH method also requires a user to have a security configuration that includes admin certificates for encryption. To find out more about these certificates, see Configuring admin certificates. curl -X PATCH -k -i --cert &lt;admin_cert file name&gt; --key &lt;admin_cert_key file name&gt; &lt;domain&gt;/_opendistro/_security/api/audit -H 'Content-Type: application/json' -d '[{\"op\":\"add\",\"path\":\"/config/enabled\",\"value\":\"true\"}]' copy OpenSearch Dashboards Dev Tools do not currently support the PATCH method. You can use curl, Postman, or another alternative process to update the configuration using this method. To follow the GitHub issue for support of the PATCH method in Dashboards, see issue #2343.\nExample response\nThe GET call produces a response that appears similar to the following: { \"_readonly\": [ \"/audit/exclude_sensitive_headers\", \"/compliance/internal_config\", \"/compliance/external_config\"], \"config\": { \"compliance\": { \"enabled\": true, \"write_log_diffs\": false, \"read_watched_fields\": { }, \"read_ignore_users\": [], \"write_watched_indices\": [], \"write_ignore_users\": [], \"read_metadata_only\": true, \"write_metadata_only\": true, \"external_config\": false, \"internal_config\": true }, \"enabled\": true, \"audit\": { \"ignore_users\": [], \"ignore_requests\": [], \"disabled_rest_categories\": [ \"AUTHENTICATED\", \"GRANTED_PRIVILEGES\"], \"disabled_transport_categories\": [ \"AUTHENTICATED\", \"GRANTED_PRIVILEGES\"], \"log_request_body\": true, \"resolve_indices\": true, \"resolve_bulk_requests\": true, \"exclude_sensitive_headers\": true, \"enable_transport\": true, \"enable_rest\": true } } } The PUT request produces a response that appears similar to the following: { \"status\": \"OK\", \"message\": \"'config' updated.\" } The PATCH request produces a response similar to the following: HTTP/1.1 200 OK\ncontent-type: application/json; charset = UTF-8\ncontent-length: 45",
    "ancestors": [
      "Security in OpenSearch",
      "Access control"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/access-control/cross-cluster-search/",
    "title": "Cross-cluster search",
    "content": "Cross-cluster search is exactly what it sounds like: it lets any node in a cluster execute search requests against other clusters. The Security plugin supports cross-cluster search out of the box.\nTable of contents Authentication flow Permissions Walkthrough Authentication flow\nWhen accessing a remote cluster from a coordinating cluster using cross-cluster search:\nThe Security plugin authenticates the user on the coordinating cluster.\nThe Security plugin fetches the user’s backend roles on the coordinating cluster.\nThe call, including the authenticated user, is forwarded to the remote cluster.\nThe user’s permissions are evaluated on the remote cluster.\nYou can have different authentication and authorization configurations on the remote and coordinating cluster, but we recommend using the same settings on both.\nPermissions\nTo query indexes on remote clusters, users need to have READ or SEARCH permissions. Furthermore, when the search request includes the query parameter ccs_minimize_roundtrips=false – which tells OpenSearch not to minimize outgoing and ingoing requests to remote clusters – users need to have the following additional permission for the index: indices:admin/shards/search_shards For more information about the ccs_minimize_roundtrips parameter, see the list of URL Parameters for the Search API.\nSample roles.yml configuration humanresources: cluster: - CLUSTER_COMPOSITE_OPS_RO indices: ' humanresources': ' *': - READ - indices:admin/shards/search_shards # needed when the search request includes parameter setting 'ccs_minimize_roundtrips=false'. Sample role in OpenSearch Dashboards Walkthrough\nSave this file as docker-compose.yml and run docker-compose up to start two single-node clusters on the same network: version: ' 3' services: opensearch-ccs-node1: image: opensearchproject/opensearch:2.7.0 container_name: opensearch-ccs-node1 environment: - cluster.name=opensearch-ccs-cluster1 - discovery.type=single-node - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping - \" OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # minimum and maximum Java heap size, recommend setting both to 50% of system RAM ulimits: memlock: soft: -1 hard: -1 volumes: - opensearch-data1:/usr/share/opensearch/data ports: - 9200:9200 - 9600:9600 # required for Performance Analyzer networks: - opensearch-net opensearch-ccs-node2: image: opensearchproject/opensearch:2.7.0 container_name: opensearch-ccs-node2 environment: - cluster.name=opensearch-ccs-cluster2 - discovery.type=single-node - bootstrap.memory_lock=true # along with the memlock settings below, disables swapping - \" OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # minimum and maximum Java heap size, recommend setting both to 50% of system RAM ulimits: memlock: soft: -1 hard: -1 volumes: - opensearch-data2:/usr/share/opensearch/data ports: - 9250:9200 - 9700:9600 # required for Performance Analyzer networks: - opensearch-net volumes: opensearch-data1: opensearch-data2: networks: opensearch-net: After the clusters start, verify the names of each: curl -XGET -u 'admin:admin' -k 'https://localhost: 9200 ' { \"cluster_name\": \"opensearch-ccs-cluster1\",... } curl -XGET -u 'admin:admin' -k 'https://localhost: 9250 ' { \"cluster_name\": \"opensearch-ccs-cluster2\",... } Both clusters run on localhost, so the important identifier is the port number. In this case, use port 9200 ( opensearch-ccs-node1) as the remote cluster, and port 9250 ( opensearch-ccs-node2) as the coordinating cluster.\nTo get the IP address for the remote cluster, first identify its container ID: docker ps\nCONTAINER ID IMAGE PORTS NAMES\n6fe89ebc5a8e opensearchproject/opensearch:2.7.0 0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9300/tcp opensearch-ccs-node1\n2da08b6c54d8 opensearchproject/opensearch:2.7.0 9300/tcp, 0.0.0.0:9250-&gt;9200/tcp, 0.0.0.0:9700-&gt;9600/tcp opensearch-ccs-node2 Then get that container’s IP address: docker inspect --format = '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' 6fe89ebc5a8e\n172.31.0.3 On the coordinating cluster, add the remote cluster name and the IP address (with port 9300) for each “seed node.” In this case, you only have one seed node: curl -k -XPUT -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost: 9250 /_cluster/settings' -d ' { \"persistent\": { \"cluster.remote\": { \"opensearch-ccs-cluster1\": { \"seeds\": [ \"172.31.0.3:9300\"] } } } } ' On the remote cluster, index a document: curl -XPUT -k -H 'Content-Type: application/json' -u 'admin:admin' 'https://localhost:9200/books/_doc/1' -d '{\"Dracula\": \"Bram Stoker\"}' At this point, cross-cluster search works. You can test it using the admin user: curl -XGET -k -u 'admin:admin' 'https://localhost:9250/opensearch-ccs-cluster1:books/_search?pretty' {... \"hits\": [{ \"_index\": \"opensearch-ccs-cluster1:books\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"Dracula\": \"Bram Stoker\" } }] } To continue testing, create a new user on both clusters: curl -XPUT -k -u 'admin:admin' 'https://localhost:9200/_plugins/_security/api/internalusers/booksuser' -H 'Content-Type: application/json' -d '{\"password\":\"password\"}' curl -XPUT -k -u 'admin:admin' 'https://localhost:9250/_plugins/_security/api/internalusers/booksuser' -H 'Content-Type: application/json' -d '{\"password\":\"password\"}' Then run the same search as before with booksuser: curl -XGET -k -u booksuser:password 'https://localhost: 9250 /opensearch-ccs-cluster 1:books/_search?pretty' { \"error\": { \"root_cause\": [ { \"type\": \"security_exception\", \"reason\": \"no permissions for [indices:admin/shards/search_shards, indices:data/read/search] and User [name=booksuser, roles=[], requestedTenant=null]\" }], \"type\": \"security_exception\", \"reason\": \"no permissions for [indices:admin/shards/search_shards, indices:data/read/search] and User [name=booksuser, roles=[], requestedTenant=null]\" }, \"status\": 403 } Note the permissions error. On the remote cluster, create a role with the appropriate permissions, and map booksuser to that role: curl -XPUT -k -u 'admin:admin' -H 'Content-Type: application/json' 'https://localhost:9200/_plugins/_security/api/roles/booksrole' -d '{\"index_permissions\":[{\"index_patterns\":[\"books\"],\"allowed_actions\":[\"indices:admin/shards/search_shards\",\"indices:data/read/search\"]}]}' curl -XPUT -k -u 'admin:admin' -H 'Content-Type: application/json' 'https://localhost:9200/_plugins/_security/api/rolesmapping/booksrole' -d '{\"users\": [\"booksuser\"]}' Both clusters must have the user, but only the remote cluster needs the role and mapping; in this case, the coordinating cluster handles authentication (i.e. “Does this request include valid user credentials?”), and the remote cluster handles authorization (i.e. “Can this user access this data?”).\nFinally, repeat the search: curl -XGET -k -u booksuser:password 'https://localhost:9250/opensearch-ccs-cluster1:books/_search?pretty' {... \"hits\": [{ \"_index\": \"opensearch-ccs-cluster1:books\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"Dracula\": \"Bram Stoker\" } }] }",
    "ancestors": [
      "Security in OpenSearch",
      "Access control"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/access-control/default-action-groups/",
    "title": "Default action groups",
    "content": "This page catalogs all default action groups. Often, the most coherent way to create new action groups is to use a combination of these default groups and individual permissions.\nGeneral Name Description unlimited\nGrants complete access. Can be used on an cluster- or index-level. Equates to \"*\". Cluster-level Name Description cluster_all\nGrants all cluster permissions. Equates to cluster:*.\ncluster_monitor\nGrants all cluster monitoring permissions. Equates to cluster:monitor/*.\ncluster_composite_ops_ro\nGrants read-only permissions to execute requests like mget, msearch, or mtv, plus permissions to query for aliases.\ncluster_composite_ops\nSame as CLUSTER_COMPOSITE_OPS_RO, but also grants bulk permissions and all aliases permissions.\nmanage_snapshots\nGrants permissions to manage snapshots and repositories.\ncluster_manage_pipelines\nGrants permissions to manage ingest pipelines.\ncluster_manage_index_templates\nGrants permissions to manage index templates. Index-level Name Description indices_all\nGrants all permissions on the index. Equates to indices:*.\nget\nGrants permissions to use get and mget actions only.\nread\nGrants read permissions such as search, get field mappings, get, and mget.\nwrite\nGrants permissions to create and update documents within existing indices. To create new indices, see create_index.\ndelete\nGrants permissions to delete documents.\ncrud\nCombines the read, write, and delete action groups. Included in the data_access action group.\nsearch\nGrants permissions to search documents. Includes suggest.\nsuggest\nGrants permissions to use the suggest API. Included in the read action group.\ncreate_index\nGrants permissions to create indices and mappings.\nindices_monitor\nGrants permissions to execute all index monitoring actions (e.g. recovery, segments info, index stats, and status).\nindex\nA more limited version of the write action group.\ndata_access\nCombines the crud action group with indices:data/*.\nmanage_aliases\nGrants permissions to manage aliases.\nmanage\nGrants all monitoring and administration permissions for indices.",
    "ancestors": [
      "Security in OpenSearch",
      "Access control"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/access-control/document-level-security/",
    "title": "Document-level security",
    "content": "Document-level security lets you restrict a role to a subset of documents in an index. The easiest way to get started with document- and field-level security is open OpenSearch Dashboards and choose Security. Then choose Roles, create a new role, and review the Index permissions section. Simple roles\nDocument-level security uses the OpenSearch query DSL to define which documents a role grants access to. In OpenSearch Dashboards, choose an index pattern and provide a query in the Document level security section: { \"bool\": { \"must\": { \"match\": { \"genres\": \"Comedy\" } } } } This query specifies that for the role to have access to a document, its genres field must include Comedy.\nA typical request to the _search API includes { \"query\": {... } } around the query, but in this case, you only need to specify the query itself.\nIn the REST API, you provide the query as a string, so you must escape your quotes. This role allows a user to read any document in any index with the field public set to true: PUT _plugins/_security/api/roles/public_data { \"cluster_permissions\": [ \"*\"], \"index_permissions\": [{ \"index_patterns\": [ \"pub*\"], \"dls\": \"{ \\\" term \\\": { \\\" public \\\": true}}\", \"allowed_actions\": [ \"read\"] }] } These queries can be as complex as you want, but we recommend keeping them simple to minimize the performance impact that the document-level security feature has on the cluster.\nA note on Unicode special characters in text fields\nDue to word boundaries associated with Unicode special characters, the Unicode standard analyzer cannot index a text field type value as a whole value when it includes one of these special characters. As a result, a text field value that includes a special character is parsed by the standard analyzer as multiple values separated by the special character, effectively tokenizing the different elements on either side of it. This can lead to unintentional filtering of documents and potentially compromise control over their access.\nThe examples below illustrate values containing special characters that will be parsed improperly by the standard analyzer. In this example, the existence of the hyphen/minus sign in the value prevents the analyzer from distinguishing between the two different users for user.id and interprets them as one and the same: { \"bool\": { \"must\": { \"match\": { \"user.id\": \"User-1\" } } } } { \"bool\": { \"must\": { \"match\": { \"user.id\": \"User-2\" } } } } To avoid this circumstance when using either Query DSL or the REST API, you can use a custom analyzer or map the field as keyword, which performs an exact-match search. See Keyword field type for the latter option.\nFor a list of characters that should be avoided when field type is text, see Word Boundaries.\nParameter substitution\nA number of variables exist that you can use to enforce rules based on the properties of a user. For example, ${user.name} is replaced with the name of the current user.\nThis rule allows a user to read any document where the username is a value of the readable_by field: PUT _plugins/_security/api/roles/user_data { \"cluster_permissions\": [ \"*\"], \"index_permissions\": [{ \"index_patterns\": [ \"pub*\"], \"dls\": \"{ \\\" term \\\": { \\\" readable_by \\\": \\\" ${user.name} \\\" }}\", \"allowed_actions\": [ \"read\"] }] } This table lists substitutions. Term Replaced with ${user.name} Username. ${user.roles} A comma-separated, quoted list of user backend roles. ${user.securityRoles} A comma-separated, quoted list of user security roles. ${attr.&lt;TYPE&gt;.&lt;NAME&gt;} An attribute with name &lt;NAME&gt; defined for a user. &lt;TYPE&gt; is internal, jwt, proxy or ldap Attribute-based security\nYou can use roles and parameter substitution with the terms_set query to enable attribute-based security.\nNote that the security_attributes of the index need to be of type keyword.\nUser definition PUT _plugins/_security/api/internalusers/user 1 { \"password\": \"asdf\", \"backend_roles\": [ \"abac\"], \"attributes\": { \"permissions\": \" \\\" att1 \\\", \\\" att2 \\\", \\\" att3 \\\" \" } } Role definition PUT _plugins/_security/api/roles/abac { \"index_permissions\": [{ \"index_patterns\": [ \"*\"], \"dls\": \"{ \\\" terms_set \\\": { \\\" security_attributes \\\": { \\\" terms \\\": [${attr.internal.permissions}], \\\" minimum_should_match_script \\\": { \\\" source \\\": \\\" doc['security_attributes'].length \\\" }}}}\", \"allowed_actions\": [ \"read\"] }] } Use term-level lookup queries (TLQs) with DLS\nYou can perform term-level lookup queries (TLQs) with document-level security (DLS) using either of two modes: adaptive or filter level. The default mode is adaptive, where OpenSearch automatically switches between Lucene-level or filter-level mode depending on whether or not there is a TLQ. DLS queries without TLQs are executed in Lucene-level mode, whereas DLS queries with TLQs are executed in filter-level mode.\nBy default, the Security plugin detects if a DLS query contains a TLQ or not and chooses the appropriate mode automatically at runtime.\nTo learn more about OpenSearch queries, see Term-level queries.\nHow to set the DLS evaluation mode in opensearch.yml By default, the DLS evaluation mode is set to adaptive. You can also explicitly set the mode in opensearch.yml with the plugins.security.dls.mode setting. Add a line to opensearch.yml with the desired evaluation mode.\nFor example, to set it to filter level, add this line: plugins.security.dls.mode: filter-level DLS evaluation modes Evaluation mode Parameter Description Usage Lucene-level DLS lucene-level This setting makes all DLS queries apply to the Lucene level.\nLucene-level DLS modifies Lucene queries and data structures directly. This is the most efficient mode but does not allow certain advanced constructs in DLS queries, including TLQs.\nFilter-level DLS filter-level This setting makes all DLS queries apply to the filter level.\nIn this mode, OpenSearch applies DLS by modifying queries that OpenSearch receives. This allows for term-level lookup queries in DLS queries, but you can only use the get, search, mget, and msearch operations to retrieve data from the protected index. Additionally, cross-cluster searches are limited with this mode.\nAdaptive adaptive-level The default setting that allows OpenSearch to automatically choose the mode.\nDLS queries without TLQs are executed in Lucene-level mode, while DLS queries that contain TLQ are executed in filter- level mode.",
    "ancestors": [
      "Security in OpenSearch",
      "Access control"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/access-control/field-level-security/",
    "title": "Field-level security",
    "content": "Field-level security lets you control which document fields a user can see. Just like document-level security, you control access by index within a role.\nThe easiest way to get started with document- and field-level security is open OpenSearch Dashboards and choose Security. Then choose Roles, create a new role, and review the Index permissions section.\nTable of contents Include or exclude fields OpenSearch Dashboards roles.yml REST API Interaction with multiple roles Interaction with document-level security Include or exclude fields\nYou have two options when you configure field-level security: include or exclude fields. If you include fields, users see only those fields when they retrieve a document. For example, if you include the actors, title, and year fields, a search result might look like this: { \"_index\": \"movies\", \"_source\": { \"year\": 2013, \"title\": \"Rush\", \"actors\": [ \"Daniel Brühl\", \"Chris Hemsworth\", \"Olivia Wilde\"] } } If you exclude fields, users see everything but those fields when they retrieve a document. For example, if you exclude those same fields, the same search result might look like this: { \"_index\": \"movies\", \"_source\": { \"directors\": [ \"Ron Howard\"], \"plot\": \"A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\", \"genres\": [ \"Action\", \"Biography\", \"Drama\", \"Sport\"] } } You can achieve the same outcomes using inclusion or exclusion, so choose whichever makes sense for your use case. Mixing the two doesn’t make sense and is not supported.\nYou can specify field-level security settings using OpenSearch Dashboards, roles.yml, and the REST API.\nTo exclude fields in roles.yml or the REST API, add ~ before the field name.\nField names support wildcards ( *).\nWildcards are especially useful for excluding subfields. For example, if you index a document that has a string (e.g. {\"title\": \"Thor\"}), OpenSearch creates a title field of type text, but it also creates a title.keyword subfield of type keyword. In this example, to prevent unauthorized access to data in the title field, you must also exclude the title.keyword subfield. Use title* to match all fields that begin with title.\nOpenSearch Dashboards\nChoose a role and Add index permission.\nChoose an index pattern.\nUnder Field level security, use the drop-down to select your preferred option. Then specify one or more fields and press Enter.\nroles.yml someonerole: cluster: [] indices: movies: ' *': - \" READ\" _fls_: - \" ~actors\" - \" ~title\" - \" ~year\" REST API\nSee Create role.\nInteraction with multiple roles\nIf you map a user to multiple roles, we recommend that those roles use either include or exclude statements for each index. The Security plugin evaluates field-level security settings using the AND operator, so combining include and exclude statements can lead to neither behavior working properly.\nFor example, in the movies index, if you include actors, title, and year in one role, exclude actors, title, and genres in another role, and then map both roles to the same user, a search result might look like this: { \"_index\": \"movies\", \"_source\": { \"year\": 2013, \"directors\": [ \"Ron Howard\"], \"plot\": \"A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\" } } Interaction with document-level security Document-level security relies on OpenSearch queries, which means that all fields in the query must be visible in order for it to work properly. If you use field-level security in conjunction with document-level security, make sure you don’t restrict access to the fields that document-level security uses.",
    "ancestors": [
      "Security in OpenSearch",
      "Access control"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/access-control/field-masking/",
    "title": "Field masking",
    "content": "If you don’t want to remove fields from a document using field-level security, you can mask their values. Currently, field masking is only available for string-based fields and replaces the field’s value with a cryptographic hash.\nField masking works alongside field-level security on the same per-role, per-index basis. You can allow certain roles to see sensitive fields in plain text and mask them for others. A search result with a masked field might look like this: { \"_index\": \"movies\", \"_source\": { \"year\": 2013, \"directors\": [ \"Ron Howard\"], \"title\": \"ca998e768dd2e6cdd84c77015feb29975f9f498a472743f159bec6f1f1db109e\" } } Set the salt\nYou set the salt (a random string used to hash your data) in opensearch.yml: plugins.security.compliance.salt: abcdefghijklmnopqrstuvqxyz1234567890 Property Description plugins.security.compliance.salt The salt to use when generating the hash value. Must be at least 32 characters. Only ASCII characters are allowed. Optional. Setting the salt is optional, but we highly recommend it.\nConfigure field masking\nYou configure field masking using OpenSearch Dashboards, roles.yml, or the REST API.\nOpenSearch Dashboards\nChoose a role.\nChoose an index permission.\nFor Anonymization, specify one or more fields and press Enter.\nroles.yml someonerole: cluster: [] indices: movies: _masked_fields_: - \" title\" - \" genres\" ' *': - \" READ\" REST API\nSee Create role.\n(Advanced) Use an alternative hash algorithm\nBy default, the Security plugin uses the BLAKE2b algorithm, but you can use any hashing algorithm that your JVM provides. This list typically includes MD5, SHA-1, SHA-384, and SHA-512.\nTo specify a different algorithm, add it after the masked field: someonerole: cluster: [] indices: movies: _masked_fields_: - \" title::SHA-512\" - \" genres\" ' *': - \" READ\" (Advanced) Pattern-based field masking\nRather than creating a hash, you can use one or more regular expressions and replacement strings to mask a field. The syntax is &lt;field&gt;::/&lt;regular-expression&gt;/::&lt;replacement-string&gt;. If you use multiple regular expressions, the results are passed from left to right, like piping in a shell: hr_employee: index_permissions: - index_patterns: - ' humanresources' allowed_actions: -... masked_fields: - ' lastname::/.*/::*' - ' *ip_source::/[0-9]{1,3}$/::XXX::/^[0-9]{1,3}/::***' someonerole: cluster: [] indices: movies: _masked_fields_: - \" title::/./::*\" - \" genres::/^[a-zA-Z]{1,3}/::XXX::/[a-zA-Z]{1,3}$/::YYY\" ' *': - \" READ\" The title statement changes each character in the field to *, so you can still discern the length of the masked string. The genres statement changes the first three characters of the string to XXX and the last three characters to YYY.\nEffect on audit logging\nThe read history feature lets you track read access to sensitive fields in your documents. For example, you might track access to the email field of your customer records. Access to masked fields are excluded from read history, because the user only saw the hash value, not the clear text value of the field.",
    "ancestors": [
      "Security in OpenSearch",
      "Access control"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/access-control/impersonation/",
    "title": "User impersonation",
    "content": "User impersonation allows specially privileged users to act as another user without knowledge of nor access to the impersonated user’s credentials.\nImpersonation can be useful for testing and troubleshooting, or for allowing system services to safely act as a user.\nImpersonation can occur on either the REST interface or at the transport layer.\nREST interface\nTo allow one user to impersonate another, add the following to opensearch.yml: plugins.security.authcz.rest_impersonation_user: &lt;AUTHENTICATED_USER&gt;: - &lt;IMPERSONATED_USER_1&gt; - &lt;IMPERSONATED_USER_2&gt; The impersonated user field supports wildcards. Setting it to * allows AUTHENTICATED_USER to impersonate any user.\nTransport interface\nIn a similar fashion, add the following to enable transport layer impersonation: plugins.security.authcz.impersonation_dn: \" CN=spock,OU=client,O=client,L=Test,C=DE\": - worf Impersonating Users\nTo impersonate another user, submit a request to the system with the HTTP header opendistro_security_impersonate_as set to the name of the user to be impersonated. A good test is to make a GET request to the _plugins/_security/authinfo URI: curl -XGET -u 'admin:admin' -k -H \"opendistro_security_impersonate_as: user_1\" https://localhost:9200/_plugins/_security/authinfo?pretty",
    "ancestors": [
      "Security in OpenSearch",
      "Access control"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/access-control/index/",
    "title": "Access control",
    "content": "After you configure the Security plugin to use your own certificates and preferred authentication backend, you can start adding users, creating roles, and mapping roles to users.\nThis section of the documentation covers what a user is allowed to see and do after successfully authenticating.\nConcepts Term Description Permission\nAn individual action, such as creating an index (e.g. indices:admin/create). For a complete list, see Permissions.\nAction group\nA set of permissions. For example, the predefined SEARCH action group authorizes roles to use the _search and _msearch APIs.\nRole\nSecurity roles define the scope of a permission or action group: cluster, index, document, or field. For example, a role named delivery_analyst might have no cluster permissions, the READ action group for all indices that match the delivery-data-* pattern, access to all document types within those indices, and access to all fields except delivery_driver_name.\nBackend role\n(Optional) Arbitrary strings that you specify or that come from an external authentication system (e.g. LDAP/Active Directory). Backend roles can help simplify the role mapping process. Rather than mapping a role to 100 individual users, you can map the role to a single backend role that all 100 users share.\nUser\nUsers make requests to OpenSearch clusters. A user has credentials (e.g. a username and password), zero or more backend roles, and zero or more custom attributes.\nRole mapping\nUsers assume roles after they successfully authenticate. Role mappings, well, map roles to users (or backend roles). For example, a mapping of kibana_user (role) to jdoe (user) means that John Doe gains all the permissions of kibana_user after authenticating. Likewise, a mapping of all_access (role) to admin (backend role) means that any user with the backend role of admin gains all the permissions of all_access after authenticating. You can map each role to many users and/or backend roles. The Security plugin comes with a number of predefined action groups, roles, mappings, and users. These entities serve as sensible defaults and are good examples of how to use the plugin.",
    "ancestors": [
      "Security in OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/access-control/permissions/",
    "title": "Permissions",
    "content": "Each permission in the Security plugin controls access to some action that the OpenSearch cluster can perform, such as indexing a document or checking cluster health.\nMost permissions are self-describing. For example, cluster:admin/ingest/pipeline/get lets you retrieve information about ingest pipelines. In many cases, a permission correlates to a specific REST API operation, such as GET _ingest/pipeline.\nDespite this correlation, permissions do not directly map to REST API operations. Operations such as POST _bulk and GET _msearch can access many indices and perform many actions in a single request. Even a simple request, such as GET _cat/nodes, performs several actions in order to generate its response.\nIn short, controlling access to the REST API is insufficient. Instead, the Security plugin controls access to the underlying OpenSearch actions.\nFor example, consider the following _bulk request: POST _bulk { \"delete\": { \"_index\": \"test-index\", \"_id\": \"tt2229499\" } } { \"index\": { \"_index\": \"test-index\", \"_id\": \"tt1979320\" } } { \"title\": \"Rush\", \"year\": 2013 } { \"create\": { \"_index\": \"test-index\", \"_id\": \"tt1392214\" } } { \"title\": \"Prisoners\", \"year\": 2013 } { \"update\": { \"_index\": \"test-index\", \"_id\": \"tt0816711\" } } { \"doc\": { \"title\": \"World War Z\" } } For this request to succeed, you must have the following permissions for test-index:\nindices:data/write/bulk*\nindices:data/write/delete\nindices:data/write/index\nindices:data/write/update\nThese permissions also allow you add, update, or delete documents (e.g. PUT test-index/_doc/tt0816711), because they govern the underlying OpenSearch actions of indexing and deleting documents rather than a specific API path and HTTP method.\nTest permissions\nIf you want a user to have the absolute minimum set of permissions necessary to perform some function—the principle of least privilege —the best way is to send representative requests to your cluster as a new test user. In the case of a permissions error, the Security plugin is very explicit about which permissions are missing. Consider this request and response: GET _cat/shards?v { \"error\": { \"root_cause\": [{ \"type\": \"security_exception\", \"reason\": \"no permissions for [indices:monitor/stats] and User [name=test-user, backend_roles=[], requestedTenant=null]\" }] }, \"status\": 403 } Create a user and a role, map the role to the user, and start sending signed requests using curl, Postman, or any other client. Then gradually add permissions to the role as you encounter errors. Even after you resolve one permissions error, the same request might generate new errors; the plugin only returns the first error it encounters, so keep trying until the request succeeds.\nRather than individual permissions, you can often achieve your desired security posture using a combination of the default action groups. See Default action groups for descriptions of the permissions that each group grants.\nCluster permissions\nThese permissions are for the cluster and can’t be applied granularly. For example, you either have permissions to take snapshots ( cluster:admin/snapshot/create) or you don’t. The cluster permission, therefore, cannot grant a user privileges to take snapshots of a select set of indexes while preventing the user from taking snapshots of others.\nCross-references to API documentation in the permissions that follow are only intended to provide an understanding of the permissions. As stated at the beginning of this section, permissions often correlate to APIs but do not map directly to them.\nIngest API permissions\nSee Ingest APIs.\ncluster:admin/ingest/pipeline/delete\ncluster:admin/ingest/pipeline/get\ncluster:admin/ingest/pipeline/put\ncluster:admin/ingest/pipeline/simulate\ncluster:admin/ingest/processor/grok/get\nAnomaly Detection permissions\nSee Anomaly detection API.\ncluster:admin/opendistro/ad/detector/delete\ncluster:admin/opendistro/ad/detector/info\ncluster:admin/opendistro/ad/detector/jobmanagement\ncluster:admin/opendistro/ad/detector/preview\ncluster:admin/opendistro/ad/detector/run\ncluster:admin/opendistro/ad/detector/search\ncluster:admin/opendistro/ad/detector/stats\ncluster:admin/opendistro/ad/detector/write\ncluster:admin/opendistro/ad/detector/validate\ncluster:admin/opendistro/ad/detectors/get\ncluster:admin/opendistro/ad/result/search\ncluster:admin/opendistro/ad/result/topAnomalies\ncluster:admin/opendistro/ad/tasks/search\nAlerting permissions\nSee Alerting API.\ncluster:admin/opendistro/alerting/alerts/ack\ncluster:admin/opendistro/alerting/alerts/get\ncluster:admin/opendistro/alerting/destination/delete\ncluster:admin/opendistro/alerting/destination/email_account/delete\ncluster:admin/opendistro/alerting/destination/email_account/get\ncluster:admin/opendistro/alerting/destination/email_account/search\ncluster:admin/opendistro/alerting/destination/email_account/write\ncluster:admin/opendistro/alerting/destination/email_group/delete\ncluster:admin/opendistro/alerting/destination/email_group/get\ncluster:admin/opendistro/alerting/destination/email_group/search\ncluster:admin/opendistro/alerting/destination/email_group/write\ncluster:admin/opendistro/alerting/destination/get\ncluster:admin/opendistro/alerting/destination/write\ncluster:admin/opendistro/alerting/monitor/delete\ncluster:admin/opendistro/alerting/monitor/execute\ncluster:admin/opendistro/alerting/monitor/get\ncluster:admin/opendistro/alerting/monitor/search\ncluster:admin/opendistro/alerting/monitor/write\nAsynchronous Search permissions\nSee Asynchronous search.\ncluster:admin/opendistro/asynchronous_search/stats\ncluster:admin/opendistro/asynchronous_search/delete\ncluster:admin/opendistro/asynchronous_search/get\ncluster:admin/opendistro/asynchronous_search/submit\nIndex State Management permissions\nSee ISM API.\ncluster:admin/opendistro/ism/managedindex/add\ncluster:admin/opendistro/ism/managedindex/change\ncluster:admin/opendistro/ism/managedindex/remove\ncluster:admin/opendistro/ism/managedindex/explain\ncluster:admin/opendistro/ism/managedindex/retry\ncluster:admin/opendistro/ism/policy/write\ncluster:admin/opendistro/ism/policy/get\ncluster:admin/opendistro/ism/policy/search\ncluster:admin/opendistro/ism/policy/delete\nIndex rollups permissions\nSee Index rollups API.\ncluster:admin/opendistro/rollup/index\ncluster:admin/opendistro/rollup/get\ncluster:admin/opendistro/rollup/search\ncluster:admin/opendistro/rollup/delete\ncluster:admin/opendistro/rollup/start\ncluster:admin/opendistro/rollup/stop\ncluster:admin/opendistro/rollup/explain\nReporting permissions\nSee Creating reports with the Dashboards interface.\ncluster:admin/opendistro/reports/definition/create\ncluster:admin/opendistro/reports/definition/update\ncluster:admin/opendistro/reports/definition/on_demand\ncluster:admin/opendistro/reports/definition/delete\ncluster:admin/opendistro/reports/definition/get\ncluster:admin/opendistro/reports/definition/list\ncluster:admin/opendistro/reports/instance/list\ncluster:admin/opendistro/reports/instance/get\ncluster:admin/opendistro/reports/menu/download\nTransform job permissions\nSee Transforms APIs cluster:admin/opendistro/transform/index\ncluster:admin/opendistro/transform/get\ncluster:admin/opendistro/transform/preview\ncluster:admin/opendistro/transform/delete\ncluster:admin/opendistro/transform/start\ncluster:admin/opendistro/transform/stop\ncluster:admin/opendistro/transform/explain\nObservability permissions\nSee Observability security.\ncluster:admin/opensearch/observability/create\ncluster:admin/opensearch/observability/update\ncluster:admin/opensearch/observability/delete\ncluster:admin/opensearch/observability/get\nCross-cluster replication\nSee Cross-cluster replication security.\ncluster:admin/plugins/replication/autofollow/update\nReindex\nSee Reindex document.\ncluster:admin/reindex/rethrottle\nSnapshot repository permissions\nSee Snapshot APIs.\ncluster:admin/repository/delete\ncluster:admin/repository/get\ncluster:admin/repository/put\ncluster:admin/repository/verify\nReroute\nSee Cluster manager task throttling.\ncluster:admin/reroute\nScript permissions\nSee Script APIs.\ncluster:admin/script/delete\ncluster:admin/script/get\ncluster:admin/script/put\nUpdate settings permission\nSee Update settings on the Index APIs page.\ncluster:admin/settings/update\nSnapshot permissions\nSee Snapshot APIs.\ncluster:admin/snapshot/create\ncluster:admin/snapshot/delete\ncluster:admin/snapshot/get\ncluster:admin/snapshot/restore\ncluster:admin/snapshot/status\ncluster:admin/snapshot/status*\nTask permissions\nSee Tasks in the API Reference section.\ncluster:admin/tasks/cancel\ncluster:admin/tasks/test\ncluster:admin/tasks/testunblock\nSecurity Analytics permissions\nSee API tools. Permission Description cluster:admin/opensearch/securityanalytics/alerts/get\nPermission to get alerts\ncluster:admin/opensearch/securityanalytics/alerts/ack\nPermission to acknowledge alerts\ncluster:admin/opensearch/securityanalytics/detector/get\nPermission to get detectors\ncluster:admin/opensearch/securityanalytics/detector/search\nPermission to search detectors\ncluster:admin/opensearch/securityanalytics/detector/write\nPermission to create and update detectors\ncluster:admin/opensearch/securityanalytics/detector/delete\nPermission to delete detectors\ncluster:admin/opensearch/securityanalytics/findings/get\nPermission to get findings\ncluster:admin/opensearch/securityanalytics/mapping/get\nPermission to get field mappings by index\ncluster:admin/opensearch/securityanalytics/mapping/view/get\nPermission to get field mappings by index and view mapped and unmapped fields\ncluster:admin/opensearch/securityanalytics/mapping/create\nPermission to create field mappings\ncluster:admin/opensearch/securityanalytics/mapping/update\nPermission to update field mappings\ncluster:admin/opensearch/securityanalytics/rules/categories\nPermission to get all rule categories\ncluster:admin/opensearch/securityanalytics/rule/write\nPermission to create and update rules\ncluster:admin/opensearch/securityanalytics/rule/search\nPermission to search for rules\ncluster:admin/opensearch/securityanalytics/rules/validate\nPermission to validate rules\ncluster:admin/opensearch/securityanalytics/rule/delete\nPermission to delete rules Monitoring permissions\nCluster permissions for monitoring the cluster apply to read-only operations, such as checking cluster health and getting information about usage on nodes or tasks running in the cluster.\nSee REST API reference.\ncluster:monitor/allocation/explain\ncluster:monitor/health\ncluster:monitor/main\ncluster:monitor/nodes/hot_threads\ncluster:monitor/nodes/info\ncluster:monitor/nodes/liveness\ncluster:monitor/nodes/stats\ncluster:monitor/nodes/usage\ncluster:monitor/remote/info\ncluster:monitor/state\ncluster:monitor/stats\ncluster:monitor/task\ncluster:monitor/task/get\ncluster:monitor/tasks/list\nIndex templates\nThe index template permissions are for indexes but apply globally to the cluster.\nSee Index templates.\nindices:admin/index_template/delete\nindices:admin/index_template/get\nindices:admin/index_template/put\nindices:admin/index_template/simulate\nindices:admin/index_template/simulate_index\nIndex permissions\nThese permissions apply to an index or index pattern. You might want a user to have read access to all indices (i.e. *), but write access to only a few (e.g. web-logs and product-catalog).\nindices:admin/aliases\nindices:admin/aliases/exists\nindices:admin/aliases/get\nindices:admin/analyze\nindices:admin/cache/clear\nindices:admin/close\nindices:admin/close*\nindices:admin/create (create indices)\nindices:admin/data_stream/create\nindices:admin/data_stream/delete\nindices:admin/data_stream/get\nindices:admin/delete (delete indices)\nindices:admin/exists\nindices:admin/flush\nindices:admin/flush*\nindices:admin/forcemerge\nindices:admin/get (retrieve index and mapping)\nindices:admin/mapping/put\nindices:admin/mappings/fields/get\nindices:admin/mappings/fields/get*\nindices:admin/mappings/get\nindices:admin/open\nindices:admin/plugins/replication/index/setup/validate\nindices:admin/plugins/replication/index/start\nindices:admin/plugins/replication/index/pause\nindices:admin/plugins/replication/index/resume\nindices:admin/plugins/replication/index/stop\nindices:admin/plugins/replication/index/update\nindices:admin/plugins/replication/index/status_check\nindices:admin/refresh\nindices:admin/refresh*\nindices:admin/resolve/index\nindices:admin/rollover\nindices:admin/seq_no/global_checkpoint_sync\nindices:admin/settings/update\nindices:admin/shards/search_shards\nindices:admin/shrink\nindices:admin/synced_flush\nindices:admin/template/delete\nindices:admin/template/get\nindices:admin/template/put\nindices:admin/types/exists\nindices:admin/upgrade\nindices:admin/validate/query\nindices:data/read/explain\nindices:data/read/field_caps\nindices:data/read/field_caps*\nindices:data/read/get\nindices:data/read/mget\nindices:data/read/mget*\nindices:data/read/msearch\nindices:data/read/msearch/template\nindices:data/read/mtv (multi-term vectors)\nindices:data/read/mtv*\nindices:data/read/plugins/replication/file_chunk\nindices:data/read/plugins/replication/changes\nindices:data/read/scroll\nindices:data/read/scroll/clear\nindices:data/read/search\nindices:data/read/search*\nindices:data/read/search/template\nindices:data/read/tv (term vectors)\nindices:data/write/bulk\nindices:data/write/bulk*\nindices:data/write/delete (delete documents)\nindices:data/write/delete/byquery\nindices:data/write/plugins/replication/changes\nindices:data/write/index (add documents to existing indices)\nindices:data/write/reindex\nindices:data/write/update\nindices:data/write/update/byquery\nindices:monitor/data_stream/stats\nindices:monitor/recovery\nindices:monitor/segments\nindices:monitor/settings/get\nindices:monitor/shard_stores\nindices:monitor/stats\nindices:monitor/upgrade",
    "ancestors": [
      "Security in OpenSearch",
      "Access control"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/access-control/users-roles/",
    "title": "Users and roles",
    "content": "The Security plugin includes an internal user database. Use this database in place of or in addition to an external authentication system such as LDAP or Active Directory.\nRoles are the core way of controlling access to your cluster. Roles contain any combination of cluster-wide permissions, index-specific permissions, document- and field-level security, and tenants. Then you map users to these roles so that users gain those permissions.\nUnless you need to create new reserved or hidden users, we highly recommend using OpenSearch Dashboards or the REST API to create new users, roles, and role mappings. The.yml files are for initial setup, not ongoing use.\nTable of contents Create users OpenSearch Dashboards internal_users.yml REST API Create roles OpenSearch Dashboards roles.yml REST API Map users to roles OpenSearch Dashboards roles_mapping.yml REST API Predefined roles Sample roles Set up a read-only user in OpenSearch Dashboards Set up a bulk access role in OpenSearch Dashboards Create users\nYou can create users using OpenSearch Dashboards, internal_users.yml, or the REST API. When creating a user, you can map users to roles using internal_users.yml or the REST API, but that feature is not currently available in OpenSearch Dashboards.\nOpenSearch Dashboards\nChoose Security, Internal Users, and Create internal user.\nProvide a username and password. The Security plugin automatically hashes the password and stores it in the.opendistro_security index.\nIf desired, specify user attributes.\nAttributes are optional user properties that you can use for variable substitution in index permissions or document-level security.\nChoose Submit.\ninternal_users.yml\nSee YAML files.\nREST API\nSee Create user.\nCreate roles\nJust like users, you can create roles using OpenSearch Dashboards, roles.yml, or the REST API.\nOpenSearch Dashboards\nChoose Security, Roles, and Create role.\nProvide a name for the role.\nAdd permissions as desired.\nFor example, you might give a role no cluster permissions, read permissions to two indices, unlimited permissions to a third index, and read permissions to the analysts tenant.\nChoose Submit.\nroles.yml\nSee YAML files.\nREST API\nSee Create role.\nMap users to roles\nIf you didn’t specify roles when you created your user, you can map roles to it afterwards.\nJust like users and roles, you create role mappings using OpenSearch Dashboards, roles_mapping.yml, or the REST API.\nOpenSearch Dashboards\nChoose Security, Roles, and a role.\nChoose the Mapped users tab and Manage mapping.\nSpecify users or external identities (also known as backend roles).\nChoose Map.\nroles_mapping.yml\nSee YAML files.\nREST API\nSee Create role mapping.\nPredefined roles\nThe Security plugin includes several predefined roles that serve as useful defaults. Role Description alerting_ack_alerts Grants permissions to view and acknowledge alerts, but not to modify destinations or monitors. alerting_full_access Grants full permissions to all alerting actions. alerting_read_access Grants permissions to view alerts, destinations, and monitors, but not to acknowledge alerts or modify destinations or monitors. anomaly_full_access Grants full permissions to all anomaly detection actions. anomaly_read_access Grants permissions to view detectors, but not to create, modify, or delete detectors. all_access Grants full access to the cluster, including all cluster-wide operations, permission to write to all cluster indexes, and permission to write to all tenants. For more information on access using the REST API, see Access control for the API. cross_cluster_replication_follower_full_access Grants full access to perform cross-cluster replication actions on the follower cluster. cross_cluster_replication_leader_full_access Grants full access to perform cross-cluster replication actions on the leader cluster. observability_full_access Grants full access to perform actions on Observability objects such as visualizations, notebooks, and operational panels. observability_read_access Grants permission to view Observability objects such as visualizations, notebooks, and operational panels, but not to create, modify, or delete them. kibana_read_only A special role that prevents users from making changes to visualizations, dashboards, and other OpenSearch Dashboards objects. To enable read-only mode in Dashboards, add the opensearch_security.readonly_mode.roles setting to the opensearch_dashboards.yml file and include the role as a setting value. See the example configuration in Dashboards documentation. kibana_user Grants permissions to use OpenSearch Dashboards: cluster-wide searches, index monitoring, and write to various OpenSearch Dashboards indexes. logstash Grants permissions for Logstash to interact with the cluster: cluster-wide searches, cluster monitoring, and write to the various Logstash indexes. manage_snapshots Grants permissions to manage snapshot repositories, take snapshots, and restore snapshots. readall Grants permissions for cluster-wide searches like msearch and search permissions for all indexes. readall_and_monitor Same as readall but with added cluster permissions for monitoring. security_rest_api_access A special role that allows access to the REST API. See plugins.security.restapi.roles_enabled in opensearch.yml and Access control for the API. reports_read_access Grants permissions to generate on-demand reports, download existing reports, and view report definitions but not to create report definitions. reports_instances_read_access Grants permissions to generate on-demand reports and download existing reports but not to view or create report definitions. reports_full_access Grants full permissions to reports. asynchronous_search_full_access Grants full permissions to all asynchronous search actions. asynchronous_search_read_access Grants permissions to view asynchronous searches but not to submit, modify, or delete them. index_management_full_access Grants full permissions to all index management actions, including Index State Management (ISM), transforms, and rollups. snapshot_management_full_access Grants full permissions to all snapshot management actions. snapshot_management_read_access Grants permissions to view policies but not to create, modify, start, stop, or delete them. point_in_time_full_access Grants full permissions to all Point in Time operations. security_analytics_full_access Grants full permissions to all Security Analytics functionality. security_analytics_read_access Grants permissions to view the various components in Security Analytics, such as detectors, alerts, and findings. It also includes permissions that allow users to search for detectors and rules. This role does not allow a user to perform actions such as modfying or deleting a detector. security_analytics_ack_alerts Grants permissions to view and acknowledge alerts. For more detailed summaries of the permissions for each role, reference their action groups against the descriptions in Default action groups.\nSample roles\nThe following examples demonstrate how you might set up a read-only role and a bulk access role.\nSet up a read-only user in OpenSearch Dashboards\nCreate a new read_only_index role:\nOpen OpenSearch Dashboards.\nChoose Security, Roles.\nCreate a new role named read_only_index.\nFor Cluster permissions, add the cluster_composite_ops_ro action group.\nFor Index Permissions, add an index pattern. For example, you might specify my-index-*.\nFor index permissions, add the read action group.\nChoose Create.\nMap three roles to the read-only user:\nChoose the Mapped users tab and Manage mapping.\nFor Internal users, add your read-only user.\nChoose Map.\nRepeat these steps for the opensearch_dashboards_user and opensearch_dashboards_read_only roles.\nSet up a bulk access role in OpenSearch Dashboards\nCreate a new bulk_access role:\nOpen OpenSearch Dashboards.\nChoose Security, Roles.\nCreate a new role named bulk_access.\nFor Cluster permissions, add the cluster_composite_ops action group.\nFor Index Permissions, add an index pattern. For example, you might specify my-index-*.\nFor index permissions, add the write action group.\nChoose Create.\nMap the role to your user:\nChoose the Mapped users tab and Manage mapping.\nFor Internal users, add your bulk access user.\nChoose Map.",
    "ancestors": [
      "Security in OpenSearch",
      "Access control"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/audit-logs/field-reference/",
    "title": "Audit log field reference",
    "content": "This page contains descriptions for all audit log fields.\nCommon attributes\nThe following attributes are logged for all event categories, independent of the layer. Name Description audit_format_version The audit log message format version. audit_category The audit log category. FAILED_LOGIN, MISSING_PRIVILEGES, BAD_HEADERS, SSL_EXCEPTION, OPENSEARCH_SECURITY_INDEX_ATTEMPT, AUTHENTICATED, or GRANTED_PRIVILEGES. audit_node_id The ID of the node where the event was generated. audit_node_name The name of the node where the event was generated. audit_node_host_address The host address of the node where the event was generated. audit_node_host_name The host name of the node where the event was generated. audit_request_layer The layer on which the event has been generated, either TRANSPORT or REST. audit_request_origin The layer from which the event originated, either TRANSPORT or REST. audit_request_effective_user_is_admin True if the request was made with a TLS admin certificate, otherwise false. REST FAILED_LOGIN attributes Name Description audit_request_effective_user The username that failed to authenticate. audit_rest_request_path The REST endpoint URI. audit_rest_request_params The HTTP request parameters, if any. audit_rest_request_headers The HTTP headers, if any. audit_request_initiating_user The user that initiated the request. Only logged if it differs from the effective user. audit_request_body The HTTP request body, if any (and if request body logging is enabled). REST AUTHENTICATED attributes Name Description audit_request_effective_user The username that failed to authenticate. audit_request_initiating_user The user that initiated the request. Only logged if it differs from the effective user. audit_rest_request_path The REST endpoint URI. audit_rest_request_params The HTTP request parameters, if any. audit_rest_request_headers The HTTP headers, if any. audit_request_body The HTTP request body, if any (and if request body logging is enabled). REST SSL_EXCEPTION attributes Name Description audit_request_exception_stacktrace The stack trace of the SSL exception. REST BAD_HEADERS attributes Name Description audit_rest_request_path The REST endpoint URI. audit_rest_request_params The HTTP request parameters, if any. audit_rest_request_headers The HTTP headers, if any. audit_request_body The HTTP request body, if any (and if request body logging is enabled). Transport FAILED_LOGIN attributes Name Description audit_trace_task_id The ID of the request. audit_transport_headers The headers of the request, if any. audit_request_effective_user The username that failed to authenticate. audit_request_initiating_user The user that initiated the request. Only logged if it differs from the effective user. audit_transport_request_type The type of request (e.g. IndexRequest). audit_request_body The HTTP request body, if any (and if request body logging is enabled). audit_trace_indices The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. audit_trace_resolved_indices The resolved index name(s) affected by the request. Only logged if resolve_indices is true. audit_trace_doc_types The document types affected by the request. Only logged if resolve_indices is true. Transport AUTHENTICATED attributes Name Description audit_trace_task_id The ID of the request. audit_transport_headers The headers of the request, if any. audit_request_effective_user The username that failed to authenticate. audit_request_initiating_user The user that initiated the request. Only logged if it differs from the effective user. audit_transport_request_type The type of request (e.g. IndexRequest). audit_request_body The HTTP request body, if any (and if request body logging is enabled). audit_trace_indices The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. audit_trace_resolved_indices The resolved index name(s) affected by the request. Only logged if resolve_indices is true. audit_trace_doc_types The document types affected by the request. Only logged if resolve_indices is true. Transport MISSING_PRIVILEGES attributes Name Description audit_trace_task_id The ID of the request. audit_trace_task_parent_id The parent ID of this request, if any. audit_transport_headers The headers of the request, if any. audit_request_effective_user The username that failed to authenticate. audit_request_initiating_user The user that initiated the request. Only logged if it differs from the effective user. audit_transport_request_type The type of request (e.g. IndexRequest). audit_request_privilege The required privilege of the request (e.g. indices:data/read/search). audit_request_body The HTTP request body, if any (and if request body logging is enabled). audit_trace_indices The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. audit_trace_resolved_indices The resolved index name(s) affected by the request. Only logged if resolve_indices is true. audit_trace_doc_types The document types affected by the request. Only logged if resolve_indices is true. Transport GRANTED_PRIVILEGES attributes Name Description audit_trace_task_id The ID of the request. audit_trace_task_parent_id The parent ID of this request, if any. audit_transport_headers The headers of the request, if any. audit_request_effective_user The username that failed to authenticate. audit_request_initiating_user The user that initiated the request. Only logged if it differs from the effective user. audit_transport_request_type The type of request (e.g. IndexRequest). audit_request_privilege The required privilege of the request (e.g. indices:data/read/search). audit_request_body The HTTP request body, if any (and if request body logging is enabled). audit_trace_indices The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. audit_trace_resolved_indices The resolved index name(s) affected by the request. Only logged if resolve_indices is true. audit_trace_doc_types The document types affected by the request. Only logged if resolve_indices is true. Transport SSL_EXCEPTION attributes Name Description audit_request_exception_stacktrace The stack trace of the SSL exception. Transport BAD_HEADERS attributes Name Description audit_trace_task_id The ID of the request. audit_trace_task_parent_id The parent ID of this request, if any. audit_transport_headers The headers of the request, if any. audit_request_effective_user The username that failed to authenticate. audit_request_initiating_user The user that initiated the request. Only logged if it differs from the effective user. audit_transport_request_type The type of request (e.g. IndexRequest). audit_request_body The HTTP request body, if any (and if request body logging is enabled). audit_trace_indices The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. audit_trace_resolved_indices The resolved index name(s) affected by the request. Only logged if resolve_indices is true. audit_trace_doc_types The document types affected by the request. Only logged if resolve_indices is true. Transport opensearch_SECURITY_INDEX_ATTEMPT attributes Name Description audit_trace_task_id The ID of the request. audit_transport_headers The headers of the request, if any. audit_request_effective_user The username that failed to authenticate. audit_request_initiating_user The user that initiated the request. Only logged if it differs from the effective user. audit_transport_request_type The type of request (e.g. IndexRequest). audit_request_body The HTTP request body, if any (and if request body logging is enabled). audit_trace_indices The index name(s) included in the request. Can contain wildcards, date patterns, and aliases. Only logged if resolve_indices is true. audit_trace_resolved_indices The resolved index name(s) affected by the request. Only logged if resolve_indices is true. audit_trace_doc_types The document types affected by the request. Only logged if resolve_indices is true.",
    "ancestors": [
      "Security in OpenSearch",
      "Audit logs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/audit-logs/index/",
    "title": "Audit logs",
    "content": "Audit logs let you track access to your OpenSearch cluster and are useful for compliance purposes or in the aftermath of a security breach. You can configure the categories to be logged, the detail level of the logged messages, and where to store the logs.\nTo enable audit logging:\nAdd the following line to opensearch.yml on each node: plugins.security.audit.type: internal_opensearch This setting stores audit logs on the current cluster. For other storage options, see Audit Log Storage Types.\nRestart each node.\nAfter this initial setup, you can use OpenSearch Dashboards to manage your audit log categories and other settings. In OpenSearch Dashboards, choose Security, Audit logs.\nTable of contents Tracked events Exclude categories Disable REST or the transport layer Disable request body logging Log index names Configure bulk request handling Exclude requests Exclude users Configure the audit log index name (Advanced) Tune the thread pool Tracked events\nAudit logging records events in two ways: HTTP requests (REST) and the transport layer. Event Logged on REST Logged on transport Description FAILED_LOGIN Yes\nYes\nThe credentials of a request could not be validated, most likely because the user does not exist or the password is incorrect. AUTHENTICATED Yes\nYes\nA user successfully authenticated. MISSING_PRIVILEGES No\nYes\nThe user does not have the required permissions to execute the request. GRANTED_PRIVILEGES No\nYes\nA user made a successful request to OpenSearch. SSL_EXCEPTION Yes\nYes\nAn attempt was made to access OpenSearch without a valid SSL/TLS certificate. opensearch_SECURITY_INDEX_ATTEMPT No\nYes\nAn attempt was made to modify the Security plugin internal user and privileges index without the required permissions or TLS admin certificate. BAD_HEADERS Yes\nYes\nAn attempt was made to spoof a request to OpenSearch with the Security plugin internal headers. These default log settings work well for most use cases, but you can change settings to save storage space or adapt the information to your exact needs.\nExclude categories\nTo exclude categories, set: plugins.security.audit.config.disabled_rest_categories: &lt;disabled categories&gt; plugins.security.audit.config.disabled_transport_categories: &lt;disabled categories&gt; For example: plugins.security.audit.config.disabled_rest_categories: AUTHENTICATED, opensearch_SECURITY_INDEX_ATTEMPT plugins.security.audit.config.disabled_transport_categories: GRANTED_PRIVILEGES If you want to log events in all categories, use NONE: plugins.security.audit.config.disabled_rest_categories: NONE plugins.security.audit.config.disabled_transport_categories: NONE Disable REST or the transport layer\nBy default, the Security plugin logs events on both REST and the transport layer. You can disable either type: plugins.security.audit.enable_rest: false plugins.security.audit.enable_transport: false Disable request body logging\nBy default, the Security plugin includes the body of the request (if available) for both REST and the transport layer. If you do not want or need the request body, you can disable it: plugins.security.audit.log_request_body: false Log index names\nBy default, the Security plugin logs all indices affected by a request. Because index names can be aliases and contain wildcards/date patterns, the Security plugin logs the index name that the user submitted and the actual index name to which it resolves.\nFor example, if you use an alias or a wildcard, the audit event might look like: audit_trace_indices: [ \"human*\"], audit_trace_resolved_indices: [ \"humanresources\"] You can disable this feature by setting: plugins.security.audit.resolve_indices: false Disabling this feature only takes effect if plugins.security.audit.log_request_body is also set to false.\nConfigure bulk request handling\nBulk requests can contain many indexing operations. By default, the Security plugin only logs the single bulk request, not each individual operation.\nThe Security plugin can be configured to log each indexing operation as a separate event: plugins.security.audit.resolve_bulk_requests: true This change can create a massive number of events in the audit logs, so we don’t recommend enabling this setting if you make heavy use of the _bulk API.\nExclude requests\nYou can exclude certain requests from being logged completely, by either configuring actions (for transport requests) and/or HTTP request paths (REST): plugins.security.audit.ignore_requests: [ \" indices:data/read/*\", \" SearchRequest\"] Exclude users\nBy default, the Security plugin logs events from all users, but excludes the internal OpenSearch Dashboards server user kibanaserver. You can exclude other users: plugins.security.audit.ignore_users: - kibanaserver - admin If requests from all users should be logged, use NONE: plugins.security.audit.ignore_users: NONE Configure the audit log index name\nBy default, the Security plugin stores audit events in a daily rolling index named auditlog-YYYY.MM.dd. You can configure the name of the index in opensearch.yml: plugins.security.audit.config.index: myauditlogindex Use a date pattern in the index name to configure daily, weekly, or monthly rolling indices: plugins.security.audit.config.index: \" 'auditlog-'YYYY.MM.dd\" For a reference on the date pattern format, see the Joda DateTimeFormat documentation.\n(Advanced) Tune the thread pool\nThe Search plugin logs events asynchronously, which keeps performance impact on your cluster minimal. The plugin uses a fixed thread pool to log events. You can define the number of threads in the pool in opensearch.yml: plugins.security.audit.threadpool.size: &lt;integer&gt; The default setting is 10. Setting this value to 0 disables the thread pool, which means the plugin logs events synchronously. To set the maximum queue length per thread: plugins.security.audit.threadpool.max_queue_len: 100000",
    "ancestors": [
      "Security in OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/audit-logs/storage-types/",
    "title": "Audit log storage types",
    "content": "Audit logs can take up quite a bit of space, so the Security plugin offers several options for storage locations. Setting Description debug\nOutputs to stdout. Useful for testing and debugging.\ninternal_opensearch\nWrites to an audit index on the current OpenSearch cluster.\nexternal_opensearch\nWrites to an audit index on a remote OpenSearch cluster.\nwebhook\nSends events to an arbitrary HTTP endpoint.\nlog4j\nWrites the events to a Log4j logger. You can use any Log4j appender, such as SNMP, JDBC, Cassandra, and Kafka. You configure the output location in opensearch.yml: plugins.security.audit.type: &lt;debug|internal_opensearch|external_opensearch|webhook|log4j&gt; external_opensearch, webhook, and log4j all have additional configuration options. Details follow.\nExternal OpenSearch\nThe external_opensearch storage type requires one or more OpenSearch endpoints with a host/IP address and port. Optionally, provide the index name and a document type. plugins.security.audit.type: external_opensearch plugins.security.audit.config.http_endpoints: [ &lt;endpoints&gt;] plugins.security.audit.config.index: &lt;indexname&gt; plugins.security.audit.config.type: _doc The Security plugin uses the OpenSearch REST API to send events, just like any other indexing request. For plugins.security.audit.config.http_endpoints, use a comma-separated list of hosts/IP addresses and the REST port (default 9200). plugins.security.audit.config.http_endpoints: [192.168.178.1:9200,192.168.178.2:9200] If you use external_opensearch and the remote cluster also uses the Security plugin, you must supply some additional parameters for authentication. These parameters depend on which authentication type you configured for the remote cluster.\nTLS settings Name Data type Description plugins.security.audit.config.enable_ssl Boolean\nIf you enabled SSL/TLS on the receiving cluster, set to true. The default is false. plugins.security.audit.config.verify_hostnames Boolean\nWhether to verify the hostname of the SSL/TLS certificate of the receiving cluster. Default is true. plugins.security.audit.config.pemtrustedcas_filepath String\nThe trusted root certificate of the external OpenSearch cluster, relative to the config directory. plugins.security.audit.config.pemtrustedcas_content String\nInstead of specifying the path ( plugins.security.audit.config.pemtrustedcas_filepath), you can configure the Base64-encoded certificate content directly. plugins.security.audit.config.enable_ssl_client_auth Boolean\nWhether to enable SSL/TLS client authentication. If you set this to true, the audit log module sends the node’s certificate along with the request. The receiving cluster can use this certificate to verify the identity of the caller. plugins.security.audit.config.pemcert_filepath String\nThe path to the TLS certificate to send to the external OpenSearch cluster, relative to the config directory. plugins.security.audit.config.pemcert_content String\nInstead of specifying the path ( plugins.security.audit.config.pemcert_filepath), you can configure the Base64-encoded certificate content directly. plugins.security.audit.config.pemkey_filepath String\nThe path to the private key of the TLS certificate to send to the external OpenSearch cluster, relative to the config directory. plugins.security.audit.config.pemkey_content String\nInstead of specifying the path ( plugins.security.audit.config.pemkey_filepath), you can configure the Base64-encoded certificate content directly. plugins.security.audit.config.pemkey_password String\nThe password of the private key. Basic auth settings\nIf you enabled HTTP basic authentication on the receiving cluster, use these settings to specify the username and password: plugins.security.audit.config.username: &lt;username&gt; plugins.security.audit.config.password: &lt;password&gt; Webhook\nUse the following keys to configure the webhook storage type. Name Data type Description plugins.security.audit.config.webhook.url String\nThe HTTP or HTTPS URL to send the logs to. plugins.security.audit.config.webhook.ssl.verify Boolean\nIf true, the TLS certificate provided by the endpoint (if any) will be verified. If set to false, no verification is performed. You can disable this check if you use self-signed certificates. plugins.security.audit.config.webhook.ssl.pemtrustedcas_filepath String\nThe path to the trusted certificate against which the webhook’s TLS certificate is validated. plugins.security.audit.config.webhook.ssl.pemtrustedcas_content String\nSame as plugins.security.audit.config.webhook.ssl.pemtrustedcas_content, but you can configure the base 64 encoded certificate content directly. plugins.security.audit.config.webhook.format String\nThe format in which the audit log message is logged, can be one of URL_PARAMETER_GET, URL_PARAMETER_POST, TEXT, JSON, SLACK. See Formats. Formats Format Description URL_PARAMETER_GET Uses HTTP GET to send logs to the webhook URL. All logged information is appended to the URL as request parameters. URL_PARAMETER_POST Uses HTTP POST to send logs to the webhook URL. All logged information is appended to the URL as request parameters. TEXT Uses HTTP POST to send logs to the webhook URL. The request body contains the audit log message in plain text format. JSON Uses HTTP POST to send logs to the webhook URL. The request body contains the audit log message in JSON format. SLACK Uses HTTP POST to send logs to the webhook URL. The request body contains the audit log message in JSON format suitable for consumption by Slack. The default implementation returns \"text\": \"&lt;AuditMessage#toText&gt;\". Log4j\nThe log4j storage type lets you specify the name of the logger and log level. plugins.security.audit.config.log4j.logger_name: audit plugins.security.audit.config.log4j.level: INFO By default, the Security plugin uses the logger name audit and logs the events on INFO level. Audit events are stored in JSON format.",
    "ancestors": [
      "Security in OpenSearch",
      "Audit logs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/authentication-backends/authc-index/",
    "title": "Authentication backends",
    "content": "Authentication backend configurations determine the method or methods you use for authenticating users and the way users pass their credentials and sign in to OpenSearch. Having an understanding of the basic authentication flow before getting started can help with the configuration process for whichever backend you choose. Consider the high-level sequence of events in the description that follows, and then refer to the detailed steps for configuring the authentication type you choose to use with OpenSearch.\nAuthentication flow\nTo identify a user who wants to access the cluster, the Security plugin needs the user’s credentials.\nThese credentials differ depending on how you’ve configured the plugin. For example, if you use basic authentication, the credentials are a username and password. If you use a JSON web token, the credentials (username and roles) are stored within the token itself. If you use TLS certificates, the credentials are the distinguished name (DN) of the certificate. No matter which backend you use, these credentials are included in the request for authentication.\nThe Security plugin authenticates a request against a backend configured for an authentication provider. Some examples of authentication providers used with OpenSearch include Basic Auth (which uses the internal user database), LDAP/Active Directory, JSON web tokens, SAML, or another authentication protocol.\nThe plugin supports chaining backends in config/opensearch-security/config.yml. If more than one backend is present, the plugin tries to authenticate the user sequentially against each until one succeeds. A common use case is to combine the internal user database of the Security plugin with LDAP/Active Directory.\nAfter a backend verifies the user’s credentials, the plugin collects any backend roles. The authentication provider determines the way these roles are retrieved. For example, LDAP extracts backend roles from its directory service based on their mappings to roles in OpenSearch, while SAML stores the roles as attributes. When basic authentication is used, the internal user database refers to role mappings configured in OpenSearch.\nAfter the user is authenticated and any backend roles are retrieved, the Security plugin uses the role mapping to assign security roles to the user.\nIf the role mapping doesn’t include the user (or the user’s backend roles), the user is successfully authenticated, but has no permissions.\nThe user can now perform actions as defined by the mapped security roles. For example, a user might map to the kibana_user role and thus have permissions to access OpenSearch Dashboards.",
    "ancestors": [
      "Security in OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/authentication-backends/client-auth/",
    "title": "Client certificate authentication",
    "content": "After obtaining your own certificates either from a certificate authority (CA) or by generating your own certificates using OpenSSL, you can start configuring OpenSearch to authenticate a user using a client certificate.\nClient certificate authentication offers more security advantages than just using basic authentication (username and password). Because client certificate authentication requires both a client certificate and its private key, which are often in the user’s possession, it is less vulnerable to brute force attacks in which malicious individuals try to guess a user’s password.\nAnother benefit of client certificate authentication is you can use it along with basic authentication, providing two layers of security.\nEnabling client certificate authentication\nTo enable client certificate authentication, you must first set clientauth_mode in opensearch.yml to either OPTIONAL or REQUIRE: plugins.security.ssl.http.clientauth_mode: OPTIONAL Next, enable client certificate authentication in the client_auth_domain section of config.yml. clientcert_auth_domain: description: \" Authenticate via SSL client certificates\" http_enabled: true transport_enabled: true order: 1 http_authenticator: type: clientcert config: username_attribute: cn #optional, if omitted DN becomes username challenge: false authentication_backend: type: noop Assigning roles to your common name\nYou can now assign your certificate’s common name (CN) to a role. For this step, you must know your certificate’s CN and the role you want to assign to. To get a list of all predefined roles in OpenSearch, refer to our list of predefined roles. If you want to first create a role, refer to how to create a role, and then map your certificate’s CN to that role.\nAfter deciding which role you want to map your certificate’s CN to, you can use OpenSearch Dashboards, roles_mapping.yml, or the REST API to map your certificate’s CN to the role. The following example uses the REST API to map the common name CLIENT1 to the role readall. Example request PUT _plugins/_security/api/rolesmapping/readall { \"backend_roles\": [ \"sample_role\"], \"hosts\": [ \"example.host.com\"], \"users\": [ \"CLIENT1\"] } Example response { \"status\": \"OK\", \"message\": \"'readall' updated.\" } After mapping a role to your client certificate’s CN, you’re ready to connect to your cluster using those credentials.\nThe code example below uses the Python requests library to connect to a local OpenSearch cluster and sends a GET request to the movies index. import requests import json base_url = 'https://localhost:9200/' headers = { 'Content-Type': 'application/json' } cert_file_path = \"/full/path/to/client-cert.pem\" key_file_path = \"/full/path/to/client-cert-key.pem\" root_ca_path = \"/full/path/to/root-ca.pem\" # Send the request. path = 'movies/_doc/3' url = base_url + path response = requests. get ( url, cert = ( cert_file_path, key_file_path), verify = root_ca_path) print ( response. text) Using certificates with Docker\nWhile we recommend using the tarball installation of ODFE to test client certificate authentication configurations, you can also use any of the other install types. For instructions on using Docker security, see Configuring basic security settings.",
    "ancestors": [
      "Security in OpenSearch",
      "Authentication backends"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/authentication-backends/ldap/",
    "title": "Active Directory and LDAP",
    "content": "Active Directory and LDAP can be used for both authentication and authorization (the authc and authz sections of the configuration, respectively). Authentication checks whether the user has entered valid credentials. Authorization retrieves any backend roles for the user.\nIn most cases, you want to configure both authentication and authorization. You can also use authentication only and map the users retrieved from LDAP directly to Security plugin roles.\nDocker example\nWe provide a fully functional example that can help you understand how to use an LDAP server for both authentication and authorization.\nDownload and unzip the example zip file.\nAt the command line, run docker-compose up.\nReview the files: docker-compose.yml defines a single OpenSearch node, an LDAP server, and a PHP administration tool for the LDAP server.\nYou can access the administration tool at https://localhost:6443. Acknowledge the security warning and log in using cn=admin,dc=example,dc=org and changethis. directory.ldif seeds the LDAP server with three users and two groups. psantos is in the Administrator and Developers groups. jroe and jdoe are in the Developers group. The Security plugin loads these groups as backend roles. roles_mapping.yml maps the Administrator and Developers LDAP groups (as backend roles) to security roles so that users gain the appropriate permissions after authenticating. internal_users.yml removes all default users except administrator and kibanaserver. config.yml includes all necessary LDAP settings.\nIndex a document as psantos: curl -XPUT 'https://localhost:9200/new-index/_doc/1' -H 'Content-Type: application/json' -d '{\"title\": \"Spirited Away\"}' -u 'psantos:password' -k If you try the same request as jroe, it fails. The Developers group is mapped to the readall, manage_snapshots, and kibana_user roles and has no write permissions.\nSearch for the document as jroe: curl -XGET 'https://localhost:9200/new-index/_search?pretty' -u 'jroe:password' -k This request succeeds, because the Developers group is mapped to the readall role.\nIf you want to examine the contents of the various containers, run docker ps to find the container ID and then docker exec -it &lt;container-id&gt; /bin/bash.\nConnection settings\nTo enable LDAP authentication and authorization, add the following lines to config/opensearch-security/config.yml: authc: ldap: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: basic challenge: false authentication_backend: type: ldap config:... authz: ldap: http_enabled: true transport_enabled: true authorization_backend: type: ldap config:... The connection settings are identical for authentication and authorization and are added to the config sections.\nHostname and port\nTo configure the hostname and port of your Active Directory servers, use the following: config: hosts: - primary.ldap.example.com:389 - secondary.ldap.example.com:389 You can configure more than one server here. If the Security plugin cannot connect to the first server, it tries to connect to the remaining servers sequentially.\nTimeouts\nTo configure connection and response timeouts to your Active Directory server, use the following (values are in milliseconds): config: connect_timeout: 5000 response_timeout: 0 If your server supports two-factor authentication (2FA), the default timeout settings might result in login errors. You can increase connect_timeout to accommodate the 2FA process. Setting response_timeout to 0 (the default) indicates an indefinite waiting period.\nBind DN and password\nTo configure the bind_dn and password that the Security plugin uses when issuing queries to your server, use the following: config: bind_dn: cn=admin,dc=example,dc=com password: password If your server supports anonymous authentication, both bind_dn and password can be set to null.\nTLS settings\nUse the following parameters to configure TLS for connecting to your server: config: enable_ssl: &lt;true|false&gt; enable_start_tls: &lt;true|false&gt; enable_ssl_client_auth: &lt;true|false&gt; verify_hostnames: &lt;true|false&gt; Name Description enable_ssl Whether to use LDAP over SSL (LDAPS). enable_start_tls Whether to use STARTTLS. Can’t be used in combination with LDAPS. enable_ssl_client_auth Whether to send the client certificate to the LDAP server. verify_hostnames Whether to verify the hostnames of the server’s TLS certificate. Certificate validation\nBy default, the Security plugin validates the TLS certificate of the LDAP servers against the root CA configured in opensearch.yml, either as a PEM certificate or a truststore: plugins.security.ssl.transport.pemtrustedcas_filepath:...\nplugins.security.ssl.http.truststore_filepath:... If your server uses a certificate signed by a different CA, import this CA into your truststore or add it to your trusted CA file on each node.\nYou can also use a separate root CA in PEM format by setting one of the following configuration options: config: pemtrustedcas_filepath: /full/path/to/trusted_cas.pem config: pemtrustedcas_content: |- MIID/jCCAuagAwIBAgIBATANBgkqhkiG9w0BAQUFADCBjzETMBEGCgmSJomT8ixk ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w bGUgQ29tIEluYy4xITAfBgNVBAsMGEV4YW1wbGUgQ29tIEluYy4gUm9vdCBDQTEh... Name Description pemtrustedcas_filepath Absolute path to the PEM file containing the root CAs of your Active Directory/LDAP server. pemtrustedcas_content The root CA content of your Active Directory/LDAP server. Cannot be used when pemtrustedcas_filepath is set. Client authentication\nIf you use TLS client authentication, the Security plugin sends the PEM certificate of the node, as configured in opensearch.yml. Set one of the following configuration options: config: pemkey_filepath: /full/path/to/private.key.pem pemkey_password: private_key_password pemcert_filepath: /full/path/to/certificate.pem or config: pemkey_content: |- MIID2jCCAsKgAwIBAgIBBTANBgkqhkiG9w0BAQUFADCBlTETMBEGCgmSJomT8ixk ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w bGUgQ29tIEluYy4xJDAiBgNVBAsMG0V4YW1wbGUgQ29tIEluYy4gU2lnbmluZyBD... pemkey_password: private_key_password pemcert_content: |- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCHRZwzwGlP2FvL oEzNeDu2XnOF+ram7rWPT6fxI+JJr3SDz1mSzixTeHq82P5A7RLdMULfQFMfQPfr WXgB4qfisuDSt+CPocZRfUqqhGlMG2l8LgJMr58tn0AHvauvNTeiGlyXy0ShxHbD... Name Description pemkey_filepath Absolute path to the file containing the private key of your certificate. pemkey_content The content of the private key of your certificate. Cannot be used when pemkey_filepath is set. pemkey_password The password of your private key, if any. pemcert_filepath Absolute path to the client certificate. pemcert_content The content of the client certificate. Cannot be used when pemcert_filepath is set. Enabled ciphers and protocols\nYou can limit the allowed ciphers and TLS protocols for the LDAP connection. For example, you can allow only strong ciphers and limit the TLS versions to the most recent ones: ldap: http_enabled: true transport_enabled: true... authentication_backend: type: ldap config: enabled_ssl_ciphers: - \" TLS_DHE_RSA_WITH_AES_256_CBC_SHA\" - \" TLS_DHE_DSS_WITH_AES_128_CBC_SHA256\" enabled_ssl_protocols: - \" TLSv1.1\" - \" TLSv1.2\" Name Description enabled_ssl_ciphers Array, enabled TLS ciphers. Only the Java format is supported. enabled_ssl_protocols Array, enabled TLS protocols. Only the Java format is supported. Use Active Directory and LDAP for authentication\nTo use Active Directory/LDAP for authentication, first configure a respective authentication domain in the authc section of config/opensearch-security/config.yml: authc: ldap: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: basic challenge: true authentication_backend: type: ldap config:... Next, add the connection settings for your Active Directory/LDAP server to the config section of the authentication domain: config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts: - ldap.example.com:8389 bind_dn: cn=admin,dc=example,dc=com password: passw0rd Authentication works by issuing an LDAP query containing the user name against the user subtree of the LDAP tree.\nThe Security plugin first takes the configured LDAP query and replaces the placeholder {0} with the user name from the user’s credentials. usersearch: ' (sAMAccountName={0})' Then it issues this query against the user subtree. Currently, the entire subtree under the configured userbase is searched: userbase: ' ou=people,dc=example,dc=com' If the query is successful, the Security plugin retrieves the user name from the LDAP entry. You can specify which attribute from the LDAP entry the Security plugin should use as the user name: username_attribute: uid If this key is not set or null, then the distinguished name (DN) of the LDAP entry is used.\nConfiguration summary Name Description userbase Specifies the subtree in the directory where user information is stored. usersearch The actual LDAP query that the Security plugin executes when trying to authenticate a user. The variable {0} is substituted with the user name. username_attribute The Security plugin uses this attribute of the directory entry to look for the user name. If set to null, the DN is used (default). Complete authentication example ldap: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: basic challenge: true authentication_backend: type: ldap config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts: - ldap.example.com:636 bind_dn: cn=admin,dc=example,dc=com password: password userbase: ' ou=people,dc=example,dc=com' usersearch: ' (sAMAccountName={0})' username_attribute: uid Use Active Directory and LDAP for authorization\nTo use Active Directory/LDAP for authorization, first configure a respective authorization domain in the authz section of config.yml: authz: ldap: http_enabled: true transport_enabled: true authorization_backend: type: ldap config:... Authorization is the process of retrieving backend roles for an authenticated user from an LDAP server. This is typically the same servers that you use for authentication, but you can also use a different server. The only requirement is that the user you use to fetch the roles actually exists on the LDAP server.\nBecause the Security plugin always checks if a user exists in the LDAP server, you must also configure userbase, usersearch and username_attribute in the authz section.\nAuthorization works similarly to authentication. The Security plugin issues an LDAP query containing the user name against the role subtree of the LDAP tree.\nAs an alternative, the Security plugin can also fetch roles that are defined as a direct attribute of the user entry in the user subtree.\nApproach 1: Query the role subtree\nThe Security plugin first takes the LDAP query for fetching roles (“rolesearch”) and substitutes any variables found in the query. For example, for a standard Active Directory installation, you would use the following role search: rolesearch: ' (member={0})' You can use the following variables: {0} is substituted with the DN of the user. {1} is substituted with the user name, as defined by the username_attribute setting. {2} is substituted with an arbitrary attribute value from the authenticated user’s directory entry.\nThe variable {2} refers to an attribute from the user’s directory entry. The attribute that you should use is specified by the userroleattribute setting: userroleattribute: myattribute The Security plugin then issues the substituted query against the configured role subtree. The entire subtree under rolebase is searched: rolebase: ' ou=groups,dc=example,dc=com' If you use nested roles (roles that are members of other roles), you can configure the Security plugin to resolve them: resolve_nested_roles: false After all roles have been fetched, the Security plugin extracts the final role names from a configurable attribute of the role entries: rolename: cn If this is not set, the DN of the role entry is used. You can now use this role name for mapping it to one or more of the Security plugin roles, as defined in roles_mapping.yml.\nApproach 2: Use a user’s attribute as the role name\nIf you store the roles as a direct attribute of the user entries in the user subtree, you need to configure only the attribute name: userrolename: roles You can configure multiple attribute names: userrolename: roles, otherroles This approach can be combined with querying the role subtree. The Security plugin fetches the roles from the user’s role attribute and then executes the role search.\nIf you don’t use or have a role subtree, you can disable the role search completely: rolesearch_enabled: false (Advanced) Control LDAP user attributes\nBy default, the Security plugin reads all LDAP user attributes and makes them available for index name variable substitution and DLS query variable substitution. If your LDAP entries have a lot of attributes, you might want to control which attributes should be made available. The fewer the attributes, the better the performance.\nNote that this setting is made in the authentication authc section of the config.yml file. Name Description custom_attr_allowlist String array. Specifies the LDAP attributes that should be made available for variable substitution. custom_attr_maxval_len Integer. Specifies the maximum allowed length of each attribute. All attributes longer than this value are discarded. A value of 0 disables custom attributes altogether. Default is 36. Example: authc: ldap: http_enabled: true transport_enabled: true authentication_backend: type: ldap config: custom_attr_allowlist: - attribute1 - attribute2 custom_attr_maxval_len: 36... (Advanced) Exclude certain users from role lookup\nIf you are using multiple authentication methods, it can make sense to exclude certain users from the LDAP role lookup.\nConsider the following scenario for a typical OpenSearch Dashboards setup: All OpenSearch Dashboards users are stored in an LDAP/Active Directory server.\nHowever, you also have an OpenSearch Dashboards server user. OpenSearch Dashboards uses this user to manage stored objects and perform monitoring and maintenance tasks. You do not want to add this user to your Active Directory installation, but rather store it in the Security plugin internal user database.\nIn this case, it makes sense to exclude the OpenSearch Dashboards server user from the LDAP authorization because we already know that there is no corresponding entry. You can use the skip_users configuration setting to define which users should be skipped. Wildcards and regular expressions are supported: skip_users: - kibanaserver - ' cn=Jane Doe,ou*people,o=TEST' - ' /\\S*/' (Advanced) Exclude roles from nested role lookups\nIf the users in your LDAP installation have a large number of roles, and you have the requirement to resolve nested roles as well, you might run into performance issues.\nIn most cases, however, not all user roles are related to OpenSearch and OpenSearch Dashboards. You might need only a couple of roles. In this case, you can use the nested role filter feature to define a list of roles that are filtered out from the list of the user’s roles. Wildcards and regular expressions are supported.\nThis has an effect only if resolve_nested_roles is true: nested_role_filter: - ' cn=Jane Doe,ou*people,o=TEST' -... Configuration summary Name Description rolebase Specifies the subtree in the directory where role/group information is stored. rolesearch The actual LDAP query that the Security plugin executes when trying to determine the roles of a user. You can use three variables here (see below). userroleattribute The attribute in a user entry to use for {2} variable substitution. userrolename If the roles/groups of a user are not stored in the groups subtree, but as an attribute of the user’s directory entry, define this attribute name here. rolename The attribute of the role entry that should be used as the role name. resolve_nested_roles Boolean. Whether or not to resolve nested roles. Default is false. max_nested_depth Integer. When resolve_nested_roles is true, this defines the maximum number of nested roles to traverse. Setting smaller values can reduce the amount of data retrieved from LDAP and improve authentication times at the cost of failing to discover deeply nested roles. Default is 30. skip_users Array of users that should be skipped when retrieving roles. Wildcards and regular expressions are supported. nested_role_filter Array of role DNs that should be filtered before resolving nested roles. Wildcards and regular expressions are supported. rolesearch_enabled Boolean. Enable or disable the role search. Default is true. custom_attr_allowlist String array. Specifies the LDAP attributes that should be made available for variable substitution. custom_attr_maxval_len Integer. Specifies the maximum allowed length of each attribute. All attributes longer than this value are discarded. A value of 0 disables custom attributes altogether. Default is 36. Complete authorization example authz: ldap: http_enabled: true transport_enabled: true authorization_backend: type: ldap config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts: - ldap.example.com:636 bind_dn: cn=admin,dc=example,dc=com password: password userbase: ' ou=people,dc=example,dc=com' usersearch: ' (uid={0})' username_attribute: uid rolebase: ' ou=groups,dc=example,dc=com' rolesearch: ' (member={0})' userroleattribute: null userrolename: none rolename: cn resolve_nested_roles: true skip_users: - kibanaserver - ' cn=Jane Doe,ou*people,o=TEST' - ' /\\S*/' (Advanced) Configuring multiple user and role bases\nTo configure multiple user bases in the authc and/or authz section, use the following syntax:... bind_dn: cn=admin,dc=example,dc=com password: password users: primary-userbase: base: ' ou=people,dc=example,dc=com' search: ' (uid={0})' secondary-userbase: base: ' cn=users,dc=example,dc=com' search: ' (uid={0})' username_attribute: uid... Similarly, use the following setup to configure multiple role bases in the authz section:... username_attribute: uid roles: primary-rolebase: base: ' ou=groups,dc=example,dc=com' search: ' (uniqueMember={0})' secondary-rolebase: base: ' ou=othergroups,dc=example,dc=com' search: ' (member={0})' userroleattribute: null... Complete authentication and authorization with multiple user and role bases example: authc:... ldap: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: basic challenge: true authentication_backend: type: ldap config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts: - ldap.example.com:636 bind_dn: cn=admin,dc=example,dc=com password: password users: primary-userbase: base: ' ou=people,dc=example,dc=com' search: ' (uid={0})' secondary-userbase: base: ' cn=users,dc=example,dc=com' search: ' (uid={0})' username_attribute: uid authz: ldap: http_enabled: true transport_enabled: true authorization_backend: type: ldap config: enable_ssl: true enable_start_tls: false enable_ssl_client_auth: false verify_hostnames: true hosts: - ldap.example.com:636 bind_dn: cn=admin,dc=example,dc=com password: password users: primary-userbase: base: ' ou=people,dc=example,dc=com' search: ' (uid={0})' secondary-userbase: base: ' cn=users,dc=example,dc=com' search: ' (uid={0})' username_attribute: uid roles: primary-rolebase: base: ' ou=groups,dc=example,dc=com' search: ' (uniqueMember={0})' secondary-rolebase: base: ' ou=othergroups,dc=example,dc=com' search: ' (member={0})' userroleattribute: null userrolename: none rolename: cn resolve_nested_roles: true",
    "ancestors": [
      "Security in OpenSearch",
      "Authentication backends"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/authentication-backends/openid-connect/",
    "title": "OpenID Connect",
    "content": "The Security plugin can integrate with identify providers that use the OpenID Connect standard. This feature enables the following:\nAutomatic configuration\nPoint the Security plugin to the metadata of your identity provider (IdP), and the Security plugin uses that data for configuration.\nAutomatic key fetching\nThe Security plugin automatically retrieves the public key for validating the JSON Web Tokens (JWTs) from the JSON Web Key Set (JWKS) endpoint of your IdP. You don’t have to configure keys or shared secrets in config.yml.\nKey rollover\nYou can change the keys used for signing the JWTs directly in your IdP. If the Security plugin detects an unknown key, it tries to retrieve it from the IdP. This rollover is transparent to the user.\nOpenSearch Dashboards as single sign-on or as one option among multiple authentication types in the Dashboards sign-in window.\nConfigure OpenID Connect integration\nTo integrate with an OpenID IdP, set up an authentication domain and choose openid as the HTTP authentication type. JWTs already contain all of the information required to verify the request, so set challenge to false and authentication_backend to noop.\nThis is the minimal configuration: openid_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: openid challenge: false config: subject_key: preferred_username roles_key: roles openid_connect_url: https://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration authentication_backend: type: noop The following table shows the configuration parameters. Name Description openid_connect_url The URL of your IdP where the Security plugin can find the OpenID Connect metadata/configuration settings. This URL differs between IdPs. Required. jwt_header The HTTP header that stores the token. Typically the Authorization header with the Bearer schema: Authorization: Bearer &lt;token&gt;. Optional. Default is Authorization. jwt_url_parameter If the token is not transmitted in the HTTP header, but as an URL parameter, define the name of the parameter here. Optional. subject_key The key in the JSON payload that stores the user’s name. If not defined, the subject registered claim is used. Most IdP providers use the preferred_username claim. Optional. roles_key The key in the JSON payload that stores the user’s roles. The value of this key must be a comma-separated list of roles. Required only if you want to use roles in the JWT. OpenID Connect URL\nOpenID Connect specifies various endpoints for integration purposes. The most important endpoint is well-known, which lists endpoints and other configuration options for the Security plugin.\nThe URL differs between IdPs, but usually ends in /.well-known/openid-configuration.\nKeycloak example: http(s)://&lt;server&gt;:&lt;port&gt;/auth/realms/&lt;realm&gt;/.well-known/openid-configuration The main information that the Security plugin needs is jwks_uri. This URI specifies where the IdP’s public keys in JWKS format can be found. For example: jwks_uri: \"https://keycloak.example.com:8080/auth/realms/master/protocol/openid-connect/certs\" {\nkeys:[\n{\nkid:\"V-diposfUJIk5jDBFi_QRouiVinG5PowskcSWy5EuCo\",\nkty:\"RSA\",\nalg:\"RS256\",\nuse:\"sig\",\nn:\"rI8aUrAcI_auAdF10KUopDOmEFa4qlUUaNoTER90XXWADtKne6VsYoD3ZnHGFXvPkRAQLM5d65ScBzWungcbLwZGWtWf5T2NzQj0wDyquMRwwIAsFDFtAZWkXRfXeXrFY0irYUS9rIJDafyMRvBbSz1FwWG7RTQkILkwiC4B8W1KdS5d9EZ8JPhrXvPMvW509g0GhLlkBSbPBeRSUlAS2Kk6nY5i3m6fi1H9CP3Y_X-TzOjOTsxQA_1pdP5uubXPUh5YfJihXcgewO9XXiqGDuQn6wZ3hrF6HTlhNWGcSyQPKh1gEcmXWQlRENZMvYET-BuJEE7eKyM5vRhjNoYR3w\",\ne:\"AQAB\"\n}]\n} For more information about IdP endpoints, see the following: Okta Keycloak Auth0 Connect2ID Salesforce IBM OpenID Connect Time disparity compensation for JWT validation\nOccasionally you may find that the clock times between the authentication server and the OpenSearch node are not perfectly synchronized. When this is the case, even by a few seconds, the system that either issues or receives a JWT may try to validate nbf (not before) and exp (expiration) claims and fail to authenticate the user due to the time disparity.\nBy default, Security allows for a window of 30 seconds to compensate for possible misalignment between server clock times. To set a custom value for this feature and override the default, you can add the jwt_clock_skew_tolerance_seconds setting to the config.yml: http_authenticator: type: openid challenge: false config: subject_key: preferred_username roles_key: roles openid_connect_url: https://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration jwt_clock_skew_tolerance_seconds: 20 Fetching public keys\nWhen an IdP generates and signs a JWT, it must add the ID of the key to the JWT header. For example: {\n\"alg\": \"RS256\",\n\"typ\": \"JWT\",\n\"kid\": \"V-diposfUJIk5jDBFi_QRouiVinG5PowskcSWy5EuCo\"\n} As per the OpenID Connect specification, the kid (key ID) is mandatory. Token verification does not work if an IdP fails to add the kid field to the JWT.\nIf the Security plugin receives a JWT with an unknown kid, it visits the IdP’s jwks_uri and retrieves all available, valid keys. These keys are used and cached until a refresh is triggered by retrieving another unknown key ID.\nKey rollover and multiple public keys\nThe Security plugin can maintain multiple valid public keys at once. The OpenID specification does not allow for a validity period of public keys, so a key is valid until it has been removed from the list of valid keys in your IdP and the list of valid keys has been refreshed.\nIf you want to roll over a key in your IdP, follow these best practices:\nCreate a new key pair in your IdP, and give the new key a higher priority than the currently used key.\nYour IdP uses this new key over the old key.\nUpon first appearance of the new kid in a JWT, the Security plugin refreshes the key list.\nAt this point, both the old key and the new key are valid. Tokens signed with the old key are also still valid.\nThe old key can be removed from your IdP when the last JWT signed with this key has timed out.\nIf you have to immediately change your public key, you can also delete the old key first and then create a new one. In this case, all JWTs signed with the old key become invalid immediately.\nTLS settings\nTo prevent man-in-the-middle attacks, you should secure the connection between the Security plugin and your IdP with TLS.\nEnabling TLS\nUse the following parameters to enable TLS for connecting to your IdP: config: openid_connect_idp: enable_ssl: &lt;true|false&gt; verify_hostnames: &lt;true|false&gt; Name Description enable_ssl Whether to use TLS. Default is false. verify_hostnames Whether to verify the hostnames of the IdP’s TLS certificate. Default is true. Certificate validation\nTo validate the TLS certificate of your IdP, configure either the path to the IdP’s root CA or the root certificate’s content: config: openid_connect_idp: enable_ssl: true pemtrustedcas_filepath: /full/path/to/trusted_cas.pem config: openid_connect_idp: enable_ssl: true pemtrustedcas_content: |- MIID/jCCAuagAwIBAgIBATANBgkqhkiG9w0BAQUFADCBjzETMBEGCgmSJomT8ixk ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w bGUgQ29tIEluYy4xITAfBgNVBAsMGEV4YW1wbGUgQ29tIEluYy4gUm9vdCBDQTEh... Name Description pemtrustedcas_filepath Absolute path to the PEM file containing the root CAs of your IdP. pemtrustedcas_content The root CA content of your IdP. Cannot be used if pemtrustedcas_filepath is set. TLS client authentication\nTo use TLS client authentication, configure the PEM certificate and private key the Security plugin should send for TLS client authentication (or its content): config: openid_connect_idp: enable_ssl: true pemkey_filepath: /full/path/to/private.key.pem pemkey_password: private_key_password pemcert_filepath: /full/path/to/certificate.pem config: openid_connect_idp: enable_ssl: true pemkey_content: |- MIID2jCCAsKgAwIBAgIBBTANBgkqhkiG9w0BAQUFADCBlTETMBEGCgmSJomT8ixk ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w bGUgQ29tIEluYy4xJDAiBgNVBAsMG0V4YW1wbGUgQ29tIEluYy4gU2lnbmluZyBD... pemkey_password: private_key_password pemcert_content: |- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCHRZwzwGlP2FvL oEzNeDu2XnOF+ram7rWPT6fxI+JJr3SDz1mSzixTeHq82P5A7RLdMULfQFMfQPfr WXgB4qfisuDSt+CPocZRfUqqhGlMG2l8LgJMr58tn0AHvauvNTeiGlyXy0ShxHbD... Name Description enable_ssl_client_auth Whether to send the client certificate to the IdP server. Default is false. pemcert_filepath Absolute path to the client certificate. pemcert_content The content of the client certificate. Cannot be used when pemcert_filepath is set. pemkey_filepath Absolute path to the file containing the private key of the client certificate. pemkey_content The content of the private key of your client certificate. Cannot be used when pemkey_filepath is set. pemkey_password The password of your private key, if any. Enabled ciphers and protocols\nYou can limit the allowed ciphers and TLS protocols by using the following keys. Name Description enabled_ssl_ciphers Array. Enabled TLS cipher suites. Only Java format is supported. enabled_ssl_protocols Array. Enabled TLS protocols. Only Java format is supported. (Advanced) DoS protection\nTo help protect against denial-of-service (DoS) attacks, the Security plugin only allows a maximum number of new key IDs in a certain span of time. If the number of new key IDs exceeds this threshold, the Security plugin returns HTTP status code 503 (Service Unavailable) and refuses to query the IdP. By default, the Security plugin does not allow for more than 10 unknown key IDs within 10 seconds. The following table shows how to modify these settings. Name Description refresh_rate_limit_count The maximum number of unknown key IDs in the time frame. Default is 10. refresh_rate_limit_time_window_ms The time frame to use when checking the maximum number of unknown key IDs, in milliseconds. Default is 10000 (10 seconds). OpenSearch Dashboards single sign-on\nActivate OpenID Connect by adding the following to opensearch_dashboards.yml: opensearch_security.auth.type: \"openid\" Configuration\nOpenID Connect providers usually publish their configuration in JSON format under the metadata url. Therefore, most settings can be pulled in automatically, so the OpenSearch Dashboards configuration becomes minimal. The most important settings are the following: Connect URL Client ID\nEvery IdP can host multiple clients (sometimes called applications) with different settings and authentication protocols. When enabling OpenID Connect, you should create a new client for OpenSearch Dashboards in your IdP. The client ID uniquely identifies OpenSearch Dashboards.\nClient secret\nBeyond the ID, each client also has a client secret assigned. The client secret is usually generated when the client is created. Applications can obtain an identity token only when they provide a client secret. You can find this secret in the settings of the client on your IdP.\nConfiguration settings Name Description opensearch_security.openid.connect_url The URL where the IdP publishes the OpenID metadata. Required. opensearch_security.openid.client_id The ID of the OpenID Connect client configured in your IdP. Required. opensearch_security.openid.client_secret The client secret of the OpenID Connect client configured in your IdP. Required. opensearch_security.openid.scope The scope of the identity token issued by the IdP. Optional. Default is openid profile email address phone. opensearch_security.openid.header HTTP header name of the JWT token. Optional. Default is Authorization. opensearch_security.openid.logout_url The logout URL of your IdP. Optional. Only necessary if your IdP does not publish the logout URL in its metadata. opensearch_security.openid.base_redirect_url The base of the redirect URL that will be sent to your IdP. Optional. Only necessary when OpenSearch Dashboards is behind a reverse proxy, in which case it should be different than server.host and server.port in opensearch_dashboards.yml. opensearch_security.openid.trust_dynamic_headers Compute base_redirect_url from the reverse proxy HTTP headers ( X-Forwarded-Host / X-Forwarded-Proto). Optional. Default is false. Configuration example # Enable OpenID authentication opensearch_security.auth.type: \" openid\" # The IdP metadata endpoint opensearch_security.openid.connect_url: \" http://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration\" # The ID of the OpenID Connect client in your IdP opensearch_security.openid.client_id: \" opensearch-dashboards-sso\" # The client secret of the OpenID Connect client opensearch_security.openid.client_secret: \" a59c51f5-f052-4740-a3b0-e14ba355b520\" # Use HTTPS instead of HTTP opensearch.url: \" https://&lt;hostname&gt;.com:&lt;http port&gt;\" # Configure the OpenSearch Dashboards internal server user opensearch.username: \" kibanaserver\" opensearch.password: \" kibanaserver\" # Disable SSL verification when using self-signed demo certificates opensearch.ssl.verificationMode: none # allowlist basic headers and multi-tenancy header opensearch.requestHeadersAllowlist: [ \" Authorization\", \" security_tenant\"] To include OpenID Connect with other authentication types in the Dashboards sign-in window, see Configuring sign-in options.\nSession management with additional cookies\nTo improve session management—especially for users who have multiple roles assigned to them—Dashboards provides an option to split cookie payloads into multiple cookies and then recombine the payloads when receiving them. This can help prevent larger OpenID Connect assertions from exceeding size limits for each cookie. The two settings in the following example allow you to set a prefix name for additional cookies and specify the number of them. They are added to the opensearch_dashboards.yml file. The default number of additional cookies is three: opensearch_security.openid.extra_storage.cookie_prefix: security_authentication_oidc opensearch_security.openid.extra_storage.additional_cookies: 3 Note that reducing the number of additional cookies can cause some of the cookies that were in use before the change to stop working. We recommend establishing a fixed number of additional cookies and not changing the configuration after that.\nIf the ID token from the IdP is especially large, OpenSearch may throw a server log authentication error indicating that the HTTP header is too large. In this case, you can increase the value for the http.max_header_size setting in the opensearch.yml file.\nOpenSearch security configuration\nBecause OpenSearch Dashboards requires that the internal OpenSearch Dashboards server user can authenticate through HTTP basic authentication, you must configure two authentication domains. For OpenID Connect, the HTTP basic domain has to be placed first in the chain. Make sure you set the challenge flag to false.\nModify and apply the following example settings in config.yml: basic_internal_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: basic challenge: false authentication_backend: type: internal openid_auth_domain: http_enabled: true transport_enabled: true order: 1 http_authenticator: type: openid challenge: false config: subject_key: preferred_username roles_key: roles openid_connect_url: https://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration authentication_backend: type: noop",
    "ancestors": [
      "Security in OpenSearch",
      "Authentication backends"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/authentication-backends/proxy/",
    "title": "Proxy-based authentication",
    "content": "If you already have a single sign-on (SSO) solution in place, you might want to use it as an authentication backend.\nMost solutions work as a proxy in front of OpenSearch and the Security plugin. If proxy authentication succeeds, the proxy adds the (verified) username and its (verified) roles in HTTP header fields. The names of these fields depend on the SSO solution you have in place.\nThe Security plugin then extracts these HTTP header fields from the request and uses the values to determine the user’s permissions.\nEnable proxy detection\nTo enable proxy detection for OpenSearch, configure it in the xff section of config.yml: --- _meta: type: \" config\" config_version: 2 config: dynamic: http: anonymous_auth_enabled: false xff: enabled: true internalProxies: ' 192\\.168\\.0\\.10|192\\.168\\.0\\.11' remoteIpHeader: ' x-forwarded-for' You can configure the following settings: Name Description enabled Enables or disables proxy support. Default is false. internalProxies A regular expression containing the IP addresses of all trusted proxies. The pattern.* trusts all internal proxies. remoteIpHeader Name of the HTTP header field that has the hostname chain. Default is x-forwarded-for. To determine whether a request comes from a trusted internal proxy, the Security plugin compares the remote address of the HTTP request with the list of configured internal proxies. If the remote address is not in the list, the plugin treats the request like a client request.\nEnable proxy authentication\nConfigure the names of the HTTP header fields that carry the authenticated username and role(s) in in the proxy HTTP authenticator section: proxy_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: proxy challenge: false config: user_header: \" x-proxy-user\" roles_header: \" x-proxy-roles\" authentication_backend: type: noop Name Description user_header The HTTP header field containing the authenticated username. Default is x-proxy-user. roles_header The HTTP header field containing the comma-separated list of authenticated role names. The Security plugin uses the roles found in this header field as backend roles. Default is x-proxy-roles. roles_separator The separator for roles. Default is,. Enable extended proxy authentication\nThe Security plugin has an extended version of the proxy type that lets you pass additional user attributes for use with document-level security. Aside from type: extended-proxy and attr_header_prefix, configuration is identical: proxy_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: extended-proxy challenge: false config: user_header: \" x-proxy-user\" roles_header: \" x-proxy-roles\" attr_header_prefix: \" x-proxy-ext-\" authentication_backend: type: noop Name Description attr_header_prefix The header prefix that the proxy uses to provide user attributes. For example, if the proxy provides x-proxy-ext-namespace: my-namespace, use ${attr.proxy.namespace} in document-level security queries. Example\nThe following example uses an nginx proxy in front of a three-node OpenSearch cluster. For simplicity, we use hardcoded values for x-proxy-user and x-proxy-roles. In a real world example you would set these headers dynamically. The example also includes a commented header for use with the extended proxy. events {\nworker_connections 1024;\n}\nhttp {\nupstream opensearch {\nserver node1.example.com:9200;\nserver node2.example.com:9200;\nserver node3.example.com:9200;\nkeepalive 15;\n}\nserver {\nlisten 8090;\nserver_name nginx.example.com;\nlocation / {\nproxy_pass https://opensearch;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header x-proxy-user test;\nproxy_set_header x-proxy-roles test;\n#proxy_set_header x-proxy-ext-namespace my-namespace;\n}\n}\n} The corresponding minimal config.yml looks like: --- _meta: type: \" config\" config_version: 2 config: dynamic: http: xff: enabled: true internalProxies: ' 172.16.0.203' # the nginx proxy authc: proxy_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: proxy #type: extended-proxy challenge: false config: user_header: \" x-proxy-user\" roles_header: \" x-proxy-roles\" #attr_header_prefix: \"x-proxy-ext-\" authentication_backend: type: noop The important part is to enable the X-Forwarded-For (XFF) resolution and set the IP(s) of the internal proxies correctly: enabled: true internalProxies: ' 172.16.0.203' # nginx proxy In this case, nginx.example.com runs on 172.16.0.203, so add this IP to the list of internal proxies. Be sure to set internalProxies to the minimum number of IP addresses so that the Security plugin only accepts requests from trusted IPs.\nOpenSearch Dashboards proxy authentication\nTo use proxy authentication with OpenSearch Dashboards, the most common configuration is to place the proxy in front of OpenSearch Dashboards and let OpenSearch Dashboards pass the user and role headers to the Security plugin.\nIn this case, the remote address of the HTTP call is the IP of OpenSearch Dashboards, because it sits directly in front of OpenSearch. Add the IP of OpenSearch Dashboards to the list of internal proxies: --- _meta: type: \" config\" config_version: 2 config: dynamic: http: xff: enabled: true remoteIpHeader: \" x-forwarded-for\" internalProxies: ' &lt;opensearch-dashboards-ip-address&gt;' To pass the user and role headers that the authenticating proxy adds from OpenSearch Dashboards to the Security plugin, add them to the HTTP header allow list in opensearch_dashboards.yml: opensearch.requestHeadersAllowlist: [ \" securitytenant\", \" Authorization\", \" x-forwarded-for\", \" x-proxy-user\", \" x-proxy-roles\"] You must also enable the authentication type in opensearch_dashboards.yml: opensearch_security.auth.type: \" proxy\" opensearch_security.proxycache.user_header: \" x-proxy-user\" opensearch_security.proxycache.roles_header: \" x-proxy-roles\"",
    "ancestors": [
      "Security in OpenSearch",
      "Authentication backends"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/authentication-backends/saml/",
    "title": "SAML",
    "content": "The Security plugin supports user authentication through SAML single sign-on. The Security plugin implements the web browser SSO profile of the SAML 2.0 protocol.\nThis profile is meant for use with web browsers. It is not a general-purpose way of authenticating users against the Security plugin, so its primary use case is to support OpenSearch Dashboards single sign-on.\nDocker example\nWe provide a fully functional example that can help you understand how to use SAML with OpenSearch Dashboards.\nDownload and unzip the example zip file.\nAt the command line, run docker-compose up.\nReview the files: docker-compose.yml defines two OpenSearch nodes, an OpenSearch Dashboards server, and a SAML server. custom-opensearch_dashboards.yml add a few SAML settings to the default opensearch_dashboards.yml file. config.yml configures SAML for authentication.\nAccess OpenSearch Dashboards at http://localhost:5601. Note that OpenSearch Dashboards immediately redirects you to the SAML login page.\nLog in as admin with a password of admin.\nAfter logging in, note that your user in the upper-right is SAMLAdmin, as defined in /var/www/simplesamlphp/config/authsources.php of the SAML server.\nIf you want to examine the SAML server, run docker ps to find its container ID and then docker exec -it &lt;container-id&gt; /bin/bash.\nIn particular, you might find it helpful to review the contents of the /var/www/simplesamlphp/config/ and /var/www/simplesamlphp/metadata/ directories.\nActivating SAML\nTo use SAML for authentication, you need to configure a respective authentication domain in the authc section of config/opensearch-security/config.yml. Because SAML works solely on the HTTP layer, you do not need any authentication_backend and can set it to noop. Place all SAML-specific configuration options in this chapter in the config section of the SAML HTTP authenticator: authc: saml_auth_domain: http_enabled: true transport_enabled: false order: 1 http_authenticator: type: saml challenge: true config: idp: metadata_file: okta.xml... authentication_backend: type: noop After you have configured SAML in config.yml, you must also activate it in OpenSearch Dashboards.\nRunning multiple authentication domains\nWe recommend adding at least one other authentication domain, such as LDAP or the internal user database, to support API access to OpenSearch without SAML. For OpenSearch Dashboards and the internal OpenSearch Dashboards server user, you also must add another authentication domain that supports basic authentication. This authentication domain should be placed first in the chain, and the challenge flag must be set to false: authc: basic_internal_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: basic challenge: false authentication_backend: type: internal saml_auth_domain: http_enabled: true transport_enabled: false order: 1 http_authenticator: type: saml challenge: true config:... authentication_backend: type: noop Identity provider metadata\nA SAML identity provider (IdP) provides a SAML 2.0 metadata file describing the IdP’s capabilities and configuration. The Security plugin can read IdP metadata either from a URL or a file. The choice that you make depends on your IdP and your preferences. The SAML 2.0 metadata file is required. Name Description idp.metadata_file The path to the SAML 2.0 metadata file of your IdP. Place the metadata file in the config directory of OpenSearch. The path has to be specified relative to the config directory. Required if idp.metadata_url is not set. idp.metadata_url The SAML 2.0 metadata URL of your IdP. Required if idp.metadata_file is not set. IdP and service provider entity ID\nAn entity ID is a globally unique name for a SAML entity, either an IdP or a service provider (SP). The IdP entity ID is usually provided by your IdP. The SP entity ID is the name of the configured application or client in your IdP. We recommend adding a new application for OpenSearch Dashboards and using the URL of your OpenSearch Dashboards installation as the SP entity ID. Name Description idp.entity_id The entity ID of your IdP. Required. sp.entity_id The entity ID of the service provider. Required. Time disparity compensation for JWT validation\nOccasionally you may find that the clock times between the authentication server and the OpenSearch node are not perfectly synchronized. When this is the case, even by a few seconds, the system that either issues or receives a JSON Web Token (JWT) may try to validate nbf (not before) and exp (expiration) claims and fail to authenticate the user due to the time disparity.\nBy default, OpenSearch Security allows for a window of 30 seconds to compensate for possible misalignment between server clock times. To set a custom value for this feature and override the default, you can add the jwt_clock_skew_tolerance_seconds setting to the config.yml. http_authenticator: type: saml challenge: true config: idp: metadata_file: okta.xml jwt_clock_skew_tolerance_seconds: 20 OpenSearch Dashboards settings\nThe web browser SSO profile exchanges information through HTTP GET or POST. For example, after you log in to your IdP, it sends an HTTP POST back to OpenSearch Dashboards containing the SAML response. You must configure the base URL of your OpenSearch Dashboards installation where the HTTP requests are being sent to. Name Description kibana_url The OpenSearch Dashboards base URL. Required. Username and Role attributes\nSubjects (for example, user names) are usually stored in the NameID element of a SAML response: &lt;saml2:Subject&gt;\n&lt;saml2:NameID&gt;admin&lt;/saml2:NameID&gt;...\n&lt;/saml2:Subject&gt; If your IdP is compliant with the SAML 2.0 specification, you do not need to set anything special. If your IdP uses a different element name, you can also specify its name explicitly.\nRole attributes are optional. However, most IdPs can be configured to add roles in the SAML assertions as well. If present, you can use these roles in your role mappings: &lt;saml2:Attribute Name='Role'&gt;\n&lt;saml2:AttributeValue &gt;Everyone&lt;/saml2:AttributeValue&gt;\n&lt;saml2:AttributeValue &gt;Admins&lt;/saml2:AttributeValue&gt;\n&lt;/saml2:Attribute&gt; If you want to extract roles from the SAML response, you need to specify the element name that contains the roles. Name Description subject_key The attribute in the SAML response where the subject is stored. Optional. If not configured, the NameID attribute is used. roles_key The attribute in the SAML response where the roles are stored. Optional. If not configured, no roles are used. Request signing\nRequests from the Security plugin to the IdP can optionally be signed. Use the following settings to configure request signing. Name Description sp.signature_private_key The private key used to sign the requests or to decode encrypted assertions. Optional. Cannot be used when private_key_filepath is set. sp.signature_private_key_password The password of the private key, if any. sp.signature_private_key_filepath Path to the private key. The file must be placed under the OpenSearch config directory, and the path must be specified relative to that same directory. sp.signature_algorithm The algorithm used to sign the requests. See the next table for possible values. The Security plugin supports the following signature algorithms. Algorithm Value DSA_SHA1\nhttp://www.w3.org/2000/09/xmldsig#dsa-sha1;\nRSA_SHA1\nhttp://www.w3.org/2000/09/xmldsig#rsa-sha1;\nRSA_SHA256\nhttp://www.w3.org/2001/04/xmldsig-more#rsa-sha256;\nRSA_SHA384\nhttp://www.w3.org/2001/04/xmldsig-more#rsa-sha384;\nRSA_SHA512\nhttp://www.w3.org/2001/04/xmldsig-more#rsa-sha512; Logout\nUsually, IdPs provide information about their individual logout URL in their SAML 2.0 metadata. If this is the case, the Security plugin uses them to render the correct logout link in OpenSearch Dashboards. If your IdP does not support an explicit logout, you can force a re-login when the user visits OpenSearch Dashboards again. Name Description sp.forceAuthn Force a re-login even if the user has an active session with the IdP. Currently, the Security plugin supports only the HTTP-Redirect logout binding. Make sure this is configured correctly in your IdP.\nExchange key settings\nSAML, unlike other protocols, is not meant to be used for exchanging user credentials with each request. The Security plugin trades the SAML response for a lightweight JWT that stores the validated user attributes. This token is signed by an exchange key of your choice. Note that when you change this key, all tokens signed with it become invalid immediately. Name Description exchange_key The key to sign the token. The algorithm is HMAC256, so it should have at least 32 characters. TLS settings\nIf you are loading the IdP metadata from a URL, we recommend that you use SSL/TLS. If you use an external IdP like Okta or Auth0 that uses a trusted certificate, you usually do not need to configure anything. If you host the IdP yourself and use your own root CA, you can customize the TLS settings as follows. These settings are used only for loading SAML metadata over HTTPS. Name Description idp.enable_ssl Whether to enable the custom TLS configuration. Default is false (JDK settings are used). idp.verify_hostnames Whether to verify the hostnames of the server’s TLS certificate. Example: authc: saml_auth_domain: http_enabled: true transport_enabled: false order: 1 http_authenticator: type: saml challenge: true config: idp: enable_ssl: true verify_hostnames: true... authentication_backend: type: noop Certificate validation\nConfigure the root CA used for validating the IdP TLS certificate by setting one of the following configuration options: config: idp: pemtrustedcas_filepath: path/to/trusted_cas.pem config: idp: pemtrustedcas_content: |- MIID/jCCAuagAwIBAgIBATANBgkqhkiG9w0BAQUFADCBjzETMBEGCgmSJomT8ixk ARkWA2NvbTEXMBUGCgmSJomT8ixkARkWB2V4YW1wbGUxGTAXBgNVBAoMEEV4YW1w bGUgQ29tIEluYy4xITAfBgNVBAsMGEV4YW1wbGUgQ29tIEluYy4gUm9vdCBDQTEh... Name Description idp.pemtrustedcas_filepath Path to the PEM file containing the root CAs of your IdP. The files must be placed under the OpenSearch config directory, and you must specify the path relative to that same directory. idp.pemtrustedcas_content The root CA content of your IdP server. Cannot be used when pemtrustedcas_filepath is set. Client authentication\nThe Security plugin can use TLS client authentication when fetching the IdP metadata. If enabled, the Security plugin sends a TLS client certificate to the IdP for each metadata request. Use the following keys to configure client authentication. Name Description idp.enable_ssl_client_auth Whether to send a client certificate to the IdP server. Default is false. idp.pemcert_filepath Path to the PEM file containing the client certificate. The file must be placed under the OpenSearch config directory, and the path must be specified relative to the config directory. idp.pemcert_content The content of the client certificate. Cannot be used when pemcert_filepath is set. idp.pemkey_filepath Path to the private key of the client certificate. The file must be placed under the OpenSearch config directory, and the path must be specified relative to the config directory. idp.pemkey_content The content of the private key of your certificate. Cannot be used when pemkey_filepath is set. idp.pemkey_password The password of your private key, if any. Enabled ciphers and protocols\nYou can limit the allowed ciphers and TLS protocols for the IdP connection. For example, you can only enable strong ciphers and limit the TLS versions to the most recent ones. Name Description idp.enabled_ssl_ciphers Array of enabled TLS ciphers. Only the Java format is supported. idp.enabled_ssl_protocols Array of enabled TLS protocols. Only the Java format is supported. Minimal configuration example\nThe following example shows the minimal configuration: authc: saml_auth_domain: http_enabled: true transport_enabled: false order: 1 http_authenticator: type: saml challenge: true config: idp: metadata_file: metadata.xml entity_id: http://idp.example.com/ sp: entity_id: https://opensearch-dashboards.example.com kibana_url: https://opensearch-dashboards.example.com:5601/ roles_key: Role exchange_key: ' peuvgOLrjzuhXf...' authentication_backend: type: noop OpenSearch Dashboards configuration\nBecause most of the SAML-specific configuration is done in the Security plugin, just activate SAML in your opensearch_dashboards.yml by adding the following: opensearch_security.auth.type: \" saml\" In addition, you must add the OpenSearch Dashboards endpoint for validating the SAML assertions to your allow list: server.xsrf.allowlist: [ \" /_opendistro/_security/saml/acs\"] If you use the logout POST binding, you also need to ad the logout endpoint to your allow list: server.xsrf.allowlist: [ \" /_opendistro/_security/saml/acs\", \" /_opendistro/_security/saml/logout\"] To include SAML with other authentication types in the Dashboards sign-in window, see Configuring sign-in options.\nSession management with additional cookies\nTo improve session management—especially for users who have multiple roles assigned to them—Dashboards provides an option to split cookie payloads into multiple cookies and then recombine the payloads when receiving them. This can help prevent larger SAML assertions from exceeding size limits for each cookie. The two settings in the following example allow you to set a prefix name for additional cookies and specify the number of them. They are added to the opensearch_dashboards.yml file. The default number of additional cookies is three: opensearch_security.saml.extra_storage.cookie_prefix: security_authentication_saml opensearch_security.saml.extra_storage.additional_cookies: 3 Note that reducing the number of additional cookies can cause some of the cookies that were in use before the change to stop working. We recommend establishing a fixed number of additional cookies and not changing the configuration after that.\nIf the ID token from the IdP is especially large, OpenSearch may throw a server log authentication error indicating that the HTTP header is too large. In this case, you can increase the value for the http.max_header_size setting in the opensearch.yml file.\nIdP-initiated SSO\nTo use IdP-initiated SSO, set the Assertion Consumer Service endpoint of your IdP to this: /_opendistro/_security/saml/acs/idpinitiated Then add this endpoint to server.xsrf.allowlist in opensearch_dashboards.yml: server.xsrf.allowlist: [ \" /_opendistro/_security/saml/acs/idpinitiated\", \" /_opendistro/_security/saml/acs\", \" /_opendistro/_security/saml/logout\"]",
    "ancestors": [
      "Security in OpenSearch",
      "Authentication backends"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/configuration/configuration/",
    "title": "Configuring the Security backend",
    "content": "One of the first steps to using the Security plugin is to decide on an authentication backend, which handles steps 2-3 of the authentication flow. The plugin has an internal user database, but many people prefer to use an existing authentication backend, such as an LDAP server, or some combination of the two.\nThe main configuration file for authentication and authorization backends is config/opensearch-security/config.yml. It defines how the Security plugin retrieves the user credentials, how it verifies these credentials, and how to fetch additional roles from backend systems (optional). config.yml has three main parts: opensearch_security: dynamic: http:... authc:... authz:... For a more complete example, see the sample file on GitHub.\nHTTP\nThe http section has the following format: anonymous_auth_enabled: &lt;true|false&gt; xff: # optional section enabled: &lt;true|false&gt; internalProxies: &lt;string&gt; # Regex pattern remoteIpHeader: &lt;string&gt; # Name of the header in which to look. Typically: x-forwarded-for proxiesHeader: &lt;string&gt; trustedProxies: &lt;string&gt; # Regex pattern If you disable anonymous authentication, the Security plugin won’t initialize if you have not provided at least one authc.\nAuthentication\nThe authc section has the following format: &lt;name&gt;: http_enabled: &lt;true|false&gt; transport_enabled: &lt;true|false&gt; order: &lt;integer&gt; http_authenticator:... authentication_backend:... An entry in the authc section is called an authentication domain. It specifies where to get the user credentials and against which backend they should be authenticated.\nYou can use more than one authentication domain. Each authentication domain has a name (for example, basic_auth_internal), enabled flags, and an order. The order makes it possible to chain authentication domains together. The Security plugin uses them in the order that you provide. If the user successfully authenticates with one domain, the Security plugin skips the remaining domains. http_authenticator specifies which authentication method that you want to use on the HTTP layer.\nThis is the syntax for defining an authenticator on the HTTP layer: http_authenticator: type: &lt;type&gt; challenge: &lt;true|false&gt; config:... These are the allowed values for type: basic: HTTP basic authentication. No additional configuration is needed. kerberos: Kerberos authentication. Additional Kerberos-specific configuration is needed. jwt: JSON Web Token (JWT) authentication. Additional JWT-specific configuration is needed. clientcert: Authentication through a client TLS certificate. This certificate must be trusted by one of the root CAs in the truststore of your nodes.\nAfter setting an HTTP authenticator, you must specify against which backend system you want to authenticate the user: authentication_backend: type: &lt;type&gt; config:... These are the possible values for type: noop: No further authentication against any backend system is performed. Use noop if the HTTP authenticator has already authenticated the user completely, as in the case of JWT, Kerberos, or client certificate authentication. internal: Use the users and roles defined in internal_users.yml for authentication. ldap: Authenticate users against an LDAP server. This setting requires additional, LDAP-specific configuration settings.\nAuthorization\nAfter the user has been authenticated, the Security plugin can optionally collect additional roles from backend systems. The authorization configuration has the following format: authz: &lt;name&gt;: http_enabled: &lt;true|false&gt; transport_enabled: &lt;true|false&gt; authorization_backend: type: &lt;type&gt; config:... You can define multiple entries in this section the same way as you can for authentication entries. In this case, execution order is not relevant, so there is no order field.\nThese are the possible values for type: noop: Skip this step altogether. ldap: Fetch additional roles from an LDAP server. This setting requires additional, LDAP-specific configuration settings.\nConfiguration examples\nThe default config/opensearch-security/config.yml that ships with OpenSearch contains many configuration examples. Use these examples as a starting point, and customize them to your needs.\nHTTP basic\nTo set up HTTP basic authentication, you must enable it in the http_authenticator section of the configuration: http_authenticator: type: basic challenge: true In most cases, you set the challenge flag to true. The flag defines the behavior of the Security plugin if the Authorization field in the HTTP header is not set.\nIf challenge is set to true, the Security plugin sends a response with status UNAUTHORIZED (401) back to the client. If the client is accessing the cluster with a browser, this triggers the authentication dialog box, and the user is prompted to enter a user name and password.\nIf challenge is set to false and no Authorization header field is set, the Security plugin does not send a WWW-Authenticate response back to the client, and authentication fails. You might want to use this setting if you have another challenge http_authenticator in your configured authentication domains. One such scenario is when you plan to use basic authentication and Kerberos together.\nKerberos\nKerberos authentication does not work with OpenSearch Dashboards. To track OpenSearch’s progress in adding support for Kerberos in OpenSearch Dashboards, see issue #907 in the Dashboard’s Security plugin repository.\nDue to the nature of Kerberos, you must define some settings in opensearch.yml and some in config.yml.\nIn opensearch.yml, define the following: plugins.security.kerberos.krb5_filepath: ' /etc/krb5.conf' plugins.security.kerberos.acceptor_keytab_filepath: ' eskeytab.tab' plugins.security.kerberos.krb5_filepath defines the path to your Kerberos configuration file. This file contains various settings regarding your Kerberos installation, for example, the realm names, hostnames, and ports of the Kerberos key distribution center (KDC). plugins.security.kerberos.acceptor_keytab_filepath defines the path to the keytab file, which contains the principal that the Security plugin uses to issue requests against Kerberos. plugins.security.kerberos.acceptor_principal: 'HTTP/localhost' defines the principal that the Security plugin uses to issue requests against Kerberos. This value must be present in the keytab file.\nDue to security restrictions, the keytab file must be placed in config or a subdirectory, and the path in opensearch.yml must be relative, not absolute.\nDynamic configuration\nA typical Kerberos authentication domain in config.yml looks like this: authc: kerberos_auth_domain: enabled: true order: 1 http_authenticator: type: kerberos challenge: true config: krb_debug: false strip_realm_from_principal: true authentication_backend: type: noop Authentication against Kerberos through a browser on an HTTP level is achieved using SPNEGO. Kerberos/SPNEGO implementations vary, depending on your browser and operating system. This is important when deciding if you need to set the challenge flag to true or false.\nAs with HTTP Basic Authentication, this flag determines how the Security plugin should react when no Authorization header is found in the HTTP request or if this header does not equal negotiate.\nIf set to true, the Security plugin sends a response with status code 401 and a WWW-Authenticate header set to negotiate. This tells the client (browser) to resend the request with the Authorization header set. If set to false, the Security plugin cannot extract the credentials from the request, and authentication fails. Setting challenge to false thus makes sense only if the Kerberos credentials are sent in the initial request.\nAs the name implies, setting krb_debug to true will output Kerberos-specific debugging messages to stdout. Use this setting if you encounter problems with your Kerberos integration.\nIf you set strip_realm_from_principal to true, the Security plugin strips the realm from the user name.\nAuthentication backend\nBecause Kerberos/SPNEGO authenticates users on an HTTP level, no additional authentication_backend is needed. Set this value to noop.\nJSON Web Token\nJWTs are JSON-based access tokens that assert one or more claims. They are commonly used to implement single sign-on (SSO) solutions and fall in the category of token-based authentication systems:\nA user logs in to an authentication server by providing credentials (for example, a user name and password).\nThe authentication server validates the credentials.\nThe authentication server creates an access token and signs it.\nThe authentication server returns the token to the user.\nThe user stores the access token.\nThe user sends the access token alongside every request to the service that it wants to use.\nThe service verifies the token and grants or denies access.\nA JWT is self-contained in the sense that it carries within itself all of the information necessary to verify a user. The tokens are base64-encoded, signed JSON objects.\nJWTs consist of three parts:\nHeader\nPayload\nSignature\nHeader\nThe header contains information about the used signing mechanism, as shown in the following example: { \"alg\": \"HS256\", \"typ\": \"JWT\" } In this case, the header states that the message was signed using HMAC-SHA256.\nPayload\nThe payload of a JWT contains the JWT claims. A claim can be any piece of information about the user that the application that created the token has verified.\nThe specification defines a set of standard claims with reserved names, referred to as registered claims. Some examples of these claims include token issuer (iss), expiration time (exp), and subject (sub).\nPublic claims, on the other hand, can be created freely by the token issuer. They can contain arbitrary information, such as the user name and the roles of the user. { \"iss\": \"example.com\", \"exp\": 1300819380, \"name\": \"John Doe\", \"roles\": \"admin, devops\" } Signature\nThe issuer of the token calculates the signature of the token by applying a cryptographic hash function on the base64-encoded header and payload. These three parts are then concatenated using periods to form a complete JWT: encoded = base64UrlEncode(header) + \".\" + base64UrlEncode(payload)\nsignature = HMACSHA256(encoded, 'secretkey');\njwt = encoded + \".\" + base64UrlEncode(signature) Example: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJsb2dnZWRJbkFzIjoiYWRtaW4iLCJpYXQiOjE0MjI3Nzk2Mzh9.gzSraSYS8EXBxLN_oWnFSRgCzcmJmMjLiuyu5CSpyHI Configure JWTs\nIf you use a JWT as your only authentication method, disable the user cache by setting plugins.security.cache.ttl_minutes: 0.\nSet up an authentication domain and choose jwt as the HTTP authentication type. Because the tokens already contain all required information to verify the request, challenge must be set to false and authentication_backend to noop. jwt_auth_domain: http_enabled: true transport_enabled: true order: 0 http_authenticator: type: jwt challenge: false config: signing_key: \" base64 encoded key\" jwt_header: \" Authorization\" jwt_url_parameter: null subject_key: null roles_key: null jwt_clock_skew_tolerance_seconds: 20 authentication_backend: I type: noop The following table shows the configuration parameters. Name Description signing_key The signing key to use when verifying the token. If you use a symmetric key algorithm, it is the base64-encoded shared secret. If you use an asymmetric algorithm, it contains the public key. jwt_header The HTTP header in which the token is transmitted. This typically is the Authorization header with the Bearer schema: Authorization: Bearer &lt;token&gt;. Default is Authorization. jwt_url_parameter If the token is not transmitted in the HTTP header, but as an URL parameter, define the name of this parameter here. subject_key The key in the JSON payload that stores the user name. If not set, the subject registered claim is used. roles_key The key in the JSON payload that stores the user’s roles. The value of this key must be a comma-separated list of roles. jwt_clock_skew_tolerance_seconds Sets a window of time, in seconds, to prevent authentication failures due to a misalignment between the JWT authentication server and OpenSearch node clock times. Security sets 30 seconds as the default. Use this setting to apply a custom value. Because JWTs are self-contained and the user is authenticated at the HTTP level, no additional authentication_backend is needed. Set this value to noop.\nSymmetric key algorithms: HMAC\nHash-based message authentication codes (HMACs) are a group of algorithms that provide a way of signing messages by means of a shared key. The key is shared between the authentication server and the Security plugin. It must be configured as a base64-encoded value in the signing_key setting: jwt_auth_domain:... config: signing_key: \" a3M5MjEwamRqOTAxOTJqZDE=\"... Asymmetric key algorithms: RSA and ECDSA\nRSA and ECDSA are asymmetric encryption and digital signature algorithms and use a public/private key pair to sign and verify tokens. This means that they use a private key for signing the token, while the Security plugin needs to know only the public key to verify it.\nBecause you cannot issue new tokens with the public key—and because you can make valid assumptions about the creator of the token—RSA and ECDSA are considered more secure than using HMAC.\nTo use RS256, you need to configure only the (non-base64-encoded) public RSA key as signing_key in the JWT configuration: jwt_auth_domain:... config: signing_key: |- -----BEGIN PUBLIC KEY----- MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQK... -----END PUBLIC KEY-----... The Security plugin automatically detects the algorithm (RSA/ECDSA), and if necessary you can break the key into multiple lines.\nBearer authentication for HTTP requests\nThe most common way of transmitting a JWT in an HTTP request is to add it as an HTTP header with the bearer authentication schema: Authorization: Bearer &lt;JWT&gt; The default name of the header is Authorization. If required by your authentication server or proxy, you can also use a different HTTP header name using the jwt_header configuration key.\nAs with HTTP basic authentication, you should use HTTPS instead of HTTP when transmitting JWTs in HTTP requests.\nURL parameters for HTTP requests\nAlthough the most common way to transmit JWTs in HTTP requests is to use a header field, the Security plugin also supports parameters. Configure the name of the GET parameter using the following key: config: signing_key:... jwt_url_parameter: \" parameter_name\" subject_key:... roles_key:... As with HTTP basic authentication, you should use HTTPS instead of HTTP.\nValidated registered claims\nThe following registered claims are validated automatically:\n“iat” (Issued At) Claim\n“nbf” (Not Before) Claim\n“exp” (Expiration Time) Claim\nSupported formats and algorithms\nThe Security plugin supports digitally signed, compact JWTs with all standard algorithms: HS256: HMAC using SHA-256\nHS384: HMAC using SHA-384\nHS512: HMAC using SHA-512\nRS256: RSASSA-PKCS-v1_5 using SHA-256\nRS384: RSASSA-PKCS-v1_5 using SHA-384\nRS512: RSASSA-PKCS-v1_5 using SHA-512\nPS256: RSASSA-PSS using SHA-256 and MGF1 with SHA-256\nPS384: RSASSA-PSS using SHA-384 and MGF1 with SHA-384\nPS512: RSASSA-PSS using SHA-512 and MGF1 with SHA-512\nES256: ECDSA using P-256 and SHA-256\nES384: ECDSA using P-384 and SHA-384\nES512: ECDSA using P-521 and SHA-512 Troubleshooting common issues\nThis section details how to troubleshoot common issues with your Security plugin configuration.\nCorrect iat\nEnsure that the JWT token contains the correct iat (issued at), nbf (not before), and exp (expiry) claims, all of which are validated automatically by OpenSearch.\nJWT URL parameter\nWhen using the JWT URL parameter containing the default admin role all_access against OpenSearch (for example, curl http://localhost:9200?jwtToken=&lt;jwt-token&gt;) the request fails with: { \"error\":{ \"root_cause\":[ { \"type\": \"security_exception\", \"reason\": \"no permissions for [cluster:monitor/main] and User [name=admin, backend_roles=[all_access], requestedTenant=null]\" }], \"type\": \"security_exception\", \"reason\": \"no permissions for [cluster:monitor/main] and User [name=admin, backend_roles=[all_access], requestedTenant=null]\" }, \"status\": 403 } To solve this, ensure that the role all_access is mapped directly to the internal user and not a backend role. To do this, navigate to Security &gt; Roles &gt; all_access and switch to the tab to Mapped Users. Select Manage mapping and add “admin” to the Users section. The user should appear in the Mapped Users tab. OpenSearch Dashboards configuration\nEven though JWT URL parameter authentication works when querying OpenSearch directly, it fails when used to access OpenSearch Dashboards. Solution: Ensure the following lines are present in the OpenSearch Dashboards config file opensearch_dashboards.yml opensearch_security.auth.type: \" jwt\" opensearch_security.jwt.url_param: &lt;your-param-name-here&gt;",
    "ancestors": [
      "Security in OpenSearch",
      "Configuration"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/configuration/disable/",
    "title": "Disabling security",
    "content": "You might want to temporarily disable the Security plugin to make testing or internal usage more straightforward. To disable the plugin, add the following line in opensearch.yml: plugins.security.disabled: true A more permanent option is to remove the Security plugin entirely:\nDelete the plugins/opensearch-security folder on all nodes.\nDelete all plugins.security.* configuration entries from opensearch.yml.\nTo perform these steps on the Docker image, see Working with plugins.\nDisabling or removing the plugin exposes the configuration index for the Security plugin. If the index contains sensitive information, be sure to protect it through some other means. If you no longer need the index, delete it.\nRemove OpenSearch Dashboards plugin\nThe Security plugin is actually two plugins: one for OpenSearch and one for OpenSearch Dashboards. You can use the OpenSearch plugin independently, but the OpenSearch Dashboards plugin depends on a secured OpenSearch cluster.\nIf you disable the Security plugin in opensearch.yml (or delete the plugin entirely) and still want to use OpenSearch Dashboards, you must remove the corresponding OpenSearch Dashboards plugin. For more information, see OpenSearch Dashboards remove plugins.\nDocker\nCreate a new Dockerfile: FROM opensearchproject/opensearch-dashboards:2.7.0\nRUN /usr/share/opensearch-dashboards/bin/opensearch-dashboards-plugin remove securityDashboards\nCOPY --chown=opensearch-dashboards:opensearch-dashboards opensearch_dashboards.yml /usr/share/opensearch-dashboards/config/ In this case, opensearch_dashboards.yml is a “vanilla” version of the file with no entries for the Security plugin. It might look like this: --- server.name: opensearch-dashboards server.host: \" 0.0.0.0\" opensearch.hosts: http://localhost:9200 To build the new Docker image, run the following command: docker build --tag = opensearch-dashboards-no-security. In docker-compose.yml, change opensearchproject/opensearch-dashboards:2.7.0 to opensearch-dashboards-no-security.\nChange OPENSEARCH_HOSTS or opensearch.hosts to http:// rather than https://.\nEnter docker-compose up.",
    "ancestors": [
      "Security in OpenSearch",
      "Configuration"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/configuration/generate-certificates/",
    "title": "Generating self-signed certificates",
    "content": "If you don’t have access to a certificate authority (CA) for your organization and want to use OpenSearch for non-demo purposes, you can generate your own self-signed certificates using OpenSSL.\nYou can probably find OpenSSL in the package manager for your operating system.\nOn CentOS, use Yum: sudo yum install openssl On macOS, use Homebrew: brew install openssl Generate a private key\nThe first step in this process is to generate a private key using the openssl genrsa command. As the name suggests, you should keep this file private.\nPrivate keys must be of sufficient length to be secure, so specify 2048: openssl genrsa -out root-ca-key.pem 2048 You can optionally add the -aes256 option to encrypt the key using the AES-256 standard. This option requires a password.\nGenerate a root certificate\nNext, use the private key to generate a self-signed certificate for the root CA: openssl req -new -x509 -sha256 -key root-ca-key.pem -out root-ca.pem -days 730 The default -days value of 30 is only useful for testing purposes. This sample command specifies 730 (two years) for the certificate expiration date, but use whatever value makes sense for your organization.\nThe -x509 option specifies that you want a self-signed certificate rather than a certificate request.\nThe -sha256 option sets the hash algorithm to SHA-256. SHA-256 is the default in later versions of OpenSSL, but earlier versions might use SHA-1.\nFollow the prompts to specify details for your organization. Together, these details form the distinguished name (DN) of your CA.\nGenerate an admin certificate\nTo generate an admin certificate, first create a new key: openssl genrsa -out admin-key-temp.pem 2048 Then convert that key to PKCS#8 format for use in Java using a PKCS#12-compatible algorithm (3DES): openssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem Next, create a certificate signing request (CSR). This file acts as an application to a CA for a signed certificate: openssl req -new -key admin-key.pem -out admin.csr Follow the prompts to fill in the details. You don’t need to specify a challenge password. As noted in the OpenSSL Cookbook, “Having a challenge password does not increase the security of the CSR in any way.”\nIf you generate TLS certificates and have enabled hostname verification by setting plugins.security.ssl.transport.enforce_hostname_verification to true (default), be sure to specify a common name (CN) for each certificate signing request (CSR) that matches the corresponding DNS A record of the intended node.\nIf you want to use the same node certificate on all nodes (not recommended), set hostname verification to false. For more information, see Configure TLS certificates.\nNow that the private key and signing request have been created, generate the certificate: openssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem -days 730 Just like the root certificate, use the -days option to specify an expiration date of longer than 30 days.\n(Optional) Generate node and client certificates\nSimilar to the steps in Generate an admin certificate, you will generate keys and CSRs with new file names for each node and as many client certificates as you need. For example, you might generate one client certificate for OpenSearch Dashboards and another for a Python client. Each certificate should use its own private key and should be generated from a unique CSR with matching SAN extension specific to the intended host. A SAN extension is not needed for the admin cert because that cert is not tied to a specific host.\nTo generate a node or client certificate, first create a new key: openssl genrsa -out node1-key-temp.pem 2048 Then convert that key to PKCS#8 format for use in Java using a PKCS#12-compatible algorithm (3DES): openssl pkcs8 -inform PEM -outform PEM -in node1-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node1-key.pem Next, create the CSR: openssl req -new -key node1-key.pem -out node1.csr For all host and client certificates, you should specify a subject alternative name (SAN) to ensure compliance with RFC 2818 (HTTP Over TLS). The SAN should match the corresponding CN so that both refer to the same DNS A record.\nBefore generating a signed certificate, create a SAN extension file which describes the DNS A record for the host: echo 'subjectAltName=DNS:node1.dns.a-record' &gt; node1.ext Generate the certificate: openssl x509 -req -in node1.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node1.pem -days 730 -extfile node1.ext Sample script\nIf you already know the certificate details and don’t want to specify them interactively, use the -subj option in your root-ca.pem and CSR commands. This script creates a root certificate, admin certificate, two node certificates, and a client certificate, all with an expiration dates of two years (730 days): #!/bin/sh # Root CA openssl genrsa -out root-ca-key.pem 2048\nopenssl req -new -x509 -sha256 -key root-ca-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=root.dns.a-record\" -out root-ca.pem -days 730 # Admin cert openssl genrsa -out admin-key-temp.pem 2048\nopenssl pkcs8 -inform PEM -outform PEM -in admin-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out admin-key.pem\nopenssl req -new -key admin-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=A\" -out admin.csr\nopenssl x509 -req -in admin.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out admin.pem -days 730 # Node cert 1 openssl genrsa -out node1-key-temp.pem 2048\nopenssl pkcs8 -inform PEM -outform PEM -in node1-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node1-key.pem\nopenssl req -new -key node1-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.dns.a-record\" -out node1.csr echo 'subjectAltName=DNS:node1.dns.a-record' &gt; node1.ext\nopenssl x509 -req -in node1.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node1.pem -days 730 -extfile node1.ext # Node cert 2 openssl genrsa -out node2-key-temp.pem 2048\nopenssl pkcs8 -inform PEM -outform PEM -in node2-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out node2-key.pem\nopenssl req -new -key node2-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node2.dns.a-record\" -out node2.csr echo 'subjectAltName=DNS:node2.dns.a-record' &gt; node2.ext\nopenssl x509 -req -in node2.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out node2.pem -days 730 -extfile node2.ext # Client cert openssl genrsa -out client-key-temp.pem 2048\nopenssl pkcs8 -inform PEM -outform PEM -in client-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out client-key.pem\nopenssl req -new -key client-key.pem -subj \"/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=client.dns.a-record\" -out client.csr echo 'subjectAltName=DNS:client.dns.a-record' &gt; client.ext\nopenssl x509 -req -in client.csr -CA root-ca.pem -CAkey root-ca-key.pem -CAcreateserial -sha256 -out client.pem -days 730 -extfile client.ext # Cleanup rm admin-key-temp.pem rm admin.csr rm node1-key-temp.pem rm node1.csr rm node1.ext rm node2-key-temp.pem rm node2.csr rm node2.ext rm client-key-temp.pem rm client.csr rm client.ext Add distinguished names to opensearch.yml\nYou must specify the distinguished names (DNs) for all admin and node certificates in opensearch.yml on all nodes. Using the certificates from the sample script above, part of opensearch.yml might look like this: plugins.security.authcz.admin_dn: - ' CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' plugins.security.nodes_dn: - ' CN=node1.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' - ' CN=node2.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' But if you look at the subject of the certificate after creating it, you might see different formatting: subject=/C=CA/ST=ONTARIO/L=TORONTO/O=ORG/OU=UNIT/CN=node1.dns.a-record If you compare this string to the ones above, you can see that you need to invert the order of elements and use commas rather than slashes. Enter this command to get the correct string: openssl x509 -subject -nameopt RFC2253 -noout -in node.pem Then copy and paste the output into opensearch.yml.\nAdd certificate files to opensearch.yml\nThis process generates many files, but these are the ones you need to add to each node: root-ca.pem admin.pem admin-key.pem (Optional) node1.pem (Optional) node1-key.pem On one node, the security configuration portion of opensearch.yml might look like this: plugins.security.ssl.transport.pemcert_filepath: node1.pem plugins.security.ssl.transport.pemkey_filepath: node1-key.pem plugins.security.ssl.transport.pemtrustedcas_filepath: root-ca.pem plugins.security.ssl.transport.enforce_hostname_verification: false plugins.security.ssl.http.enabled: true plugins.security.ssl.http.pemcert_filepath: node1.pem plugins.security.ssl.http.pemkey_filepath: node1-key.pem plugins.security.ssl.http.pemtrustedcas_filepath: root-ca.pem plugins.security.authcz.admin_dn: - ' CN=A,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' plugins.security.nodes_dn: - ' CN=node1.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' - ' CN=node2.dns.a-record,OU=UNIT,O=ORG,L=TORONTO,ST=ONTARIO,C=CA' For more information about adding and using these certificates in your own setup, see Configuring basic security settings for Docker, Configure TLS certificates, and Client certificate authentication.\nRun securityadmin.sh\nAfter configuring your certificates and starting OpenSearch, run securityadmin.sh to initialize the Security plugin. For information about how to use this script, see Applying changes to configuration files.\nOpenSearch Dashboards\nFor information on using your root CA and a client certificate to enable TLS for OpenSearch Dashboards, see Configure TLS for OpenSearch Dashboards.",
    "ancestors": [
      "Security in OpenSearch",
      "Configuration"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/configuration/index/",
    "title": "Configuration",
    "content": "The plugin includes demo certificates so that you can get up and running quickly. To use OpenSearch in a production environment, you must configure it manually: Replace the demo certificates. Reconfigure opensearch.yml to use your certificates. Reconfigure config.yml to use your authentication backend (if you don’t plan to use the internal user database). Modify the configuration YAML files.\nIf you plan to use the internal user database, set a password policy in opensearch.yml. Apply changes using the securityadmin script.\nStart OpenSearch. Add users, roles, role mappings, and tenants.\nIf you don’t want to use the plugin, see Disable security.\nThe Security plugin has several default users, roles, action groups, permissions, and settings for OpenSearch Dashboards that use kibana in their names. We will change these names in a future release.",
    "ancestors": [
      "Security in OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/configuration/multi-auth/",
    "title": "Configuring sign-in options",
    "content": "You can configure the sign-in window for OpenSearch Dashboards to provide either a single option for authenticating users at sign-in or multiple options. Currently, Dashboards supports basic authentication, OpenID Connect, and SAML as the multiple options.\nGeneral steps for configuring multiple authentication options\nConsider the following sequence of steps before configuring the sign-in window for multiple authentication options.\nDecide which types of authentication to make available at sign-in.\nConfigure each authentication type, including an authentication domain for the identity provider (IdP) and the essential settings that give each type sign-in access to OpenSearch Dashboards. For OpenId Connect backend configuration, see OpenID Connect. For SAML backend configuration, see SAML.\nAdd, enable, and configure multiple option authentication settings in the opensearch_dashboards.yml file.\nEnabling multiple authentication options\nBy default, Dashboards provides basic authentication for sign-in. To enable multiple options for authentication, begin by adding opensearch_security.auth.multiple_auth_enabled to the opensearch_dashboards.yml file and setting it to true.\nTo specify the multiple authentication types as options during sign-in, add the opensearch_security.auth.type setting to the opensearch_dashboards.yml file and enter multiple types as values. When more than one authentication type is added to the setting, the Dashboards sign-in window recognizes multiple types and adjusts to accommodate the sign-in options.\nWhen setting up Dashboards to provide multiple authentication options, basic authentication is always required as one of the values for the setting.\nAdd a single value to the setting when only one authentication type is needed. opensearch_security.auth.type: \" openid\" For multiple authentication options, add values to the setting as an array separated by commas. As a reminder, Dashboards currently supports a combination of basic authentication, OpenID Connect, and SAML as a valid set of values. In the setting, these values are expressed as \"basicauth\", \"openid\", and \"saml\". opensearch_security.auth.type: [ \" basicauth\", \" openid\"] opensearch_security.auth.multiple_auth_enabled: true opensearch_security.auth.type: [ \" basicauth\", \" saml\"] opensearch_security.auth.multiple_auth_enabled: true opensearch_security.auth.type: [ \" basicauth\", \" saml\", \" openid\"] opensearch_security.auth.multiple_auth_enabled: true When the opensearch_security.auth.type setting contains basicauth and one other authentication type, the sign-in window appears as in the following example. With all three valid authentication types specified, the sign-in window appears as in the following example. Customizing the sign-in environment\nIn addition to the essential sign-in settings for each authentication type, you can configure additional settings in the opensearch_dashboards.yml file to customize the sign-in window so that it clearly represents the options that are available. For example, you can replace the label on the sign-in button with the name and icon of the IdP. Refer to the settings and descriptions that follow. Basic authentication settings\nThese settings allow you to customize the basic username and password sign-in button. Setting Description opensearch_security.ui.basicauth.login.brandimage Login button logo. Supported file types are SVG, PNG, and GIF. opensearch_security.ui.basicauth.login.showbrandimage Determines whether a logo for the login button is displayed or not. Default is true. OpenID Connect authentication settings\nThese settings allow you to customize the sign-in button associated with OpenID Connect authentication. For the essential settings required to use OpenID Connect as a single sign-in option, see OpenSearch Dashboards single sign-on. Setting Description opensearch_security.ui.openid.login.buttonname Display name for the login button. “Log in with single sign-on” by default. opensearch_security.ui.openid.login.brandimage Login button logo. Supported file types are SVG, PNG, and GIF. opensearch_security.ui.openid.login.showbrandimage Determines whether a logo for the login button is displayed or not. Default is false. SAML authentication settings\nThese settings allow you to customize the sign-in button associated with SAML authentication. For the essential settings required to use SAML as a sign-in option, see OpenSearch Dashboards configuration. Setting Description opensearch_security.ui.saml.login.buttonname Display name for the login button. “Log in with single sign-on” by default. opensearch_security.ui.saml.login.brandimage Login button logo. Supported file types are SVG, PNG, and GIF. opensearch_security.ui.saml.login.showbrandimage Determines whether a logo for the login button is displayed or not. Default is false. Sample setup\nThe following example shows basic settings in the opensearch_dashboards.yml file when it is configured for two types of authentication at sign-in. # The several settings directly below are typical of all `opensearch_dashboards.yml` configurations. # server.host: 0.0.0.0 server.port: 5601 opensearch.hosts: [ \" https://localhost:9200\"] opensearch.ssl.verificationMode: none opensearch.username: &lt;preferred username&gt; opensearch.password: &lt;preferred password&gt; opensearch.requestHeadersAllowlist: [ \" securitytenant\", \" Authorization\"] opensearch_security.multitenancy.enabled: true opensearch_security.multitenancy.tenants.preferred: [ \" Private\", \" Global\"] opensearch_security.readonly_mode.roles: [ \" &lt;role_for_read_only&gt;\"] # Settings that enable multiple option authentication in the sign-in window # opensearch_security.auth.multiple_auth_enabled: true opensearch_security.auth.type: [ \" basicauth\", \" openid\"] # Basic authentication customization # opensearch_security.ui.basicauth.login.brandimage: &lt;path/to/OSlogo.png&gt; opensearch_security.ui.basicauth.login.showbrandimage: true # OIDC auth customization and start settings # opensearch_security.ui.openid.login.buttonname: Log in with &lt;IdP name or other&gt; opensearch_security.ui.openid.login.brandimage: &lt;path/to/brand-logo.png&gt; opensearch_security.ui.openid.login.showbrandimage: true opensearch_security.openid.base_redirect_url: &lt;\"OIDC redirect URL\"&gt; opensearch_security.openid.verify_hostnames: false opensearch_security.openid.refresh_tokens: false opensearch_security.openid.logout_url: &lt;\"OIDC logout URL\"&gt; opensearch_security.openid.connect_url: &lt;\"OIDC connect URL\"&gt; opensearch_security.openid.client_id: &lt;Client ID&gt; opensearch_security.openid.client_secret: &lt;Client secret&gt;",
    "ancestors": [
      "Security in OpenSearch",
      "Configuration"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/configuration/security-admin/",
    "title": "Applying changes to configuration files",
    "content": "On Windows, use securityadmin.bat in place of securityadmin.sh. For more information, see Windows usage.\nThe Security plugin stores its configuration—including users, roles, permissions, and backend settings—in a system index on the OpenSearch cluster. Storing these settings in an index lets you change settings without restarting the cluster and eliminates the need to edit configuration files on every individual node. This is accomplished by running the securityadmin.sh script.\nThe first job of the script is to initialize the.opendistro_security index. This loads your initial configuration into the index using the configuration files in /config/opensearch-security. After the.opendistro_security index is initialized, you can use OpenSearch Dashboards or the REST API to manage your users, roles, and permissions.\nThe script can be found at /plugins/opensearch-security/tools/securityadmin.sh. This is a relative path showing where the securityadmin.sh script is located. The absolute path depends on the directory where you’ve installed OpenSearch. For example, if you use Docker to install OpenSearch, the path will resemble the following: /usr/share/opensearch/plugins/opensearch-security/tools/securityadmin.sh.\nA word of caution\nIf you make changes to the configuration files in config/opensearch-security, OpenSearch does not automatically apply these changes. Instead, you must run securityadmin.sh to load the updated files into the index.\nRunning securityadmin.sh overwrites one or more portions of the.opendistro_security index. Run it with extreme care to avoid losing your existing resources. Consider the following example:\nYou initialize the.opendistro_security index.\nYou create ten users using the REST API.\nYou decide to create a new reserved user using internal_users.yml.\nYou run securityadmin.sh again to load the new reserved user into the index.\nYou lose all ten users that you created using the REST API.\nTo avoid this situation, back up your current configuration before making changes and re-running the script:./securityadmin.sh -backup my-backup-directory \\ -icl \\ -nhnv \\ -cacert../../../config/root-ca.pem \\ -cert../../../config/kirk.pem \\ -key../../../config/kirk-key.pem If you use the -f argument rather than -cd, you can load a single YAML file into the index rather than the entire directory of YAML files. For example, if you create ten new roles, you can safely load internal_users.yml into the index without losing your roles; only the internal users get overwritten../securityadmin.sh -f../../../config/opensearch-security/internal_users.yml \\ -t internalusers \\ -icl \\ -nhnv \\ -cacert../../../config/root-ca.pem \\ -cert../../../config/kirk.pem \\ -key../../../config/kirk-key.pem To resolve all environment variables before applying the security configurations, use the -rev parameter../securityadmin.sh -cd../../../config/opensearch-security/ \\ -rev \\ -cacert../../../root-ca.pem \\ -cert../../../kirk.pem \\ -key../../../kirk.key.pem The following example shows an environment variable in the config.yml file: password: ${env.LDAP_PASSWORD} Configure the admin certificate\nIn order to use securityadmin.sh, you must add the distinguished names (DNs) of all admin certificates to opensearch.yml. If you use the demo certificates, for example, opensearch.yml might contain the following lines for the kirk certificate: plugins.security.authcz.admin_dn: - CN=kirk,OU=client,O=client,L=test,C=DE You can’t use node certificates as admin certificates. The two must be separate. Also, do not add whitespace between the parts of the DN.\nBasic usage\nThe securityadmin.sh tool can be run from any machine that has access to the HTTP port of your OpenSearch cluster (the default port is 9200). You can change the Security plugin configuration without having to access your nodes through SSH. securityadmin.sh requires that SSL/TLS transport is enabled on your opensearch cluster. In other words, make sure that the plugins.security.ssl.http.enabled: true is set in opensearch.yml before proceeding.\nEach node also includes the tool at plugins/opensearch-security/tools/securityadmin.sh. You might need to make the script executable before running it: chmod +x plugins/opensearch-security/tools/securityadmin.sh To print all available command line options, run the script with no arguments:./plugins/opensearch-security/tools/securityadmin.sh To load your initial configuration (all YAML files), you might use the following command:./securityadmin.sh -cd../../../config/opensearch-security/ -icl -nhnv \\ -cacert../../../config/root-ca.pem \\ -cert../../../config/kirk.pem \\ -key../../../config/kirk-key.pem The -cd option specifies where the Security plugin configuration files can be found.\nThe -icl ( --ignore-clustername) option tells the Security plugin to upload the configuration regardless of the cluster name. As an alternative, you can also specify the cluster name with the -cn ( --clustername) option.\nBecause the demo certificates are self-signed, this command disables hostname verification with the -nhnv ( --disable-host-name-verification) option.\nThe -cacert, -cert and -key options define the location of your root CA certificate, the admin certificate, and the private key for the admin certificate. If the private key has a password, specify it with the -keypass option.\nThe following table shows the PEM options. Name Description -cert The location of the PEM file containing the admin certificate and all intermediate certificates, if any. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. -key The location of the PEM file containing the private key of the admin certificate. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. The key must be in PKCS#8 format. -keypass The password of the private key of the admin certificate, if any. -cacert The location of the PEM file containing the root certificate. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. Sample commands\nApply all YAML files in config/opensearch-security/ using PEM certificates: /usr/share/opensearch/plugins/opensearch-security/tools/securityadmin.sh \\ -cacert /etc/opensearch/root-ca.pem \\ -cert /etc/opensearch/kirk.pem \\ -key /etc/opensearch/kirk-key.pem \\ -cd /usr/share/opensearch/config/opensearch-security/ Apply a single YAML file ( config.yml) using PEM certificates:./securityadmin.sh \\ -f../../../config/opensearch-security/config.yml \\ -icl -nhnv -cert /etc/opensearch/kirk.pem \\ -cacert /etc/opensearch/root-ca.pem \\ -key /etc/opensearch/kirk-key.pem \\ -t config Apply all YAML files in config/opensearch-security/ with keystore and truststore files:./securityadmin.sh \\ -cd /usr/share/opensearch/config/opensearch-security/ \\ -ks /path/to/keystore.jks \\ -kspass changeit \\ -ts /path/to/truststore.jks \\ -tspass changeit -nhnv -icl Using securityadmin with keystore and truststore files\nYou can also use keystore files in JKS format in conjunction with securityadmin.sh:./securityadmin.sh -cd../../../config/opensearch-security -icl -nhnv -ts &lt;path/to/truststore&gt; -tspass &lt;truststore password&gt; -ks &lt;path/to/keystore&gt; -kspass &lt;keystore password&gt; Use the following options to control the key and truststore settings. Name Description -ks The location of the keystore containing the admin certificate and all intermediate certificates, if any. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. -kspass The password for the keystore. -kst The key store type, either JKS or PKCS#12/PFX. If not specified, the Security plugin tries to determine the type from the file extension. -ksalias The alias of the admin certificate, if any. -ts The location of the truststore containing the root certificate. You can use an absolute or relative path. Relative paths are resolved relative to the execution directory of securityadmin.sh. -tspass The password for the truststore. -tst The truststore type, either JKS or PKCS#12/PFX. If not specified, the Security plugin tries to determine the type from the file extension. -tsalias The alias for the root certificate, if any. OpenSearch settings\nIf you run a default OpenSearch installation, which listens on port 9200 and uses opensearch as a cluster name, you can omit the following settings altogether. Otherwise, specify your OpenSearch settings by using the following switches. Name Description -h OpenSearch hostname. Default is localhost. -p OpenSearch port. Default is 9200 - not the HTTP port. -cn Cluster name. Default is opensearch. -icl Ignore cluster name. -sniff Sniff cluster nodes. Sniffing detects available nodes using the OpenSearch _cluster/state API. -arc,--accept-red-cluster Execute securityadmin.sh even if the cluster state is red. Default is false, which means the script will not execute on a red cluster. Certificate validation settings\nUse the following options to control certificate validation. Name Description -nhnv Do not validate hostname. Default is false. -nrhn Do not resolve hostname. Only relevant if -nhnv is not set. -noopenssl Do not use OpenSSL, even if available. Default is to use OpenSSL if it is available. Configuration files settings\nThe following switches define which configuration files you want to push to the Security plugin. You can either push a single file or specify a directory containing one or more configuration files. Name Description -cd Directory containing multiple Security plugin configuration files. -f Single configuration file. Can’t be used with -cd. -t File type. -rl Reload the current configuration and flush the internal cache. To upload all configuration files in a directory, use this:./securityadmin.sh -cd../../../config/opensearch-security -ts... -tspass... -ks... -kspass... If you want to push a single configuration file, use this:./securityadmin.sh -f../../../config/opensearch-security/internal_users.yml -t internalusers \\ -ts... -tspass... -ks... -kspass... The file type must be one of the following:\nconfig\nroles\nrolesmapping\ninternalusers\nactiongroups\nCipher settings\nYou probably won’t need to change cipher settings. If you need to, use the following options. Name Description -ec Comma-separated list of enabled TLS ciphers. -ep Comma-separated list of enabled TLS protocols. Backup, restore, and migrate\nYou can download all current configuration files from your cluster with the following command:./securityadmin.sh -backup my-backup-directory -ts... -tspass... -ks... -kspass... This command dumps the current Security plugin configuration from your cluster to individual files in the directory you specify. You can then use these files as backups or to load the configuration into a different cluster. This command is useful when moving a proof-of-concept to production or if you need to add additional reserved or hidden resources:./securityadmin.sh \\ -backup my-backup-directory \\ -icl \\ -nhnv \\ -cacert../../../config/root-ca.pem \\ -cert../../../config/kirk.pem \\ -key../../../config/kirk-key.pem To upload the dumped files to another cluster:./securityadmin.sh -h production.example.com -p 9301 -cd /etc/backup/ -ts... -tspass... -ks... -kspass... To migrate configuration YAML files from the Open Distro for Elasticsearch 0.x.x format to the OpenSearch 1.x.x format:./securityadmin.sh -migrate../../../config/opensearch-security -ts... -tspass... -ks... -kspass... Name Description -backup Retrieve the current Security plugin configuration from a running cluster and dump it to the working directory. -migrate Migrate configuration YAML files from Open Distro for Elasticsearch 0.x.x to OpenSearch 1.x.x. Other options Name Description -dci Delete the Security plugin configuration index and exit. This option is useful if the cluster state is red due to a corrupted Security plugin index. -esa Enable shard allocation and exit. This option is useful if you disabled shard allocation while performing a full cluster restart and need to recreate the Security plugin index. -w Displays information about the used admin certificate. -rl By default, the Security plugin caches authenticated users, along with their roles and permissions, for one hour. This option reloads the current Security plugin configuration stored in your cluster, invalidating any cached users, roles, and permissions. -i The Security plugin index name. Default is.opendistro_security. -er Set explicit number of replicas or auto-expand expression for the opensearch_security index. -era Enable replica auto-expand. -dra Disable replica auto-expand. -us Update the replica settings. Windows usage\nOn Windows, the equivalent of securityadmin.sh is the securityadmin.bat script located in the \\path\\to\\opensearch-2.7.0\\plugins\\opensearch-security\\tools\\ directory.\nWhen running the example commands in the preceding sections, use the command prompt or Powershell. Open the command prompt by entering cmd or Powershell by entering powershell in the search box next to Start on the taskbar.\nFor example, to print all available command line options, run the script with no arguments:.\\plugins\\opensearch -security \\tools\\securityadmin.bat When entering a multiline command, use the caret ( ^) character to escape the next character in the command line.\nFor example, to load your initial configuration (all YAML files), use the following command:.\\securityadmin.bat -cd..\\..\\..\\config\\opensearch -security \\ -icl -nhnv ^ -cacert..\\..\\..\\config\\root -ca.pem ^ -cert..\\..\\..\\config\\kirk.pem ^ -key..\\..\\..\\config\\kirk -key.pem",
    "ancestors": [
      "Security in OpenSearch",
      "Configuration"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/configuration/system-indices/",
    "title": "System indexes",
    "content": "By default, OpenSearch has a protected system index,.opendistro_security, which is used to store the Security configuration YAML files. You create this index using securityadmin.sh. Even with a user account that has read permissions for all indexes, you can’t directly access the data in this system index.\nInstead, you first need to authenticate with an admin certificate to gain access: curl -k --cert./kirk.pem --key./kirk-key.pem -XGET 'https://localhost:9200/.opendistro_security/_search' When Security is installed, the demo configuration automatically creates the.opendistro_security system index. It also adds several other indexes for the various OpenSearch plugins that integrate with the Security plugin: plugins.security.system_indices.enabled: true plugins.security.system_indices.indices: [ \".opendistro-alerting-config\", \".opendistro-alerting-alert*\", \".opendistro-anomaly-results*\", \".opendistro-anomaly-detector*\", \".opendistro-anomaly-checkpoints\", \".opendistro-anomaly-detection-state\", \".opendistro-reports-*\", \".opendistro-notifications-*\", \".opendistro-notebooks\", \".opendistro-asynchronous-search-response*\"] You can add additional system indexes in opensearch.yml. An alternative way to remove a system index is to delete it from the plugins.security.system_indices.indices list on each node and restart OpenSearch.",
    "ancestors": [
      "Security in OpenSearch",
      "Configuration"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/configuration/tls/",
    "title": "Configuring TLS certificates",
    "content": "TLS is configured in opensearch.yml. Certificates are used to secure transport-layer traffic (node-to-node communication within your cluster) and REST-layer traffic (communication between a client and a node within your cluster). TLS is optional for the REST layer and mandatory for the transport layer.\nYou can find an example configuration template with all options on GitHub.\nX.509 PEM certificates and PKCS #8 keys\nThe following tables contain the settings you can use to configure the location of your PEM certificates and private keys.\nTransport layer TLS Name Description plugins.security.ssl.transport.pemkey_filepath Path to the certificate’s key file (PKCS #8), which must be under the config directory, specified using a relative path. Required. plugins.security.ssl.transport.pemkey_password Key password. Omit this setting if the key has no password. Optional. plugins.security.ssl.transport.pemcert_filepath Path to the X.509 node certificate chain (PEM format), which must be under the config directory, specified using a relative path. Required. plugins.security.ssl.transport.pemtrustedcas_filepath Path to the root CAs (PEM format), which must be under the config directory, specified using a relative path. Required. REST layer TLS Name Description plugins.security.ssl.http.pemkey_filepath Path to the certificate’s key file (PKCS #8), which must be under the config directory, specified using a relative path. Required. plugins.security.ssl.http.pemkey_password Key password. Omit this setting if the key has no password. Optional. plugins.security.ssl.http.pemcert_filepath Path to the X.509 node certificate chain (PEM format), which must be under the config directory, specified using a relative path. Required. plugins.security.ssl.http.pemtrustedcas_filepath Path to the root CAs (PEM format), which must be under the config directory, specified using a relative path. Required. Keystore and truststore files\nAs an alternative to certificates and private keys in PEM format, you can instead use keystore and truststore files in JKS or PKCS12/PFX format. For the Security plugin to operate, you need certificates and private keys.\nThe following settings configure the location and password of your keystore and truststore files. If you want, you can use different keystore and truststore files for the REST and the transport layer.\nTransport layer TLS Name Description plugins.security.ssl.transport.keystore_type The type of the keystore file, JKS or PKCS12/PFX. Optional. Default is JKS. plugins.security.ssl.transport.keystore_filepath Path to the keystore file, which must be under the config directory, specified using a relative path. Required. plugins.security.ssl.transport.keystore_alias: my_alias Alias name. Optional. Default is the first alias. plugins.security.ssl.transport.keystore_password Keystore password. Default is changeit. plugins.security.ssl.transport.truststore_type The type of the truststore file, JKS or PKCS12/PFX. Default is JKS. plugins.security.ssl.transport.truststore_filepath Path to the truststore file, which must be under the config directory, specified using a relative path. Required. plugins.security.ssl.transport.truststore_alias Alias name. Optional. Default is all certificates. plugins.security.ssl.transport.truststore_password Truststore password. Default is changeit. REST layer TLS Name Description plugins.security.ssl.http.enabled Whether to enable TLS on the REST layer. If enabled, only HTTPS is allowed. Optional. Default is false. plugins.security.ssl.http.keystore_type The type of the keystore file, JKS or PKCS12/PFX. Optional. Default is JKS. plugins.security.ssl.http.keystore_filepath Path to the keystore file, which must be under the config directory, specified using a relative path. Required. plugins.security.ssl.http.keystore_alias Alias name. Optional. Default is the first alias. plugins.security.ssl.http.keystore_password Keystore password. Default is changeit. plugins.security.ssl.http.truststore_type The type of the truststore file, JKS or PKCS12/PFX. Default is JKS. plugins.security.ssl.http.truststore_filepath Path to the truststore file, which must be under the config directory, specified using a relative path. Required. plugins.security.ssl.http.truststore_alias Alias name. Optional. Default is all certificates. plugins.security.ssl.http.truststore_password Truststore password. Default is changeit. Configuring node certificates\nOpenSearch Security needs to identify requests between the nodes in the cluster. It uses node certificates to secure these requests. The simplest way to configure node certificates is to list the Distinguished Names (DNs) of these certificates in opensearch.yml. All DNs must be included in opensearch.yml on all nodes. Keep in mind that the Security plugin supports wildcards and regular expressions: plugins.security.nodes_dn: - ' CN=node.other.com,OU=SSL,O=Test,L=Test,C=DE' - ' CN=*.example.com,OU=SSL,O=Test,L=Test,C=DE' - ' CN=elk-devcluster*' - ' /CN=.*regex/' If your node certificates have an Object ID (OID) identifier in the SAN section, you can omit this configuration.\nConfiguring admin certificates\nAdmin certificates are regular client certificates that have elevated rights to perform administrative tasks. You need an admin certificate to change the Security plugin configuration using plugins/opensearch-security/tools/securityadmin.sh or the REST API. Admin certificates are configured in opensearch.yml by stating their DN(s): plugins.security.authcz.admin_dn: - CN=admin,OU=SSL,O=Test,L=Test,C=DE For security reasons, you can’t use wildcards or regular expressions here.\n(Advanced) OpenSSL\nThe Security plugin supports OpenSSL, but we only recommend it if you use Java 8. If you use Java 11, we recommend the default configuration.\nTo use OpenSSL, you must install OpenSSL, the Apache Portable Runtime, and a Netty version with OpenSSL support matching your platform on all nodes.\nIf OpenSSL is enabled, but for one reason or another the installation does not work, the Security plugin falls back to the Java JCE as the security engine. Name Description plugins.security.ssl.transport.enable_openssl_if_available Enable OpenSSL on the transport layer if available. Optional. Default is true. plugins.security.ssl.http.enable_openssl_if_available Enable OpenSSL on the REST layer if available. Optional. Default is true. (Advanced) Hostname verification and DNS lookup\nIn addition to verifying the TLS certificates against the root CA and/or intermediate CA(s), the Security plugin can apply additional checks on the transport layer.\nWith enforce_hostname_verification enabled, the Security plugin verifies that the hostname of the communication partner matches the hostname in the certificate. The hostname is taken from the subject or SAN entries of your certificate. For example, if the hostname of your node is node-0.example.com, then the hostname in the TLS certificate has to be set to node-0.example.com, as well. Otherwise, errors are thrown: [ERROR][c.a.o.s.s.t.opensearchSecuritySSLNettyTransport] [WX6omJY] SSL Problem No name matching &lt;hostname&gt; found\n[ERROR][c.a.o.s.s.t.opensearchSecuritySSLNettyTransport] [WX6omJY] SSL Problem Received fatal alert: certificate_unknown In addition, when resolve_hostname is enabled, the Security plugin resolves the (verified) hostname against your DNS. If the hostname does not resolve, errors are thrown: Name Description plugins.security.ssl.transport.enforce_hostname_verification Whether to verify hostnames on the transport layer. Optional. Default is true. plugins.security.ssl.transport.resolve_hostname Whether to resolve hostnames against DNS on the transport layer. Optional. Default is true. Only works if hostname verification is also enabled. (Advanced) Client authentication\nWith TLS client authentication enabled, REST clients can send a TLS certificate with the HTTP request to provide identity information to the Security plugin. There are three main usage scenarios for TLS client authentication:\nProviding an admin certificate when using the REST management API.\nConfiguring roles and permissions based on a client certificate.\nProviding identity information for tools like OpenSearch Dashboards, Logstash, or Beats.\nTLS client authentication has three modes: NONE: The Security plugin does not accept TLS client certificates. If one is sent, it is discarded. OPTIONAL: The Security plugin accepts TLS client certificates if they are sent, but does not require them. REQUIRE: The Security plugin only accepts REST requests when a valid client TLS certificate is sent.\nFor the REST management API, the client authentication modes has to be OPTIONAL at a minimum.\nYou can configure the client authentication mode by using the following setting: Name Description plugins.security.ssl.http.clientauth_mode\nThe TLS client authentication mode to use. Can be one of NONE, OPTIONAL (default) or REQUIRE. Optional. (Advanced) Enabled ciphers and protocols\nYou can limit the allowed ciphers and TLS protocols for the REST layer. For example, you can only allow strong ciphers and limit the TLS versions to the most recent ones.\nIf this setting is not enabled, the ciphers and TLS versions are negotiated between the browser and the Security plugin automatically, which in some cases can lead to a weaker cipher suite being used. You can configure the ciphers and protocols using the following settings. Name Data type Description plugins.security.ssl.http.enabled_ciphers Array\nEnabled TLS cipher suites for the REST layer. Only Java format is supported. plugins.security.ssl.http.enabled_protocols Array\nEnabled TLS protocols for the REST layer. Only Java format is supported. plugins.security.ssl.transport.enabled_ciphers Array\nEnabled TLS cipher suites for the transport layer. Only Java format is supported. plugins.security.ssl.transport.enabled_protocols Array\nEnabled TLS protocols for the transport layer. Only Java format is supported. Example settings plugins.security.ssl.http.enabled_ciphers: - \" TLS_DHE_RSA_WITH_AES_256_CBC_SHA\" - \" TLS_DHE_DSS_WITH_AES_128_CBC_SHA256\" plugins.security.ssl.http.enabled_protocols: - \" TLSv1.1\" - \" TLSv1.2\" Because it is insecure, the Security plugin disables TLSv1 by default. If you need to use TLSv1 and accept the risks, you can still enable it: plugins.security.ssl.http.enabled_protocols: - \" TLSv1\" - \" TLSv1.1\" - \" TLSv1.2\" (Advanced) Disabling client initiated renegotiation for Java 8\nSet -Djdk.tls.rejectClientInitiatedRenegotiation=true to disable secure client initiated renegotiation, which is enabled by default. This can be set via OPENSEARCH_JAVA_OPTS in config/jvm.options.",
    "ancestors": [
      "Security in OpenSearch",
      "Configuration"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/configuration/yaml/",
    "title": "Modifying the YAML files",
    "content": "The Security installation provides a number of YAML confguration files that are used to store the necessary settings that define the way Security manages users, roles, and activity within the cluster. These settings range from configurations for authentication backends to lists of allowed endpoints and HTTP requests.\nBefore running securityadmin.sh to load the settings into the.opendistro_security index, perform an initial configuration of the YAML files. The files can be found in the config/opensearch-security directory. It’s also good practice to back up these files so that you can reuse them for other clusters.\nThe approach we recommend for using the YAML files is to first configure reserved and hidden resources, such as the admin and kibanaserver users. Thereafter you can create other users, roles, mappings, action groups, and tenants using OpenSearch Dashboards or the REST API.\ninternal_users.yml\nThis file contains any initial users that you want to add to the Security plugin’s internal user database.\nThe file format requires a hashed password. To generate one, run plugins/opensearch-security/tools/hash.sh -p &lt;new-password&gt;. If you decide to keep any of the demo users, change their passwords and re-run securityadmin.sh to apply the new passwords. --- # This is the internal user database # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh _meta: type: \" internalusers\" config_version: 2 # Define your internal users here new-user: hash: \" $2y$12$88IFVl6IfIwCFh5aQYfOmuXVL9j2hz/GusQb35o.4sdTDAEMTOD.K\" reserved: false hidden: false opendistro_security_roles: - \" specify-some-security-role-here\" backend_roles: - \" specify-some-backend-role-here\" attributes: attribute1: \" value1\" static: false ## Demo users admin: hash: \" $2a$12$VcCDgh2NDk07JGN0rjGbM.Ad41qVR/YFJcgHp0UGns5JDymv..TOG\" reserved: true backend_roles: - \" admin\" description: \" Demo admin user\" kibanaserver: hash: \" $2a$12$4AcgAt3xwOWadA5s5blL6ev39OXDNhmOesEoo33eZtrq2N0YrU3H.\" reserved: true description: \" Demo user for the OpenSearch Dashboards server\" kibanaro: hash: \" $2a$12$JJSXNfTowz7Uu5ttXfeYpeYE0arACvcwlPBStB1F.MI7f0U9Z4DGC\" reserved: false backend_roles: - \" kibanauser\" - \" readall\" attributes: attribute1: \" value1\" attribute2: \" value2\" attribute3: \" value3\" description: \" Demo read-only user for OpenSearch dashboards\" logstash: hash: \" $2a$12$u1ShR4l4uBS3Uv59Pa2y5.1uQuZBrZtmNfqB3iM/.jL0XoV9sghS2\" reserved: false backend_roles: - \" logstash\" description: \" Demo logstash user\" readall: hash: \" $2a$12$ae4ycwzwvLtZxwZ82RmiEunBbIPiAmGZduBAjKN0TXdwQFtCwARz2\" reserved: false backend_roles: - \" readall\" description: \" Demo readall user\" snapshotrestore: hash: \" $2y$12$DpwmetHKwgYnorbgdvORCenv4NAK8cPUg8AI6pxLCuWf/ALc0.v7W\" reserved: false backend_roles: - \" snapshotrestore\" description: \" Demo snapshotrestore user\" opensearch.yml\nIn addition to many OpenSearch settings, this file contains paths to TLS certificates and their attributes, such as distinguished names and trusted certificate authorities. plugins.security.ssl.transport.pemcert_filepath: esnode.pem plugins.security.ssl.transport.pemkey_filepath: esnode-key.pem plugins.security.ssl.transport.pemtrustedcas_filepath: root-ca.pem plugins.security.ssl.transport.enforce_hostname_verification: false plugins.security.ssl.http.enabled: true plugins.security.ssl.http.pemcert_filepath: esnode.pem plugins.security.ssl.http.pemkey_filepath: esnode-key.pem plugins.security.ssl.http.pemtrustedcas_filepath: root-ca.pem plugins.security.allow_unsafe_democertificates: true plugins.security.allow_default_init_securityindex: true plugins.security.authcz.admin_dn: - CN=kirk,OU=client,O=client,L=test, C=de plugins.security.audit.type: internal_opensearch plugins.security.enable_snapshot_restore_privilege: true plugins.security.check_snapshot_restore_write_privileges: true plugins.security.cache.ttl_minutes: 60 plugins.security.restapi.roles_enabled: [ \" all_access\", \" security_rest_api_access\"] plugins.security.system_indices.enabled: true plugins.security.system_indices.indices: [ \".opendistro-alerting-config\", \".opendistro-alerting-alert*\", \".opendistro-anomaly-results*\", \".opendistro-anomaly-detector*\", \".opendistro-anomaly-checkpoints\", \".opendistro-anomaly-detection-state\", \".opendistro-reports-*\", \".opendistro-notifications-*\", \".opendistro-notebooks\", \".opendistro-asynchronous-search-response*\"] node.max_local_storage_nodes: 3 If you want to run your users’ passwords against some validation, specify a regular expression (regex) in this file. You can also include an error message that loads when passwords don’t pass validation. The following example demonstrates how to include a regex so OpenSearch requires new passwords to be a minimum of eight characters with at least one uppercase, one lowercase, one digit, and one special character.\nNote that OpenSearch validates only users and passwords created through OpenSearch Dashboards or the REST API. plugins.security.restapi.password_validation_regex: ' (?=.*[A-Z])(?=.*[^a-zA-Z\\d])(?=.*[0-9])(?=.*[a-z]).{8,}' plugins.security.restapi.password_validation_error_message: \" Password must be minimum 8 characters long and must contain at least one uppercase letter, one lowercase letter, one digit, and one special character.\" The opensearch.yml file also contains the plugins.security.allow_default_init_securityindex property. When set to true, the Security plugin uses default security settings if an attempt to create the security index fails when OpenSearch launches. Default security settings are stored in YAML files contained in the opensearch-project/security/config directory. By default, this setting is false. plugins.security.allow_default_init_securityindex: true Authentication cache for the Security plugin exists to help speed up authentication by temporarily storing user objects returned from the backend so that the Security plugin is not required to make repeated requests for them. To determine how long it takes for caching to time out, you can use the plugins.security.cache.ttl_minutes property to set a value in minutes. The default is 60. You can disable caching by setting the value to 0. plugins.security.cache.ttl_minutes: 60 allowlist.yml\nYou can use allowlist.yml to add any endpoints and HTTP requests to a list of allowed endpoints and requests. If enabled, all users except the super admin are allowed access to only the specified endpoints and HTTP requests, and all other HTTP requests associated with the endpoint are denied. For example, if GET _cluster/settings is added to the allow list, users cannot submit PUT requests to _cluster/settings to update cluster settings.\nNote that while you can configure access to endpoints this way, for most cases, it is still best to configure permissions using the Security plugin’s users and roles, which have more granular settings. --- _meta: type: \" allowlist\" config_version: 2 # Description: # enabled - feature flag. # if enabled is false, all endpoints are accessible. # if enabled is true, all users except the SuperAdmin can only submit the allowed requests to the specified endpoints. # SuperAdmin can access all APIs. # SuperAdmin is defined by the SuperAdmin certificate, which is configured with the opensearch.yml setting plugins.security.authcz.admin_dn: # Refer to the example setting in opensearch.yml to learn more about configuring SuperAdmin. # # requests - map of allow listed endpoints and HTTP requests #this name must be config config: enabled: true requests: /_cluster/settings: - GET /_cat/nodes: - GET To enable PUT requests to cluster settings, add PUT to the list of allowed operations under /_cluster/settings. requests: /_cluster/settings: - GET - PUT You can also add custom indices to the allow list. allowlist.yml doesn’t support wildcards, so you must manually specify all of the indexes you want to add. requests: # Only allow GET requests to /sample-index1/_doc/1 and /sample-index2/_doc/1 /sample-index1/_doc/1: - GET /sample-index2/_doc/1: - GET roles.yml\nThis file contains any initial roles that you want to add to the Security plugin. Aside from some metadata, the default file is empty, because the Security plugin has a number of static roles that it adds automatically. --- complex-role: reserved: false hidden: false cluster_permissions: - \" read\" - \" cluster:monitor/nodes/stats\" - \" cluster:monitor/task/get\" index_permissions: - index_patterns: - \" opensearch_dashboards_sample_data_*\" dls: \" { \\\" match \\\": { \\\" FlightDelay \\\": true}}\" fls: - \" ~FlightNum\" masked_fields: - \" Carrier\" allowed_actions: - \" read\" tenant_permissions: - tenant_patterns: - \" analyst_*\" allowed_actions: - \" kibana_all_write\" static: false _meta: type: \" roles\" config_version: 2 roles_mapping.yml --- manage_snapshots: reserved: true hidden: false backend_roles: - \" snapshotrestore\" hosts: [] users: [] and_backend_roles: [] logstash: reserved: false hidden: false backend_roles: - \" logstash\" hosts: [] users: [] and_backend_roles: [] own_index: reserved: false hidden: false backend_roles: [] hosts: [] users: - \" *\" and_backend_roles: [] description: \" Allow full access to an index named like the username\" kibana_user: reserved: false hidden: false backend_roles: - \" kibanauser\" hosts: [] users: [] and_backend_roles: [] description: \" Maps kibanauser to kibana_user\" complex-role: reserved: false hidden: false backend_roles: - \" ldap-analyst\" hosts: [] users: - \" new-user\" and_backend_roles: [] _meta: type: \" rolesmapping\" config_version: 2 all_access: reserved: true hidden: false backend_roles: - \" admin\" hosts: [] users: [] and_backend_roles: [] description: \" Maps admin to all_access\" readall: reserved: true hidden: false backend_roles: - \" readall\" hosts: [] users: [] and_backend_roles: [] kibana_server: reserved: true hidden: false backend_roles: [] hosts: [] users: - \" kibanaserver\" and_backend_roles: [] action_groups.yml\nThis file contains any initial action groups that you want to add to the Security plugin.\nAside from some metadata, the default file is empty, because the Security plugin has a number of static action groups that it adds automatically. These static action groups cover a wide variety of use cases and are a great way to get started with the plugin. --- my-action-group: reserved: false hidden: false allowed_actions: - \" indices:data/write/index*\" - \" indices:data/write/update*\" - \" indices:admin/mapping/put\" - \" indices:data/write/bulk*\" - \" read\" - \" write\" static: false _meta: type: \" actiongroups\" config_version: 2 tenants.yml\nYou can use this file to specify and add any number of OpenSearch Dashboards tenants to your OpenSearch cluster. For more information about tenants, see OpenSearch Dashboards multi-tenancy.\nLike all of the other YAML files, we recommend you use tenants.yml to add any tenants you must have in your cluster, and then use OpenSearch Dashboards or the REST API if you need to further configure or create any other tenants. --- _meta: type: \" tenants\" config_version: 2 admin_tenant: reserved: false description: \" Demo tenant for admin user\" nodes_dn.yml nodes_dn.yml lets you add certificates’ distinguished names (DNs) an allow list to enable communication between any number of nodes and/or clusters. For example, a node that has the DN CN=node1.example.com in its allow list accepts communication from any other node or certificate that uses that DN.\nThe DNs get indexed into a system index that only a super admin or an admin with a Transport Layer Security (TLS) certificate can access. If you want to programmatically add DNs to your allow lists, use the REST API. --- _meta: type: \" nodesdn\" config_version: 2 # Define nodesdn mapping name and corresponding values # cluster1: # nodes_dn: # - CN=*.example.com",
    "ancestors": [
      "Security in OpenSearch",
      "Configuration"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/index/",
    "title": "About Security",
    "content": "Security in OpenSearch is built around four main features that work together to safeguard data and track activity within a cluster. Separately, these features are:\nEncryption.\nAuthentication.\nAccess control.\nAudit logging and compliance.\nUsed together they provide effective protection of sensitive data by placing it behind multiple layers of defense and granting or restricting access to the data at different levels in the OpenSearch data structure. Most implementations use a combination of options for these features to meet specific security needs.\nFeatures at a glance\nThe following topics provide a general description of the features that define security in OpenSearch.\nEncryption\nEncryption typically addresses the protection of data both at rest and in transit. OpenSearch Security is responsible for managing encryption in transit.\nIn transit, Security encrypts data moving to, from, and within the cluster. OpenSearch uses the TLS protocol, which covers both client-to-node encryption (the REST layer) and node-to-node encryption (the transport layer). This combination of in-transit encryption helps ensure that both requests to OpenSearch and the movement of data among different nodes are safe from tampering.\nYou can find out more about configuring TLS in the Configuring TLS certificates section.\nEncryption at rest, on the other hand, protects data stored in the cluster, including indexes, logs, swap files, automated snapshots, and all data in the application directory. This type of encryption is managed by the operating system on each OpenSearch node. For information about enabling encryption at rest, see Encryption at rest.\nAuthentication\nAuthentication is used to validate the identity of users and works by verifying an end user’s credentials against a backend configuration. These credentials can be a simple name and password, a JSON web token, or a TLS certificate. Once the authentication domain extracts those credentials from a user’s request, it can check their validity against the authentication backend.\nThe backend used for validation can be OpenSearch’s built-in internal user database—used for storing user and role configurations and hashed passwords—or one of a wide range of industry-standard identification protocols such as LDAP, Active Directory, SAML, or OpenID Connect. A common practice is to chain together more than one authentication method to create a more robust defense against unauthorized access. This might involve, for example, HTTP basic authentication followed by a backend configuration that specifies the LDAP protocol. See the Configuring the Security backend section to learn more about setting up the backend.\nAccess control\nAccess control (or authorization) generally involves selectively assigning permissions to users that allow them to perform specific tasks, such as clearing the cache for a particular index or taking a snapshot of a cluster. However, rather than assign individual permissions directly to users, OpenSearch assigns these permissions to roles and then maps the roles to users. For more on setting up these relationships, see Users and roles. Roles, therefore, define the actions that users can perform, including the data they can read, the cluster settings they can modify, the indexes to which they can write, and so on. Roles are reusable across multiple users, and users can have multiple roles.\nAnother notable characteristic of access control in OpenSearch is the ability to assign user access through levels of increasing granularity. Fine-grained access control (FGAC) means that a role can control permissions for users at not only the cluster level but also the index level, the document level, and even the field level. For example, a role may provide a user access to certain cluster-level permissions but at the same time prevent the user from accessing a given group of indexes. Likewise, that role may grant access to certain types of documents but not others, or it may even include access to specific fields within a document but exclude access to other sensitive fields. Field masking further extends FGAC by providing options to mask certain types of data, such as a list of emails, which can still be aggregated but not made viewable to a role.\nTo learn more about this feature, see the Access control section of the security documentation.\nAudit logging and compliance\nFinally, audit logging and compliance refer to mechanisms that allow for tracking and analysis of activity within a cluster. This is important after data breaches (unauthorized access) or when data suffers unintended exposure, as could happen when the data is left vulnerable in an unsecured location. However, audit logging can be just as valuable a tool for assessing excessive loads on a cluster or surveying trends for a given task. This feature allows you to review changes made anywhere in a cluster and track access patterns and API requests of all types, whether valid or invalid.\nHow OpenSearch archives logging is configurable at many levels of detail, and there are a number of options for where those logs are stored. Compliance features also ensure that all data is available if and when compliance auditing is required. In this case, the logging can be automated to focus on data especially pertinent to those compliance requirements.\nSee the Audit logs section of the security documentation to read more about this feature.\nOther features and functionality\nOpenSearch includes other features that complement the security infrastructure.\nDashboards multi-tenancy\nOne such feature is OpenSearch Dashboards multi-tenancy. Tenants are work spaces that include visualizations, index patterns, and other Dashboards objects. Multi-tenancy allows for the sharing of tenants among users of Dashboards and leverages OpenSearch roles to manage access to tenants and safely make them available to others.\nFor more information on creating tenants, see OpenSearch Dashboards multi-tenancy.\nCross-cluster search\nAnother notable feature is cross-cluster search. This feature provides users with the ability to perform searches from one node in a cluster across other clusters that have been set up to coordinate this type of search. As with other features, cross-cluster search is supported by the OpenSearch access control infrastructure, which defines the permissions users have for working with this feature.\nTo learn more, see Cross-cluster search.\nNext steps\nTo get started, see the configuration overview in the Security configuration section, which provides the basic steps for setting up security in your OpenSearch implementation and includes links to information about customizing security for your business needs.",
    "ancestors": [
      "Security in OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/multi-tenancy/dynamic-config/",
    "title": "Dynamic configuration in OpenSearch Dashboards",
    "content": "Multi-tenancy includes dynamic configuration options in OpenSearch Dashboards so you can manage common settings for tenancy without having to make changes to the configuration YAML files on each node and then restart the cluster. You can take advantage of this functionality by using the Dashboards interface or the REST API. The following list includes descriptions of the options currently covered by dynamic configuration: Disable or enable multi-tenancy - Administrators can disable and enable multi-tenancy dynamically. Disabling multi-tenancy does not pose a risk of data loss. If and when an administrator chooses to reenable tenancy, all previously saved objects are preserved and made available. The default is true.\nThis setting does not have an impact on the global tenant, which always remains enabled. Disable or enable private tenant - This option allows administrators to disable and enable private tenants. As with the enable multi-tenancy setting, when private tenants are reenabled all previously saved objects are preserved and made available. Default tenant - This option allows an administrator to choose either a global, private, or custom tenant as the default when users log in. In cases where a user doesn’t have access to the default tenant (for example, if a custom tenant unavailable to the user was specified as the default), the default transitions to the preferred tenant, which is specified by the opensearch_security.multitenancy.tenants.preferred setting in the opensearch-dashboards.yml file. See Multi-tenancy configuration for more information about this setting.\nDepending on the specific changes made to multi-tenancy using dynamic configuration, some users may be logged out of their Dashboards session once the changes are saved. For example, if an admin user disables multi-tenancy, users with either a private or custom tenant as their selected tenant will be logged out and will need to log back in. Similarly, if an admin user disables private tenants, users with the private tenant selected will be logged out and will need to log back in.\nThe global tenant, however, is a special case. Because this tenant is never disabled, users with the global tenant selected as their active tenant will experience no interruption to their session. Furthermore, changing the default tenant has no impact on a user’s session.\nConfiguring multi-tenancy in OpenSearch Dashboards\nTo configure multi-tenancy in Dashboards, follow these steps:\nBegin by selecting Security in the Dashboards home page menu. Then select Tenancy from the Security menu on the left side of the screen. The Multi-tenancy page is displayed.\nBy default, the Manage tab is displayed. Select the Configure tab to display the dynamic settings for multi-tenancy.\nIn the Multi-tenancy field, select the Enable tenancy check box to enable multi-tenancy. Clear the check box to disable the feature. The default is true.\nIn the Tenants field, you can enable or disable private tenants for users. By default the check box is selected and the feature is enabled.\nIn the Default tenant field, use the dropdown menu to select a default tenant. The menu includes Global, Private, and any other custom tenants that are available to users.\nAfter making your preferred changes, select Save changes in the lower right corner of the window. A pop-up window appears listing the configuration items you’ve changed and asks you to review your changes.\nSelect the check boxes beside the items you want to confirm and then select Apply changes. The changes are implemented dynamically.\nConfiguring multi-tenancy with the REST API\nIn addition to using the Dashboards interface, you can manage dynamic configurations using the REST API.\nGet tenancy configuration\nThe GET call retrieves settings for the dynamic configuration: GET /_plugins/_security/api/tenancy/config copy Example response { \"mulitenancy_enabled\": true, \"private_tenant_enabled\": true, \"default_tenant\": \"global tenant\" } Update tenant configuration\nThe PUT call updates settings for dynamic configuration: PUT /_plugins/_security/api/tenancy/config { \"default_tenant\": \"custom tenant 1\", \"private_tenant_enabled\": false, \"mulitenancy_enabled\": true } copy Example response { \"mulitenancy_enabled\": true, \"private_tenant_enabled\": false, \"default_tenant\": \"custom tenant 1\" } Dashboardsinfo API\nYou can also use the Dashboardsinfo API to retrieve the status of multi-tenancy settings for the user logged in to Dashboards: GET /_plugins/_security/dashboardsinfo copy Example response { \"user_name\": \"admin\", \"not_fail_on_forbidden_enabled\": false, \"opensearch_dashboards_mt_enabled\": true, \"opensearch_dashboards_index\": \".kibana\", \"opensearch_dashboards_server_user\": \"kibanaserver\", \"multitenancy_enabled\": true, \"private_tenant_enabled\": true, \"default_tenant\": \"Private\" }",
    "ancestors": [
      "Security in OpenSearch",
      "OpenSearch Dashboards multi-tenancy"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/multi-tenancy/mt-agg-view/",
    "title": "Multi-tenancy aggregate view for saved objects",
    "content": "Aggregate view for saved objects is an experimental feature released in OpenSearch 2.4. Therefore, we do not recommend enabling the feature in a production environment at this time. For updates on the progress of aggregate view for saved objects, or if you’d like to leave feedback that could help improve the feature, see the Dashboards object sharing GitHub issue. For a more comprehensive view of the proposed future development of multi-tenancy, see the Dashboards object sharing issue.\nAggregate view for saved objects allows a user who has access to multiple tenants to see all saved objects associated with those tenants in a single view without having to switch between tenants to do so. This includes both tenants created by the user and tenants shared with the user. Aggregate view introduces a Tenant dropdown menu and column in the Saved Objects table that gives the user the option to filter by tenants and make visible their associated saved objects.\nOnce you identify a saved object of interest, you can then switch to that tenant to work with the object.\nTo access saved objects, expand the top menu and select Management &gt; Stack Management &gt; Saved Objects. The Saved Objects window opens. By default, all tenants the user has permissions for are displayed along with all saved objects associated with the tenants. As an experimental feature, aggregate view for saved objects is kept behind a feature flag and must be enabled in the opensearch_dashboards.yml file before the feature is made available. See Enabling aggregate view for more information.\nFeature benefits\nImplementing an aggregate view for all saved objects on one screen allows you to quickly locate an object of interest and determine which tenant is associated with it. Once you locate an object, you can select the appropriate tenant and work with the object.\nThis feature also adds a Tenant dropdown menu to the Saved Objects table, which allows you to filter the view by tenants and their associated saved objects.\nPlans for future development\nIn subsequent releases, we plan to expand the functionality of this feature to include the ability to perform actions directly from aggregate view and share items without having to first select a specific tenant. In the longer term, OpenSearch plans to evolve multi-tenancy so that it becomes a much more flexible tool for sharing objects among users and employs a more sophisticated way of assigning the roles and permissions that facilitate sharing. To learn more about the features being proposed for future releases, see the GitHub issue Dashboards object sharing.\nKnown limitations\nIn this first experimental phase of development, there are some limitations that should be observed before enabling the feature and using it in a test environment:\nThe feature can only be used in a new cluster. At this time, the feature is not supported by clusters already in use.\nAlso, the feature should be used only in a test environment, not in production.\nFinally, once the feature has been enabled and used in a test cluster, the feature cannot be disabled for the cluster. Disabling the feature once it has been used to work with tenants and saved objects can result in the loss of saved objects and can have an impact on tenant-to-tenant functionality. This can occur when disabling the feature in any one of three ways: disabling the aggregate view feature with the feature flag; disabling multi-tenancy with the traditional multi-tenancy configuration setting; or disabling multi-tenancy with dynamic configuration settings.\nThese limitations will be addressed in upcoming releases.\nEnabling aggregate view for saved objects\nBy default, the aggregate view in the Saved Objects table is disabled. To enable the feature, add the opensearch_security.multitenancy.enable_aggregation_view flag to the opensearch_dashboards.yml file and set it to true: opensearch_security.multitenancy.enable_aggregation_view: true After enabling the feature you can start the new cluster and then launch Dashboards.\nWorking in aggregate view\nSelect the Tenant dropdown arrow to display the list of tenants available to the user. You can select multiple tenants while the menu is open. Each time you select a tenant in the menu, the list of saved objects is filtered by that tenant and any others with a check mark beside their name. After you finish specifying tenants, select anywhere outside the menu to collapse it.\nThe Title column displays the names of the available saved objects.\nThe Tenant column displays the tenants associated with the saved objects.\nAlso, the number of tenants selected for filtering is shown in a red box beside the Tenant dropdown menu label. Use the Type dropdown menu to filter saved objects by type. The behavior of the Type dropdown menu is the same as the behavior of the Tenant dropdown menu.\nSelecting and working with a saved object\nAfter identifying a saved object that you would like to work with, follow these steps to access the object:\nNote the tenant associated with the object in the Tenant column.\nIn the upper-right corner of the window, open the user menu and select Switch tenants. In the Select your tenant window, choose either the Global or Private option, or one of the custom tenant options, to specify the correct tenant. Select the Confirm button. The tenant becomes active and is displayed in the user menu.\nAfter the tenant is active, you can use the controls in the Actions column to work with saved objects associated with the tenant. When a tenant is not active, you cannot use the Actions column controls to work with its associated objects. To work with those objects, follow the preceding steps to make the tenant active.",
    "ancestors": [
      "Security in OpenSearch",
      "OpenSearch Dashboards multi-tenancy"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/multi-tenancy/multi-tenancy-config/",
    "title": "Multi-tenancy configuration",
    "content": "Multi-tenancy is enabled by default, but you can disable it or change its settings using config/opensearch-security/config.yml: config: dynamic: kibana: multitenancy_enabled: true private_tenant_enabled: true default_tenant: global tenant server_username: kibanaserver index: '.kibana' do_not_fail_on_forbidden: false Setting Description multitenancy_enabled Enable or disable multi-tenancy. Default is true. private_tenant_enabled Enable or disable the private tenant. Default is true. default_tenant Use to set the tenant that is available when users log in. server_username Must match the name of the OpenSearch Dashboards server user from opensearch_dashboards.yml. Default is kibanaserver. index Must match the name of the OpenSearch Dashboards index from opensearch_dashboards.yml. Default is.kibana. do_not_fail_on_forbidden When true, the Security plugin removes any content that a user is not allowed to see from the search results. When false, the plugin returns a security exception. Default is false. The opensearch_dashboards.yml file includes additional settings: opensearch.username: kibanaserver opensearch.password: kibanaserver opensearch.requestHeadersAllowlist: [ \" securitytenant\", \" Authorization\"] opensearch_security.multitenancy.enabled: true opensearch_security.multitenancy.tenants.enable_global: true opensearch_security.multitenancy.tenants.enable_private: true opensearch_security.multitenancy.tenants.preferred: [ \" Private\", \" Global\"] opensearch_security.multitenancy.enable_filter: false Setting Description opensearch.requestHeadersAllowlist OpenSearch Dashboards requires that you add all HTTP headers to the allow list so that the headers pass to OpenSearch. Multi-tenancy uses a specific header, securitytenant, that must be present with the standard Authorization header. If the securitytenant header is not on the allow list, OpenSearch Dashboards starts with a red status. opensearch_security.multitenancy.enabled Enables or disables multi-tenancy in OpenSearch Dashboards. Default is true. opensearch_security.multitenancy.tenants.enable_global Enables or disables the global tenant. Default is true. opensearch_security.multitenancy.tenants.enable_private Enables or disables private tenants. Default is true. opensearch_security.multitenancy.tenants.preferred Lets you change ordering in the Tenants tab of OpenSearch Dashboards. By default, the list starts with Global and Private (if enabled) and then proceeds alphabetically. You can add tenants here to move them to the top of the list. opensearch_security.multitenancy.enable_filter If you have many tenants, you can add a search bar to the top of the list. Default is false. Add tenants\nTo create tenants, use OpenSearch Dashboards, the REST API, or tenants.yml.\nOpenSearch Dashboards\nOpen OpenSearch Dashboards.\nChoose Security, Tenants, and Create tenant.\nGive the tenant a name and description.\nChoose Create.\nREST API\nSee Create tenant.\ntenants.yml --- _meta: type: \" tenants\" config_version: 2 ## Demo tenants admin_tenant: reserved: false description: \" Demo tenant for admin user\" Give roles access to tenants\nAfter creating a tenant, give a role access to it using OpenSearch Dashboards, the REST API, or roles.yml.\nRead-write ( kibana_all_write) permissions let the role view and modify objects in the tenant.\nRead-only ( kibana_all_read) permissions let the role view objects, but not modify them.\nOpenSearch Dashboards\nOpen OpenSearch Dashboards.\nChoose Security, Roles, and a role.\nFor Tenant permissions, add tenants, press Enter, and give the role read and/or write permissions to it.\nREST API\nSee Create role.\nroles.yml --- test-role: reserved: false hidden: false cluster_permissions: - \" cluster_composite_ops\" - \" indices_monitor\" index_permissions: - index_patterns: - \" movies*\" dls: \" \" fls: [] masked_fields: [] allowed_actions: - \" read\" tenant_permissions: - tenant_patterns: - \" human_resources\" allowed_actions: - \" kibana_all_read\" static: false _meta: type: \" roles\" config_version: 2 Manage OpenSearch Dashboards indices\nThe open source version of OpenSearch Dashboards saves all objects to a single index:.kibana. The Security plugin uses this index for the global tenant, but separate indices for every other tenant. Each user also has a private tenant, so you might see a large number of indices that follow two patterns:.kibana_&lt;hash&gt;_&lt;tenant_name&gt;.kibana_&lt;hash&gt;_&lt;username&gt; The Security plugin scrubs these index names of special characters, so they might not be a perfect match of tenant names and usernames.\nTo back up your OpenSearch Dashboards data, take a snapshot of all tenant indexes using an index pattern such as.kibana*.",
    "ancestors": [
      "Security in OpenSearch",
      "OpenSearch Dashboards multi-tenancy"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security/multi-tenancy/tenant-index/",
    "title": "OpenSearch Dashboards multi-tenancy",
    "content": "Tenants in OpenSearch Dashboards are spaces for saving index patterns, visualizations, dashboards, and other OpenSearch Dashboards objects. OpenSearch allows users to create multiple tenants for multiple uses. Tenants are useful for safely sharing your work with other OpenSearch Dashboards users. You can control which roles have access to a tenant and whether those roles have read or write access. By default, all OpenSearch Dashboards users have access to two independent tenants: the global tenant and a private tenant. Multi-tenancy also provides the option to create custom tenants. Global – This tenant is shared between every OpenSearch Dashboards user. It does allow for sharing objects among users who have access to it. Private – This tenant is exclusive to each user and can’t be shared. It does not allow you to access routes or index patterns created by the user’s global tenant. Custom – Administrators can create custom tenants and assign them to specific roles. Once created, these tenants can then provide spaces for specific groups of users.\nThe global tenant is not a primary tenant in the sense that it replicates its content in a private tenant. To the contrary, if you make a change to your global tenant, you won’t see that change reflected in your private tenant. Some example changes include the following:\nChange advanced settings\nCreate visualizations\nCreate index patterns\nTo provide a practical example, you might use the private tenant for exploratory work, create detailed visualizations with your team in an analysts tenant, and maintain a summary dashboard for corporate leadership in an executive tenant.\nIf you share a visualization or dashboard with someone, you can see that the URL includes the tenant: http://&lt;opensearch_dashboards_host&gt;:5601/app/opensearch-dashboards?security_tenant=analysts#/visualize/edit/c501fa50-7e52-11e9-ae4e-b5d69947d32e?_g=() Next steps\nTo get started with tenants, see Multi-tenancy configuration for information about enabling multi-tenancy, adding tenants, and assigning roles to tenants.\nFor information about making dynamic changes to the multi-tenancy configuration, see Dynamic configuration in OpenSearch Dashboards.",
    "ancestors": [
      "Security in OpenSearch"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/api-tools/alert-finding-api/",
    "title": "Alerts and findings APIs",
    "content": "The following APIs can be used for tasks related to alerts and findings.\nGet Alerts\nProvides an option for retrieving alerts related to a specific detector type or detector ID.\nParameters\nYou can specify the following parameters when requesting an alert. Parameter Description detectorId The ID of the detector used to fetch alerts. Optional when the detectorType is specified. Otherwise required. detectorType The type of detector used to fetch alerts. Optional when the detectorId is specified. Otherwise required. severityLevel Used to filter by alert severity level. Optional. alertState Used to filter by alert state. Possible values: ACTIVE, ACKNOWLEDGED, COMPLETED, ERROR, DELETED. Optional. sortString This field specifies which string Security Analytics uses to sort the alerts. Optional. sortOrder The order used to sort the list of findings, either ascending or descending. Optional. missing A list of fields for which there are no found alias mappings. Optional. size An optional limit for the maximum number of results returned in the response. Optional. startIndex The pagination indicator. Optional. searchString The alert attribute you want returned in the search. Optional. Example request GET /_plugins/_security_analytics/alerts?detectorType=windows Example response { \"alerts\": [{ \"detector_id\": \"detector_12345\", \"id\": \"alert_id_1\", \"version\": -3, \"schema_version\": 0, \"trigger_id\": \"trigger_id_1\", \"trigger_name\": \"my_trigger\", \"finding_ids\": [ \"finding_id_1\"], \"related_doc_ids\": [ \"docId1\"], \"state\": \"ACTIVE\", \"error_message\": null, \"alert_history\": [], \"severity\": null, \"action_execution_results\": [{ \"action_id\": \"action_id_1\", \"last_execution_time\": 1665693544996, \"throttled_count\": 0 }], \"start_time\": \"2022-10-13T20:39:04.995023Z\", \"last_notification_time\": \"2022-10-13T20:39:04.995028Z\", \"end_time\": \"2022-10-13T20:39:04.995027Z\", \"acknowledged_time\": \"2022-10-13T20:39:04.995028Z\" }], \"total_alerts\": 1, \"detectorType\": \"windows\" } Response fields\nAlerts persist until you resolve the root cause and have the following states: State Description ACTIVE The alert is ongoing and unacknowledged. Alerts remain in this state until you acknowledge them, delete the trigger associated with the alert, or delete the monitor entirely. ACKNOWLEDGED Someone has acknowledged the alert but not fixed the root cause. COMPLETED The alert is no longer ongoing. Alerts enter this state after the corresponding trigger evaluates to false. ERROR An error occurred while executing the trigger. This error is usually the result of a bad trigger or destination. DELETED Someone deleted the detector or trigger associated with this alert while the alert was ongoing. Acknowledge Alerts\nExample request POST /_plugins/_security_analytics/&lt;detector_id&gt;/_acknowledge/alerts { \"alerts\":[ \"4dc7f5a9-2c82-4786-81ca-433a209d5205\"]} Example response { \"acknowledged\": [ { \"detector_id\": \"8YT5fYQBZ8IUM4axics6\", \"id\": \"4dc7f5a9-2c82-4786-81ca-433a209d5205\", \"version\": 1, \"schema_version\": 4, \"trigger_id\": \"1TP5fYQBMkkIGY6Pg-q8\", \"trigger_name\": \"test-trigger\", \"finding_ids\": [ \"2e167f4b-8063-40ef-80f8-2afd9bf095b8\"], \"related_doc_ids\": [ \"1|windows\"], \"state\": \"ACTIVE\", \"error_message\": null, \"alert_history\": [], \"severity\": \"1\", \"action_execution_results\": [ { \"action_id\": \"BopdoIJKXd\", \"last_execution_time\": 1668560817925, \"throttled_count\": 0 }], \"start_time\": \"2022-11-16T01:06:57.748Z\", \"last_notification_time\": \"2022-11-16T01:06:57.748Z\", \"end_time\": null, \"acknowledged_time\": null }], \"failed\": [], \"missing\": [] } Get Findings\nThe Get findings API based on detector attributes.\nExample request GET /_plugins/_security_analytics/findings/_search?*detectorType*= { \"total_findings\": 2, \"findings\":[ { \"detectorId\": \"12345\", \"id\": \"2b9663f4-ae77-4df8-b84f-688a0195723b\", \"related_doc_ids\":[ \"5\"], \"index\": \"sbwhrzgdlg\", \"queries\":[ { \"id\": \"f1bff160-587b-4500-b60c-ab22c7abc652\", \"name\": \"3\", \"query\": \"test_field: \\\" us-west-2 \\\" \", \"tags\":[] }], \"timestamp\": 1664401088804, \"document_list\":[ { \"index\": \"sbwhrzgdlg\", \"id\": \"5\", \"found\": true, \"document\": \"{ \\n \\\" message \\\": \\\" This is an error from IAD region \\\", \\n \\\" test_strict_date_time \\\": \\\" 2022-09-28T21:38:02.888Z \\\", \\n \\\" test_field \\\": \\\" us-west-2 \\\"\\n }\" }] }, { \"detectorId\": \"12345\", \"id\": \"f43a2701-0ef5-4931-8254-bdf510f73952\", \"related_doc_ids\":[ \"1\"], \"index\": \"sbwhrzgdlg\", \"queries\":[ { \"id\": \"f1bff160-587b-4500-b60c-ab22c7abc652\", \"name\": \"3\", \"query\": \"test_field: \\\" us-west-2 \\\" \", \"tags\":[] }], \"timestamp\": 1664401088746, \"document_list\":[ { \"index\": \"sbwhrzgdlg\", \"id\": \"1\", \"found\": true, \"document\": \"{ \\n \\\" message \\\": \\\" This is an error from IAD region \\\", \\n \\\" test_strict_date_time \\\": \\\" 2022-09-28T21:38:02.888Z \\\", \\n \\\" test_field \\\": \\\" us-west-2 \\\"\\n }\" }] }] }",
    "ancestors": [
      "Security analytics",
      "API tools"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/api-tools/detector-api/",
    "title": "Detector APIs",
    "content": "The following APIs can be used for a number of tasks related to detectors, from creating detectors to updating and searching for detectors. Many API calls use the detector ID in the request, which can be retrieved using the Search Detector API.\nCreate Detector API\nCreates a new detector. POST _plugins/_security_analytics/detectors Request fields\nYou can specify the following fields when creating a detector. Field Type Description enabled Boolean\nSets the detector as either active (true) or inactive (false). Default is true when a new detector is created. Required. name String\nName of the detector. Name should only consist of upper and lowercase letters, numbers 0-9, hyphens, spaces, and underscores. Use between 5 and 50 characters. Required. detector_type String\nThe log type that defines the detector. Options are linux, network, windows, ad_ldap, apache_access, cloudtrail, dns, and s3. Required. schedule Object\nThe schedule that determines how often the detector runs. For information on specifying fixed intervals in the API, see Cron expression reference. schedule.period Object\nDetails for the frequency of the schedule. schedule.period.interval Integer\nThe interval at which the detector runs. schedule.period.unit String\nThe interval’s unit of time. inputs Object\nDetector inputs. inputs.detector_input Array\nAn array that contains the index and definition used to create the detector. Currently, only one log data source is allowed for the detector. inputs.detector_input.description String\nDescription of the detector. Optional. inputs.detector_input.custom_rules Array\nDetector inputs for custom rules. At least one rule must be specified for a detector. Optional if pre-packaged rules are specified. inputs.detector_input.custom_rules.id String\nA valid rule ID for the custom rule generated by the user. Valid rules are formatted as a globally, or, universally unique identifier (UUID) See Universally unique identifier for more information. inputs.detector_input.indices Array\nThe log data source used for the detector, which can be either an index name or index pattern. Currently, only one entry is supported with plans to support multiple indexes in a future release. Required. inputs.detector_input.pre_packaged_rules Array\nDetector inputs for pre-packaged rules (as opposed to custom rules). At least one rule must be specified for a detector. Optional if custom rules are specified. inputs.detector_input.pre_packaged_rules.id String\nThe rule ID for pre-packaged rules. See Search Pre-Packaged Rules for information on how to use the API to search for rules and obtain rule IDs in results. triggers Array\nTrigger settings for alerts. triggers.ids Array\nA list of rule IDs that become part of the trigger condition. triggers.tags Array\nTags are specified in a security rule. Tags can then be selected and applied to the alert trigger to focus the trigger conditions for alerts. See an example of how tags are used in a Sigma rule in Sigma’s Rule Creation Guide. triggers.id String\nThe unique ID for the trigger. triggers.sev_levels Array\nSigma rule severity levels: informational; low; medium; high; criticial. See Level in the Sigma Rule Creation Guide. triggers.name String\nThe name of the trigger. Name should only consist of upper and lowercase letters, numbers 0-9, hyphens, spaces, and underscores. Use between 5 and 50 characters. Required. triggers.severity Integer\nSeverity level for the trigger expressed as an integer: 1 = highest; 2 = high; 3 = medium; 4 = low; 5 = lowest. Trigger severity is part of the alert definition. triggers.actions Object\nActions send notifications when trigger conditions are met. Optional, as a notification message is not required as part of an alert. triggers.actions.id String\nUnique ID for the action. User generated. triggers.actions.destination_id String\nUnique ID for the notification destination. User generated. triggers.actions.subject_template Object\nContains the information for the subject field of the notification message. Optional. triggers.actions.subject_template.source String\nThe subject for the notification message. triggers.actions.subject_template.lang String\nThe scripting language used to define the subject. Must be Mustache. See the Mustache Manual for more information about templates. triggers.actions.name String\nName for the trigger alert. Name should only consist of upper and lowercase letters, numbers 0-9, hyphens, spaces, and underscores. Use between 5 and 50 characters. triggers.actions.message_template String\nContains the information for the body of the notification message. Optional. triggers.actions.message_template.source String\nThe body of the notification message. triggers.actions.message_template.lang String\nThe scripting language used to define the message. Must be Mustache. triggers.actions.throttle_enabled Boolean\nEnables throttling for alert notifications. Optional. Default is false. triggers.actions.throttle Object\nThrottling limits the number of notifications you receive within a given span of time. triggers.actions.throttle.unit String\nUnit of time for throttling. triggers.actions.throttle.value Integer\nThe value for the unit of time. Example request POST _plugins/_security_analytics/detectors { \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"detector_type\": \"WINDOWS\", \"type\": \"detector\", \"inputs\": [ { \"detector_input\": { \"description\": \"windows detector for security analytics\", \"custom_rules\": [ { \"id\": \"bc2RB4QBrbtylUb_1Pbm\" }], \"indices\": [ \"windows\"], \"pre_packaged_rules\": [ { \"id\": \"06724a9a-52fc-11ed-bdc3-0242ac120002\" }] } }], \"triggers\": [ { \"ids\": [ \"06724a9a-52fc-11ed-bdc3-0242ac120002\"], \"types\": [], \"tags\": [ \"attack.defense_evasion\"], \"severity\": \"1\", \"actions\": [{ \"id\": \"hVTLkZYzlA\", \"destination_id\": \"6r8ZBoQBKW_6dKriacQb\", \"subject_template\": { \"source\": \"Trigger: \", \"lang\": \"mustache\" }, \"name\": \"hello_world\", \"throttle_enabled\": false, \"message_template\": { \"source\": \"Detector just entered alert status. Please investigate the issue.\" + \"- Trigger: \" + \"- Severity: \", \"lang\": \"mustache\" }, \"throttle\": { \"unit\": \"MINUTES\", \"value\": 108 } }], \"id\": \"8qhrBoQBYK1JzUUDzH-N\", \"sev_levels\": [], \"name\": \"test-trigger\" }], \"name\": \"nbReFCjlfn\" } copy Example response { \"_id\": \"dc2VB4QBrbtylUb_Hfa3\", \"_version\": 1, \"detector\": { \"name\": \"nbReFCjlfn\", \"detector_type\": \"windows\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"detector_input\": { \"description\": \"windows detector for security analytics\", \"indices\": [ \"windows\"], \"custom_rules\": [ { \"id\": \"bc2RB4QBrbtylUb_1Pbm\" }], \"pre_packaged_rules\": [ { \"id\": \"06724a9a-52fc-11ed-bdc3-0242ac120002\" }] } }], \"triggers\": [ { \"id\": \"8qhrBoQBYK1JzUUDzH-N\", \"name\": \"test-trigger\", \"severity\": \"1\", \"types\": [], \"ids\": [ \"06724a9a-52fc-11ed-bdc3-0242ac120002\"], \"sev_levels\": [], \"tags\": [ \"attack.defense_evasion\"], \"actions\": [ { \"id\": \"hVTLkZYzlA\", \"name\": \"hello_world\", \"destination_id\": \"6r8ZBoQBKW_6dKriacQb\", \"message_template\": { \"source\": \"Trigger: \", \"lang\": \"mustache\" }, \"throttle_enabled\": false, \"subject_template\": { \"source\": \"Detector just entered alert status. Please investigate the issue.\" + \"- Trigger: \" + \"- Severity: \", \"lang\": \"mustache\" }, \"throttle\": { \"value\": 108, \"unit\": \"MINUTES\" } }] }], \"last_update_time\": \"2022-10-24T01:22:03.738379671Z\", \"enabled_time\": \"2022-10-24T01:22:03.738376103Z\" } } Update Detector API\nThe Update Detector API can be used to update a detector definition. It requires the detector ID to specify the detector. PUT /_plugins/_security_analytics/detectors/&lt;detector_Id&gt; Request fields\nYou can specify the following fields when updating a detector. Field Type Description detector_type String\nThe log type that defines the detector. Options are linux, network, windows, ad_ldap, apache_access, cloudtrail, dns, and s3. name String\nThe name of the detector. Name should only consist of upper and lowercase letters, numbers 0-9, hyphens, spaces, and underscores. Use between 5 and 50 characters. Required. enabled Boolean\nSets the detector as either Active (true) or Inactive (false). schedule.period.interval Integer\nThe interval at which the detector runs. schedule.period.unit String\nThe interval’s unit of time. inputs.input.description String\nDescription of the detector. inputs.input.indices Array\nThe log data source used for the detector. Only one source is allowed at this time. inputs.input.rules.id Array\nA list of security rules for the detector definition. triggers.sev_levels Array\nSigma rule severity levels: informational; low; medium; high; criticial. See Level in the Sigma Rule Creation Guide. triggers.tags Array\nTags are specified in a security rule. Tags can then be selected and applied to the alert trigger to focus the trigger conditions for alerts. See an example of how tags are used in a Sigma rule in Sigma’s Rule Creation Guide. triggers.actions Object\nActions send notifications when trigger conditions are met. See trigger actions for Create Detector API. Example request PUT /_plugins/_security_analytics/detectors/J 1 RX 1 IMByX 0 LvTiGTddR { \"type\": \"detector\", \"detector_type\": \"windows\", \"name\": \"windows_detector\", \"enabled\": true, \"createdBy\": \"chip\", \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"input\": { \"description\": \"windows detector for security analytics\", \"indices\": [ \"windows\"], \"custom_rules\": [], \"pre_packaged_rules\": [ { \"id\": \"73a883d0-0348-4be4-a8d8-51031c2564f8\" }, { \"id\": \"1a4bd6e3-4c6e-405d-a9a3-53a116e341d4\" }] } }], \"triggers\": [ { \"sev_levels\": [], \"tags\": [], \"actions\": [], \"types\": [ \"windows\"], \"name\": \"test-trigger\", \"id\": \"fyAy1IMBK2A1DZyOuW_b\" }] } copy Example response { \"_id\": \"J1RX1IMByX0LvTiGTddR\", \"_version\": 1, \"detector\": { \"name\": \"windows_detector\", \"detector_type\": \"windows\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"detector_input\": { \"description\": \"windows detector for security analytics\", \"indices\": [ \"windows\"], \"rules\": [ { \"id\": \"LFRY1IMByX0LvTiGZtfh\" }] } }], \"triggers\": [], \"last_update_time\": \"2022-10-14T02:36:32.909581688Z\", \"enabled_time\": \"2022-10-14T02:33:34.197Z\" } } Response fields Field Type Description _version String\nVersion number for the update. detector.last_update_time String\nDate and time of the last update. detector.enabled_time String\nDate and time when the detector was last enabled. Delete Detector API\nThis API uses the detector ID to specify and delete a detector.\nPath and HTTP methods DELETE /_plugins/_security_analytics/detectors/IJAXz 4 QBrmVplM 4 JYxx_ Example request DELETE /_plugins/_security_analytics/detectors/&lt;detector Id&gt; copy Example response { \"_id\": \"IJAXz4QBrmVplM4JYxx_\", \"_version\": 1 } Get Detector API\nThe Get Detector API retrieves the detector details. Use the detector ID in the call to fetch detector details.\nPath and HTTP methods GET /_plugins/_security_analytics/detectors/x-dwFIYBT 6 _n 8 WeuQjo 4 Example request GET /_plugins/_security_analytics/detectors/&lt;detector Id&gt; copy Example response { \"_id\": \"x-dwFIYBT6_n8WeuQjo4\", \"_version\": 1, \"detector\": { \"name\": \"DetectorTest1\", \"detector_type\": \"windows\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"detector_input\": { \"description\": \"Test and delete\", \"indices\": [ \"windows1\"], \"custom_rules\": [], \"pre_packaged_rules\": [ { \"id\": \"847def9e-924d-4e90-b7c4-5f581395a2b4\" }] } }], \"last_update_time\": \"2023-02-02T23:22:26.454Z\", \"enabled_time\": \"2023-02-02T23:22:26.454Z\" } } Search Detector API\nThe Search Detector API searches for detector matches by detector ID, detector name, or detector type.\nRequest fields Field Type Description _id String\nVersion number for the update. detector.name String\nName of the detector. detector_type String\nThe log type for the detector. Options are linux, network, windows, ad_ldap, apache_access, cloudtrail, dns, and s3. Example request Detector ID POST /_plugins/_security_analytics/detectors/_search { \"query\": { \"match\": { \"_id\": \"MFRg1IMByX0LvTiGHtcN\" } } } copy Detector name POST /_plugins/_security_analytics/detectors/_search { \"size\": 30, \"query\": { \"nested\": { \"path\": \"detector\", \"query\": { \"bool\": { \"must\": [ { \"match\": { \"detector.name\": \"DetectorTest1\" } }] } } } } } copy Example response { \"took\": 0, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 3.671739, \"hits\": [ { \"_index\": \".opensearch-sap-detectors-config\", \"_id\": \"x-dwFIYBT6_n8WeuQjo4\", \"_version\": 1, \"_seq_no\": 76, \"_primary_term\": 17, \"_score\": 3.671739, \"_source\": { \"type\": \"detector\", \"name\": \"DetectorTest1\", \"detector_type\": \"windows\", \"enabled\": true, \"enabled_time\": 1675380146454, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"detector_input\": { \"description\": \"Test and delete\", \"indices\": [ \"windows1\"], \"custom_rules\": [], \"pre_packaged_rules\": [ { \"id\": \"847def9e-924d-4e90-b7c4-5f581395a2b4\" }] } }], \"triggers\": [ { \"id\": \"w-dwFIYBT6_n8WeuQToW\", \"name\": \"trigger 1\", \"severity\": \"1\", \"types\": [ \"windows\"], \"ids\": [ \"847def9e-924d-4e90-b7c4-5f581395a2b4\"], \"sev_levels\": [ \"critical\"], \"tags\": [ \"attack.t1003.002\"], \"actions\": [ { \"id\": \"\", \"name\": \"Triggered alert condition: - Severity: 1 (Highest) - Threat detector: DetectorTest1\", \"destination_id\": \"\", \"message_template\": { \"source\": \"\"\"Triggered alert condition:\nSeverity: 1 (Highest)\nThreat detector: DetectorTest1\nDescription: Test and delete\nDetector data sources:\nwindows1\"\"\", \"lang\": \"mustache\" }, \"throttle_enabled\": false, \"subject_template\": { \"source\": \"Triggered alert condition: - Severity: 1 (Highest) - Threat detector: DetectorTest1\", \"lang\": \"mustache\" }, \"throttle\": { \"value\": 10, \"unit\": \"MINUTES\" } }] }], \"last_update_time\": 1675380146454, \"monitor_id\": [ \"xOdwFIYBT6_n8WeuQToa\"], \"bucket_monitor_id_rule_id\": { \"-1\": \"xOdwFIYBT6_n8WeuQToa\" }, \"rule_topic_index\": \".opensearch-sap-windows-detectors-queries\", \"alert_index\": \".opensearch-sap-windows-alerts\", \"alert_history_index\": \".opensearch-sap-windows-alerts-history\", \"alert_history_index_pattern\": \"&lt;.opensearch-sap-windows-alerts-history-{now/d}-1&gt;\", \"findings_index\": \".opensearch-sap-windows-findings\", \"findings_index_pattern\": \"&lt;.opensearch-sap-windows-findings-{now/d}-1&gt;\" } }] } }",
    "ancestors": [
      "Security analytics",
      "API tools"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/api-tools/index/",
    "title": "API tools",
    "content": "Security Analytics includes a number of APIs to help administrators maintain and update an implementation. The APIs often mimic the same controls available for setting up Security Analytics in OpenSearch Dashboards, and they provide another option for administering the plugin.\nThe APIs for Security Analytics are separated into the following categories: Detector APIs Rules APIs Mappings APIs Alerts and findings APIs",
    "ancestors": [
      "Security analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/api-tools/mappings-api/",
    "title": "Mappings APIs",
    "content": "The following APIs can be used for a number of tasks related to mappings, from creating to getting and updating mappings.\nGet Mappings View\nExample request GET /_plugins/_security_analytics/mappings/view { \"index_name\": \"windows\", \"rule_topic\": \"windows\" } Example response { \"properties\": { \"windows-event_data-CommandLine\": { \"path\": \"CommandLine\", \"type\": \"alias\" }, \"event_uid\": { \"path\": \"EventID\", \"type\": \"alias\" } }, \"unmapped_index_fields\": [ \"windows-event_data-CommandLine\", \"unmapped_HiveName\", \"src_ip\", \"sha1\", \"processPath\", \"CallerProcessName\", \"CallTrace\", \"AuthenticationPackageName\", \"AuditSourceName\", \"AuditPolicyChanges\", \"AttributeValue\", \"AttributeLDAPDisplayName\", \"ApplicationPath\", \"Application\", \"AllowedToDelegateTo\", \"Address\", \"Action\", \"AccountType\", \"AccountName\", \"Accesses\", \"AccessMask\", \"AccessList\"] } Create Mappings\nExample request POST /_plugins/_security_analytics/mappings { \"index_name\": \"windows\", \"rule_topic\": \"windows\", \"partial\": true, \"alias_mappings\": { \"properties\": { \"event_uid\": { \"type\": \"alias\", \"path\": \"EventID\" } } } } Example response { \"acknowledged\": true } Get Mappings\nExample request GET /_plugins/_security_analytics/mappings Example response { \"windows\": { \"mappings\": { \"properties\": { \"windows-event_data-CommandLine\": { \"type\": \"alias\", \"path\": \"CommandLine\" }, \"event_uid\": { \"type\": \"alias\", \"path\": \"EventID\" } } } } } Update Mappings\nExample request PUT /_plugins/_security_analytics/mappings { \"index_name\": \"windows\", \"field\": \"CommandLine\", \"alias\": \"windows-event_data-CommandLine\" } Example response { \"acknowledged\": true }",
    "ancestors": [
      "Security analytics",
      "API tools"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/api-tools/rule-api/",
    "title": "Rule APIs",
    "content": "The following APIs can be used for a number of tasks related to rules, from searching for pre-packaged rules to creating and updating custom rules.\nCreate Custom Rule\nThe Create custom rule API uses Sigma security rule formatting to create a custom rule. For information on how to write a rule in Sigma format, see information provided at Sigma’s GitHub repository. POST /_plugins/_security_analytics/rules?category=windows Example request Header: Content-Type: application/json Body: title: Moriya Rootkit id: 25b9c01c-350d-4b95-bed1-836d04a4f324 description: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report status: experimental author: Bhabesh Raj date: 2021/05/06 modified: 2021/11/30 references: - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831 tags: - attack.persistence - attack.privilege_escalation - attack.t1543.003 logsource: product: windows service: system detection: selection: Provider_Name: ' Service Control Manager' EventID: 7045 ServiceName: ZzNetSvc condition: selection level: critical falsepositives: - Unknown Example response Sample 1: { \"_id\": \"M1Rm1IMByX0LvTiGvde2\", \"_version\": 1, \"rule\": { \"category\": \"windows\", \"title\": \"Moriya Rootkit\", \"log_source\": \"\", \"description\": \"Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report\", \"tags\": [ { \"value\": \"attack.persistence\" }, { \"value\": \"attack.privilege_escalation\" }, { \"value\": \"attack.t1543.003\" }], \"references\": [ { \"value\": \"https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831\" }], \"level\": \"critical\", \"false_positives\": [ { \"value\": \"Unknown\" }], \"author\": \"Bhabesh Raj\", \"status\": \"experimental\", \"last_update_time\": \"2021-05-06T00:00:00.000Z\", \"rule\": \"title: Moriya Rootkit \\n id: 25b9c01c-350d-4b95-bed1-836d04a4f324 \\n description: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report \\n status: experimental \\n author: Bhabesh Raj \\n date: 2021/05/06 \\n modified: 2021/11/30 \\n references: \\n - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831 \\n tags: \\n - attack.persistence \\n - attack.privilege_escalation \\n - attack.t1543.003 \\n logsource: \\n product: windows \\n service: system \\n detection: \\n selection: \\n Provider_Name: 'Service Control Manager' \\n EventID: 7045 \\n ServiceName: ZzNetSvc \\n condition: selection \\n level: critical \\n falsepositives: \\n - Unknown\" } } Sample 2: { \"error\": { \"root_cause\": [ { \"type\": \"security_analytics_exception\", \"reason\": \"{ \\\" error \\\": \\\" Sigma rule must have a log source \\\", \\\" error \\\": \\\" Sigma rule must have a detection definitions \\\" }\" }], \"type\": \"security_analytics_exception\", \"reason\": \"{ \\\" error \\\": \\\" Sigma rule must have a log source \\\", \\\" error \\\": \\\" Sigma rule must have a detection definitions \\\" }\", \"caused_by\": { \"type\": \"exception\", \"reason\": \"java.util.Arrays$ArrayList: { \\\" error \\\": \\\" Sigma rule must have a log source \\\", \\\" error \\\": \\\" Sigma rule must have a detection definitions \\\" }\" } }, \"status\": 400 } Update Custom Rule (not forced)\nExample request PUT /_plugins/_security_analytics/rules/ZaFv 1 IMBdLpXWBiBa 1 XI?category=windows Content-Type: application/json Body: title: Moriya Rooskit id: 25 b 9 c 01 c -350 d -4 b 95 -bed 1-836 d 04 a 4 f 324 description: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report status: experimental author: Bhabesh Raj date: 2021 / 05 / 06 modified: 2021 / 11 / 30 references: - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/ 101831 tags: - attack.persistence - attack.privilege_escalation - attack.t 1543.003 logsource: product: windows service: system detection: selection: Provider_Name: 'Service Control Manager' EventID: 7045 ServiceName: ZzNetSvc condition: selection level: critical false positives: - Unknown Example response { \"error\": { \"root_cause\": [ { \"type\": \"security_analytics_exception\", \"reason\": \"Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Update can be forced by setting forced flag to true\" }], \"type\": \"security_analytics_exception\", \"reason\": \"Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Update can be forced by setting forced flag to true\", \"caused_by\": { \"type\": \"exception\", \"reason\": \"org.opensearch.OpenSearchStatusException: Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Update can be forced by setting forced flag to true\" } }, \"status\": 500 } Update Custom Rule (forced)\nExample request PUT /_plugins/_security_analytics/rules/ZaFv 1 IMBdLpXWBiBa 1 XI?category=windows&amp;forced= true Content-Type: application/json Body: title: Moriya Rooskit id: 25 b 9 c 01 c -350 d -4 b 95 -bed 1-836 d 04 a 4 f 324 description: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report status: experimental author: Bhabesh Raj date: 2021 / 05 / 06 modified: 2021 / 11 / 30 references: - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/ 101831 tags: - attack.persistence - attack.privilege_escalation - attack.t 1543.003 logsource: product: windows service: system detection: selection: Provider_Name: 'Service Control Manager' EventID: 7045 ServiceName: ZzNetSvc condition: selection level: critical false positives: - Unknown Example response { \"_id\": \"ZaFv1IMBdLpXWBiBa1XI\", \"_version\": 1, \"rule\": { \"category\": \"windows\", \"title\": \"Moriya Rooskit\", \"log_source\": \"\", \"description\": \"Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report\", \"tags\": [ { \"value\": \"attack.persistence\" }, { \"value\": \"attack.privilege_escalation\" }, { \"value\": \"attack.t1543.003\" }], \"references\": [ { \"value\": \"https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831\" }], \"level\": \"critical\", \"false_positives\": [ { \"value\": \"Unknown\" }], \"author\": \"Bhabesh Raj\", \"status\": \"experimental\", \"last_update_time\": \"2021-05-06T00:00:00.000Z\", \"rule\": \"title: Moriya Rooskit \\n id: 25b9c01c-350d-4b95-bed1-836d04a4f324 \\n description: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report \\n status: experimental \\n author: Bhabesh Raj \\n date: 2021/05/06 \\n modified: 2021/11/30 \\n references: \\n - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831 \\n tags: \\n - attack.persistence \\n - attack.privilege_escalation \\n - attack.t1543.003 \\n logsource: \\n product: windows \\n service: system \\n detection: \\n selection: \\n Provider_Name: 'Service Control Manager' \\n EventID: 7045 \\n ServiceName: ZzNetSvc \\n condition: selection \\n level: critical \\n falsepositives: \\n - Unknown\" } } Search Pre-Packaged Rules\nExample request POST /_plugins/_security_analytics/rules/_search?pre_packaged= true { \"from\": 0, \"size\": 20, \"query\": { \"nested\": { \"path\": \"rule\", \"query\": { \"bool\": { \"must\": [ { \"match\": { \"rule.category\": \"windows\" } }] } } } } } Example response { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1580, \"relation\": \"eq\" }, \"max_score\": 0.25863406, \"hits\": [ { \"_index\": \".opensearch-pre-packaged-rules-config\", \"_id\": \"6KFv1IMBdLpXWBiBelZg\", \"_version\": 1, \"_seq_no\": 386, \"_primary_term\": 1, \"_score\": 0.25863406, \"_source\": { \"category\": \"windows\", \"title\": \"Change Outlook Security Setting in Registry\", \"log_source\": \"registry_set\", \"description\": \"Change outlook email security settings\", \"references\": [ { \"value\": \"https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/T1137/T1137.md\" }, { \"value\": \"https://docs.microsoft.com/en-us/outlook/troubleshoot/security/information-about-email-security-settings\" }], \"tags\": [ { \"value\": \"attack.persistence\" }, { \"value\": \"attack.t1137\" }], \"level\": \"medium\", \"false_positives\": [ { \"value\": \"Administrative scripts\" }], \"author\": \"frack113\", \"status\": \"experimental\", \"last_update_time\": \"2021-12-28T00:00:00.000Z\", \"queries\": [ { \"value\": \"((TargetObject: * \\\\\\\\ SOFTWARE \\\\\\\\ Microsoft \\\\\\\\ Office \\\\\\\\ *) AND (TargetObject: * \\\\\\\\ Outlook \\\\\\\\ Security \\\\\\\\ *)) AND (EventType: \\\" SetValue \\\")\" }], \"rule\": \"title: Change Outlook Security Setting in Registry \\n id: c3cefdf4-6703-4e1c-bad8-bf422fc5015a \\n description: Change outlook email security settings \\n author: frack113 \\n date: 2021/12/28 \\n modified: 2022/03/26 \\n status: experimental \\n references: \\n - https://github.com/redcanaryco/atomic-red-team/blob/master/atomics/T1137/T1137.md \\n - https://docs.microsoft.com/en-us/outlook/troubleshoot/security/information-about-email-security-settings \\n logsource: \\n category: registry_set \\n product: windows \\n detection: \\n selection: \\n TargetObject|contains|all: \\n - ' \\\\ SOFTWARE \\\\ Microsoft \\\\ Office \\\\ ' \\n - ' \\\\ Outlook \\\\ Security \\\\ ' \\n EventType: SetValue \\n condition: selection \\n falsepositives: \\n - Administrative scripts \\n level: medium \\n tags: \\n - attack.persistence \\n - attack.t1137 \\n \" } }] } } Search Custom Rules\nExample request POST /_plugins/_security_analytics/rules/_search?pre_packaged= false Body: { \"from\": 0, \"size\": 20, \"query\": { \"nested\": { \"path\": \"rule\", \"query\": { \"bool\": { \"must\": [ { \"match\": { \"rule.category\": \"windows\" } }] } } } } } Example response { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 0.2876821, \"hits\": [ { \"_index\": \".opensearch-custom-rules-config\", \"_id\": \"ZaFv1IMBdLpXWBiBa1XI\", \"_version\": 2, \"_seq_no\": 1, \"_primary_term\": 1, \"_score\": 0.2876821, \"_source\": { \"category\": \"windows\", \"title\": \"Moriya Rooskit\", \"log_source\": \"\", \"description\": \"Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report\", \"references\": [ { \"value\": \"https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831\" }], \"tags\": [ { \"value\": \"attack.persistence\" }, { \"value\": \"attack.privilege_escalation\" }, { \"value\": \"attack.t1543.003\" }], \"level\": \"critical\", \"false_positives\": [ { \"value\": \"Unknown\" }], \"author\": \"Bhabesh Raj\", \"status\": \"experimental\", \"last_update_time\": \"2021-05-06T00:00:00.000Z\", \"queries\": [ { \"value\": \"(Provider_Name: \\\" Service_ws_Control_ws_Manager \\\") AND (event_uid: 7045) AND (ServiceName: \\\" ZzNetSvc \\\")\" }], \"rule\": \"title: Moriya Rooskit \\n id: 25b9c01c-350d-4b95-bed1-836d04a4f324 \\n description: Detects the use of Moriya rootkit as described in the securelist's Operation TunnelSnake report \\n status: experimental \\n author: Bhabesh Raj \\n date: 2021/05/06 \\n modified: 2021/11/30 \\n references: \\n - https://securelist.com/operation-tunnelsnake-and-moriya-rootkit/101831 \\n tags: \\n - attack.persistence \\n - attack.privilege_escalation \\n - attack.t1543.003 \\n logsource: \\n product: windows \\n service: system \\n detection: \\n selection: \\n Provider_Name: 'Service Control Manager' \\n EventID: 7045 \\n ServiceName: ZzNetSvc \\n condition: selection \\n level: critical \\n falsepositives: \\n - Unknown\" } }] } } Delete Custom Rule (not forced)\nExample request DELETE /_plugins/_security_analytics/rules/ZaFv 1 IMBdLpXWBiBa 1 XI Example response { \"error\": { \"root_cause\": [ { \"type\": \"security_analytics_exception\", \"reason\": \"Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Deletion can be forced by setting forced flag to true\" }], \"type\": \"security_analytics_exception\", \"reason\": \"Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Deletion can be forced by setting forced flag to true\", \"caused_by\": { \"type\": \"exception\", \"reason\": \"org.opensearch.OpenSearchStatusException: Rule with id ZaFv1IMBdLpXWBiBa1XI is actively used by detectors. Deletion can be forced by setting forced flag to true\" } }, \"status\": 500 } Delete Custom Rule (forced)\nExample request DELETE /_plugins/_security_analytics/rules/ZaFv 1 IMBdLpXWBiBa 1 XI?forced= true Example response { \"_id\": \"ZaFv1IMBdLpXWBiBa1XI\", \"_version\": 1 }",
    "ancestors": [
      "Security analytics",
      "API tools"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/index/",
    "title": "About Security Analytics",
    "content": "Security Analytics is a security information and event management (SIEM) solution for OpenSearch, designed to investigate, detect, analyze, and respond to security threats that can jeopardize the success of businesses and organizations and their online operations. These threats include the potential exposure of confidential data, cyber attacks, and other adverse security events. Security Analytics provides an out-of-the-box solution that installs automatically with any OpenSearch distribution. It includes the tools and features necessary for defining detection parameters, generating alerts, and responding effectively to potential threats.\nResources and information\nAs part of the OpenSearch Project, Security Analytics exists in the open source community and benefits from the feedback and contributions of that community. To learn more about proposals for its development, options for making contributions, and general information on the platform, see the Security Analytics repository at GitHub.\nIf you would like to leave feedback that could help improve Security Analytics, join the discussion on the OpenSearch forum.\nComponents and concepts\nSecurity Analytics includes a number of tools and features elemental to its operation. The major components that compose the plugin are summarized in the following sections.\nDetectors\nDetectors are core components that are configured to identify a range of cybersecurity threats corresponding to an ever-growing knowldege base of adversary tactics and techniques maintained by the MITRE ATT&amp;CK organization. Detectors use log data to evaluate events occuring in the system. They then apply a set of security rules specified for the detector and determine findings from these events.\nFor information on configuring detectors, see Creating detectors.\nLog types\nLog types provide the data used to evaluate events occuring in a system. OpenSearch supports several types of logs and provides out-of-the-box mappings for the most common log sources. Currently supported log sources include:\nNetwork events\nDNS logs\nApache access logs\nWindows logs\nAD/LDAP logs\nSystem logs\nAWS CloudTrail logs\nAmazon S3 access logs\nGoogle Workspace logs\nGitHub actions\nMicrosoft 365 logs\nOkta events\nMicrosoft Azure logs\nLog types are specified during the creation of detectors, including steps for mapping log fields to the detector. Security Analytics also automatically selects an appropriate set of rules based on a specific log type and populates them for the detector.\nRules\nRules, or threat detection rules, define the conditional logic applied to ingested log data that allows the system to identify an event of interest. Security Analytics uses prepackaged, open source Sigma rules as a starting point for describing relevant log events. But with their inherently flexible format and easy portability, Sigma rules provide users of Security Analytics with options for importing and customizing the rules. You can take advantage of these options using either Dashboards or the API.\nFor information on configuring rules, see Working with rules.\nFindings\nFindings are generated every time a detector matches a rule with a log event. Findings do not necessarily point to imminent threats within the system, but they always isolate an event of interest. Because they represent the result of a specific definition for a detector, findings include a unique combination of select rules, a log type, and a rule severity. As such, you can search for specific findings in the Findings window, and you can filter findings in the list based on severity and log type.\nTo learn more about findings, see Working with findings.\nAlerts\nWhen defining a detector, you can specify certain conditions that will trigger an alert. When an event triggers an alert, the system sends a notification to a preferred channel, such as Amazon Chime, Slack, or email. The alert can be triggered when the detector matches one or multiple rules. Further conditions can be set by rule severity and tags. You can also create a notification message with a customized subject line and message body.\nFor information on setting up alerts, see Step 3. Set up alerts in detector creation documentation. For information on managing alerts on the Alerts window, see Working with alerts.\nCorrelation engine\nThe correlation engine is an experimental feature released in OpenSearch 2.7. Therefore, we do not recommend using the feature in a production environment at this time. For updates on the progress of the correlation engine, see Security Analytics Correlation Engine on GitHub. To share ideas and provide feedback, join the Security Analytics forum.\nThe correlation engine gives Security Analytics the ability to compare findings from different log types and draw correlations between them. This facilitates understanding of the relationships between findings from different systems in an infrastructure and increases confidence that an event is meaningful and requires attention.\nThe correlation engine uses correlation rules to define threat scenarios involving different log types. It can then perform queries on logs to match relevant findings from those different log sources. To depict relationships between events occurring in different logs, a correlation graph provides a visual representation of findings, their connections, and the proximity of those connections. While the correlation rules define what threat scenarios to look for, the graph provides a visualization that helps you identify the relationships between different findings in a chain of security events.\nTo learn more about defining threat scenarios for correlation rules, see Creating correlation rules. To learn more about using the correlation graph, see Working with the correlation graph.\nFirst steps\nTo get started with Security Analytics, you need to define detectors, ingest log data, generate findings, define correlation rules, and configure alerts. See Setting up Security Analytics to begin configuring the platform to meet your objectives.",
    "ancestors": [
      "Security analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/sec-analytics-config/correlation-config/",
    "title": "Creating correlation rules",
    "content": "The correlation engine is an experimental feature released in OpenSearch 2.7. Therefore, we do not recommend using the feature in a production environment at this time. For updates on the progress of the correlation engine, see Security Analytics Correlation Engine on GitHub. To share ideas and provide feedback, join the Security Analytics forum.\nCorrelation rules allow you to define threat scenarios involving multiple systems in an infrastructure by matching the signatures of threat events occuring in different log types. Once a rule contains at least two different log sources and the preferred fields and field values that define an intended threat secenario, the correlation engine can query the indexes specified in the correlation rule and identify any correlations between the findings.\nConfiguring rules\nHaving at least two data sources in the rule configuration is the basis for making connections between different systems in an infrastructure and identifying correlations. Therefore, a minimum of two queries is required for each correlation rule. However, you can include more than two queries to better define a threat scenario and look for correlations between multiple systems. Follow these steps to create a correlation rule:\nBegin by selecting Security Analytics in the OpenSearch Dashboards main menu. Then select Correlation rules from the Security Analytics menu on the left side of the screen. The Correlation rules page is displayed, as shown in the following image. Select Create correlation rule. The Create correlation rule window opens.\nIn the Correlation rule details field, enter a name for the rule, as shown in the following image. The Correlation queries field contains two dropdown lists. In the Select index dropdown list, specify an index or index pattern for the data source. In the Log type dropdown list, specify the log type associated with the index, as shown in the following image. In the Field dropdown list, specify a log field. In the Field value text box, enter a value for the field, as shown in the following image. To add more fields to the query, select Add field.\nAfter configuring the first query, repeat the previous step to configure a second query. You can select Add query at the bottom of the window to add more queries for the rule, as shown in the following image. Once the rule is complete, select Create correlation rule in the lower-right corner of the window. OpenSearch creates a new rule, the screen returns to the Correlation rules window, and the new rule appears in the table of correlation rules.\nSetting a time window\nThe Cluster Settings API allows you to correlate findings within a set time window. For example, if your time window is three minutes, the system attempts to correlate findings defined in the threat scenario only when they occur within three minutes of one another. By default, the time window is five minutes. For more information about the Cluster Settings API, see Cluster settings.\nExample request\nThe following PUT call sets the time window to two minutes: PUT /_cluster/settings { \"transient\": { \"plugins.security_analytics.correlation_time_window\": \"2m\" } } copy Next steps\nAfter creating detectors and correlation rules, you can use the correlation graph to observe the correlations between findings from different log sources. For information about working with the correlation graph, see Working with the correlation graph.",
    "ancestors": [
      "Security analytics",
      "Setting up Security Analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/sec-analytics-config/detectors-config/",
    "title": "Creating detectors",
    "content": "Security Analytics provides the options and functionality to monitor and respond to a wide range of security threats. Detectors are the essential components that determine what to look for and how to respond to those threats. This section covers their creation and configuration.\nStep 1. Define a detector\nYou can define a new detector by naming the detector, selecting a data source and detector type, and specifying a detector schedule. After defining a detector, you can also configure field mappings and set up alerts. Follow the steps in this section to accomplish all three of these setup tasks.\nOn the Threat detectors page, choose Create detector. The Define detector page opens.\nIn Detector details, give the detector a name. Adding a description for the detector is optional.\nIn the Data source section, select the dropdown arrow and select one or multiple sources for the log data.\nWhen multiple data sources are selected, the logs must be of the same type. We recommend creating separate detectors for different log types.\nIn the Log types and rules section, select the log type for the data source. The Sigma security rules associated with the log data are automatically populated in the Detection rules section, as shown in the following image. When selecting Network events, CloudTrail logs, or S3 access logs as the log type, a detector dashboard is automatically created. The dashboard offers visualizations for the detector and can provide security-related insight into log source data. For more information about visualizations, see Building data visualizations.\nYou can skip the next step for applying select rules if you are satisfied with those automatically populated by the system. Otherwise, go to the next step to select rules individually.\nIn the Detection rules section, specify only those rules you want applied to the detector, as shown in the following image. Use the toggle to the left of Rule name to select or deselect rules.\nUse the Rule severity and Source dropdown lists to filter the rules you want to select from.\nUse the Search bar to search for specific rules.\nTo quickly select one or more known rules and dismiss others, first deselect all rules by moving the Rule name toggle to the left, then search for your target rule names and select each individually by moving its toggle to the right.\nIn the Detector schedule section, set how often the detector will run. Specify a unit of time and a corresponding number to set the interval.\nChoose Next in the lower-right corner of the screen to continue. The Configure field mapping page appears.\nStep 2. Create field mappings\nThe field mapping step matches field names from the rule with field names from the log index being used to provide data. Creating field mappings allows the system to accurately pass event data from the log to the detector and then use the data to trigger alerts.\nThe data source (log index), log type, and detection rules specified in the first step determine which fields are available for mapping. For example, when “Windows logs” is selected as the log type, this parameter, along with the specific detection rules, determines the list of rule field names available for the mapping. Similarly, the selected data source (log index) determines the list of log source field names that are available for the mapping.\nBecause the system uses prepackaged Sigma rules for detector creation, it can automatically map important fields for a specific log type with the corresponding fields in the Sigma rules. The field mapping step presents a view of automatically mapped fields while also providing the option to customize, change, or add new field mappings. When a detector includes custom rules, you can follow this step to manually map rule field names to log source field names.\nA note on field names\nThe field mapping process requires that you are familiar with the field names in the log index and have an understanding of the data contained in those fields. If you have an understanding of the log fields in the index, the mapping is typically a straightforward process.\nSecurity Analytics takes advantage of prepackaged Sigma rules for security event detection. Therefore, the field names are derived from a Sigma rule field standard. To make them easier to identify, however, we have created aliases for the Sigma rule fields based on the open-source Elastic Common Schema (ECS) specification. These alias rule field names are the field names used in these steps. They appear in the Detector field name column of the mapping tables.\nAlthough the ECS rule field names are largely self-explanatory, you can find predefined mappings of the Sigma rule field names to ECS rule field names, for all supported log types, in the GitHub Security Analytics repository. Navigate to the OSMappings folder, choose the folder named for the log type, and open the fieldmappings.yml file. For example, to see the Sigma rule fields that correspond to ECS rule fields for the Windows log type, open the fieldmappings.yml file in the windows folder.\nAutomatically mapped fields\nOnce you navigate to the Configure field mapping page, the system attempts to automatically map fields between the two sources. The Automatically mapped fields table contains mappings that the system created automatically after defining the detector. When the field names are similar to one another, the system can successfully match the two, as shown in the following image. Although these automatic matches are normally dependable, it’s still a good idea to review the mappings in the Automatically mapped fields table and verify that they are correct and matched as expected. If you find a mapping that doesn’t appear to be accurate, you can use the dropdown list to search for and select the correct field name. For more on matching field names, see the Pending field mappings section that follows.\nPending field mappings\nThe field names that are not automatically mapped appear in the Pending field mappings table. In this table you can manually map rule fields to log source fields, as shown in the following image. While mapping fields, consider the following:\nThe Detector field name column lists field names based on all of the prepackaged rules associated with the selected log type.\nThe Log source field name column includes a dropdown list for each of the detector fields. Each dropdown list contains field names extracted from the log index.\nTo map a detector field name to a log source field name, use the dropdown arrow to open the list of log source fields and select the log field name from the list. To search for names in the log field list, enter text in the Select a mapping field box, as shown in the following image. Once the log source field name is selected and mapped to the detector field name, the icon in the Status column to the right changes to a green check mark.\nMake as many matches between field names as possible to complete an accurate mapping for the detector and log source fields.\nAfter completing the mappings, choose Next in the lower-right corner of the screen. The Set up alerts page appears and displays settings for an alert trigger.\nStep 3. Set up alerts\nThe third step in creating a detector involves setting up alerts. Alerts are configured to create triggers that, when matched with a set of detection rule criteria, send a notification of a possible security event. You can select rule names, rule severity, and tags in any combination to define a trigger. Once a trigger is defined, the alert setup lets you choose the channel on which to be notified and provides options for customizing a message for the notification.\nAt least one alert condition is required before a detector can begin generating findings.\nYou can also configure alerts from the Findings window. To see how to set up alerts from the Findings window, see The findings list. A final option for adding additional alerts is to edit a detector and navigate to the Alert triggers tab, where you can edit existing alerts as well as add new ones. For details, see Editing a detector.\nTo set up an alert for a detector, continue with the following steps:\nIn the Trigger name box, enter a name for the trigger.\nTo define rule matches for the alert, select security rules, severity levels, and tags. Select one rule or multiple rules that will trigger the alert. Put the cursor in the Rule names box and type a name to search for it. To remove a rule name, select the X beside the name. To remove all rule names, select the X beside the dropdown list’s down arrow. Select one or more rule severities as conditions for the alert.\nSelect from a list of tags to include as conditions for the alert.\nTo define a notification for the alert, assign an alert severity, select a channel for the notification, and customize a message generated for the alert. Assign a level of severity for the alert to give the recipient an indication of its urgency.\nSelect a channel for the notification. Examples include Slack, Chime, or email. Select the Manage channels link to the right of the field to link the notification to a preferred channel.\nSelect the Show notify message label to expand message preferences. You can add a subject for the message and a note to inform recipients of the nature of the message.\nAfter configuring the conditions in the preceding fields, select Next in the lower-right corner of the screen. The Review and create page opens.\nAfter reviewing the specifications for the detector, choose Create in the lower-right corner of the screen to create the detector. The screen returns to the list of all detectors, and the new detector appears in the list.\nWhat’s next\nIf you are ready to view findings for the new detector, see the Working with findings section. If you would like to import rules or set up custom rules before working with findings, see the Working with rules section.",
    "ancestors": [
      "Security analytics",
      "Setting up Security Analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/sec-analytics-config/index/",
    "title": "Setting up Security Analytics",
    "content": "Before Security Analytics can begin generating findings and sending alerts, administrators must create detectors and make log data available to the system. Once detectors are able to generate findings, you can fine-tune your alerts to focus on specific areas of interest. The following steps outline the basic workflow for setting up components in Security Analytics.\nCreate security detectors and alerts, and ingest log data. See Creating detectors for details.\nInspect findings generated from detector output and create any additional alerts.\nIf desired, create custom rules by duplicating and then modifying pre-packaged rules. See Customizing rules for details.\nNavigate to Security Analytics\nTo get started, select the top menu on the Dashboards home page and then select Security Analytics. The Overview page for Security Analytics is displayed.\nFrom the options on the left side of the page, select Detectors to begin creating a detector.",
    "ancestors": [
      "Security analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/security/",
    "title": "OpenSearch Security for Security Analytics",
    "content": "You can use OpenSearch Security with Security Analytics to assign user permissions and manage the actions that users can and cannot perform. For example, you might want one group of users to be able to create, update, or delete detectors and another group of users to only view detectors. You may want still another group to be able to receive and acknowledge alerts but to be prevented from performing other tasks. The OpenSearch Security framework allows you to control the level of access users have to Security Analytics functionality.\nSecurity Analytics system indexes\nSecurity Analytics indexes are protected as system indexes and treated differently than other indexes in a cluster. System indexes store configurations and other system settings and, for that reason, cannot be modified using the REST API or the OpenSearch Dashboards interface. Only a user with a TLS admin certificate can access system indexes. For more information about working with this type of index, see System indexes.\nBasic permissions\nAs an administrator, you can use OpenSearch Dashboards or the Security REST API to assign specific permissions to users based on the specific APIs they need to access. For a list of supported APIs, see API tools.\nOpenSearch Security has three built-in roles that cover most Security Analytics use cases: security_analytics_full_access, security_analytics_read_access, and security_analytics_ack_alerts. For descriptions of these and other roles, see Predefined roles.\nIf these roles don’t meet your needs, mix and match individual Security Analytics permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the cluster:admin/opensearch/securityanalytics/detector/delete permission allows you to delete detectors.\n(Advanced) Limit access by backend role\nYou can use backend roles to configure fine-grained access to individual detectors based on roles. For example, backend roles can be assigned to users working in different departments of an organization so that they can view only those detectors owned by the departments in which they work.\nFirst, make sure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually.\nNext, enable the following setting: PUT /_cluster/settings { \"transient\": { \"plugins.security_analytics.filter_by_backend_roles\": \"true\" } } copy Now when users view Security Analytics resources in OpenSearch Dashboards (or make REST API calls), they only see detectors created by users who share at least one backend role.\nFor example, consider two users: alice and bob.\nThe following example assigns the user alice the analyst backend role: PUT /_plugins/_security/api/internalusers/alice { \"password\": \"alice\", \"backend_roles\": [ \"analyst\"], \"attributes\": {} } copy The next example assigns the user bob the human-resources backend role: PUT /_plugins/_security/api/internalusers/bob { \"password\": \"bob\", \"backend_roles\": [ \"human-resources\"], \"attributes\": {} } copy Finally, this last example assigns both alice and bob the role that gives them full access to Security Analytics: PUT /_plugins/_security/api/rolesmapping/security_analytics_full_access { \"backend_roles\": [], \"hosts\": [], \"users\": [ \"alice\", \"bob\"] } copy However, because they have different backend roles, alice and bob cannot view each other’s detectors or their results.\nA note on using fine-grained access control with the plugin\nWhen a trigger generates an alert, the detector configurations, the alert itself, and any notifications that are sent to a channel may include metadata describing the index being queried. By design, the plugin must extract the data and store it as metadata outside of the index. Document-level security (DLS) and field-level security (FLS) access controls are designed to protect the data in the index. But once the data is stored outside the index as metadata, users with access to the detector and monitor configurations, alerts, and their notifications will be able to view this metadata and possibly infer the contents and quality of data in the index, which would otherwise be concealed by DLS and FLS access control.\nTo reduce the chances of unintended users viewing metadata that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See Limit access by backend role for more information.",
    "ancestors": [
      "Security analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/usage/alerts/",
    "title": "Working with alerts",
    "content": "The Alerts window includes features for viewing and working with alerts. The two main features are:\nThe bar graph with alert information arranged by count, date, and alert status or alert severity.\nThe Alerts list arranged by time of the alert, the alert’s trigger name, which detector triggered it, and other details.\nYou can select the Refresh button at any time to refresh information on the Alerts page.\nThe Alerts graph\nThe Alerts graph can display alerts by their status or severity. Use the Group by dropdown list to specify either Alert status or Alert severity.\nTo specify the date range you would like the graph to display, first select the calendar dropdown arrow. The date selector window opens. You can use the Quick select settings to specify an exact window of time.\nSelect either Last or Next in the first dropdown list to set the window of time behind the current setting or ahead of the current setting.\nSelect a number in the second dropdown list to define a value for the range.\nSelect a unit of time in the third dropdown list. Available options are seconds, minutes, hours, days, weeks, months, and years.\nSelect the Apply button to apply the range of dates to the graph. Information on the graph changes accordingly. You can use the left and right arrows to move the window of time behind the current range of dates or ahead of the current range of dates. When you use these arrows, the start date and end date appear in the date range field. You can then select each one to set an absolute, relative, or current date and time. For absolute and relative changes, select the Update button to apply the changes. As an alternative, you can select an option in the Commonly used section (see the preceding image of the calendar dropdown list) to conveniently set a window of time. Options include date ranges such as Today, Yesterday, this week, and week to date.\nWhen one of the commonly used windows of time is selected, you can select the Show dates label in the date range field to populate the range of dates. Following that, you can select either the start date or end date to specify by an absolute, relative, or current date and time setting. For absolute and relative changes, select the Update button to apply the changes.\nAs one more alternative, you can select an option from the Recently used date ranges section to go back to a previous setting.\nThe Alerts list\nThe Alerts list displays all findings according to the time when the alert was triggered, the alert’s trigger name, the detector that triggered the alert, the alert status, and alert severity.\nUse the Alert severity dropdown list to filter the list of alerts by severity. Use the Status dropdown list to filter the list by alert status.",
    "ancestors": [
      "Security analytics",
      "Using Security Analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/usage/correlation-graph/",
    "title": "Working with the correlation graph",
    "content": "The correlation engine is an experimental feature released in OpenSearch 2.7. Therefore, we do not recommend using the feature in a production environment at this time. For updates on the progress of the correlation engine, see Security Analytics Correlation Engine on GitHub. To share ideas and provide feedback, join the Security Analytics forum.\nThe correlation graph is a security findings knowledge graph. It provides a visualization of information generated by the correlation engine and allows you to focus on specific correlations and inspect them in greater detail. Information on the graph includes findings by log type, the severity levels for the findings, the correlations drawn between findings, and the relevance of the correlations, among other details. You can also manipulate the graph to gain further insight into specific events of interest. This includes filtering findings by date and time, zooming in on the relationship between specific findings and their correlations, and filtering by log type and severity level. Use this section to learn more about using the graph.\nAcccessing the graph\nBegin by selecting Security Analytics in the OpenSearch Dashboards main menu. Then select Correlations from the Security Analytics menu on the left side of the screen. The Correlations page is displayed, as shown in the following image. Interpreting the graph\nThe graph displays findings as nodes with colored borders expressing their severity level. A three-letter abbreviation inside the node indicates the log type. The lines that connect the findings represent the correlations between them. A heavy line indicates a strong correlation, while a light line shows a weaker connection. Using the graph\nYou can control which findings are displayed on the graph by filtering by severity level, log type, and time filter. The time filter controls the findings that appear on the graph by setting a date range in which they were generated.\nUse the Severity dropdown list to select which findings appear on the graph according to their severity level. The number beside the list name indicates how many severity levels are being shown on the graph.\nUse the Log types dropdown list to select which log types to show on the graph. The number beside the list name indicates how many log types are being shown on the graph.\nSelect Reset filters to return the dropdown lists to their default settings, showing all items.\nUse the time filter to set the date range and show only those findings that were generated within that time span. Select Refresh to bring the current number of findings up to date.\nYou can focus on a particular area of the graph to look at correlations associated with a specific finding by selecting the finding on the graph. The graph then changes to show only the selected finding along with the constellation of findings correlated to it, as shown in the following image. After narrowing the focus of the graph, informational cards for each of the findings appear on the right-hand side of the screen. The selected finding appears at the top of the cards, and the correlated findings are listed below it in order of their correlation relevance, represented by a correlation score, as shown in the following image. You can select one of the correlated findings on the graph to shift the perspective of the correlation relationships. This sends the newly selected finding to the top of the informational cards and displays the other findings as relative correlations.\nThe cards display the following details about each finding:\nThe severity level of the finding: 1, critical; 2, high; 3, medium; 4, low; 5, informational.\nA correlation score for correlated findings. The score is based on the proximity of relevant findings in the threat scenario defined by the correlation rule.\nThe detection rule that generated the finding.\nFor correlated findings, the correlation rule used to associate it with the selected finding.",
    "ancestors": [
      "Security analytics",
      "Using Security Analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/usage/detectors/",
    "title": "Working with detectors",
    "content": "After creating a detector, it appears on the Threat detectors page along with others saved to the system. You can then perform a number of actions for each detector, from editing its details to changing its status. See the following sections for description of the available actions. Threat detector list\nThe list of threat detectors includes the search bar, the Status dropdown list, and the Log type dropdown list.\nUse the search bar to filter by detector name.\nSelect the Status dropdown list to filter detectors in the list by Active and Inactive status.\nSelect the Log type dropdown list to filter detectors by any log type that appears in the list (the options depend on the detectors present in the list and their log types).\nEditing a detector\nTo edit a detector, begin by selecting the link to the detector in the Detector name column of the list. The detector’s details window opens and shows details about the detector’s configuration. In the upper-left portion of the window, the details window shows the name of the detector and its status, either Active or Inactive.\nIn the upper-right corner of the window, you can select View alerts to go to the Alerts window or View findings to go to the Findings window. You can also select Actions to perform actions for the detector. See Detector actions.\nIn the lower portion of the window, select the Edit button for either Detector details or Detection rules to make changes accordingly.\nFinally, you can select the Field mappings tab to edit field mappings for the detector, or select the Alert triggers tab to make edits to alerts associated with the detector. After you select the Alert triggers tab, you also have the option to add additional alerts for the detector by selecting Add another alert condition at the bottom of the page.\nDetector actions\nThreat detector actions allow you to stop and start detectors or delete a detector. To enable actions, first select the checkbox beside one or more detectors in the list. Changing detector status\nSelect the detector or detectors in the list whose status you would like to change. The Actions dropdown list becomes enabled.\nDepending on whether the detector is currently active or inactive, select either Stop detector or Start detector. After a moment, the change in status of the detector appears in the detector list as either Inactive or Active.\nDeleting a detector\nSelect the detector or detectors in the list that you would like to delete. The Actions dropdown list becomes enabled.\nSelect Delete in the dropdown list. The Delete detector popup window opens and asks you to verify that you want to delete the detector or detectors.\nSelect Cancel to decline the action. Select Delete detector to delete the detector or detectors permanently from the list.",
    "ancestors": [
      "Security analytics",
      "Using Security Analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/usage/findings/",
    "title": "Working with findings",
    "content": "The Findings window includes features for viewing and working with findings. The two main features are:\nThe bar graph with findings information arranged by count, date, and log type or rule severity.\nThe Findings list arranged by time, finding ID, rule name, and other details.\nYou can choose Refresh at any time to refresh information on the Findings page.\nThe Findings graph\nThe findings graph can display findings by log type or rule severity. Use the Group by dropdown list to specify either log type or rule severity.\nTo specify the date range you would like the graph to display, first select the calendar dropdown list. The date selector window opens. You can use the Quick select settings to specify an exact window of time.\nSelect either Last or Next in the first dropdown list to set the window of time behind the current setting or ahead of the current setting.\nSelect a number in the second dropdown list to define a value for the range.\nSelect a unit of time in the third dropdown list. Available options are seconds, minutes, hours, days, weeks, months, and years.\nChoose Apply to apply the range of dates to the graph. Information on the graph changes accordingly, as shown in the following image. You can use the left and right arrows to move the window of time behind the current range of dates or ahead of the current range of dates. When you use these arrows, the start and end dates appear in the date range field. You can then select each one to set an absolute, relative, or current date and time. For absolute and relative changes, choose Update to apply the changes. As an alternative, you can select an option in the Commonly used section (see the preceding image of the calendar dropdown list) to conveniently set a window of time. Options include date ranges such as Today, Yesterday, this week, and week to date.\nWhen one of the commonly used windows of time is selected, you can choose Show dates in the date range field to populate the range of dates. Following that, you can select either the start date or end date to specify an absolute, relative, or current date and time setting. For absolute and relative changes, choose Update to apply the changes.\nAs one more alternative, you can select an option from the Recently used date ranges section to go back to a previous setting.\nThe Findings list\nThe Findings list displays all findings according to the time of the finding, the finding ID, the rule name that generated the finding, the detector that captured the finding, and other details, as shown in the following image. Use the Rule severity dropdown list to filter the list of findings by severity. Use the log type dropdown list to filter the list by log type.\nThe Actions column includes two options for each finding:\nThe diagonal arrow provides a way to open the Finding details pane, which describes the finding according to parameters defined when creating the detector and includes the document that generated the finding.\nThe bell icon allows you to open the Create detector alert trigger pane, where you can quickly set up an alert for the specific finding and modify rules and their conditions as required.\nFor details on setting up an alert, see Step 3. Set up alerts in detector creation documentation.\nFinding details\nEach finding in the list also includes a Finding ID. In addition to using the diagonal arrow in Actions, you can select the ID to open the Finding details pane. An example of Finding details is shown in the following image. Viewing surrounding documents\nThe Finding details pane contains specific information about the finding, including the document that generated the finding. To investigate the series of events that led to the finding or followed the finding, you can select View surrounding documents to open the document in the Discover panel and view other documents preceding or following it.\nOpen Finding details by selecting the Finding ID in the Findings list.\nIn the Documents section, select View surrounding documents. If an index pattern already exists for the document, the Discover panel opens and displays the document. If an index pattern does not exist, the Create index pattern to view documents window opens and prompts you to create an index pattern, as shown in the following image. In the Create index pattern to view documents window, the index pattern name is automatically populated. Enter the appropriate time field from the log index used to determine the timing for log events. For information on mapping log fields to detector fields, see Step 2. Create field mappings. Choose Create index pattern. The Create index pattern to view documents confirmation window opens.\nSelect View surrounding documents in the confirmation window. The Discover panel opens, as shown in the following image. The Discover panel displays the document that generated the finding with a highlighted background. Other documents that came either before or after the event are also displayed.\nFor details about working with Discover in OpenSearch Dashboards, see Exploring data.\nViewing correlated findings\nCorrelations between findings are generated by the correlation engine, which is an experimental feature released in OpenSearch 2.7. Therefore, we do not recommend using the feature in a production environment at this time. For updates on the progress of the correlation engine, see Security Analytics Correlation Engine on GitHub. To share ideas and provide feedback, join the Security Analytics forum.\nTo see how the finding is correlated with other findings, select the Correlations tab. Correlations are relationships between findings that express a particular threat scenario involving multiple log types. Information in the Correlated findings table shows the time at which a correlated finding was generated, a finding’s ID, the log type used to generate the finding, its threat severity, and the correlation score—a measure of its proximity to the reference finding—as shown in the following image. You can select View correlations graph to visualize correlations between the findings. For more information about using the correlation graph, see Working with the correlation graph.",
    "ancestors": [
      "Security analytics",
      "Using Security Analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/usage/index/",
    "title": "Using Security Analytics",
    "content": "After creating detectors and generating findings, functionality within the several Security Analytics windows offers visualizations and tools to help you investigate and manage findings, create focused alerts and notifications, import or customize rules, and edit detectors, among other tasks. This section discusses available features, their uses, and general navigation while working in the various windows. You can use the links below to go directly to information on a specific window. The Overview page Working with detectors Working with findings Working with rules Working with the correlation graph Working with alerts",
    "ancestors": [
      "Security analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/usage/overview/",
    "title": "The Overview page",
    "content": "When you select Security Analytics from the top menu, the Overview page is displayed. The Overview page consists of five sections:\nFindings and alert count\nTop recent alerts\nTop recent findings\nMost frequent detection rules\nDetectors\nEach section provides a summary description for each element of Security Analytics, along with controls that let you take action for each item.\nOverview and getting started\nThe upper portion of the Overview page contains two control buttons for refreshing information and getting started with Security Analytics. You can select the Refresh button to refresh all of the information on the page.\nYou can also select the Getting started link to expand the Get started with Security Analytics window, which includes a summary of the setup steps as well as control buttons that allow you to jump to any of the steps. In step 1 of setup, select Create detector to define a detector.\nIn step 2, select View findings to go to the Findings page. For details about this page, see Working with findings.\nIn step 3, select View alerts to go to the Security alerts page. For details about this page, see Working with alerts.\nIn step 4, select Manage rules to go to the Rules page. For more on rules, see Working with rules.\nFindings and alert count\nThe Findings and alert count section provides a graph showing data on the latest findings. Use the Group by dropdown list to select either All findings or Log type.\nRecent alerts\nThe Recent alerts table displays recent alerts by time, trigger name, and alert severity. Select View alerts to go to the Alerts page.\nRecent findings\nThe Recent findings table displays recent findings by time, rule name, rule severity, and detector. Select View all findings to go to the Findings page.\nMost frequent detection rules\nThis section provides a graphical representation of detection rules that trigger findings most often and how they compare to others as a percentage of the whole. The rule names represented by the graph are listed to the right. Detectors\nThe Detectors section displays a list of available detectors by detector name, status (active/inactive), and log type. Select View all detectors to go to the Detectors page. Select Create detector to go directly to the Define detector page.",
    "ancestors": [
      "Security analytics",
      "Using Security Analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/security-analytics/usage/rules/",
    "title": "Working with rules",
    "content": "The Rules window lists all security rules and provides options for filtering the list and viewing details for each rule. Further options let you import rules and create new rules by first duplicating a Sigma rule then modifying it. This section covers navigation of the Rules page and description of the actions you can perform. Viewing and filtering rules\nWhen you open the Rules page, all rules are listed in the table. Use the search bar to search for specific rules by entering a full or partial name and pressing Return/Enter on your keyboard. The list is filtered and displays matching results.\nAlternatively, you can use the Rule type, Rule severity, and Source dropdown lists to drill down in the alerts and filter for preferred results. You can select multiple options from each list and use all three in combination to narrow results. Rule details\nTo see rule details, select the rule in the Rule name column of the list. The rule details pane opens. In Visual view, rule details are arranged in fields, and the links are active. Select YAML to display the rule in YAML file format. Rule details are formatted as a YAML file according to the Sigma rule specification.\nTo copy the rule, select the copy icon in the top right corner of the rule. To quickly create a new and customized rule, you can paste the rule into the YAML editor and make any modifications before saving it. See Customizing rules for details.\nCreating rules\nThere are several ways to create rules on the Rules page. The first is to manually fill in the necessary fields that complete the rule, using either the Visual Editor or YAML Editor. To do this, select the Create new rule button in the uppper-right corner of the Rules window. The Create a rule window opens. If you choose to create the rule manually, you can refer to Sigma’s Rule Creation Guide to help understand details for each field.\nBy default, the Visual Editor is displayed. Enter the appropriate content in each field and select Create in the lower-right corner of the window to save the rule.\nThe Create a rule window also provides the YAML Editor so that you can create the rule directly in a YAML file format. Select YAML Editor and then enter information for the pre-populated field types.\nThe alternatives to manually creating a rule, however, simplify and speed up the process. They involve either importing a rule in a YAML file or duplicating an existing rule and customizing it. See the next two sections for detailed steps.\nImporting rules\nAt this time, Security Analytics supports the import of Sigma rules in YAML format. The following sample file shows the basic formatting of a rule in YAML. title: RDP Sensitive Settings Changed logsource: product: windows service: system description: ' Detects changes to RDP terminal service sensitive settings' detection: selection_reg: EventType: SetValue TargetObject|contains: - \\services\\TermService\\Parameters\\ServiceDll - \\Control\\Terminal Server\\fSingleSessionPerUser - \\Control\\Terminal Server\\fDenyTSConnections - \\Policies\\Microsoft\\Windows NT\\Terminal Services\\Shadow - \\Control\\Terminal Server\\WinStations\\RDP-Tcp\\InitialProgram condition: selection_reg level: high tags: - attack.defense_evasion - attack.t1112 references: - https://blog.menasec.net/2019/02/threat-hunting-rdp-hijacking-via.html - https://knowledge.insourcess.com/Supporting_Technologies/Wonderware/Tech_Notes/TN_WW213_How_to_shadow_an_established_RDP_Session_on_Windows_10_Pro - https://twitter.com/SagieSec/status/1469001618863624194?t=HRf0eA0W1YYzkTSHb-Ky1A&amp;s=03 - http://etutorials.org/Microsoft+Products/microsoft+windows+server+2003+terminal+services/Chapter+6+Registry/Registry+Keys+for+Terminal+Services/ falsepositives: - Unknown author: - Samir Bousseaden - David ANDRE status: experimental copy To begin, select the Import rule button in the upper-right corner of the page. The Import rule page opens.\nEither drag a YAML-formatted Sigma rule into the window or browse for the file by selecting the link and opening it. The Import a rule window opens and the rule definition fields are automatically populated in both the Visual Editor and YAML Editor.\nVerify or modify the information in the fields.\nAfter you confirm the information for the rule is accurate, select the Create button in the lower-right corner of the window. A new rule is created, and it appears in the list of rules on the main page of the Rules window.\nCustomizing rules\nAn alternative to importing a rule is duplicating a Sigma rule and then modifying it to create a custom rule. First search for or filter rules in the Rules list to locate the rule you want to duplicate. To begin, select the rule in the Rule name column. The rule details pane opens. Select the Duplicate button in the upper-right corner of the pane. The Duplicate rule window opens in Visual Editor view and all of the fields are automatically populated with the rule’s details. Details are also populated in YAML Editor view. In either Visual Editor view or YAML Editor view, modify any of the fields to customize the rule.\nAfter performing any modifications to the rule, select the Create button in the lower-right corner of the window. A new and customized rule is created, and it appears in the list of rules on the main page of the Rules window. You cannot modify the Sigma rule itself. The original Sigma rule always remains in the system. Its duplicate, after modification, becomes the custom rule that is added to the list of rules.",
    "ancestors": [
      "Security analytics",
      "Using Security Analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/async/index/",
    "title": "Asynchronous search",
    "content": "Searching large volumes of data can take a long time, especially if you’re searching across warm nodes or multiple remote clusters.\nAsynchronous search in OpenSearch lets you send search requests that run in the background. You can monitor the progress of these searches and get back partial results as they become available. After the search finishes, you can save the results to examine at a later time.\nREST API\nIntroduced 1.0\nTo perform an asynchronous search, send requests to _plugins/_asynchronous_search, with your query in the request body: POST _plugins/_asynchronous_search You can specify the following options. Options Description Default value Required wait_for_completion_timeout The amount of time that you plan to wait for the results. You can see whatever results you get within this time just like in a normal search. You can poll the remaining results based on an ID. The maximum value is 300 seconds.\n1 second\nNo keep_on_completion Whether you want to save the results in the cluster after the search is complete. You can examine the stored results at a later time. false No keep_alive The amount of time that the result is saved in the cluster. For example, 2d means that the results are stored in the cluster for 48 hours. The saved search results are deleted after this period or if the search is canceled. Note that this includes the query execution time. If the query overruns this time, the process cancels this query automatically.\n12 hours\nNo Example request POST _plugins/_asynchronous_search/?pretty&amp;size= 10 &amp;wait_for_completion_timeout= 1 ms&amp;keep_on_completion= true &amp;request_cache= false { \"aggs\": { \"city\": { \"terms\": { \"field\": \"city\", \"size\": 10 } } } } Example response { \"*id*\": \"FklfVlU4eFdIUTh1Q1hyM3ZnT19fUVEUd29KLWZYUUI3TzRpdU5wMjRYOHgAAAAAAAAABg==\", \"state\": \"RUNNING\", \"start_time_in_millis\": 1599833301297, \"expiration_time_in_millis\": 1600265301297, \"response\": { \"took\": 15, \"timed_out\": false, \"terminated_early\": false, \"num_reduce_phases\": 4, \"_shards\": { \"total\": 21, \"successful\": 4, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 807, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"city\": { \"doc_count_error_upper_bound\": 16, \"sum_other_doc_count\": 403, \"buckets\": [ { \"key\": \"downsville\", \"doc_count\": 1 },............ { \"key\": \"blairstown\", \"doc_count\": 1 }] } } } } Response parameters Options Description id The ID of an asynchronous search. Use this ID to monitor the progress of the search, get its partial results, and/or delete the results. If the asynchronous search finishes within the timeout period, the response doesn’t include the ID because the results aren’t stored in the cluster. state Specifies whether the search is still running or if it has finished, and if the results persist in the cluster. The possible states are RUNNING, COMPLETED, and PERSISTED. start_time_in_millis The start time in milliseconds. expiration_time_in_millis The expiration time in milliseconds. took The total time that the search is running. response The actual search response. num_reduce_phases The number of times that the coordinating node aggregates results from batches of shard responses (5 by default). If this number increases compared to the last retrieved results, you can expect additional results to be included in the search response. total The total number of shards that run the search. successful The number of shard responses that the coordinating node received successfully. aggregations The partial aggregation results that have been completed by the shards so far. Get partial results\nIntroduced 1.0\nAfter you submit an asynchronous search request, you can request partial responses with the ID that you see in the asynchronous search response. GET _plugins/_asynchronous_search/&lt;ID&gt;?pretty Example response { \"id\": \"Fk9lQk5aWHJIUUltR2xGWnpVcWtFdVEURUN1SWZYUUJBVkFVMEJCTUlZUUoAAAAAAAAAAg==\", \"state\": \"STORE_RESIDENT\", \"start_time_in_millis\": 1599833907465, \"expiration_time_in_millis\": 1600265907465, \"response\": { \"took\": 83, \"timed_out\": false, \"_shards\": { \"total\": 20, \"successful\": 20, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1000, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \"bank\", \"_id\": \"1\", \"_score\": 1, \"_source\": { \"email\": \"amberduke@abc.com\", \"city\": \"Brogan\", \"state\": \"IL\" } }, {.... }] }, \"aggregations\": { \"city\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 997, \"buckets\": [ { \"key\": \"belvoir\", \"doc_count\": 2 }, { \"key\": \"aberdeen\", \"doc_count\": 1 }, { \"key\": \"abiquiu\", \"doc_count\": 1 }] } } } } After the response is successfully persisted, you get back the STORE_RESIDENT state in the response.\nYou can poll the ID with the wait_for_completion_timeout parameter to wait for the results received for the time that you specify.\nFor asynchronous searches with keep_on_completion as true and a sufficiently long keep_alive time, you can keep polling the IDs until the search finishes. If you don’t want to periodically poll each ID, you can retain the results in your cluster with the keep_alive parameter and come back to it at a later time.\nDelete searches and results\nIntroduced 1.0\nTo delete an asynchronous search: DELETE _plugins/_asynchronous_search/&lt;ID&gt;?pretty If the search is still running, OpenSearch cancels it.\nIf the search is complete, OpenSearch deletes the saved results.\nExample response { \"acknowledged\": \"true\" } Monitor stats\nIntroduced 1.0\nYou can use the stats API operation to monitor asynchronous searches that are running, completed, and/or persisted. GET _plugins/_asynchronous_search/stats Example response { \"_nodes\": { \"total\": 8, \"successful\": 8, \"failed\": 0 }, \"cluster_name\": \"264071961897:asynchronous-search\", \"nodes\": { \"JKEFl6pdRC-xNkKQauy7Yg\": { \"asynchronous_search_stats\": { \"submitted\": 18236, \"initialized\": 112, \"search_failed\": 56, \"search_completed\": 56, \"rejected\": 18124, \"persist_failed\": 0, \"cancelled\": 1, \"running_current\": 399, \"persisted\": 100 } } } } Response parameters Options Description submitted The number of asynchronous search requests that were submitted. initialized The number of asynchronous search requests that were initialized. rejected The number of asynchronous search requests that were rejected. search_completed The number of asynchronous search requests that completed with a successful response. search_failed The number of asynchronous search requests that completed with a failed response. persisted The number of asynchronous search requests whose final result successfully persisted in the cluster. persist_failed The number of asynchronous search requests whose final result failed to persist in the cluster. running_current The number of asynchronous search requests that are running on a given coordinator node. cancelled The number of asynchronous search requests that were canceled while the search was running.",
    "ancestors": [
      "Search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/async/security/",
    "title": "Asynchronous search security",
    "content": "You can use the Security plugin with asynchronous searches to limit non-admin users to specific actions. For example, you might want some users to only be able to submit or delete asynchronous searches, while you might want others to only view the results.\nAll asynchronous search indices are protected as system indices. Only a super admin user or an admin user with a Transport Layer Security (TLS) certificate can access system indices. For more information, see System indices.\nBasic permissions\nAs an admin user, you can use the Security plugin to assign specific permissions to users based on which API operations they need access to. For a list of supported APIs operations, see Asynchronous search.\nThe Security plugin has two built-in roles that cover most asynchronous search use cases: asynchronous_search_full_access and asynchronous_search_read_access. For descriptions of each, see Predefined roles.\nIf these roles don’t meet your needs, mix and match individual asynchronous search permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the cluster:admin/opensearch/asynchronous_search/delete permission lets you delete a previously submitted asynchronous search.\nA note on Asynchronous Search and fine-grained access control\nBy design, the Asynchronous Search plugin extracts data from a target index and stores the data in a separate index to make search results available to users with the proper permissions. Although a user with either the asynchronous_search_read_access or cluster:admin/opensearch/asynchronous_search/get permission cannot submit the asynchronous search request itself, that user can get and view the search results using the associated search ID. Document-level security (DLS) and field-level security (FLS) access controls are designed to protect the data in the target index. But once the data is stored outside this index, users with these access permissions are able to use search IDs to get and view asynchronous search results, which may include data that is otherwise concealed by DLS and FLS access control in the target index.\nTo reduce the chances of unintended users viewing search results that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See Limit access by backend role for details.\n(Advanced) Limit access by backend role\nUse backend roles to configure fine-grained access to asynchronous searches based on roles. For example, users of different departments in an organization can view asynchronous searches owned by their own department.\nFirst, make sure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually.\nNow when users view asynchronous search resources in OpenSearch Dashboards (or make REST API calls), they only see asynchronous searches submitted by users who have a subset of the backend role.\nFor example, consider two users: judy and elon. judy has an IT backend role: PUT _plugins/_security/api/internalusers/judy { \"password\": \"judy\", \"backend_roles\": [ \"IT\"], \"attributes\": {} } elon has an admin backend role: PUT _plugins/_security/api/internalusers/elon { \"password\": \"elon\", \"backend_roles\": [ \"admin\"], \"attributes\": {} } Both judy and elon have full access to asynchronous search: PUT _plugins/_security/api/rolesmapping/async_full_access { \"backend_roles\": [], \"hosts\": [], \"users\": [ \"judy\", \"elon\"] } Because they have different backend roles, an asynchronous search submitted by judy will not be visible to elon and vice versa. judy needs to have at least the superset of all roles that elon has to see elon ’s asynchronous searches.\nFor example, if judy has five backend roles and elon has one of these roles, then judy can see asynchronous searches submitted by elon, but elon can’t see the asynchronous searches submitted by judy. This means that judy can perform GET and DELETE operations on asynchronous searches submitted by elon, but not the reverse.\nIf none of the users have any backend roles, all three will be able to see the others’ searches.\nFor example, consider three users: judy, elon, and jack. judy, elon, and jack have no backend roles set up: PUT _plugins/_security/api/internalusers/judy { \"password\": \"judy\", \"backend_roles\": [], \"attributes\": {} } PUT _plugins/_security/api/internalusers/elon { \"password\": \"elon\", \"backend_roles\": [], \"attributes\": {} } PUT _plugins/_security/api/internalusers/jack { \"password\": \"jack\", \"backend_roles\": [], \"attributes\": {} } Both judy and elon have full access to asynchronous search: PUT _plugins/_security/api/rolesmapping/async_full_access { \"backend_roles\": [], \"hosts\": [], \"users\": [ \"judy\", \"elon\"] } jack has read access to asynchronous search results: PUT _plugins/_security/api/rolesmapping/async_read_access { \"backend_roles\": [], \"hosts\": [], \"users\": [ \"jack\"] } Because none of the users have backend roles, they will be able to see each other’s asynchronous searches. So, if judy submits an asynchronous search, elon, who has full access, will be able to see that search. jack, who has read access, will also be able to see judy ’s asynchronous search.",
    "ancestors": [
      "Search",
      "Asynchronous search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/async/settings/",
    "title": "Settings",
    "content": "The Asynchronous Search plugin adds several settings to the standard OpenSearch cluster settings. They are dynamic, so you can change the default behavior of the plugin without restarting your cluster. You can mark the settings as persistent or transient.\nFor example, to update the retention period of the result index: PUT _cluster/settings { \"transient\": { \"plugins.asynchronous_search.max_wait_for_completion_timeout\": \"5m\" } } Setting Default Description plugins.asynchronous_search.max_search_running_time 12 hours\nThe maximum running time for the search beyond which the search is terminated. plugins.asynchronous_search.node_concurrent_running_searches 20\nThe concurrent searches running per coordinator node. plugins.asynchronous_search.max_keep_alive 5 days\nThe maximum amount of time that search results can be stored in the cluster. plugins.asynchronous_search.max_wait_for_completion_timeout 1 minute\nThe maximum value for the wait_for_completion_timeout parameter. plugins.asynchronous_search.persist_search_failures false\nPersist asynchronous search results that end with a search failure in the system index.",
    "ancestors": [
      "Search",
      "Asynchronous search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/knn/api/",
    "title": "API",
    "content": "The k-NN plugin adds several APIs for managing, monitoring and optimizing your k-NN workload.\nStats\nIntroduced 1.0\nThe k-NN stats API provides information about the current status of the k-NN plugin. The plugin keeps track of both cluster-level and node-level statistics. Cluster-level statistics have a single value for the entire cluster. Node-level statistics have a single value for each node in the cluster. You can filter the query by nodeId and statName: GET /_plugins/_knn/nodeId1,nodeId2/stats/statName1,statName2 Statistic Description circuit_breaker_triggered Indicates whether the circuit breaker is triggered. This statistic is only relevant to approximate k-NN search. total_load_time The time in nanoseconds that k-NN has taken to load native library indices into the cache. This statistic is only relevant to approximate k-NN search. eviction_count The number of native library indices that have been evicted from the cache due to memory constraints or idle time. This statistic is only relevant to approximate k-NN search. Note: Explicit evictions that occur because of index deletion aren’t counted. hit_count The number of cache hits. A cache hit occurs when a user queries a native library index that’s already loaded into memory. This statistic is only relevant to approximate k-NN search. miss_count The number of cache misses. A cache miss occurs when a user queries a native library index that isn’t loaded into memory yet. This statistic is only relevant to approximate k-NN search. graph_memory_usage The amount of native memory native library indices are using on the node in kilobytes. graph_memory_usage_percentage The amount of native memory native library indices are using on the node as a percentage of the maximum cache capacity. graph_index_requests The number of requests to add the knn_vector field of a document into a native library index. graph_index_errors The number of requests to add the knn_vector field of a document into a native library index that have produced an error. graph_query_requests The number of native library index queries that have been made. graph_query_errors The number of native library index queries that have produced an error. knn_query_requests The number of k-NN query requests received. cache_capacity_reached Whether knn.memory.circuit_breaker.limit has been reached. This statistic is only relevant to approximate k-NN search. load_success_count The number of times k-NN successfully loaded a native library index into the cache. This statistic is only relevant to approximate k-NN search. load_exception_count The number of times an exception occurred when trying to load a native library index into the cache. This statistic is only relevant to approximate k-NN search. indices_in_cache For each OpenSearch index with a knn_vector field and approximate k-NN turned on, this statistic provides the number of native library indices that OpenSearch index has and the total graph_memory_usage that the OpenSearch index is using, in kilobytes. script_compilations The number of times the k-NN script has been compiled. This value should usually be 1 or 0, but if the cache containing the compiled scripts is filled, the k-NN script might be recompiled. This statistic is only relevant to k-NN score script search. script_compilation_errors The number of errors during script compilation. This statistic is only relevant to k-NN score script search. script_query_requests The total number of script queries. This statistic is only relevant to k-NN score script search. script_query_errors The number of errors during script queries. This statistic is only relevant to k-NN score script search. nmslib_initialized Boolean value indicating whether the nmslib JNI library has been loaded and initialized on the node. faiss_initialized Boolean value indicating whether the faiss JNI library has been loaded and initialized on the node. model_index_status Status of model system index. Valid values are “red”, “yellow”, “green”. If the index does not exist, this will be null. indexing_from_model_degraded Boolean value indicating if indexing from a model is degraded. This will happen if there is not enough JVM memory to cache the models. training_requests The number of training requests made to the node. training_errors The number of training errors that have occurred on the node. training_memory_usage The amount of native memory training is using on the node in kilobytes. training_memory_usage_percentage The amount of native memory training is using on the node as a percentage of the maximum cache capacity. Note: Some stats contain graph in the name. In these cases, graph is synonymous with native library index. The term graph is a legacy detail, coming from when the plugin only supported the HNSW algorithm, which consists of hierarchical graphs.\nUsage GET /_plugins/_knn/stats?pretty { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"my-cluster\", \"circuit_breaker_triggered\": false, \"model_index_status\": \"YELLOW\", \"nodes\": { \"JdfxIkOS1-43UxqNz98nw\": { \"graph_memory_usage_percentage\": 3.68, \"graph_query_requests\": 1420920, \"graph_memory_usage\": 2, \"cache_capacity_reached\": false, \"load_success_count\": 179, \"training_memory_usage\": 0, \"indices_in_cache\": { \"myindex\": { \"graph_memory_usage\": 2, \"graph_memory_usage_percentage\": 3.68, \"graph_count\": 2 } }, \"script_query_errors\": 0, \"hit_count\": 1420775, \"knn_query_requests\": 147092, \"total_load_time\": 2436679306, \"miss_count\": 179, \"training_memory_usage_percentage\": 0.0, \"graph_index_requests\": 656, \"faiss_initialized\": true, \"load_exception_count\": 0, \"training_errors\": 0, \"eviction_count\": 0, \"nmslib_initialized\": false, \"script_compilations\": 0, \"script_query_requests\": 0, \"graph_query_errors\": 0, \"indexing_from_model_degraded\": false, \"graph_index_errors\": 0, \"training_requests\": 17, \"script_compilation_errors\": 0 } } } GET /_plugins/_knn/HYMrXXsBSamUkcAjhjeN 0 w/stats/circuit_breaker_triggered,graph_memory_usage?pretty { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"my-cluster\", \"circuit_breaker_triggered\": false, \"nodes\": { \"HYMrXXsBSamUkcAjhjeN0w\": { \"graph_memory_usage\": 1 } } } Warmup operation\nIntroduced 1.0\nThe native library indices used to perform approximate k-Nearest Neighbor (k-NN) search are stored as special files with other Apache Lucene segment files. In order for you to perform a search on these indices using the k-NN plugin, the plugin needs to load these files into native memory.\nIf the plugin hasn’t loaded the files into native memory, it loads them when it receives a search request. The loading time can cause high latency during initial queries. To avoid this situation, users often run random queries during a warmup period. After this warmup period, the files are loaded into native memory and their production workloads can begin. This loading process is indirect and requires extra effort.\nAs an alternative, you can avoid this latency issue by running the k-NN plugin warmup API operation on whatever indices you’re interested in searching. This operation loads all the native library files for all of the shards (primaries and replicas) of all the indices specified in the request into native memory.\nAfter the process finishes, you can start searching against the indices with no initial latency penalties. The warmup API operation is idempotent, so if a segment’s native library files are already loaded into memory, this operation has no impact. It only loads files that aren’t currently in memory.\nUsage\nThis request performs a warmup on three indices: GET /_plugins/_knn/warmup/index 1,index 2,index 3?pretty { \"_shards\": { \"total\": 6, \"successful\": 6, \"failed\": 0 } } total indicates how many shards the k-NN plugin attempted to warm up. The response also includes the number of shards the plugin succeeded and failed to warm up.\nThe call doesn’t return results until the warmup operation finishes or the request times out. If the request times out, the operation still continues on the cluster. To monitor the warmup operation, use the OpenSearch _tasks API: GET /_tasks After the operation has finished, use the k-NN _stats API operation to see what the k-NN plugin loaded into the graph.\nBest practices\nFor the warmup operation to function properly, follow these best practices:\nDon’t run merge operations on indices that you want to warm up. During merge, the k-NN plugin creates new segments, and old segments are sometimes deleted. For example, you could encounter a situation in which the warmup API operation loads native library indices A and B into native memory, but segment C is created from segments A and B being merged. The native library indices for A and B would no longer be in memory, and native library index C would also not be in memory. In this case, the initial penalty for loading native library index C is still present.\nConfirm that all native library indices you want to warm up can fit into native memory. For more information about the native memory limit, see the knn.memory.circuit_breaker.limit statistic. High graph memory usage causes cache thrashing, which can lead to operations constantly failing and attempting to run again.\nDon’t index any documents that you want to load into the cache. Writing new information to segments prevents the warmup API operation from loading the native library indices until they’re searchable. This means that you would have to run the warmup operation again after indexing finishes.\nGet Model\nIntroduced 1.2\nUsed to retrieve information about models present in the cluster. Some native library index configurations require a\ntraining step before indexing and querying can begin. The output of training is a model that can then be used to\ninitialize native library index files during indexing. The model is serialized in the k-NN model system index. GET /_plugins/_knn/models/{model_id} Response Field Description model_id The id of the fetched model. model_blob The base64 encoded string of the serialized model. state Current state of the model. Either “created”, “failed”, “training”. timestamp Time when the model was created. description User provided description of the model. error Error message explaining why the model is in the failed state. space_type Space type this model is trained for. dimension Dimension this model is for. engine Native library used to create model. Either “faiss” or “nmslib”. Usage GET /_plugins/_knn/models/test-model?pretty { \"model_id\": \"test-model\", \"model_blob\": \"SXdGbIAAAAAAAAAAAA...\", \"state\": \"created\", \"timestamp\": \"2021-11-15T18:45:07.505369036Z\", \"description\": \"Default\", \"error\": \"\", \"space_type\": \"l2\", \"dimension\": 128, \"engine\": \"faiss\" } GET /_plugins/_knn/models/test-model?pretty&amp;filter_path=model_id,state { \"model_id\": \"test-model\", \"state\": \"created\" } Search Model\nIntroduced 1.2\nUse an OpenSearch query to search for models in the index.\nUsage GET/POST /_plugins/_knn/models/_search?pretty&amp;_source_excludes=model_blob { \"query\": {... } } { \"took\": 0, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \".opensearch-knn-models\", \"_id\": \"test-model\", \"_score\": 1.0, \"_source\": { \"engine\": \"faiss\", \"space_type\": \"l2\", \"description\": \"Default\", \"model_id\": \"test-model\", \"state\": \"created\", \"error\": \"\", \"dimension\": 128, \"timestamp\": \"2021-11-15T18:45:07.505369036Z\" } }] } } Delete Model\nIntroduced 1.2\nUsed to delete a particular model in the cluster.\nUsage DELETE /_plugins/_knn/models/ { model_id } { \"model_id\": { model_id }, \"acknowledged\": true } Train Model\nIntroduced 1.2\nCreate and train a model that can be used for initializing k-NN native library indices during indexing. This API will\npull training data from a knn_vector field in a training index and then create and train a model and then serialize it\nto the model system index. Training data must match the dimension passed into the body of the request. This request\nwill return when training begins. To monitor the state of the model, use the Get model API. Query Parameter Description model_id (Optional) The id of the fetched model. If not specified, a random id will be generated. node_id (Optional) Preferred node to execute training. If set, this node will be used to perform training if it is deemed to be capable. Request Parameter Description training_index Index from where training data from. training_field knn_vector field from training_index to grab training data from. Dimension of this field must match dimension passed in to this request. dimension Dimension this model is for. max_training_vector_count (Optional) Maximum number of vectors from the training index to use for training. Defaults to all of the vectors in the index. search_size (Optional) Training data is pulled from the training index with scroll queries. Defines the number of results to return per scroll query. Defaults to 10,000. description (Optional) User provided description of the model. method Configuration of ANN method used for search. For more information on possible methods, refer to the method documentation. Method must require training to be valid. Usage POST /_plugins/_knn/models/ { model_id } /_train?preference= { node_id } { \"training_index\": \"train-index-name\", \"training_field\": \"train-field-name\", \"dimension\": 16, \"max_training_vector_count\": 1200, \"search_size\": 100, \"description\": \"My model\", \"method\": { \"name\": \"ivf\", \"engine\": \"faiss\", \"space_type\": \"l2\", \"parameters\":{ \"nlist\": 128, \"encoder\":{ \"name\": \"pq\", \"parameters\":{ \"code_size\": 8 } } } } } { \"model_id\": \"model_x\" } POST /_plugins/_knn/models/_train?preference= { node_id } { \"training_index\": \"train-index-name\", \"training_field\": \"train-field-name\", \"dimension\": 16, \"max_training_vector_count\": 1200, \"search_size\": 100, \"description\": \"My model\", \"method\": { \"name\": \"ivf\", \"engine\": \"faiss\", \"space_type\": \"l2\", \"parameters\":{ \"nlist\": 128, \"encoder\":{ \"name\": \"pq\", \"parameters\":{ \"code_size\": 8 } } } } } { \"model_id\": \"dcdwscddscsad\" }",
    "ancestors": [
      "Search",
      "k-NN"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/knn/approximate-knn/",
    "title": "Approximate search",
    "content": "Standard k-NN search methods compute similarity using a brute-force approach that measures the nearest distance between a query and a number of points, which produces exact results. This works well in many applications. However, in the case of extremely large datasets with high dimensionality, this creates a scaling problem that reduces the efficiency of the search. Approximate k-NN search methods can overcome this by employing tools that restructure indexes more efficiently and reduce the dimensionality of searchable vectors. Using this approach requires a sacrifice in accuracy but increases search processing speeds appreciably.\nThe Approximate k-NN search methods leveraged by OpenSearch use approximate nearest neighbor (ANN) algorithms from the nmslib, faiss, and Lucene libraries to power k-NN search. These search methods employ ANN to improve search latency for large datasets. Of the three search methods the k-NN plugin provides, this method offers the best search scalability for large datasets. This approach is the preferred method when a dataset reaches hundreds of thousands of vectors.\nFor details on the algorithms the plugin currently supports, see k-NN Index documentation.\nThe k-NN plugin builds a native library index of the vectors for each knn-vector field/Lucene segment pair during indexing, which can be used to efficiently find the k-nearest neighbors to a query vector during search. To learn more about Lucene segments, see the Apache Lucene documentation. These native library indexes are loaded into native memory during search and managed by a cache. To learn more about preloading native library indexes into memory, refer to the warmup API. Additionally, you can see which native library indexes are already loaded in memory. To learn more about this, see the stats API section.\nBecause the native library indexes are constructed during indexing, it is not possible to apply a filter on an index and then use this search method. All filters are applied on the results produced by the approximate nearest neighbor search.\nRecommendations for engines and cluster node sizing\nEach of the three engines used for approximate k-NN search has its own attributes that make one more sensible to use than the others in a given situation. You can follow the general information below to help determine which engine will best meet your requirements.\nIn general, nmslib outperforms both faiss and Lucene on search. However, to optimize for indexing throughput, faiss is a good option. For relatively smaller datasets (up to a few million vectors), the Lucene engine demonstrates better latencies and recall. At the same time, the size of the index is smallest compared to the other engines, which allows it to use smaller AWS instances for data nodes.\nAlso, the Lucene engine uses a pure Java implementation and does not share any of the limitations that engines using platform-native code experience. However, one exception to this is that the maximum dimension count for the Lucene engine is 1,024, compared with 16,000 for the other engines. Refer to the sample mapping parameters in the following section to see where this is configured.\nWhen considering cluster node sizing, a general approach is to first establish an even distribution of the index across the cluster. However, there are other considerations. To help make these choices, you can refer to the OpenSearch managed service guidance in the section Sizing domains.\nGet started with approximate k-NN\nTo use the k-NN plugin’s approximate search functionality, you must first create a k-NN index with index.knn set to true. This setting tells the plugin to create native library indexes for the index.\nNext, you must add one or more fields of the knn_vector data type. This example creates an index with two knn_vector fields, one using faiss and the other using nmslib fields: PUT my-knn-index -1 { \"settings\": { \"index\": { \"knn\": true, \"knn.algo_param.ef_search\": 100 } }, \"mappings\": { \"properties\": { \"my_vector1\": { \"type\": \"knn_vector\", \"dimension\": 2, \"method\": { \"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": { \"ef_construction\": 128, \"m\": 24 } } }, \"my_vector2\": { \"type\": \"knn_vector\", \"dimension\": 4, \"method\": { \"name\": \"hnsw\", \"space_type\": \"innerproduct\", \"engine\": \"faiss\", \"parameters\": { \"ef_construction\": 256, \"m\": 48 } } } } } } In the example above, both knn_vector fields are configured from method definitions. Additionally, knn_vector fields can also be configured from models. You can learn more about this in the knn_vector data type section.\nThe knn_vector data type supports a vector of floats that can have a dimension count of up to 16,000 for the nmslib and faiss engines, as set by the dimension mapping parameter. The maximum dimension count for the Lucene library is 1,024.\nIn OpenSearch, codecs handle the storage and retrieval of indexes. The k-NN plugin uses a custom codec to write vector data to native library indexes so that the underlying k-NN search library can read it.\nAfter you create the index, you can add some data to it: POST _bulk { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"1\" } } { \"my_vector1\": [ 1.5, 2.5], \"price\": 12.2 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"2\" } } { \"my_vector1\": [ 2.5, 3.5], \"price\": 7.1 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"3\" } } { \"my_vector1\": [ 3.5, 4.5], \"price\": 12.9 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"4\" } } { \"my_vector1\": [ 5.5, 6.5], \"price\": 1.2 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"5\" } } { \"my_vector1\": [ 4.5, 5.5], \"price\": 3.7 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"6\" } } { \"my_vector2\": [ 1.5, 5.5, 4.5, 6.4], \"price\": 10.3 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"7\" } } { \"my_vector2\": [ 2.5, 3.5, 5.6, 6.7], \"price\": 5.5 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"8\" } } { \"my_vector2\": [ 4.5, 5.5, 6.7, 3.7], \"price\": 4.4 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"9\" } } { \"my_vector2\": [ 1.5, 5.5, 4.5, 6.4], \"price\": 8.9 } Then you can execute an approximate nearest neighbor search on the data using the knn query type: GET my-knn-index -1 /_search { \"size\": 2, \"query\": { \"knn\": { \"my_vector2\": { \"vector\": [ 2, 3, 5, 6], \"k\": 2 } } } } k is the number of neighbors the search of each graph will return. You must also include the size option, which\nindicates how many results the query actually returns. The plugin returns k amount of results for each shard\n(and each segment) and size amount of results for the entire query. The plugin supports a maximum k value of 10,000.\nBuilding a k-NN index from a model\nFor some of the algorithms that we support, the native library index needs to be trained before it can be used. It would be expensive to training every newly created segment, so, instead, we introduce the concept of a model that is used to initialize the native library index during segment creation. A model is created by calling the Train API, passing in the source of training data as well as the method definition of the model. Once training is complete, the model will be serialized to a k-NN model system index. Then, during indexing, the model is pulled from this index to initialize the segments.\nTo train a model, we first need an OpenSearch index with training data in it. Training data can come from\nany knn_vector field that has a dimension matching the dimension of the model you want to create. Training data can be the same data that you are going to index or have in a separate set. Let’s create a training index: PUT /train-index { \"settings\": { \"number_of_shards\": 3, \"number_of_replicas\": 0 }, \"mappings\": { \"properties\": { \"train-field\": { \"type\": \"knn_vector\", \"dimension\": 4 } } } } Notice that index.knn is not set in the index settings. This ensures that you do not create native library indexes for this index.\nYou can now add some data to the index: POST _bulk { \"index\": { \"_index\": \"train-index\", \"_id\": \"1\" } } { \"train-field\": [ 1.5, 5.5, 4.5, 6.4]} { \"index\": { \"_index\": \"train-index\", \"_id\": \"2\" } } { \"train-field\": [ 2.5, 3.5, 5.6, 6.7]} { \"index\": { \"_index\": \"train-index\", \"_id\": \"3\" } } { \"train-field\": [ 4.5, 5.5, 6.7, 3.7]} { \"index\": { \"_index\": \"train-index\", \"_id\": \"4\" } } { \"train-field\": [ 1.5, 5.5, 4.5, 6.4]} After indexing into the training index completes, we can call the Train API: POST /_plugins/_knn/models/my-model/_train { \"training_index\": \"train-index\", \"training_field\": \"train-field\", \"dimension\": 4, \"description\": \"My model description\", \"method\": { \"name\": \"ivf\", \"engine\": \"faiss\", \"space_type\": \"l2\", \"parameters\": { \"nlist\": 4, \"nprobes\": 2 } } } The Train API will return as soon as the training job is started. To check its status, we can use the Get Model API: GET /_plugins/_knn/models/my-model?filter_path=state&amp;pretty { \"state\": \"training\" } Once the model enters the “created” state, you can create an index that will use this model to initialize its native\nlibrary indexes: PUT /target-index { \"settings\": { \"number_of_shards\": 3, \"number_of_replicas\": 1, \"index.knn\": true }, \"mappings\": { \"properties\": { \"target-field\": { \"type\": \"knn_vector\", \"model_id\": \"my-model\" } } } } Lastly, we can add the documents we want to be searched to the index: POST _bulk { \"index\": { \"_index\": \"target-index\", \"_id\": \"1\" } } { \"target-field\": [ 1.5, 5.5, 4.5, 6.4]} { \"index\": { \"_index\": \"target-index\", \"_id\": \"2\" } } { \"target-field\": [ 2.5, 3.5, 5.6, 6.7]} { \"index\": { \"_index\": \"target-index\", \"_id\": \"3\" } } { \"target-field\": [ 4.5, 5.5, 6.7, 3.7]} { \"index\": { \"_index\": \"target-index\", \"_id\": \"4\" } } { \"target-field\": [ 1.5, 5.5, 4.5, 6.4]}... After data is ingested, it can be search just like any other knn_vector field!\nUsing approximate k-NN with filters\nIf you use the knn query alongside filters or other clauses (e.g. bool, must, match), you might receive fewer than k results. In this example, post_filter reduces the number of results from 2 to 1: GET my-knn-index -1 /_search { \"size\": 2, \"query\": { \"knn\": { \"my_vector2\": { \"vector\": [ 2, 3, 5, 6], \"k\": 2 } } }, \"post_filter\": { \"range\": { \"price\": { \"gte\": 5, \"lte\": 10 } } } } Spaces\nA space corresponds to the function used to measure the distance between two points in order to determine the k-nearest neighbors. From the k-NN perspective, a lower score equates to a closer and better result. This is the opposite of how OpenSearch scores results, where a greater score equates to a better result. To convert distances to OpenSearch scores, we take 1 / (1 + distance). The k-NN plugin the spaces the plugin supports are below. Not every method supports each of these spaces. Be sure to check out the method documentation to make sure the space you are interested in is supported. spaceType Distance Function (d) OpenSearch Score l1\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n |x_i - y_i| \\]\n\\[ score = {1 \\over 1 + d } \\]\nl2\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n (x_i - y_i)^2 \\]\n\\[ score = {1 \\over 1 + d } \\]\nlinf\n\\[ d(\\mathbf{x}, \\mathbf{y}) = max(|x_i - y_i|) \\]\n\\[ score = {1 \\over 1 + d } \\]\ncosinesimil\n\\[ d(\\mathbf{x}, \\mathbf{y}) = 1 - cos { \\theta } = 1 - {\\mathbf{x} &middot; \\mathbf{y} \\over \\|\\mathbf{x}\\| &middot; \\|\\mathbf{y}\\|}\\]\\[ = 1 -\n{\\sum_{i=1}^n x_i y_i \\over \\sqrt{\\sum_{i=1}^n x_i^2} &middot; \\sqrt{\\sum_{i=1}^n y_i^2}}\\]\nwhere \\(\\|\\mathbf{x}\\|\\) and \\(\\|\\mathbf{y}\\|\\) represent the norms of vectors x and y respectively. nmslib and faiss: \\[ score = {1 \\over 1 + d } \\] Lucene: \\[ score = {1 + d \\over 2}\\]\ninnerproduct (not supported for Lucene)\n\\[ d(\\mathbf{x}, \\mathbf{y}) = - {\\mathbf{x} &middot; \\mathbf{y}} = - \\sum_{i=1}^n x_i y_i \\]\n\\[ \\text{If} d \\ge 0, \\] \\[score = {1 \\over 1 + d }\\] \\[\\text{If} d &lt; 0, score = &minus;d + 1\\]\nThe cosine similarity formula does not include the 1 - prefix. However, because similarity search libraries equates\nsmaller scores with closer results, they return 1 - cosineSimilarity for cosine similarity space—that’s why 1 - is\nincluded in the distance function.",
    "ancestors": [
      "Search",
      "k-NN"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/knn/filter-search-knn/",
    "title": "k-NN search with filters",
    "content": "To refine k-NN results, you can filter a k-NN search using one of the following methods: Scoring script filter: This approach involves pre-filtering a document set and then running an exact k-NN search on the filtered subset. It does not scale for large filtered subsets. Boolean filter: This approach runs an approximate nearest neighbor (ANN) search and then applies a filter to the results. Because of post-filtering, it may return significantly fewer than k results for a restrictive filter. Lucene k-NN filter: This approach applies filtering during the k-NN search, as opposed to before or after the k-NN search, which ensures that k results are returned. You can only use this method with the Hierarchical Navigable Small World (HNSW) algorithm implemented by the Lucene search engine in k-NN plugin versions 2.4 and later.\nFiltered search optimization\nDepending on your dataset and use case, you might be more interested in maximizing recall or minimizing latency. The following table provides guidance on various k-NN search configurations and the filtering methods used to optimize for higher recall or lower latency. The first three columns of the table provide several example k-NN search configurations. A search configuration consists of:\nThe number of documents in an index, where one OpenSearch document corresponds to one k-NN vector.\nThe percentage of documents left in the results after filtering. This value depends on the restrictiveness of the filter that you provide in the query. The most restrictive filter in the table returns 2.5% of documents in the index, while the least restrictive filter returns 80% of documents.\nThe desired number of returned results (k).\nOnce you’ve estimated the number of documents in your index, the restrictiveness of your filter, and the desired number of nearest neighbors, use the following table to choose a filtering method that optimizes for recall or latency. Number of documents in an index Percentage of documents the filter returns k Filtering method to use for higher recall Filtering method to use for lower latency 10M\n2.5\n100\nScoring script\nScoring script\n10M\n38\n100\nLucene filter\nBoolean filter\n10M\n80\n100\nScoring script\nLucene filter\n1M\n2.5\n100\nLucene filter\nScoring script\n1M\n38\n100\nLucene filter\nLucene filter/scoring script\n1M\n80\n100\nBoolean filter\nLucene filter Scoring script filter\nA scoring script filter first filters the documents and then uses a brute-force exact k-NN search on the results. For example, the following query searches for hotels with a rating between 8 and 10, inclusive, that provide parking and then performs a k-NN search to return the 3 hotels that are closest to the specified location: POST /hotels-index/_search { \"size\": 3, \"query\": { \"script_score\": { \"query\": { \"bool\": { \"filter\": { \"bool\": { \"must\": [ { \"range\": { \"rating\": { \"gte\": 8, \"lte\": 10 } } }, { \"term\": { \"parking\": \"true\" } }] } } } }, \"script\": { \"source\": \"knn_score\", \"lang\": \"knn\", \"params\": { \"field\": \"location\", \"query_value\": [ 5.0, 4.0], \"space_type\": \"l2\" } } } } } copy Boolean filter with ANN search\nA Boolean filter consists of a Boolean query that contains a k-NN query and a filter. For example, the following query searches for hotels that are closest to the specified location and then filters the results to return hotels with a rating between 8 and 10, inclusive, that provide parking: POST /hotels-index/_search { \"size\": 3, \"query\": { \"bool\": { \"filter\": { \"bool\": { \"must\": [ { \"range\": { \"rating\": { \"gte\": 8, \"lte\": 10 } } }, { \"term\": { \"parking\": \"true\" } }] } }, \"must\": [ { \"knn\": { \"location\": { \"vector\": [ 5, 4], \"k\": 20 } } }] } } } The response includes documents containing the matching hotels: { \"took\": 95, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 5, \"relation\": \"eq\" }, \"max_score\": 0.72992706, \"hits\": [ { \"_index\": \"hotels-index\", \"_id\": \"3\", \"_score\": 0.72992706, \"_source\": { \"location\": [ 4.9, 3.4], \"parking\": \"true\", \"rating\": 9 } }, { \"_index\": \"hotels-index\", \"_id\": \"6\", \"_score\": 0.3012048, \"_source\": { \"location\": [ 6.4, 3.4], \"parking\": \"true\", \"rating\": 9 } }, { \"_index\": \"hotels-index\", \"_id\": \"5\", \"_score\": 0.24154587, \"_source\": { \"location\": [ 3.3, 4.5], \"parking\": \"true\", \"rating\": 8 } }] } } Lucene k-NN filter implementation\nk-NN plugin version 2.2 introduced support for running k-NN searches with the Lucene engine using HNSW graphs. Starting with version 2.4, which is based on Lucene version 9.4, you can use Lucene filters for k-NN searches.\nWhen you specify a Lucene filter for a k-NN search, the Lucene algorithm decides whether to perform an exact k-NN search with pre-filtering or an approximate search with modified post-filtering. The algorithm uses the following variables:\nN: The number of documents in the index.\nP: The number of documents in the document subset after the filter is applied (P &lt;= N).\nk: The maximum number of vectors to return in the response.\nThe following flow chart outlines the Lucene algorithm. For more information about the Lucene filtering implementation and the underlying KnnVectorQuery, see the Apache Lucene documentation.\nUsing a Lucene k-NN filter\nConsider a dataset that includes 12 documents containing hotel information. The following image shows all hotels on an xy coordinate plane by location. Additionally, the points for hotels that have a rating between 8 and 10, inclusive, are depicted with orange dots, and hotels that provide parking are depicted with green circles. The search point is colored in red: In this example, you will create an index and search for the three hotels with high ratings and parking that are the closest to the search location.\nStep 1: Create a new index\nBefore you can run a k-NN search with a filter, you need to create an index with a knn_vector field. For this field, you need to specify lucene as the engine and hnsw as the method in the mapping.\nThe following request creates a new index called hotels-index with a knn-filter field called location: PUT /hotels-index { \"settings\": { \"index\": { \"knn\": true, \"knn.algo_param.ef_search\": 100, \"number_of_shards\": 1, \"number_of_replicas\": 0 } }, \"mappings\": { \"properties\": { \"location\": { \"type\": \"knn_vector\", \"dimension\": 2, \"method\": { \"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"lucene\", \"parameters\": { \"ef_construction\": 100, \"m\": 16 } } } } } } copy Step 2: Add data to your index\nNext, add data to your index.\nThe following request adds 12 documents that contain hotel location, rating, and parking information: POST /_bulk { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"1\" } } { \"location\": [ 5.2, 4.4], \"parking\": \"true\", \"rating\": 5 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"2\" } } { \"location\": [ 5.2, 3.9], \"parking\": \"false\", \"rating\": 4 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"3\" } } { \"location\": [ 4.9, 3.4], \"parking\": \"true\", \"rating\": 9 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"4\" } } { \"location\": [ 4.2, 4.6], \"parking\": \"false\", \"rating\": 6 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"5\" } } { \"location\": [ 3.3, 4.5], \"parking\": \"true\", \"rating\": 8 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"6\" } } { \"location\": [ 6.4, 3.4], \"parking\": \"true\", \"rating\": 9 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"7\" } } { \"location\": [ 4.2, 6.2], \"parking\": \"true\", \"rating\": 5 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"8\" } } { \"location\": [ 2.4, 4.0], \"parking\": \"true\", \"rating\": 8 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"9\" } } { \"location\": [ 1.4, 3.2], \"parking\": \"false\", \"rating\": 5 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"10\" } } { \"location\": [ 7.0, 9.9], \"parking\": \"true\", \"rating\": 9 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"11\" } } { \"location\": [ 3.0, 2.3], \"parking\": \"false\", \"rating\": 6 } { \"index\": { \"_index\": \"hotels-index\", \"_id\": \"12\" } } { \"location\": [ 5.0, 1.0], \"parking\": \"true\", \"rating\": 3 } copy Step 3: Search your data with a filter\nNow you can create a k-NN search with filters. In the k-NN query clause, include the point of interest that is used to search for nearest neighbors, the number of nearest neighbors to return ( k), and a filter with the restriction criteria. Depending on how restrictive you want your filter to be, you can add multiple query clauses to a single request.\nThe following request creates a k-NN query that searches for the top three hotels near the location with the coordinates [5, 4] that are rated between 8 and 10, inclusive, and provide parking: POST /hotels-index/_search { \"size\": 3, \"query\": { \"knn\": { \"location\": { \"vector\": [ 5, 4], \"k\": 3, \"filter\": { \"bool\": { \"must\": [ { \"range\": { \"rating\": { \"gte\": 8, \"lte\": 10 } } }, { \"term\": { \"parking\": \"true\" } }] } } } } } } copy The response returns the three hotels that are nearest to the search point and have met the filter criteria: { \"took\": 47, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3, \"relation\": \"eq\" }, \"max_score\": 0.72992706, \"hits\": [ { \"_index\": \"hotels-index\", \"_id\": \"3\", \"_score\": 0.72992706, \"_source\": { \"location\": [ 4.9, 3.4], \"parking\": \"true\", \"rating\": 9 } }, { \"_index\": \"hotels-index\", \"_id\": \"6\", \"_score\": 0.3012048, \"_source\": { \"location\": [ 6.4, 3.4], \"parking\": \"true\", \"rating\": 9 } }, { \"_index\": \"hotels-index\", \"_id\": \"5\", \"_score\": 0.24154587, \"_source\": { \"location\": [ 3.3, 4.5], \"parking\": \"true\", \"rating\": 8 } }] } } Note that there are multiple ways to construct a filter that returns hotels that provide parking, for example:\nA term query clause in the should clause\nA wildcard query clause in the should clause\nA regexp query clause in the should clause\nA must_not clause to eliminate hotels with parking set to false.\nThe following request illustrates these four different ways of searching for hotels with parking: POST /hotels-index/_search { \"size\": 3, \"query\": { \"knn\": { \"location\": { \"vector\": [ 5.0, 4.0], \"k\": 3, \"filter\": { \"bool\": { \"must\": { \"range\": { \"rating\": { \"gte\": 1, \"lte\": 6 } } }, \"should\": [ { \"term\": { \"parking\": \"true\" } }, { \"wildcard\": { \"parking\": { \"value\": \"t*e\" } } }, { \"regexp\": { \"parking\": \"[a-zA-Z]rue\" } }], \"must_not\": [ { \"term\": { \"parking\": \"false\" } }], \"minimum_should_match\": 1 } } } } } } copy",
    "ancestors": [
      "Search",
      "k-NN"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/knn/index/",
    "title": "k-NN",
    "content": "Short for k-nearest neighbors, the k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors. To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points.\nUse cases include recommendations (for example, an “other songs you might like” feature in a music application), image recognition, and fraud detection. For more background information on k-NN search, see Wikipedia.\nThis plugin supports three different methods for obtaining the k-nearest neighbors from an index of vectors: Approximate k-NN The first method takes an approximate nearest neighbor approach—it uses one of several algorithms to return the approximate k-nearest neighbors to a query vector. Usually, these algorithms sacrifice indexing speed and search accuracy in return for performance benefits such as lower latency, smaller memory footprints and more scalable search. To learn more about the algorithms, refer to nmslib ’s and faiss ’s documentation.\nApproximate k-NN is the best choice for searches over large indices (i.e. hundreds of thousands of vectors or more) that require low latency. You should not use approximate k-NN if you want to apply a filter on the index before the k-NN search, which greatly reduces the number of vectors to be searched. In this case, you should use either the script scoring method or painless extensions.\nFor more details about this method, including recommendations for which engine to use, see Approximate k-NN search. Script Score k-NN The second method extends OpenSearch’s script scoring functionality to execute a brute force, exact k-NN search over “knn_vector” fields or fields that can represent binary objects. With this approach, you can run k-NN search on a subset of vectors in your index (sometimes referred to as a pre-filter search).\nUse this approach for searches over smaller bodies of documents or when a pre-filter is needed. Using this approach on large indices may lead to high latencies.\nFor more details about this method, see Exact k-NN with scoring script. Painless extensions The third method adds the distance functions as painless extensions that you can use in more complex combinations. Similar to the k-NN Script Score, you can use this method to perform a brute force, exact k-NN search across an index, which also supports pre-filtering.\nThis approach has slightly slower query performance compared to the k-NN Script Score. If your use case requires more customization over the final score, you should use this approach over Script Score k-NN.\nFor more details about this method, see Painless scripting functions.\nOverall, for larger data sets, you should generally choose the approximate nearest neighbor method because it scales significantly better. For smaller data sets, where you may want to apply a filter, you should choose the custom scoring approach. If you have a more complex use case where you need to use a distance function as part of their scoring method, you should use the painless scripting approach.",
    "ancestors": [
      "Search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/knn/jni-libraries/",
    "title": "JNI libraries",
    "content": "To integrate nmslib and faiss approximate k-NN functionality (implemented in C++) into the k-NN plugin (implemented in Java), we created a Java Native Interface, which lets the k-NN plugin make calls to the native libraries. The interface includes three libraries: libopensearchknn_nmslib, the JNI library that interfaces with nmslib, libopensearchknn_faiss, the JNI library that interfaces with faiss, and libopensearchknn_common, a library containing common shared functionality between native libraries.\nThe Lucene library is not implemented using a native library.\nThe libraries libopensearchknn_faiss and libopensearchknn_nmslib are lazily loaded when they are first called in the plugin. This means that if you are only planning on using one of the libraries, the plugin never loads the other library.\nTo build the libraries from source, refer to the DEVELOPER_GUIDE.\nFor more information about JNI, see Java Native Interface on Wikipedia.",
    "ancestors": [
      "Search",
      "k-NN"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/knn/knn-index/",
    "title": "k-NN Index",
    "content": "knn_vector data type\nThe k-NN plugin introduces a custom data type, the knn_vector, that allows users to ingest their k-NN vectors\ninto an OpenSearch index and perform different kinds of k-NN search. The knn_vector field is highly configurable and can serve many different k-NN workloads. In general, a knn_vector field can be built either by providing a method definition or specifying a model id.\nMethod definitions are used when the underlying Approximate k-NN algorithm does not require training. For example, the following knn_vector field specifies that nmslib ’s implementation of hnsw should be used for Approximate k-NN search. During indexing, nmslib will build the corresponding hnsw segment files. \"my_vector\": { \"type\": \"knn_vector\", \"dimension\": 4, \"method\": { \"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": { \"ef_construction\": 128, \"m\": 24 } } } Model IDs are used when the underlying Approximate k-NN algorithm requires a training step. As a prerequisite, the\nmodel has to be created with the Train API. The\nmodel contains the information needed to initialize the native library segment files. \"type\": \"knn_vector\", \"model_id\": \"my-model\" } However, if you intend to just use painless scripting or a k-NN score script, you only need to pass the dimension. \"type\": \"knn_vector\", \"dimension\": 128 } Method Definitions\nA method definition refers to the underlying configuration of the Approximate k-NN algorithm you want to use. Method definitions are used to either create a knn_vector field (when the method does not require training) or create a model during training that can then be used to create a knn_vector field.\nA method definition will always contain the name of the method, the space_type the method is built for, the engine\n(the library) to use, and a map of parameters. Mapping Parameter Required Default Updatable Description name true\nn/a\nfalse\nThe identifier for the nearest neighbor method. space_type false\nl2\nfalse\nThe vector space used to calculate the distance between vectors. engine false\nnmslib\nfalse\nThe approximate k-NN library to use for indexing and search. The available libraries are faiss, nmslib, and Lucene. parameters false\nnull\nfalse\nThe parameters used for the nearest neighbor method. Supported nmslib methods Method Name Requires Training? Supported Spaces Description hnsw false\nl2, innerproduct, cosinesimil, l1, linf\nHierarchical proximity graph approach to Approximate k-NN search. For more details on the algorithm, see this abstract. HNSW parameters Parameter Name Required Default Updatable Description ef_construction false\n512\nfalse\nThe size of the dynamic list used during k-NN graph creation. Higher values lead to a more accurate graph but slower indexing speed. m false\n16\nfalse\nThe number of bidirectional links that the plugin creates for each new element. Increasing and decreasing this value can have a large impact on memory consumption. Keep this value between 2 and 100. For nmslib, ef_search is set in the index settings.\nSupported faiss methods Method Name Requires Training? Supported Spaces Description hnsw false\nl2, innerproduct\nHierarchical proximity graph approach to Approximate k-NN search. ivf true\nl2, innerproduct\nBucketing approach where vectors are assigned different buckets based on clustering and, during search, only a subset of the buckets is searched. For hnsw, “innerproduct” is not available when PQ is used.\nHNSW parameters Parameter Name Required Default Updatable Description ef_search false\n512\nfalse\nThe size of the dynamic list used during k-NN searches. Higher values lead to more accurate but slower searches. ef_construction false\n512\nfalse\nThe size of the dynamic list used during k-NN graph creation. Higher values lead to a more accurate graph but slower indexing speed. m false\n16\nfalse\nThe number of bidirectional links that the plugin creates for each new element. Increasing and decreasing this value can have a large impact on memory consumption. Keep this value between 2 and 100. encoder false\nflat\nfalse\nEncoder definition for encoding vectors. Encoders can reduce the memory footprint of your index, at the expense of search accuracy. IVF parameters Parameter Name Required Default Updatable Description nlist false\n4\nfalse\nNumber of buckets to partition vectors into. Higher values may lead to more accurate searches at the expense of memory and training latency. For more information about choosing the right value, refer to Guidelines to choose an index. nprobes false\n1\nfalse\nNumber of buckets to search during query. Higher values lead to more accurate but slower searches. encoder false\nflat\nfalse\nEncoder definition for encoding vectors. Encoders can reduce the memory footprint of your index, at the expense of search accuracy. For more information about setting these parameters, please refer to faiss ’s documentation.\nIVF training requirements\nThe IVF algorithm requires a training step. To create an index that uses IVF, you need to train a model with the Train API, passing the IVF method definition. IVF requires that, at a minimum, there should be nlist training\ndata points, but it is recommended that you use more.\nTraining data can be composed of either the same data that is going to be ingested or a separate dataset.\nSupported Lucene methods Method Name Requires Training? Supported Spaces Description hnsw false\nl2, cosinesimil\nHierarchical proximity graph approach to Approximate k-NN search. HNSW parameters Parameter Name Required Default Updatable Description ef_construction false\n512\nfalse\nThe size of the dynamic list used during k-NN graph creation. Higher values lead to a more accurate graph but slower indexing speed. The Lucene engine uses the proprietary term “beam_width” to describe this function, which corresponds directly to “ef_construction”. To be consistent throughout OpenSearch documentation, we retain the term “ef_construction” to label this parameter. m false\n16\nfalse\nThe number of bidirectional links that the plugin creates for each new element. Increasing and decreasing this value can have a large impact on memory consumption. Keep this value between 2 and 100. The Lucene engine uses the proprietary term “max_connections” to describe this function, which corresponds directly to “m”. To be consistent throughout OpenSearch documentation, we retain the term “m” to label this parameter. Lucene HNSW implementation ignores ef_search and dynamically sets it to the value of “k” in the search request. Therefore, there is no need to make settings for ef_search when using the Lucene engine. { \"type\": \"knn_vector\", \"dimension\": 100, \"method\": { \"name\": \"hnsw\", \"engine\": \"lucene\", \"space_type\": \"l2\", \"parameters\":{ \"m\": 2048, \"ef_construction\": 245 } } } Supported faiss encoders\nYou can use encoders to reduce the memory footprint of a k-NN index at the expense of search accuracy. faiss has\nseveral encoder types, but the plugin currently only supports flat and pq encoding.\nAn example method definition that specifies an encoder may look something like this: \"method\": { \"name\": \"hnsw\", \"engine\": \"faiss\", \"parameters\":{ \"encoder\":{ \"name\": \"pq\", \"parameters\":{ \"code_size\": 8, \"m\": 8 } } } } Encoder Name Requires Training? Description flat false\nEncode vectors as floating point arrays. This encoding does not reduce memory footprint. pq true\nShort for product quantization, it is a lossy compression technique that encodes a vector into a fixed size of bytes using clustering, with the goal of minimizing the drop in k-NN search accuracy. From a high level, vectors are broken up into m subvectors, and then each subvector is represented by a code_size code obtained from a code book produced during training. For more details on product quantization, here is a great blog post! PQ parameters Paramater Name Required Default Updatable Description m false\n1\nfalse\nDetermine how many many sub-vectors to break the vector into. sub-vectors are encoded independently of each other. This dimension of the vector must be divisible by m. Max value is 1024. code_size false\n8\nfalse\nDetermines the number of bits to encode a sub-vector into. Max value is 8. Note — for IVF, this value must be less than or equal to 8. For HNSW, this value can only be 8. Choosing the right method\nThere are a lot of options to choose from when building your knn_vector field. To determine the correct methods and parameters to choose, you should first understand what requirements you have for your workload and what trade-offs you are willing to make. Factors to consider are (1) query latency, (2) query quality, (3) memory limits, (4) indexing latency.\nIf memory is not a concern, HNSW offers a very strong query latency/query quality tradeoff.\nIf you want to use less memory and index faster than HNSW, while maintaining similar query quality, you should evaluate IVF.\nIf memory is a concern, consider adding a PQ encoder to your HNSW or IVF index. Because PQ is a lossy encoding, query quality will drop.\nMemory Estimation\nIn a typical OpenSearch cluster, a certain portion of RAM is set aside for the JVM heap. The k-NN plugin allocates\nnative library indexes to a portion of the remaining RAM. This portion’s size is determined by\nthe circuit_breaker_limit cluster setting. By default, the limit is set at 50%.\nHaving a replica doubles the total number of vectors.\nHNSW memory estimation\nThe memory required for HNSW is estimated to be 1.1 * (4 * dimension + 8 * M) bytes/vector.\nAs an example, assume you have a million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows: 1.1 * (4 * 256 + 8 * 16) * 1,000,000 ~= 1.267 GB IVF memory estimation\nThe memory required for IVF is estimated to be 1.1 * (((4 * dimension) * num_vectors) + (4 * nlist * d)) bytes.\nAs an example, assume you have a million vectors with a dimension of 256 and nlist of 128. The memory requirement can be estimated as follows: 1.1 * (((4 * 256) * 1,000,000) + (4 * 128 * 256)) ~= 1.126 GB Index settings\nAdditionally, the k-NN plugin introduces several index settings that can be used to configure the k-NN structure as well.\nAt the moment, several parameters defined in the settings are in the deprecation process. Those parameters should be set in the mapping instead of the index settings. Parameters set in the mapping will override the parameters set in the index settings. Setting the parameters in the mapping allows an index to have multiple knn_vector fields with different parameters. Setting Default Updateable Description index.knn false\nfalse\nWhether the index should build native library indices for the knn_vector fields. If set to false, the knn_vector fields will be stored in doc values, but Approximate k-NN search functionality will be disabled. index.knn.algo_param.ef_search 512\ntrue\nThe size of the dynamic list used during k-NN searches. Higher values lead to more accurate but slower searches. Only available for nmslib. index.knn.algo_param.ef_construction 512\nfalse\nDeprecated in 1.0.0. Use the mapping parameters to set this value instead. index.knn.algo_param.m 16\nfalse\nDeprecated in 1.0.0. Use the mapping parameters to set this value instead. index.knn.space_type l2\nfalse\nDeprecated in 1.0.0. Use the mapping parameters to set this value instead.",
    "ancestors": [
      "Search",
      "k-NN"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/knn/knn-score-script/",
    "title": "Exact k-NN with scoring script",
    "content": "The k-NN plugin implements the OpenSearch score script plugin that you can use to find the exact k-nearest neighbors to a given query point. Using the k-NN score script, you can apply a filter on an index before executing the nearest neighbor search. This is useful for dynamic search cases where the index body may vary based on other conditions.\nBecause the score script approach executes a brute force search, it doesn’t scale as well as the approximate approach. In some cases, it might be better to think about refactoring your workflow or index structure to use the approximate approach instead of the score script approach.\nGetting started with the score script for vectors\nSimilar to approximate nearest neighbor search, in order to use the score script on a body of vectors, you must first create an index with one or more knn_vector fields.\nIf you intend to just use the score script approach (and not the approximate approach) you can set index.knn to false and not set index.knn.space_type. You can choose the space type during search. See spaces for the spaces the k-NN score script suppports.\nThis example creates an index with two knn_vector fields: PUT my-knn-index -1 { \"mappings\": { \"properties\": { \"my_vector1\": { \"type\": \"knn_vector\", \"dimension\": 2 }, \"my_vector2\": { \"type\": \"knn_vector\", \"dimension\": 4 } } } } If you only want to use the score script, you can omit \"index.knn\": true. The benefit of this approach is faster indexing speed and lower memory usage, but you lose the ability to perform standard k-NN queries on the index.\nAfter you create the index, you can add some data to it: POST _bulk { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"1\" } } { \"my_vector1\": [ 1.5, 2.5], \"price\": 12.2 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"2\" } } { \"my_vector1\": [ 2.5, 3.5], \"price\": 7.1 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"3\" } } { \"my_vector1\": [ 3.5, 4.5], \"price\": 12.9 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"4\" } } { \"my_vector1\": [ 5.5, 6.5], \"price\": 1.2 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"5\" } } { \"my_vector1\": [ 4.5, 5.5], \"price\": 3.7 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"6\" } } { \"my_vector2\": [ 1.5, 5.5, 4.5, 6.4], \"price\": 10.3 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"7\" } } { \"my_vector2\": [ 2.5, 3.5, 5.6, 6.7], \"price\": 5.5 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"8\" } } { \"my_vector2\": [ 4.5, 5.5, 6.7, 3.7], \"price\": 4.4 } { \"index\": { \"_index\": \"my-knn-index-1\", \"_id\": \"9\" } } { \"my_vector2\": [ 1.5, 5.5, 4.5, 6.4], \"price\": 8.9 } Finally, you can execute an exact nearest neighbor search on the data using the knn script: GET my-knn-index -1 /_search { \"size\": 4, \"query\": { \"script_score\": { \"query\": { \"match_all\": {} }, \"script\": { \"source\": \"knn_score\", \"lang\": \"knn\", \"params\": { \"field\": \"my_vector2\", \"query_value\": [ 2.0, 3.0, 5.0, 6.0], \"space_type\": \"cosinesimil\" } } } } } All parameters are required. lang is the script type. This value is usually painless, but here you must specify knn. source is the name of the script, knn_score.\nThis script is part of the k-NN plugin and isn’t available at the standard _scripts path. A GET request to _cluster/state/metadata doesn’t return it, either. field is the field that contains your vector data. query_value is the point you want to find the nearest neighbors for. For the Euclidean and cosine similarity spaces, the value must be an array of floats that matches the dimension set in the field’s mapping. For Hamming bit distance, this value can be either of type signed long or a base64-encoded string (for the long and binary field types, respectively). space_type corresponds to the distance function. See the spaces section.\nThe post filter example in the approximate approach shows a search that returns fewer than k results. If you want to avoid this situation, the score script method lets you essentially invert the order of events. In other words, you can filter down the set of documents over which to execute the k-nearest neighbor search.\nThis example shows a pre-filter approach to k-NN search with the score script approach. First, create the index: PUT my-knn-index -2 { \"mappings\": { \"properties\": { \"my_vector\": { \"type\": \"knn_vector\", \"dimension\": 2 }, \"color\": { \"type\": \"keyword\" } } } } Then add some documents: POST _bulk { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"1\" } } { \"my_vector\": [ 1, 1], \"color\": \"RED\" } { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"2\" } } { \"my_vector\": [ 2, 2], \"color\": \"RED\" } { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"3\" } } { \"my_vector\": [ 3, 3], \"color\": \"RED\" } { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"4\" } } { \"my_vector\": [ 10, 10], \"color\": \"BLUE\" } { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"5\" } } { \"my_vector\": [ 20, 20], \"color\": \"BLUE\" } { \"index\": { \"_index\": \"my-knn-index-2\", \"_id\": \"6\" } } { \"my_vector\": [ 30, 30], \"color\": \"BLUE\" } Finally, use the script_score query to pre-filter your documents before identifying nearest neighbors: GET my-knn-index -2 /_search { \"size\": 2, \"query\": { \"script_score\": { \"query\": { \"bool\": { \"filter\": { \"term\": { \"color\": \"BLUE\" } } } }, \"script\": { \"lang\": \"knn\", \"source\": \"knn_score\", \"params\": { \"field\": \"my_vector\", \"query_value\": [ 9.9, 9.9], \"space_type\": \"l2\" } } } } } Getting started with the score script for binary data\nThe k-NN score script also allows you to run k-NN search on your binary data with the Hamming distance space.\nIn order to use Hamming distance, the field of interest must have either a binary or long field type. If you’re using binary type, the data must be a base64-encoded string.\nThis example shows how to use the Hamming distance space with a binary field type: PUT my-index { \"mappings\": { \"properties\": { \"my_binary\": { \"type\": \"binary\", \"doc_values\": true }, \"color\": { \"type\": \"keyword\" } } } } Then add some documents: POST _bulk { \"index\": { \"_index\": \"my-index\", \"_id\": \"1\" } } { \"my_binary\": \"SGVsbG8gV29ybGQh\", \"color\": \"RED\" } { \"index\": { \"_index\": \"my-index\", \"_id\": \"2\" } } { \"my_binary\": \"ay1OTiBjdXN0b20gc2NvcmluZyE=\", \"color\": \"RED\" } { \"index\": { \"_index\": \"my-index\", \"_id\": \"3\" } } { \"my_binary\": \"V2VsY29tZSB0byBrLU5O\", \"color\": \"RED\" } { \"index\": { \"_index\": \"my-index\", \"_id\": \"4\" } } { \"my_binary\": \"SSBob3BlIHRoaXMgaXMgaGVscGZ1bA==\", \"color\": \"BLUE\" } { \"index\": { \"_index\": \"my-index\", \"_id\": \"5\" } } { \"my_binary\": \"QSBjb3VwbGUgbW9yZSBkb2NzLi4u\", \"color\": \"BLUE\" } { \"index\": { \"_index\": \"my-index\", \"_id\": \"6\" } } { \"my_binary\": \"TGFzdCBvbmUh\", \"color\": \"BLUE\" } Finally, use the script_score query to pre-filter your documents before identifying nearest neighbors: GET my-index/_search { \"size\": 2, \"query\": { \"script_score\": { \"query\": { \"bool\": { \"filter\": { \"term\": { \"color\": \"BLUE\" } } } }, \"script\": { \"lang\": \"knn\", \"source\": \"knn_score\", \"params\": { \"field\": \"my_binary\", \"query_value\": \"U29tZXRoaW5nIEltIGxvb2tpbmcgZm9y\", \"space_type\": \"hammingbit\" } } } } } Similarly, you can encode your data with the long field and run a search: GET my-long-index/_search { \"size\": 2, \"query\": { \"script_score\": { \"query\": { \"bool\": { \"filter\": { \"term\": { \"color\": \"BLUE\" } } } }, \"script\": { \"lang\": \"knn\", \"source\": \"knn_score\", \"params\": { \"field\": \"my_long\", \"query_value\": 23, \"space_type\": \"hammingbit\" } } } } } Spaces\nA space corresponds to the function used to measure the distance between two points in order to determine the k-nearest neighbors. From the k-NN perspective, a lower score equates to a closer and better result. This is the opposite of how OpenSearch scores results, where a greater score equates to a better result. The following table illustrates how OpenSearch converts spaces to scores: spaceType Distance Function (d) OpenSearch Score l1\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n |x_i - y_i| \\]\n\\[ score = {1 \\over 1 + d } \\]\nl2\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^n (x_i - y_i)^2 \\]\n\\[ score = {1 \\over 1 + d } \\]\nlinf\n\\[ d(\\mathbf{x}, \\mathbf{y}) = max(|x_i - y_i|) \\]\n\\[ score = {1 \\over 1 + d } \\]\ncosinesimil\n\\[ d(\\mathbf{x}, \\mathbf{y}) = cos \\theta = {\\mathbf{x} &middot; \\mathbf{y} \\over \\|\\mathbf{x}\\| &middot; \\|\\mathbf{y}\\|}\\]\\[ =\n{\\sum_{i=1}^n x_i y_i \\over \\sqrt{\\sum_{i=1}^n x_i^2} &middot; \\sqrt{\\sum_{i=1}^n y_i^2}}\\]\nwhere \\(\\|\\mathbf{x}\\|\\) and \\(\\|\\mathbf{y}\\|\\) represent normalized vectors.\n\\[ score = 1 + d \\]\ninnerproduct (not supported for Lucene)\n\\[ d(\\mathbf{x}, \\mathbf{y}) = - {\\mathbf{x} &middot; \\mathbf{y}} = - \\sum_{i=1}^n x_i y_i \\]\n\\[ \\text{If} d \\ge 0, \\] \\[score = {1 \\over 1 + d }\\] \\[\\text{If} d &lt; 0, score = &minus;d + 1\\]\nhammingbit\n\\[ d(\\mathbf{x}, \\mathbf{y}) = \\text{countSetBits}(\\mathbf{x} \\oplus \\mathbf{y})\\]\n\\[ score = {1 \\over 1 + d } \\]\nCosine similarity returns a number between -1 and 1, and because OpenSearch relevance scores can’t be below 0, the k-NN plugin adds 1 to get the final score.",
    "ancestors": [
      "Search",
      "k-NN"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/knn/painless-functions/",
    "title": "k-NN Painless extensions",
    "content": "With the k-NN plugin’s Painless Scripting extensions, you can use k-NN distance functions directly in your Painless scripts to perform operations on knn_vector fields. Painless has a strict list of allowed functions and classes per context to ensure its scripts are secure. The k-NN plugin adds Painless Scripting extensions to a few of the distance functions used in k-NN score script, so you can use them to customize your k-NN workload.\nGet started with k-NN’s Painless Scripting functions\nTo use k-NN’s Painless Scripting functions, first create an index with knn_vector fields like in k-NN score script. Once the index is created and you ingest some data, you can use the painless extensions: GET my-knn-index -2 /_search { \"size\": 2, \"query\": { \"script_score\": { \"query\": { \"bool\": { \"filter\": { \"term\": { \"color\": \"BLUE\" } } } }, \"script\": { \"source\": \"1.0 + cosineSimilarity(params.query_value, doc[params.field])\", \"params\": { \"field\": \"my_vector\", \"query_value\": [ 9.9, 9.9] } } } } } field needs to map to a knn_vector field, and query_value needs to be a floating point array with the same dimension as field.\nFunction types\nThe following table describes the available painless functions the k-NN plugin provides: Function name Function signature Description l2Squared float l2Squared (float[] queryVector, doc['vector field']) This function calculates the square of the L2 distance (Euclidean distance) between a given query vector and document vectors. The shorter the distance, the more relevant the document is, so this example inverts the return value of the l2Squared function. If the document vector matches the query vector, the result is 0, so this example also adds 1 to the distance to avoid divide by zero errors.\nl1Norm float l1Norm (float[] queryVector, doc['vector field']) This function calculates the square of the L2 distance (Euclidean distance) between a given query vector and document vectors. The shorter the distance, the more relevant the document is, so this example inverts the return value of the l2Squared function. If the document vector matches the query vector, the result is 0, so this example also adds 1 to the distance to avoid divide by zero errors.\ncosineSimilarity float cosineSimilarity (float[] queryVector, doc['vector field']) Cosine similarity is an inner product of the query vector and document vector normalized to both have a length of 1. If the magnitude of the query vector doesn’t change throughout the query, you can pass the magnitude of the query vector to improve performance, instead of calculating the magnitude every time for every filtered document: float cosineSimilarity (float[] queryVector, doc['vector field'], float normQueryVector) In general, the range of cosine similarity is [-1, 1]. However, in the case of information retrieval, the cosine similarity of two documents ranges from 0 to 1 because the tf-idf statistic can’t be negative. Therefore, the k-NN plugin adds 1.0 in order to always yield a positive cosine similarity score. Constraints\nIf a document’s knn_vector field has different dimensions than the query, the function throws an IllegalArgumentException.\nIf a vector field doesn’t have a value, the function throws an IllegalStateException.\nYou can avoid this situation by first checking if a document has a value in its field: \"source\": \"doc[params.field].size() == 0? 0: 1 / (1 + l2Squared(params.query_value, doc[params.field]))\", Because scores can only be positive, this script ranks documents with vector fields higher than those without.",
    "ancestors": [
      "Search",
      "k-NN"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/knn/performance-tuning/",
    "title": "Performance tuning",
    "content": "This topic provides performance tuning recommendations to improve indexing and search performance for approximate k-NN (ANN). From a high level, k-NN works according to these principles:\nNative library indices are created per knn_vector field / (Lucene) segment pair.\nQueries execute on segments sequentially inside the shard (same as any other OpenSearch query).\nEach native library index in the segment returns &lt;=k neighbors.\nThe coordinator node picks up final size number of neighbors from the neighbors returned by each shard.\nThis topic also provides recommendations for comparing approximate k-NN to exact k-NN with score script.\nIndexing performance tuning\nTake the following steps to improve indexing performance, especially when you plan to index a large number of vectors at once: Disable the refresh interval Either disable the refresh interval (default = 1 sec), or set a long duration for the refresh interval to avoid creating multiple small segments: PUT /&lt;index_name&gt;/_settings { \"index\": { \"refresh_interval\": \"-1\" } } Note: Make sure to reenable refresh_interval after indexing finishes. Disable replicas (no OpenSearch replica shard) Set replicas to 0 to prevent duplicate construction of native library indices in both primary and replica shards. When you enable replicas after indexing finishes, the serialized native library indices are directly copied. If you have no replicas, losing nodes might cause data loss, so it’s important that the data lives elsewhere so this initial load can be retried in case of an issue. Increase the number of indexing threads If the hardware you choose has multiple cores, you can allow multiple threads in native library index construction by speeding up the indexing process. Determine the number of threads to allot with the knn.algo_param.index_thread_qty setting.\nKeep an eye on CPU utilization and choose the correct number of threads. Because native library index construction is costly, having multiple threads can cause additional CPU load.\nSearch performance tuning\nTake the following steps to improve search performance: Reduce segment count To improve search performance, you must keep the number of segments under control. Lucene’s IndexSearcher searches over all of the segments in a shard to find the ‘size’ best results.\nIdeally, having one segment per shard provides the optimal performance with respect to search latency. You can configure an index to have multiple shards to avoid giant shards and achieve more parallelism.\nYou can control the number of segments by choosing a larger refresh interval, or during indexing by asking OpenSearch to slow down segment creation by disabling the refresh interval. Warm up the index Native library indices are constructed during indexing, but they’re loaded into memory during the first search. In Lucene, each segment is searched sequentially (so, for k-NN, each segment returns up to k nearest neighbors of the query point), and the top ‘size’ number of results based on the score are returned from all the results returned by segements at a shard level (higher score = better result).\nOnce a native library index is loaded (native library indices are loaded outside OpenSearch JVM), OpenSearch caches them in memory. Initial queries are expensive and take a few seconds, while subsequent queries are faster and take milliseconds (assuming the k-NN circuit breaker isn’t hit).\nTo avoid this latency penalty during your first queries, you can use the warmup API operation on the indices you want to search: GET /_plugins/_knn/warmup/index 1,index 2,index 3?pretty { \"_shards\": { \"total\": 6, \"successful\": 6, \"failed\": 0 } } The warmup API operation loads all native library indices for all shards (primary and replica) for the specified indices into the cache, so there’s no penalty to load native library indices during initial searches. Note: This API operation only loads the segments of the indices it sees into the cache. If a merge or refresh operation finishes after the API runs, or if you add new documents, you need to rerun the API to load those native library indices into memory. Avoid reading stored fields If your use case is simply to read the IDs and scores of the nearest neighbors, you can disable reading stored fields, which saves time retrieving the vectors from stored fields. Use mmap file I/O For the Lucene-based approximate k-NN search, there is no dedicated cache layer that speeds up read/write operations. Instead, the plugin relies on the existing caching mechanism in OpenSearch core. In versions 2.4 and earlier of the Lucene-based approximate k-NN search, read/write operations were based on Java NIO by default, which can be slow, depending on the Lucene version and number of segments per shard. Starting with version 2.5, k-NN enables mmap file I/O by default when the store type is hybridfs (the default store type in OpenSearch). This leads to fast file I/O operations and improves the overall performance of both data ingestion and search. The two file extensions specific to vector values that use mmap are.vec and.vem. For more information about these file extensions, see the Lucene documentation.\nThe mmap file I/O uses the system file cache rather than memory allocated for the Java heap, so no additional allocation is required. To change the default list of extensions set by the plugin, update the index.store.hybrid.mmap.extensions setting at the cluster level using the Cluster Settings API. Note: This is an expert-level setting that requires closing the index before updating the setting and reopening it after the update.\nImproving recall\nRecall depends on multiple factors like number of vectors, number of dimensions, segments, and so on. Searching over a large number of small segments and aggregating the results leads to better recall than searching over a small number of large segments and aggregating results. The larger the native library index, the more chances of losing recall if you’re using smaller algorithm parameters. Choosing larger values for algorithm parameters should help solve this issue but sacrifices search latency and indexing time. That being said, it’s important to understand your system’s requirements for latency and accuracy, and then choose the number of segments you want your index to have based on experimentation.\nThe default parameters work on a broader set of use cases, but make sure to run your own experiments on your data sets and choose the appropriate values. For index-level settings, see Index settings.\nApproximate nearest neighbor versus score script\nThe standard k-NN query and custom scoring option perform differently. Test with a representative set of documents to see if the search results and latencies match your expectations.\nCustom scoring works best if the initial filter reduces the number of documents to no more than 20,000. Increasing shard count can improve latency, but be sure to keep shard size within the recommended guidelines.",
    "ancestors": [
      "Search",
      "k-NN"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/knn/settings/",
    "title": "Settings",
    "content": "The k-NN plugin adds several new cluster settings.\nCluster settings Setting Default Description knn.algo_param.index_thread_qty 1\nThe number of threads used for native library index creation. Keeping this value low reduces the CPU impact of the k-NN plugin, but also reduces indexing performance. knn.cache.item.expiry.enabled false\nWhether to remove native library indices that have not been accessed for a certain duration from memory. knn.cache.item.expiry.minutes 3h\nIf enabled, the idle time before removing a native library index from memory. knn.circuit_breaker.unset.percentage 75%\nThe native memory usage threshold for the circuit breaker. Memory usage must be below this percentage of knn.memory.circuit_breaker.limit for knn.circuit_breaker.triggered to remain false. knn.circuit_breaker.triggered false\nTrue when memory usage exceeds the knn.circuit_breaker.unset.percentage value. knn.memory.circuit_breaker.limit 50%\nThe native memory limit for native library indices. At the default value, if a machine has 100 GB of memory and the JVM uses 32 GB, the k-NN plugin uses 50% of the remaining 68 GB (34 GB). If memory usage exceeds this value, k-NN removes the least recently used native library indices. knn.memory.circuit_breaker.enabled true\nWhether to enable the k-NN memory circuit breaker. knn.plugin.enabled true\nEnables or disables the k-NN plugin. knn.model.index.number_of_shards 1\nNumber of shards to use for the model system index, the OpenSearch index that stores the models used for Approximate k-NN Search. knn.model.index.number_of_replicas 1\nNumber of replica shards to use for the model system index. Generally, in a multi-node cluster, this should be at least 1 to increase stability.",
    "ancestors": [
      "Search",
      "k-NN"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/neural-search/",
    "title": "Neural Search plugin",
    "content": "The Neural Search plugin is an experimental feature. For updates on the progress of the Neural Search plugin, or if you want to leave feedback that could help improve the feature, join the discussion in the Neural Search forum.\nThe OpenSearch Neural Search plugin enables the integration of machine learning (ML) language models into your search workloads. During ingestion and search, the Neural Search plugin transforms text into vectors. Then, Neural Search uses the transformed vectors in vector-based search.\nThe Neural Search plugin comes bundled with OpenSearch. For more information, see Managing plugins.\nIngest data with Neural Search\nIn order to ingest vectorized documents, you need to create a Neural Search pipeline. A pipeline consists of a series of processors that manipulate documents during ingestion, allowing the documents to be vectorized. The following API operation creates a Neural Search pipeline: PUT _ingest/pipeline/&lt;pipeline_name&gt; In the pipeline request body, The text_embedding processor, the only processor supported by Neural Search, converts a document’s text to vector embeddings. text_embedding uses field_map s to determine what fields from which to generate vector embeddings and also which field to store the embedding.\nPath parameter\nUse pipeline_name to create a name for your Neural Search pipeline.\nRequest fields Field Data type Description description\nstring\nA description of the processor.\nmodel_id\nstring\nThe ID of the model that will be used in the embedding interface. The model must be indexed in OpenSearch before it can be used in Neural Search. For more information, see Model Serving Framework input_field_name\nstring\nThe field name used to cache text for text embeddings.\noutput_field_name\nstring\nThe name of the field in which output text is stored. Example request\nUse the following example request to create a pipeline: PUT _ingest/pipeline/nlp-pipeline\n{\n\"description\": \"An example neural search pipeline\",\n\"processors\": [\n{\n\"text_embedding\": {\n\"model_id\": \"bxoDJ7IHGM14UqatWc_2j\",\n\"field_map\": {\n\"passage_text\": \"passage_embedding\"\n}\n}\n}]\n} Example response\nOpenSearch responds with an acknowledgment of the pipeline’s creation. PUT _ingest/pipeline/nlp-pipeline { \"acknowledged\": true } Create an index for ingestion\nIn order to use the text embedding processor defined in your pipelines, create an index with mapping data that aligns with the maps specified in your pipeline. For example, the output_fields defined in the field_map field of your processor request must map to the k-NN vector fields with a dimension that matches the model. Similarly, the text_fields defined in your processor should map to the text_fields in your index.\nExample request\nThe following example request creates an index that attaches to a Neural Search pipeline. Because the index maps to k-NN vector fields, the index setting field index-knn is set to true. Furthermore, mapping settings use k-NN method definitions to match the maps defined in the Neural Search pipeline. PUT /my-nlp-index -1 { \"settings\": { \"index.knn\": true, \"default_pipeline\": \"&lt;pipeline_name&gt;\" }, \"mappings\": { \"properties\": { \"passage_embedding\": { \"type\": \"knn_vector\", \"dimension\": int, \"method\": { \"name\": \"string\", \"space_type\": \"string\", \"engine\": \"string\", \"parameters\": json_object } }, \"passage_text\": { \"type\": \"text\" }, } } } Example response\nOpenSearch responds with information about your new index: { \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"my-nlp-index-1\" } Ingest documents into Neural Search\nDocument ingestion is managed by OpenSearch’s Ingest API, similarly to other OpenSearch indexes. For example, you can ingest a document that contains the passage_text: \"Hello world\" with a simple POST method: POST /my-nlp-index -1 /_doc { \"passage_text\": \"Hello world\" } With the text_embedding processor in place through a Neural Search pipeline, the example indexes “Hello world” as a text_field and converts “Hello world” into an associated k-NN vector field.\nSearch a neural index\nIf you want to use a language model to convert a text query into a k-NN vector query, use the neural query fields in your query. The neural query request fields can be used in both the k-NN plugin API and Query DSL. Furthermore, you can use a k-NN search filter to refine your neural search query.\nNeural request fields\nInclude the following request fields under the neural field in your query: Field Data type Description vector_field\nstring\nThe vector field against which to run a search query.\nquery_text\nstring\nThe query text from which to produce queries.\nmodel_id\nstring\nThe ID of the model that will be used in the embedding interface. The model must be indexed in OpenSearch before it can be used in Neural Search.\nk\ninteger\nThe number of results the k-NN search returns. Example request\nThe following example request uses a search query that returns vectors for the “Hello World” query text: GET my_index/_search { \"query\": { \"bool\": { \"filter\": { \"range\": { \"distance\": { \"lte\": 20 } } }, \"should\": [ { \"script_score\": { \"query\": { \"neural\": { \"passage_vector\": { \"query_text\": \"Hello world\", \"model_id\": \"xzy76xswsd\", \"k\": 100 } } }, \"script\": { \"source\": \"_score * 1.5\" } } }, { \"script_score\": { \"query\": { \"match\": { \"passage_text\": \"Hello world\" } }, \"script\": { \"source\": \"_score * 1.7\" } } }] } } }",
    "ancestors": [
      "Search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/point-in-time-api/",
    "title": "Point in Time API",
    "content": "Use the Point in Time (PIT) API to manage PITs.\nTable of contents Create a PIT Path and HTTP methods Path parameters Query parameters Response fields Extend a PIT time List all PITs Cross-cluster behavior Response fields Delete PITs Cross-cluster behavior Request fields Response fields PIT segments Request fields PIT settings Create a PIT\nIntroduced 2.4\nCreates a PIT. The keep_alive query parameter is required; it specifies how long to keep a PIT.\nPath and HTTP methods POST /&lt;target_indexes&gt;/_search/point_in_time?keep_alive= 1 h&amp;routing=&amp;expand_wildcards=&amp;preference= Path parameters Parameter Data type Description target_indexes\nString\nThe name(s) of the target index(es) for the PIT. May contain a comma-separated list or a wildcard index pattern. Query parameters Parameter Data type Description keep_alive\nTime\nThe amount of time to keep the PIT. Every time you access a PIT by using the Search API, the PIT lifetime is extended by the amount of time equal to the keep_alive parameter. Required.\npreference\nString\nThe node or the shard used to perform the search. Optional. Default is random.\nrouting\nString\nSpecifies to route search requests to a specific shard. Optional. Default is the document’s _id.\nexpand_wildcards\nString\nThe type of index that can match the wildcard pattern. Supports comma-separated values. Valid values are the following: - all: Match any index or data stream, including hidden ones. - open: Match open, non-hidden indexes or non-hidden data streams. - closed: Match closed, non-hidden indexes or non-hidden data streams. - hidden: Match hidden indexes or data streams. Must be combined with open, closed or both open and closed. - none: No wildcard patterns are accepted. Optional. Default is open.\nallow_partial_pit_creation\nBoolean\nSpecifies whether to create a PIT with partial failures. Optional. Default is true. Example request POST /my-index -1 /_search/point_in_time?keep_alive= 100 m Example response { \"pit_id\": \"o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAIWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\", \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"creation_time\": 1658146050064 } Response fields Field Data type Description pit_id Base64 encoded binary The PIT ID.\ncreation_time\nlong\nThe time the PIT was created, in milliseconds since the epoch. Extend a PIT time\nYou can extend a PIT time by providing a keep_alive parameter in the pit object when you perform a search: GET /_search { \"size\": 10000, \"query\": { \"match\": { \"user.id\": \"elkbee\" } }, \"pit\": { \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", \"keep_alive\": \"100m\" }, \"sort\": [ { \"@timestamp\": { \"order\": \"asc\", \"format\": \"strict_date_optional_time_nanos\" }}, { \"_shard_doc\": \"desc\" }], \"search_after\": [ \"2021-05-20T05:30:04.832Z\"] } The keep_alive parameter in a search request is optional. It specifies the amount by which to extend the time to keep a PIT.\nList all PITs\nIntroduced 2.4\nReturns all PITs in the OpenSearch cluster.\nCross-cluster behavior\nThe List All PITs API returns only local PITs or mixed PITs (PITs created in both local and remote clusters). It does not return fully remote PITs.\nExample request GET /_search/point_in_time/_all Example response { \"pits\": [ { \"pit_id\": \"o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAEWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\", \"creation_time\": 1658146048666, \"keep_alive\": 6000000 }, { \"pit_id\": \"o463QQEPbXktaW5kZXgtMDAwMDAxFnNOWU43ckt3U3IyaFVpbGE1UWEtMncAFjFyeXBsRGJmVFM2RTB6eVg1aVVqQncAAAAAAAAAAAIWcDVrM3ZIX0pRNS1XejE5YXRPRFhzUQEWc05ZTjdyS3dTcjJoVWlsYTVRYS0ydwAA\", \"creation_time\": 1658146050064, \"keep_alive\": 6000000 }] } Response fields Field Data type Description pits\nArray of JSON objects\nThe list of all PITs. Each PIT object contains the following fields. Field Data type Description pit_id Base64 encoded binary The PIT ID.\ncreation_time\nlong\nThe time the PIT was created, in milliseconds since the epoch.\nkeep_alive\nlong\nThe amount of time to keep the PIT, in milliseconds. Delete PITs\nIntroduced 2.4\nDeletes one, several, or all PITs. PITs are automatically deleted when the keep_alive time period elapses. However, to deallocate resources, you can delete a PIT using the Delete PIT API. The Delete PIT API supports deleting a list of PITs by ID or deleting all PITs at once.\nCross-cluster behavior\nThe Delete PITs by ID API fully supports deleting cross-cluster PITs.\nThe Delete All PITs API deletes only local PITs or mixed PITs (PITs created in both local and remote clusters). It does not delete fully remote PITs.\nSample Request: Delete all PITs DELETE /_search/point_in_time/_all If you want to delete one or several PITs, specify their PIT IDs in the request body.\nRequest fields Field Data type Description pit_id Base64 encoded binary or an array of binaries\nThe PIT IDs of the PITs to be deleted. Required. Example request: Delete PITs by ID DELETE /_search/point_in_time { \"pit_id\": [ \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAEWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\", \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAIWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\"] } Example response\nFor each PIT, the response contains a JSON object with a PIT ID and a successful field that specifies whether the deletion was successful. Partial failures are treated as failures. { \"pits\": [ { \"successful\": true, \"pit_id\": \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAEWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\" }, { \"successful\": false, \"pit_id\": \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAIWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\" }] } Response fields Field Data type Description successful\nBoolean\nWhether the delete operation was successful.\npit_id Base64 encoded binary The PIT ID of the PIT to be deleted. PIT segments\nIntroduced 2.4\nSimilarly to the CAT Segments API, the PIT Segments API provides low-level information about the disk utilization of a PIT by describing its Lucene segments. The PIT Segments API supports listing segment information of a specific PIT by ID or of all PITs at once.\nExample request: PIT segments of all PITs GET /_cat/pit_segments/_all If you want to list segments for one or several PITs, specify their PIT IDs in the request body.\nRequest fields Field Data type Description pit_id Base64 encoded binary or an array of binaries\nThe PIT IDs of the PITs whose segments are to be listed. Required. Example request: PIT segments of PITs by ID GET /_cat/pit_segments { \"pit_id\": [ \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAEWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\", \"o463QQEPbXktaW5kZXgtMDAwMDAxFkhGN09fMVlPUkVPLXh6MUExZ1hpaEEAFjBGbmVEZHdGU1EtaFhhUFc4ZkR5cWcAAAAAAAAAAAIWaXBPNVJtZEhTZDZXTWFFR05waXdWZwEWSEY3T18xWU9SRU8teHoxQTFnWGloQQAA\"] } Example response index shard prirep ip segment generation docs.count docs.deleted size size.memory committed searchable version compound index 1 0 r 10.212. 36.190 _ 0 0 4 0 3.8 kb 1364 false true 8.8. 2 true index 1 1 p 10.212. 36.190 _ 0 0 3 0 3.7 kb 1364 false true 8.8. 2 true index 1 2 r 10.212. 74.139 _ 0 0 2 0 3.6 kb 1364 false true 8.8. 2 true PIT settings\nYou can specify the following settings for a PIT. Setting Description Default point_in_time.max_keep_alive\nA cluster-level setting that specifies the maximum value for the keep_alive parameter.\n24h\nsearch.max_open_pit_context\nA node-level setting that specifies the maximum number of open PIT contexts for the node.\n300",
    "ancestors": [
      "Search",
      "Point in Time"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/point-in-time/",
    "title": "Point in Time",
    "content": "Point in Time (PIT) lets you run different queries against a dataset that is fixed in time.\nNormally, if you run a query on an index multiple times, the same query may return different results because documents are continually indexed, updated, and deleted. If you need to run a query against the same data, you can preserve that data’s state by creating a PIT. The main use of the PIT feature is to couple it with the search_after functionality for deep pagination of search results.\nPaginating search results\nBesides the PIT functionality, there are three ways to paginate search results in OpenSearch: using the Scroll API, specifying from and size parameters for your search, and using the search_after functionality. However, all three have limitations:\nThe Scroll API’s search results are frozen at the moment of the request, but they are bound to a particular query. Additionally, scroll can only move forward in the search, so if a request for a page fails, the subsequent request skips that page and returns the following one.\nIf you specify the from and size parameters for your search, the search results are not frozen in time, so they may be inconsistent because of documents being indexed or deleted. The from and size feature is not recommended for deep pagination because every page request requires processing of all results and filtering them for the requested page.\nThe search_after search results are not frozen in time, so they may be inconsistent because of concurrent document indexing or deletion.\nThe PIT functionality does not have the limitations of other pagination methods, because PIT search is not bound to a query, and it supports consistent pagination going forward and backward. If you have looked at page one of your results and are now on page two, you will see the same page one if you go back.\nPIT search\nPIT search has the same capabilities as regular search, except PIT search acts on an older dataset, while a regular search acts on a live dataset. PIT search is not bound to a query, so you can run different queries on the same dataset, which is frozen in time.\nYou can use the Create PIT API to create a PIT. When you create a PIT for a set of indexes, OpenSearch locks a set of segments for those indexes, freezing them in time. On a lower level, none of the resources required for this PIT are modified or deleted. If the segments that are part of a PIT are merged, OpenSearch retains a copy of those segments for the period of time specified at PIT creation by the keep_alive parameter.\nThe create PIT operation returns a PIT ID, which you can use to run multiple queries on the frozen dataset. Even though the indexes continue to ingest data and modify or delete documents, the PIT references the data that has not changed since the PIT creation. When your query contains a PIT ID, you don’t need to pass the indexes to the search because it will use that PIT. A search with a PIT ID will produce exactly the same result when you run it multiple times.\nIn case of a cluster or node failure, all PIT data is lost.\nPagination with PIT and search_after\nWhen you run a query with a PIT ID, you can use the search_after parameter to retrieve the next page of results. This gives you control over the order of documents in the pages of results.\nRun a search query with a PIT ID: GET /_search { \"size\": 10000, \"query\": { \"match\": { \"user.id\": \"elkbee\" } }, \"pit\": { \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", \"keep_alive\": \"100m\" }, \"sort\": [ { \"@timestamp\": { \"order\": \"asc\", \"format\": \"strict_date_optional_time_nanos\" }}, { \"_shard_doc\": \"desc\" }] } The response contains the first 10,000 documents that match the query. To get the next set of documents, run the same query with the last document’s sort values as the search_after parameter, keeping the same sort and pit.id. You can use the optional keep_alive parameter to extend the PIT time: GET /_search { \"size\": 10000, \"query\": { \"match\": { \"user.id\": \"elkbee\" } }, \"pit\": { \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\", \"keep_alive\": \"100m\" }, \"sort\": [ { \"@timestamp\": { \"order\": \"asc\", \"format\": \"strict_date_optional_time_nanos\" }}, { \"_shard_doc\": \"desc\" }], \"search_after\": [ \"2021-05-20T05:30:04.832Z\"] } Search slicing\nUsing search_after with PIT for pagination gives you control over ordering of the results. If you don’t need results in any specific order, or if you want the ability to jump from a page to a non-consecutive page, you can use search slicing. Search slicing splits a PIT search into multiple slices that can be consumed independently by a client application.\nFor example, if you have a PIT search query that has 1,000,000 results and you want to return 50,000 results at a time, your client application has to make 20 consecutive calls to receive each batch of results. If you use search slicing, you can parallelize these 20 calls. In your multithreaded client application you can use five slices for each PIT. As a result, you will have 5 10,000-hit slices that can be consumed by five different threads in your client, instead of having a single thread consume 50,000 results.\nTo use search slicing, you have to specify two parameters: slice.id is the slice ID you are requesting. slice.max is the number of slices to break the search response into.\nThe following PIT search query illustrates search slicing: GET /_search { \"slice\": { \"id\": 0, // id is the slice (page) number being requested. In every request we can only query for one slice \"max\": 2 // max is the total number of slices (pages) the search response will be broken down into }, \"query\": { \"match\": { \"message\": \"foo\" } }, \"pit\": { \"id\": \"46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==\" } } In every request you can only query for one slice, so the next query will be the same as the previous one, except the slice.id will be 1.\nSecurity model\nThis section describes the permissions needed to use PIT API operations if you are running OpenSearch with the Security plugin enabled.\nUsers can access all PIT API operations using the point_in_time_full_access role. If this role doesn’t meet your needs, mix and match individual PIT permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the indices:data/read/point_in_time/create permission lets you create a PIT. The following are the possible permissions: indices:data/read/point_in_time/create – Create API indices:data/read/point_in_time/delete – Delete API indices:data/read/point_in_time/readall – List All PITs API indices:data/read/search – Search API indices:monitor/point_in_time/segments – PIT Segments API\nFor all API operations, such as list all and delete all, the user needs the all indexes (*) permission. For API operations such as search, create PIT, or delete list, the user only needs individual index permissions.\nThe PIT IDs always contain the underlying (resolved) indexes when saved. The following sections describe the required permissions for aliases and data streams.\nAlias permissions\nFor aliases, users must have either index or alias permissions for any PIT operation.\nData stream permissions\nFor data streams, users must have both the data stream and the data stream’s backing index permissions for any PIT operation. For example, the user must have permissions for the data-stream-11 data stream and for its backing index.ds-my-data-stream11-000001.\nIf users have the data stream permissions only, they will be able to create a PIT, but they will not be able to use the PIT ID for other operations, such as search, without the backing index permissions.\nAPI\nThe following table lists all Point in Time API functions. Function API Description Create PIT POST /&lt;target_indexes&gt;/_search/point_in_time?keep_alive=1h Creates a PIT. List PIT GET /_search/point_in_time/_all Lists all PITs. Delete PIT DELETE /_search/point_in_time DELETE /_search/point_in_time/_all Deletes a PIT or all PITs. PIT segments GET /_cat/pit_segments/_all Provides information about the disk utilization of a PIT by describing its Lucene segments. For information about the relevant cluster and node settings, see PIT Settings.",
    "ancestors": [
      "Search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/querqy/index/",
    "title": "Querqy",
    "content": "Querqy is a community plugin for query rewriting that helps to solve relevance issues, making search engines more precise regarding matching and scoring.\nQuerqy is currently only supported in OpenSearch 2.3.\nQuerqy plugin installation\nThe Querqy plugin is now available for OpenSearch 2.3.0. Run the following command to install the Querqy plugin../bin/opensearch-plugin install \\ \"https://repo1.maven.org/maven2/org/querqy/opensearch-querqy/1.0.os2.3.0/opensearch-querqy-1.0.os2.3.0.zip\" Answer yes to the security prompts during the installation as Querqy requires additional permissions to load query rewriters.\nAfter installing the Querqy plugin you can find comprehensive documentation on the Querqy.org site: Querqy Path and HTTP methods POST /myindex/_search Example query { \"query\": { \"querqy\": { \"matching_query\": { \"query\": \"books\" }, \"query_fields\": [ \"title^3.0\", \"words^2.1\", \"shortSummary\"] } } }",
    "ancestors": [
      "Search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/search-relevance/index/",
    "title": "Search relevance",
    "content": "Search relevance evaluates the accuracy of the search results returned by a query. The higher the relevance, the better the search engine. Compare Search Results is the first search relevance feature in OpenSearch.\nCompare Search Results\nCompare Search Results is an experimental feature. For updates on the progress of Compare Search Results and other search relevance features, or if you want to leave feedback that could help improve the feature, join the discussion on the OpenSearch forum.\nCompare Search Results in OpenSearch Dashboards lets you compare results from two queries side by side to determine whether one query produces better results than the other. Using this tool, you can evaluate search quality by experimenting with queries.\nFor example, you can see how results change when you apply one of the following query changes:\nWeighting different fields differently\nDifferent stemming or lemmatization strategies\nShingling\nPrerequisites\nBefore you get started, you must index data in OpenSearch. To learn how to create a new index, see Index data.\nAlternatively, you can add sample data in OpenSearch Dashboards using the following steps:\nOn the top menu bar, go to OpenSearch Dashboards &gt; Overview.\nSelect View app directory.\nSelect Add sample data.\nChoose one of the built-in datasets and select Add data.\nUsing Compare Search Results in OpenSearch Dashboards\nTo compare search results in OpenSearch Dashboards, perform the following steps. Step 1: On the top menu bar, go to OpenSearch Plugins &gt; Search Relevance. Step 2: Enter the search text in the search bar. Step 3: Select an index for Query 1 and enter a query (request body only) in OpenSearch Query DSL. The GET HTTP method and the _search endpoint are implicit. Use the %SearchText% variable to refer to the text in the search bar.\nThe following is an example query: { \"query\": { \"multi_match\": { \"query\": \"%SearchText%\", \"fields\": [ \"description\", \"item_name\"] } } } Step 4: Select an index for Query 2 and enter a query (request body only).\nThe following example query boosts the title field in search results: { \"query\": { \"multi_match\": { \"query\": \"%SearchText%\", \"fields\": [ \"description\", \"item_name^3\"] } } } Step 5: Select Search and compare the results in Result 1 and Result 2.\nThe following example screen shows a search for the word “cup” in the description and item_name fields with and without boosting the item_name: If a result in Result 1 appears in Result 2, the Up and Down indicators below the result number signify how many places the result moved up or down compared to the same result in Result 2. In this example, the document with the ID 2 is Up 1 place in Result 2 compared to Result 1 and Down 1 place in Result 1 compared to Result 2.\nChanging the number of results\nBy default, OpenSearch returns the top 10 results. To change the number of returned results to a different value, specify the size parameter in the query: { \"size\": 15, \"query\": { \"multi_match\": { \"query\": \"%SearchText%\", \"fields\": [ \"title^3\", \"text\"] } } } Setting size to a high value (for example, larger than 250 documents) may degrade performance.\nYou cannot save a given comparison for future use, so Compare Search Results is not suitable for systematic testing.\nComparing OpenSearch search results with re-ranked results\nOne use case for Compare Search Results is to compare raw OpenSearch results with the same results processed by a re-ranking application. An example of such a re-ranker is Kendra Intelligent Ranking for OpenSearch, contributed by the Amazon Kendra team. This plugin takes search results from OpenSearch and applies Amazon Kendra’s semantic relevance rankings calculated using vector embeddings and other semantic search techniques. For many applications, this provides better result rankings.\nTo try Kendra Intelligent Ranking, you must first set up the Amazon Kendra service. To get started, see Amazon Kendra. For detailed information, including plugin setup instructions, see Intelligently ranking OpenSearch (self managed) results using Amazon Kendra.\nOnce you’ve set up Kendra Intelligent Ranking, enter a query in Query 1 and enter the same query using Kendra Intelligent Ranking in Query 2. Then compare the search results from OpenSearch and Amazon Kendra.\nExample\nThe following example searches for the text “snacking nuts” in the abo index. The documents in the index contain snack descriptions in the bullet_point array. Enter snacking nuts in the search bar.\nEnter the following query, which searches the bullet_point field for the search text “snacking nuts”, in Query 1: { \"query\": { \"match\": { \"bullet_point\": \"%SearchText%\" } }, \"size\": 25 } Enter the same query with intelligent ranking in Query 2: { \"query\": { \"match\": { \"bullet_point\": \"%SearchText%\" } }, \"size\": 25, \"ext\": { \"search_configuration\":{ \"result_transformer\": { \"kendra_intelligent_ranking\": { \"order\": 1, \"properties\": { \"title_field\": \"item_name\", \"body_field\": \"bullet_point\" } } } } } } In the preceding query, body_field refers to the body field of the documents in the index, which Kendra Intelligent Ranking uses to rank the results. The body_field is required, while the title_field is optional.\nSelect Search and compare the results in Result 1 and Result 2.",
    "ancestors": [
      "Search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/search-relevance/stats-api/",
    "title": "Search Relevance Stats API",
    "content": "Introduced 2.7\nThe Search Relevance Stats API provides information about Search Relevance plugin operations. The Search Relevance plugin processes operations sent by the Compare Search Results Dashboards tool.\nThe Search Relevance Stats API captures statistics for a one-minute interval during which it receives a request. For example, if a request is received at 23:59:59.004, statistics are collected for the 23:58:00.000–23:58:59.999 time interval.\nTo change the default time interval for which statistics are collected, update the searchRelevanceDashboards.metrics.metricInterval setting in the opensearch_dashboards.yml file with the new time interval in milliseconds. The opensearch_dashboards.yml file is located in the config folder of your OpenSearch Dashboards installation. For example, the following sets the interval to one second: searchRelevanceDashboards.metrics.metricInterval: 1000 Example request\nYou can access the Search Relevance Stats API by providing its URL address in the following format: &lt;opensearch-dashboards-endpoint-address&gt;/api/relevancy/stats The OpenSearch Dashboards endpoint address may contain a port number if it is specified in the OpenSearch configuration file. The specific URL format depends on the type of OpenSearch deployment and the network environment in which it is hosted.\nYou can query the endpoint in two ways:\nBy accessing the endpoint address (for example, http://localhost:5601/api/relevancy/stats) in a browser\nBy using the curl command in the terminal: curl -X GET http://localhost:5601/api/relevancy/stats copy Example response\nThe following is the response for the preceding request: { \"data\": { \"search_relevance\": { \"fetch_index\": { \"200\": { \"response_time_total\": 28.79286289215088, \"count\": 1 } }, \"single_search\": { \"200\": { \"response_time_total\": 29.817723274230957, \"count\": 1 } }, \"comparison_search\": { \"200\": { \"response_time_total\": 13.265346050262451, \"count\": 2 } } } }, \"overall\": { \"response_time_avg\": 17.968983054161072, \"requests_per_second\": 0.06666666666666667 }, \"counts_by_component\": { \"search_relevance\": 4 }, \"counts_by_status_code\": { \"200\": 4 } } Response fields\nThe following table lists all response fields. Field Data type Description data.search_relevance Object\nStatistics related to Search Relevance operations. overall Object\nThe average statistics for all operations. overall.response_time_avg Double\nThe average response time for all operations, in milliseconds. overall.requests_per_second Double\nThe average number of requests per second for all operations. counts_by_component Object\nThe sum of all count values for all child objects of the data object. counts_by_component.search_relevance The total number of responses for all operations in the search_relevance object.\n  counts_by_status_code Object\nContains a list of all response codes and their counts for all Search Relevance operations. The data.search_relevance object\nThe data.search_relevance object contains the fields described in the following table. Field Data type Description comparison_search Object\nStatistics related to the comparison search operation. A comparison search operation is a request that compares two queries when both Query 1 and Query 2 are entered in the Compare Search Results tool. single_search Object\nStatistics related to a single search operation. A single search operation is a request to run a single query when only Query 1 or Query 2, not both, is entered in the Compare Search Results tool. fetch_index Object\nStatistics related to the operation of fetching the index or indexes for a comparison search or single search. Each of the comparison_search, single_search, and fetch_index objects contains a list of HTTP response codes. The following table lists the fields for each response code. Field Data type Description response_time_total Double\nThe sum of the response times for the responses with this HTTP code, in milliseconds. count Integer\nThe total number of responses with this HTTP code.",
    "ancestors": [
      "Search",
      "Search relevance"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/search-template/",
    "title": "Search templates",
    "content": "You can convert your full-text queries into a search template to accept user input and dynamically insert it into your query.\nFor example, if you use OpenSearch as a backend search engine for your application or website, you can take in user queries from a search bar or a form field and pass them as parameters into a search template. That way, the syntax to create OpenSearch queries is abstracted from your end users.\nWhen you’re writing code to convert user input into OpenSearch queries, you can simplify your code with search templates. If you need to add fields to your search query, you can just modify the template without making changes to your code.\nSearch templates use the Mustache language. For a list of all syntax options, see the Mustache manual.\nCreate search templates\nA search template has two components: the query and the parameters. Parameters are user-inputted values that get placed into variables. Variables are represented with double braces in Mustache notation. When encountering a variable like {{var}} in the query, OpenSearch goes to the params section, looks for a parameter called var, and replaces it with the specified value.\nYou can code your application to ask your user what they want to search for and then plug that value into the params object at runtime.\nThis command defines a search template to find a play by its name. The {{play_name}} in the query is replaced by the value Henry IV: GET _search/template { \"source\": { \"query\": { \"match\": { \"play_name\": \"{{play_name}}\" } } }, \"params\": { \"play_name\": \"Henry IV\" } } This template runs the search on your entire cluster.\nTo run this search on a specific index, add the index name to the request: GET shakespeare/_search/template Specify the from and size parameters: GET _search/template { \"source\": { \"from\": \"{{from}}\", \"size\": \"{{size}}\", \"query\": { \"match\": { \"play_name\": \"{{play_name}}\" } } }, \"params\": { \"play_name\": \"Henry IV\", \"from\": 10, \"size\": 10 } } To improve the search experience, you can define defaults so the user doesn’t have to specify every possible parameter. If the parameter is not defined in the params section, OpenSearch uses the default value.\nThe syntax for defining the default value for a variable var is as follows: {{ var }}{{ ^var }} default value {{ /var }} This command sets the defaults for from as 10 and size as 10: GET _search/template { \"source\": { \"from\": \"{{from}}{{^from}}10{{/from}}\", \"size\": \"{{size}}{{^size}}10{{/size}}\", \"query\": { \"match\": { \"play_name\": \"{{play_name}}\" } } }, \"params\": { \"play_name\": \"Henry IV\" } } Save and execute search templates\nAfter the search template works the way you want it to, you can save the source of that template as a script, making it reusable for different input parameters.\nWhen saving the search template as a script, you need to specify the lang parameter as mustache: POST _scripts/play_search_template { \"script\": { \"lang\": \"mustache\", \"source\": { \"from\": \"{{from}}{{^from}}0{{/from}}\", \"size\": \"{{size}}{{^size}}10{{/size}}\", \"query\": { \"match\": { \"play_name\": \"\" } } }, \"params\": { \"play_name\": \"Henry IV\" } } } Now you can reuse the template by referring to its id parameter.\nYou can reuse this source template for different input values. GET _search/template { \"id\": \"play_search_template\", \"params\": { \"play_name\": \"Henry IV\", \"from\": 0, \"size\": 1 } } Sample output { \"took\": 7, \"timed_out\": false, \"_shards\": { \"total\": 6, \"successful\": 6, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3205, \"relation\": \"eq\" }, \"max_score\": 3.641852, \"hits\": [ { \"_index\": \"shakespeare\", \"_type\": \"_doc\", \"_id\": \"4\", \"_score\": 3.641852, \"_source\": { \"type\": \"line\", \"line_id\": 5, \"play_name\": \"Henry IV\", \"speech_number\": 1, \"line_number\": \"1.1.2\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"Find we a time for frighted peace to pant,\" } }] } } If you have a stored template and want to validate it, use the render operation: POST _render/template { \"id\": \"play_search_template\", \"params\": { \"play_name\": \"Henry IV\" } } Sample output { \"template_output\": { \"from\": \"0\", \"size\": \"10\", \"query\": { \"match\": { \"play_name\": \"Henry IV\" } } } } The following render operations are supported: GET /_render/template POST /_render/template GET /_render/template/&lt;id&gt; POST /_render/template/&lt;id&gt; Advanced parameter conversion with search templates\nYou have a lot of different syntax options in Mustache to transpose the input parameters into a query.\nYou can specify conditions, run loops, join arrays, convert arrays to JSON, and so on.\nConditions\nUse the section tag in Mustache to represent conditions: {{ #var }} var {{ /var }} When var is a boolean value, this syntax acts as an if condition. The {{#var}} and {{/var}} tags insert the values placed between them only if var evaluates to true.\nUsing section tags would make your JSON invalid, so you must write your query in a string format instead.\nThis command includes the size parameter in the query only when the limit parameter is set to true.\nIn the following example, the limit parameter is true, so the size parameter is activated. As a result, you would get back only two documents. GET _search/template { \"source\": \"{ {{#limit}} \\\" size \\\": \\\" {{size}} \\\", {{/limit}} \\\" query \\\":{ \\\" match \\\":{ \\\" play_name \\\": \\\" {{play_name}} \\\" }}}\", \"params\": { \"play_name\": \"Henry IV\", \"limit\": true, \"size\": 2 } } You can also design an if-else condition.\nThis command sets size to 2 if limit is true. Otherwise, it sets size to 10. GET _search/template { \"source\": \"{ {{#limit}} \\\" size \\\": \\\" 2 \\\", {{/limit}} {{^limit}} \\\" size \\\": \\\" 10 \\\", {{/limit}} \\\" query \\\":{ \\\" match \\\":{ \\\" play_name \\\": \\\" {{play_name}} \\\" }}}\", \"params\": { \"play_name\": \"Henry IV\", \"limit\": true } } Loops\nYou can also use the section tag to implement a foreach loop: {{#var}}{{.}}}{{/var}} When var is an array, the search template iterates through it and creates a terms query. GET _search/template { \"source\": \"{ \\\" query \\\":{ \\\" terms \\\":{ \\\" play_name \\\":[ \\\" {{#play_name}} \\\", \\\" {{.}} \\\", \\\" {{/play_name}} \\\"]}}}\", \"params\": { \"play_name\": [ \"Henry IV\", \"Othello\"] } } This template is rendered as: GET _search/template { \"source\": { \"query\": { \"terms\": { \"play_name\": [ \"Henry IV\", \"Othello\"] } } } } Join\nYou can use the join tag to concatenate values of an array (separated by commas): GET _search/template { \"source\": { \"query\": { \"match\": { \"text_entry\": \"{{#join}}{{text_entry}}{{/join}}\" } } }, \"params\": { \"text_entry\": [ \"To be\", \"or not to be\"] } } Renders as: GET _search/template { \"source\": { \"query\": { \"match\": { \"text_entry\": \"{0=To be, 1=or not to be}\" } } } } Convert to JSON\nYou can use the toJson tag to convert parameters to their JSON representation: GET _search/template { \"source\": \"{ \\\" query \\\":{ \\\" bool \\\":{ \\\" must \\\":[{ \\\" terms \\\": { \\\" text_entries \\\": {{#toJson}}text_entries{{/toJson}} }}] }}}\", \"params\": { \"text_entries\": [ { \"term\": { \"text_entry\": \"love\" } }, { \"term\": { \"text_entry\": \"soldier\" } }] } } Renders as: GET _search/template { \"source\": { \"query\": { \"bool\": { \"must\": [ { \"terms\": { \"text_entries\": [ { \"term\": { \"text_entry\": \"love\" } }, { \"term\": { \"text_entry\": \"soldier\" } }] } }] } } } } Multiple search templates\nYou can bundle multiple search templates and send them to your OpenSearch cluster in a single request using the msearch operation.\nThis saves network round trip time, so you get back the response more quickly as compared to independent requests. GET _msearch/template { \"index\": \"shakespeare\" } { \"id\": \"if_search_template\", \"params\":{ \"play_name\": \"Henry IV\", \"limit\": false, \"size\": 2 }} { \"index\": \"shakespeare\" } { \"id\": \"play_search_template\", \"params\":{ \"play_name\": \"Henry IV\" }} Manage search templates\nTo list all scripts, run the following command: GET _cluster/state/metadata?pretty&amp;filter_path=**.stored_scripts To retrieve a specific search template, run the following command: GET _scripts/&lt;name_of_search_template&gt; To delete a search template, run the following command: DELETE _scripts/&lt;name_of_search_template&gt;",
    "ancestors": [
      "Search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/searching-data/autocomplete/",
    "title": "Autocomplete",
    "content": "Autocomplete shows suggestions to users while they type.\nFor example, if a user types “pop,” OpenSearch provides suggestions like “popcorn” or “popsicles.” These suggestions preempt your user’s intention and lead them to a possible search term more quickly.\nOpenSearch lets you design autocomplete that updates with each keystroke, provides a few relevant suggestions, and tolerates typos.\nImplement autocomplete using one of the following methods: Prefix matching Edge n-gram matching Search as you type Completion suggesters While prefix matching happens at query time, the other three methods happen at index time. All methods are described in the following sections.\nPrefix matching\nPrefix matching finds documents that match the last term in a query string.\nFor example, assume that the user types “qui” into a search UI. To autocomplete this phrase, use the match_phrase_prefix query to search for all text_entry field values that begin with the prefix “qui”: GET shakespeare/_search { \"query\": { \"match_phrase_prefix\": { \"text_entry\": { \"query\": \"qui\", \"slop\": 3 } } } } To make the word order and relative positions flexible, specify a slop value. To learn about the slop option, see Other advanced options.\nPrefix matching doesn’t require any special mappings. It works with your data as is.\nHowever, it’s a fairly resource-intensive operation. A prefix of a could match hundreds of thousands of terms and not be useful to your user.\nTo limit the impact of prefix expansion, set max_expansions to a reasonable number: GET shakespeare/_search { \"query\": { \"match_phrase_prefix\": { \"text_entry\": { \"query\": \"qui\", \"slop\": 3, \"max_expansions\": 10 } } } } To learn about the max_expansions option, see Other advanced options.\nThe ease of implementing query-time autocomplete comes at the cost of performance.\nWhen implementing this feature on a large scale, we recommend an index-time solution. With an index-time solution, you might experience slower indexing, but it’s a price you pay only once and not for every query. The edge n-gram, search-as-you-type, and completion suggester methods are index-time solutions.\nEdge n-gram matching\nDuring indexing, edge n-grams split a word into a sequence of n characters to support a faster lookup of partial search terms.\nIf you n-gram the word “quick,” the results depend on the value of n. n Type n-gram 1\nUnigram\n[ q, u, i, c, k]\n2\nBigram\n[ qu, ui, ic, ck]\n3\nTrigram\n[ qui, uic, ick]\n4\nFour-gram\n[ quic, uick]\n5\nFive-gram\n[ quick] Autocomplete needs only the beginning n-grams of a search phrase, so OpenSearch uses a special type of n-gram called edge n-gram.\nEdge n-gramming the word “quick” results in the following: q qu qui quic quick This follows the same sequence the user types.\nTo configure a field to use edge n-grams, create an autocomplete analyzer with an edge_ngram filter: PUT shakespeare { \"mappings\": { \"properties\": { \"text_entry\": { \"type\": \"text\", \"analyzer\": \"autocomplete\" } } }, \"settings\": { \"analysis\": { \"filter\": { \"edge_ngram_filter\": { \"type\": \"edge_ngram\", \"min_gram\": 1, \"max_gram\": 20 } }, \"analyzer\": { \"autocomplete\": { \"type\": \"custom\", \"tokenizer\": \"standard\", \"filter\": [ \"lowercase\", \"edge_ngram_filter\"] } } } } } This example creates the index and instantiates the edge n-gram filter and analyzer.\nThe edge_ngram_filter produces edge n-grams with a minimum n-gram length of 1 (a single letter) and a maximum length of 20. So it offers suggestions for words of up to 20 letters.\nThe autocomplete analyzer tokenizes a string into individual terms, lowercases the terms, and then produces edge n-grams for each term using the edge_ngram_filter.\nUse the analyze operation to test this analyzer: POST shakespeare/_analyze { \"analyzer\": \"autocomplete\", \"text\": \"quick\" } It returns edge n-grams as tokens: q qu qui quic quick Use the standard analyzer at search time. Otherwise, the search query splits into edge n-grams and you get results for everything that matches q, u, and i.\nThis is one of the few occasions when you use different analyzers at index time and at query time: GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": { \"query\": \"qui\", \"analyzer\": \"standard\" } } } } The response contains the matching documents: { \"took\": 5, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 533, \"relation\": \"eq\" }, \"max_score\": 9.712725, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"22006\", \"_score\": 9.712725, \"_source\": { \"type\": \"line\", \"line_id\": 22007, \"play_name\": \"Antony and Cleopatra\", \"speech_number\": 12, \"line_number\": \"5.2.44\", \"speaker\": \"CLEOPATRA\", \"text_entry\": \"Quick, quick, good hands.\" } }, { \"_index\": \"shakespeare\", \"_id\": \"54665\", \"_score\": 9.712725, \"_source\": { \"type\": \"line\", \"line_id\": 54666, \"play_name\": \"Loves Labours Lost\", \"speech_number\": 21, \"line_number\": \"5.1.52\", \"speaker\": \"HOLOFERNES\", \"text_entry\": \"Quis, quis, thou consonant?\" } }...] } } Alternatively, specify the search_analyzer in the mapping itself: \"mappings\": { \"properties\": { \"text_entry\": { \"type\": \"text\", \"analyzer\": \"autocomplete\", \"search_analyzer\": \"standard\" } } } Completion suggester\nThe completion suggester accepts a list of suggestions and builds them into a finite-state transducer (FST), an optimized data structure that is essentially a graph. This data structure lives in memory and is optimized for fast prefix lookups. To learn more about FSTs, see Wikipedia.\nAs the user types, the completion suggester moves through the FST graph one character at a time along a matching path. After it runs out of user input, it examines the remaining endings to produce a list of suggestions.\nThe completion suggester makes your autocomplete solution as efficient as possible and lets you have explicit control over its suggestions.\nUse a dedicated field type called completion, which stores the FST-like data structures in the index: PUT shakespeare { \"mappings\": { \"properties\": { \"text_entry\": { \"type\": \"completion\" } } } } To get suggestions, use the search endpoint with the suggest parameter: GET shakespeare/_search { \"suggest\": { \"autocomplete\": { \"prefix\": \"To be\", \"completion\": { \"field\": \"text_entry\" } } } } The phrase “to be” is prefix matched with the FST of the text_entry field: { \"took\": 29, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"autocomplete\": [ { \"text\": \"To be\", \"offset\": 0, \"length\": 5, \"options\": [ { \"text\": \"To be a comrade with the wolf and owl,--\", \"_index\": \"shakespeare\", \"_id\": \"50652\", \"_score\": 1.0, \"_source\": { \"type\": \"line\", \"line_id\": 50653, \"play_name\": \"King Lear\", \"speech_number\": 68, \"line_number\": \"2.4.230\", \"speaker\": \"KING LEAR\", \"text_entry\": \"To be a comrade with the wolf and owl,--\" } }, { \"text\": \"To be a make-peace shall become my age:\", \"_index\": \"shakespeare\", \"_id\": \"78566\", \"_score\": 1.0, \"_source\": { \"type\": \"line\", \"line_id\": 78567, \"play_name\": \"Richard II\", \"speech_number\": 20, \"line_number\": \"1.1.160\", \"speaker\": \"JOHN OF GAUNT\", \"text_entry\": \"To be a make-peace shall become my age:\" } }, { \"text\": \"To be a party in this injury.\", \"_index\": \"shakespeare\", \"_id\": \"75259\", \"_score\": 1.0, \"_source\": { \"type\": \"line\", \"line_id\": 75260, \"play_name\": \"Othello\", \"speech_number\": 57, \"line_number\": \"5.1.93\", \"speaker\": \"IAGO\", \"text_entry\": \"To be a party in this injury.\" } }, { \"text\": \"To be a preparation gainst the Polack;\", \"_index\": \"shakespeare\", \"_id\": \"33591\", \"_score\": 1.0, \"_source\": { \"type\": \"line\", \"line_id\": 33592, \"play_name\": \"Hamlet\", \"speech_number\": 17, \"line_number\": \"2.2.67\", \"speaker\": \"VOLTIMAND\", \"text_entry\": \"To be a preparation gainst the Polack;\" } }, { \"text\": \"To be a public spectacle to all:\", \"_index\": \"shakespeare\", \"_id\": \"3709\", \"_score\": 1.0, \"_source\": { \"type\": \"line\", \"line_id\": 3710, \"play_name\": \"Henry VI Part 1\", \"speech_number\": 6, \"line_number\": \"1.4.41\", \"speaker\": \"TALBOT\", \"text_entry\": \"To be a public spectacle to all:\" } }] }] } } To specify the number of suggestions that you want to return, use the size parameter: GET shakespeare/_search { \"suggest\": { \"autocomplete\": { \"prefix\": \"To n\", \"completion\": { \"field\": \"text_entry\", \"size\": 3 } } } } The maximum of three documents is returned: { \"took\": 4109, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"autocomplete\": [ { \"text\": \"To n\", \"offset\": 0, \"length\": 4, \"options\": [ { \"text\": \"To NESTOR\", \"_index\": \"shakespeare\", \"_id\": \"99707\", \"_score\": 1.0, \"_source\": { \"type\": \"line\", \"line_id\": 99708, \"play_name\": \"Troilus and Cressida\", \"speech_number\": 3, \"line_number\": \"\", \"speaker\": \"ULYSSES\", \"text_entry\": \"To NESTOR\" } }, { \"text\": \"To name the bigger light, and how the less,\", \"_index\": \"shakespeare\", \"_id\": \"91884\", \"_score\": 1.0, \"_source\": { \"type\": \"line\", \"line_id\": 91885, \"play_name\": \"The Tempest\", \"speech_number\": 91, \"line_number\": \"1.2.394\", \"speaker\": \"CALIBAN\", \"text_entry\": \"To name the bigger light, and how the less,\" } }, { \"text\": \"To nature none more bound; his training such,\", \"_index\": \"shakespeare\", \"_id\": \"40510\", \"_score\": 1.0, \"_source\": { \"type\": \"line\", \"line_id\": 40511, \"play_name\": \"Henry VIII\", \"speech_number\": 18, \"line_number\": \"1.2.126\", \"speaker\": \"KING HENRY VIII\", \"text_entry\": \"To nature none more bound; his training such,\" } }] }] } } The suggest parameter finds suggestions using only prefix matching.\nFor example, the document “To be, or not to be” is not part of the results. If you want specific documents returned as suggestions, you can manually add curated suggestions and add weights to prioritize your suggestions.\nIndex a document with input suggestions and assign a weight: PUT shakespeare/_doc/ 1?refresh= true { \"text_entry\": { \"input\": [ \"To n\", \"To be, or not to be: that is the question:\"], \"weight\": 10 } } Perform the same search: GET shakespeare/_search { \"suggest\": { \"autocomplete\": { \"prefix\": \"To n\", \"completion\": { \"field\": \"text_entry\", \"size\": 3 } } } } You see the indexed document as the first result: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"autocomplete\": [ { \"text\": \"To n\", \"offset\": 0, \"length\": 4, \"options\": [ { \"text\": \"To n\", \"_index\": \"shakespeare\", \"_id\": \"1\", \"_score\": 10.0, \"_source\": { \"text_entry\": { \"input\": [ \"To n\", \"To be, or not to be: that is the question:\"], \"weight\": 10 } } }, { \"text\": \"To NESTOR\", \"_index\": \"shakespeare\", \"_id\": \"99707\", \"_score\": 1.0, \"_source\": { \"type\": \"line\", \"line_id\": 99708, \"play_name\": \"Troilus and Cressida\", \"speech_number\": 3, \"line_number\": \"\", \"speaker\": \"ULYSSES\", \"text_entry\": \"To NESTOR\" } }, { \"text\": \"To name the bigger light, and how the less,\", \"_index\": \"shakespeare\", \"_id\": \"91884\", \"_score\": 1.0, \"_source\": { \"type\": \"line\", \"line_id\": 91885, \"play_name\": \"The Tempest\", \"speech_number\": 91, \"line_number\": \"1.2.394\", \"speaker\": \"CALIBAN\", \"text_entry\": \"To name the bigger light, and how the less,\" } }] }] } } You can also allow for misspellings in queries by specifying the fuzzy parameter: GET shakespeare/_search { \"suggest\": { \"autocomplete\": { \"prefix\": \"rosenkrantz\", \"completion\": { \"field\": \"text_entry\", \"size\": 3, \"fuzzy\": { \"fuzziness\": \"AUTO\" } } } } } The result matches the correct spelling: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"autocomplete\": [ { \"text\": \"rosenkrantz\", \"offset\": 0, \"length\": 11, \"options\": [ { \"text\": \"ROSENCRANTZ:\", \"_index\": \"shakespeare\", \"_id\": \"35196\", \"_score\": 5.0, \"_source\": { \"type\": \"line\", \"line_id\": 35197, \"play_name\": \"Hamlet\", \"speech_number\": 2, \"line_number\": \"4.2.1\", \"speaker\": \"HAMLET\", \"text_entry\": \"ROSENCRANTZ:\" } }] }] } } You can use a regular expression to define the prefix for the completion suggester query: GET shakespeare/_search { \"suggest\": { \"autocomplete\": { \"prefix\": \"rosen*\", \"completion\": { \"field\": \"text_entry\", \"size\": 3 } } } } For more information, see the completion field type documentation.\nSearch as you type\nOpenSearch has a dedicated search_as_you_type field type that is optimized for search-as-you-type functionality and can match terms using both prefix and infix completion. The search_as_you_type field does not require you to set up a custom analyzer or index suggestions beforehand.\nFirst, map the field as search_as_you_type: PUT shakespeare { \"mappings\": { \"properties\": { \"text_entry\": { \"type\": \"search_as_you_type\" } } } } After you index a document, OpenSearch automatically creates and stores its n-grams and edge n-grams. For example, consider the string that is the question. First, it is split into terms using the standard analyzer, and the terms are stored in the text_entry field: [ \"that\", \"is\", \"the\", \"question\"] In addition to storing these terms, the following 2-grams for this field are stored in the field text_entry._2gram: [ \"that is\", \"is the\", \"the question\"] The following 3-grams for this field are stored in the field text_entry._3gram: [ \"that is the\", \"is the question\"] Finally, after an edge n-gram token filter is applied, the resulting terms are stored in the text_entry._index_prefix field: [ \"t\", \"th\", \"tha\", \"that\",...] You can then match terms in any order using the bool_prefix type of a multi-match query: GET shakespeare/_search { \"query\": { \"multi_match\": { \"query\": \"uncle what\", \"type\": \"bool_prefix\", \"fields\": [ \"text_entry\", \"text_entry._2gram\", \"text_entry._3gram\"] } }, \"size\": 3 } The documents in which the words appear in the same order as in the query are ranked higher in the results: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4759, \"relation\": \"eq\" }, \"max_score\": 10.437667, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"2817\", \"_score\": 10.437667, \"_source\": { \"type\": \"line\", \"line_id\": 2818, \"play_name\": \"Henry IV\", \"speech_number\": 5, \"line_number\": \"5.2.31\", \"speaker\": \"HOTSPUR\", \"text_entry\": \"Uncle, what news?\" } }, { \"_index\": \"shakespeare\", \"_id\": \"37085\", \"_score\": 9.437667, \"_source\": { \"type\": \"line\", \"line_id\": 37086, \"play_name\": \"Henry V\", \"speech_number\": 26, \"line_number\": \"1.2.262\", \"speaker\": \"KING HENRY V\", \"text_entry\": \"What treasure, uncle?\" } }, { \"_index\": \"shakespeare\", \"_id\": \"79274\", \"_score\": 9.358302, \"_source\": { \"type\": \"line\", \"line_id\": 79275, \"play_name\": \"Richard II\", \"speech_number\": 29, \"line_number\": \"2.1.187\", \"speaker\": \"KING RICHARD II\", \"text_entry\": \"Why, uncle, whats the matter?\" } }] } } To match terms in order, you can use a match_phrase_prefix query: GET shakespeare/_search { \"query\": { \"match_phrase_prefix\": { \"text_entry\": \"uncle wha\" } }, \"size\": 3 } The response contains documents that match the prefix: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 6, \"relation\": \"eq\" }, \"max_score\": 16.37664, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"2817\", \"_score\": 16.37664, \"_source\": { \"type\": \"line\", \"line_id\": 2818, \"play_name\": \"Henry IV\", \"speech_number\": 5, \"line_number\": \"5.2.31\", \"speaker\": \"HOTSPUR\", \"text_entry\": \"Uncle, what news?\" } }, { \"_index\": \"shakespeare\", \"_id\": \"6789\", \"_score\": 16.37664, \"_source\": { \"type\": \"line\", \"line_id\": 6790, \"play_name\": \"Henry VI Part 2\", \"speech_number\": 60, \"line_number\": \"1.3.202\", \"speaker\": \"KING HENRY VI\", \"text_entry\": \"Uncle, what shall we say to this in law?\" } }, { \"_index\": \"shakespeare\", \"_id\": \"7877\", \"_score\": 16.37664, \"_source\": { \"type\": \"line\", \"line_id\": 7878, \"play_name\": \"Henry VI Part 2\", \"speech_number\": 13, \"line_number\": \"3.2.28\", \"speaker\": \"KING HENRY VI\", \"text_entry\": \"Where is our uncle? whats the matter, Suffolk?\" } }] } } Finally, to match the last term exactly and not as a prefix, you can use a match_phrase query: GET shakespeare/_search { \"query\": { \"match_phrase\": { \"text_entry\": \"uncle what\" } }, \"size\": 5 } The response contains exact matches: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3, \"relation\": \"eq\" }, \"max_score\": 14.437452, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"2817\", \"_score\": 14.437452, \"_source\": { \"type\": \"line\", \"line_id\": 2818, \"play_name\": \"Henry IV\", \"speech_number\": 5, \"line_number\": \"5.2.31\", \"speaker\": \"HOTSPUR\", \"text_entry\": \"Uncle, what news?\" } }, { \"_index\": \"shakespeare\", \"_id\": \"6789\", \"_score\": 9.461917, \"_source\": { \"type\": \"line\", \"line_id\": 6790, \"play_name\": \"Henry VI Part 2\", \"speech_number\": 60, \"line_number\": \"1.3.202\", \"speaker\": \"KING HENRY VI\", \"text_entry\": \"Uncle, what shall we say to this in law?\" } }, { \"_index\": \"shakespeare\", \"_id\": \"100955\", \"_score\": 8.947967, \"_source\": { \"type\": \"line\", \"line_id\": 100956, \"play_name\": \"Troilus and Cressida\", \"speech_number\": 28, \"line_number\": \"3.2.98\", \"speaker\": \"CRESSIDA\", \"text_entry\": \"Well, uncle, what folly I commit, I dedicate to you.\" } }] } } If you modify the text in the previous match_phrase query and omit the last letter, none of the documents in the previous response are returned: GET shakespeare/_search { \"query\": { \"match_phrase\": { \"text_entry\": \"uncle wha\" } } } The result is empty: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] } } For more information, see the search_as_you_type field type documentation.",
    "ancestors": [
      "Search",
      "Searching data"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/searching-data/did-you-mean/",
    "title": "Did-you-mean",
    "content": "The Did-you-mean suggester shows suggested corrections for misspelled search terms.\nFor example, if a user types “fliud,” OpenSearch suggests a corrected search term like “fluid.” You can then suggest the corrected term to the user or even automatically correct the search term.\nYou can implement the did-you-mean suggester using one of the following methods:\nUse a term suggester to suggest corrections for individual words.\nUse a phrase suggester to suggest corrections for phrases.\nTerm suggester\nUse the term suggester to suggest corrected spellings for individual words.\nThe term suggester uses an edit distance to compute suggestions.\nThe edit distance is the number of single-character insertions, deletions, or substitutions that need to be performed for a term to match another term. For example, to change the word “cat” to “hats”, you need to substitute “h” for “c” and insert an “s”, so the edit distance in this case is 2.\nTo use the term suggester, you don’t need any special field mappings for your index. By default, string field types are mapped as text. A text field is analyzed, so the title in the following example is tokenized into individual words. Indexing the following documents creates a books index where title is a text field: PUT books/_doc/ 1 { \"title\": \"Design Patterns (Object-Oriented Software)\" } PUT books/_doc/ 2 { \"title\": \"Software Architecture Patterns Explained\" } To check how a string is split into tokens, you can use the _analyze endpoint. To apply the same analyzer that the field uses, you can specify the field’s name in the field parameter: GET books/_analyze { \"text\": \"Design Patterns (Object-Oriented Software)\", \"field\": \"title\" } The default analyzer ( standard) splits a string at word boundaries, removes punctuation, and lowercases the tokens: { \"tokens\": [ { \"token\": \"design\", \"start_offset\": 0, \"end_offset\": 6, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"patterns\", \"start_offset\": 7, \"end_offset\": 15, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"object\", \"start_offset\": 17, \"end_offset\": 23, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }, { \"token\": \"oriented\", \"start_offset\": 24, \"end_offset\": 32, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 3 }, { \"token\": \"software\", \"start_offset\": 33, \"end_offset\": 41, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 4 }] } To get suggestions for a misspelled search term, use the term suggester. Specify the input text that needs suggestions in the text field, and specify the field from which to get suggestions in the field field: GET books/_search { \"suggest\": { \"spell-check\": { \"text\": \"patern\", \"term\": { \"field\": \"title\" } } } } The term suggester returns a list of corrections for the input text in the options array: { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"spell-check\": [ { \"text\": \"patern\", \"offset\": 0, \"length\": 6, \"options\": [ { \"text\": \"patterns\", \"score\": 0.6666666, \"freq\": 2 }] }] } } The score value is calculated based on the edit distance. The higher the score, the better the suggestion. The freq is the frequency that represents the number of times the term appears in the documents of the specified index.\nYou can include several suggestions in one request. The following example uses the term suggester for two different suggestions: GET books/_search { \"suggest\": { \"spell-check1\": { \"text\": \"patern\", \"term\": { \"field\": \"title\" } }, \"spell-check2\": { \"text\": \"desing\", \"term\": { \"field\": \"title\" } } } } To receive suggestions for the same input text in multiple fields, you can define the text globally to avoid duplication: GET books/_search { \"suggest\": { \"text\": \"patern\", \"spell-check1\": { \"term\": { \"field\": \"title\" } }, \"spell-check2\": { \"term\": { \"field\": \"subject\" } } } } If text is specified both at the global and individual suggestion levels, the suggestion-level value overrides the global value.\nTerm suggester options\nYou can specify the following options to the term suggester. Option Description field\nThe field from which to source suggestions. Required. Can be set for each suggestion or globally.\nanalyzer\nThe analyzer with which to analyze the input text. Defaults to the analyzer configured for the field.\nsize\nThe maximum number of suggestions to return for each token in the input text.\nsort\nSpecifies how suggestions should be sorted in the response. Valid values are: - score: Sort by similarity score, then document frequency, and then the term itself. - frequency: Sort by document frequency, then similarity score, and then the term itself.\nsuggest_mode\nThe suggest mode specifies the terms for which suggestions should be included in the response. Valid values are: - missing: Return suggestions only for the input text terms that are not in the index. - popular: Return suggestions only if they occur in the documents more frequently than in the original input text. - always: Always return suggestions for each term in the input text. Default is missing.\nmax_edits\nThe maximum edit distance for suggestions. Valid values are in the [1, 2] range. Default is 2.\nprefix_length\nAn integer that specifies the minimum length the matched prefix must be to start returning suggestions. If the prefix of prefix_length is not matched, but the search term is still within the edit distance, no suggestions are returned. Default is 1. Higher values improve spellcheck performance because misspellings don’t tend to occur in the beginning of words.\nmin_word_length\nThe minimum length a suggestion must be in order to be included in the response. Default is 4.\nshard_size\nThe maximum number of candidate suggestions to obtain from each shard. After all candidate suggestions are considered, the top shard_size suggestions are returned. Default is equal to the size value. Shard-level document frequencies may not be exact because terms may reside in different shards. If shard_size is larger than size, the document frequencies for suggestions are more accurate, at the cost of decreased performance.\nmax_inspections\nThe multiplication factor for shard_size. The maximum number of candidate suggestions OpenSearch inspects to find suggestions is calculated as shard_size multiplied by max_inspection. May improve accuracy at the cost of decreased performance. Default is 5.\nmin_doc_freq\nThe minimum number or percentage of documents in which a suggestion should appear for it to be returned. May improve accuracy by returning only suggestions with high shard-level document frequencies. Valid values are integers that represent the document frequency or floats in the [0, 1] range that represent the percentage of documents. Default is 0 (feature disabled).\nmax_term_freq\nThe maximum number of documents in which a suggestion should appear in order for it to be returned. Valid values are integers that represent the document frequency or floats in the [0, 1] range that represent the percentage of documents. Default is 0.01. Excluding high-frequency terms improves spellcheck performance because high-frequency terms are usually spelled correctly. Uses shard-level document frequencies.\nstring_distance\nThe edit distance algorithm to use to determine similarity. Valid values are: - internal: The default algorithm that is based on the Damerau-Levenshtein algorithm but is highly optimized for comparing edit distances for terms in the index. - damerau_levenshtein: The edit distance algorithm based on the Damerau-Levenshtein algorithm. - levenshtein: The edit distance algorithm based on the Levenshtein edit distance algorithm. - jaro_winkler: The edit distance algorithm based on the Jaro-Winkler algorithm. - ngram: The edit distance algorithm based on character n-grams. Phrase suggester\nTo implement did-you-mean, use a phrase suggester.\nThe phrase suggester is similar to the term suggester, except it uses n-gram language models to suggest whole phrases instead of individual words.\nTo set up a phrase suggester, create a custom analyzer called trigram that uses a shingle filter and lowercases tokens. This filter is similar to the edge_ngram filter, but it applies to words instead of letters. Then configure the field from which you’ll be sourcing suggestions with the custom analyzer you created: PUT books 2 { \"settings\": { \"index\": { \"analysis\": { \"analyzer\": { \"trigram\": { \"type\": \"custom\", \"tokenizer\": \"standard\", \"filter\": [ \"lowercase\", \"shingle\"] } }, \"filter\": { \"shingle\": { \"type\": \"shingle\", \"min_shingle_size\": 2, \"max_shingle_size\": 3 } } } } }, \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\", \"fields\": { \"trigram\": { \"type\": \"text\", \"analyzer\": \"trigram\" } } } } } } Index the documents into the new index: PUT books 2 /_doc/ 1 { \"title\": \"Design Patterns\" } PUT books 2 /_doc/ 2 { \"title\": \"Software Architecture Patterns Explained\" } Suppose the user searches for an incorrect phrase: GET books 2 /_search { \"suggest\": { \"phrase-check\": { \"text\": \"design paterns\", \"phrase\": { \"field\": \"title.trigram\" } } } } The phrase suggester returns the corrected phrase: { \"took\": 4, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"phrase-check\": [ { \"text\": \"design paterns\", \"offset\": 0, \"length\": 14, \"options\": [ { \"text\": \"design patterns\", \"score\": 0.31666178 }] }] } } To highlight suggestions, set up the highlight field for the phrase suggester: GET books 2 /_search { \"suggest\": { \"phrase-check\": { \"text\": \"design paterns\", \"phrase\": { \"field\": \"title.trigram\", \"gram_size\": 3, \"highlight\": { \"pre_tag\": \"&lt;em&gt;\", \"post_tag\": \"&lt;/em&gt;\" } } } } } The results contain the highlighted text: { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"phrase-check\": [ { \"text\": \"design paterns\", \"offset\": 0, \"length\": 14, \"options\": [ { \"text\": \"design patterns\", \"highlighted\": \"design &lt;em&gt;patterns&lt;/em&gt;\", \"score\": 0.31666178 }] }] } } Phrase suggester options\nYou can specify the following options to the phrase suggester. Option Description field\nThe field to use for n-gram lookups. The phrase suggester uses this field to calculate suggestion scores. Required.\ngram_size\nThe maximum size n of the n-grams (shingles) in the field. If the field does not contain n-grams (shingles), omit this option or set it to 1. If the field uses a shingle filter, and gram_size is not set, gram_size is set to max_shingle_size.\nreal_word_error_likelihood\nThe probability that a term is misspelled, even if it exists in the dictionary. Default is 0.95 (5% of the words in the dictionary are misspelled).\nconfidence\nThe confidence level is a float factor that is multiplied by the input phrase’s score to calculate a threshold score for other suggestions. Only suggestions with higher scores than the threshold are returned. A confidence level of 1.0 will only return suggestions that score higher than the input phrase. If confidence is set to 0, the top size candidates are returned. Default is 1.\nmax_errors\nThe maximum number or percentage of the terms that can be erroneous (spelled incorrectly) in order to return a suggestion. Valid values are integers that represent the number of terms or floats in the (0, 1) range that represent the percentage of the terms. Default is 1 (return only suggestions with at most one misspelled term). Setting this value to a high number can decrease performance. We recommend setting max_errors to a low number like 1 or 2 to reduce the time spent in suggest calls relative to the time spent in query execution.\nseparator\nThe separator for the terms in the bigram field. Defaults to the space character.\nsize\nThe number of candidate suggestions to generate for each query term. Specifying a higher value can result in terms with higher edit distances being returned. Default is 5.\nanalyzer\nThe analyzer with which to analyze the suggestion text. Defaults to the analyzer configured for the field.\nshard_size\nThe maximum number of candidate suggestions to obtain from each shard. After all candidate suggestions are considered, the top shard_size suggestions are returned. Default is 5. collate Used to prune suggestions for which there are no matching documents in the index.\ncollate.query\nSpecifies a query against which suggestions are checked to prune the suggestions for which there are no matching documents in the index.\ncollate.prune\nSpecifies whether to return all suggestions. If prune is set to false, only those suggestions that have matching documents are returned. If prune is set to true, all suggestions are returned; each suggestion has an additional collate_match field that is true if the suggestion has matching documents and is false otherwise. Default is false.\nhighlight\nConfigures suggestion highlighting. Both pre_tag and post_tag values are required.\nhighlight.pre_tag\nThe starting tag for highlighting.\nhighlight.post_tag\nThe ending tag for highlighting. smoothing Smoothing model to balance the weight of the shingles that exist in the index frequently with the weight of the shingles that exist in the index infrequently. Collate field\nTo filter out spellchecked suggestions that will not return any results, you can use the collate field. This field contains a scripted query that is run for each returned suggestion. See Search templates for information on constructing a templated query. You can specify the current suggestion using the {{suggestion}} variable, or you can pass your own template parameters in the params field (the suggestion value will be added to the variables you specify).\nThe collate query for a suggestion is run only on the shard from which the suggestion was sourced. The query is required.\nAdditionally, if the prune parameter is set to true, a collate_match field is added to each suggestion. If a query returns no results, the collate_match value is false. You can then filter out suggestions based on the collate_match field. The prune parameter’s default value is false.\nFor example, the following query configures the collate field to run a match_phrase query matching the title field to the current suggestion: GET books 2 /_search { \"suggest\": { \"phrase-check\": { \"text\": \"design paterns\", \"phrase\": { \"field\": \"title.trigram\", \"collate\": { \"query\": { \"source\": { \"match_phrase\": { \"title\": \"\" } } }, \"prune\": \"true\" } } } } } The resulting suggestion contains the collate_match field set to true, which means the match_phrase query will return matching documents for the suggestion: { \"took\": 7, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"phrase-check\": [ { \"text\": \"design paterns\", \"offset\": 0, \"length\": 14, \"options\": [ { \"text\": \"design patterns\", \"score\": 0.56759655, \"collate_match\": true }] }] } } Smoothing models\nFor most use cases, when calculating a suggestion’s score, you want to take into account not only the frequency of a shingle but also the shingle’s size. Smoothing models are used to calculate scores for shingles of different sizes, balancing the weight of frequent and infrequent shingles.\nThe following smoothing models are supported. Model Description stupid_backoff\nBacks off to lower-order n-gram models if the higher-order n-gram count is 0 and multiplies the lower-order n-gram model by a constant factor ( discount). This is the default smoothing model.\nstupid.backoff.discount\nThe factor by which to multiply the lower-order n-gram model. Optional. Default is 0.4.\nlaplace\nUses additive smoothing, adding a constant alpha to all counts to balance weights.\nlaplace.alpha\nThe constant added to all counts to balance weights, typically 1.0 or smaller. Optional. Default is 0.5. By default, OpenSearch uses the Stupid Backoff model—a simple algorithm that starts with the shingles of the highest order and takes lower-order shingles if higher-order shingles are not found. For example, if you set up the phrase suggester to have 3-grams, 2-grams, and 1-grams, the Stupid Backoff model first inspects the 3-grams. If there are no 3-grams, it inspects 2-grams but multiplies the score by the discount factor. If there are no 2-grams, it inspects 1-grams but again multiplies the score by the discount factor. The Stupid Backoff model works well in most cases. If you need to choose the Laplace smoothing model, specify it in the smoothing parameter: GET books 2 /_search { \"suggest\": { \"phrase-check\": { \"text\": \"design paterns\", \"phrase\": { \"field\": \"title.trigram\", \"size\": 1, \"smoothing\": { \"laplace\": { \"alpha\": 0.7 } } } } } } Candidate generators\nCandidate generators provide possible suggestion terms based on the terms in the input text. There is one candidate generator available— direct_generator. A direct generator functions similarly to a term suggester: It is also called for each term in the input text. The phrase suggester supports multiple candidate generators, where each generator is called for each term in the input text. It also lets you specify a pre-filter (an analyzer that analyzes the input text terms before they enter the spellcheck phase) and a post-filter (an analyzer that analyzes the generated suggestions before they are returned).\nSet up a direct generator for a phrase suggester: GET books 2 /_search { \"suggest\": { \"text\": \"design paterns\", \"phrase-check\": { \"phrase\": { \"field\": \"title.trigram\", \"size\": 1, \"direct_generator\": [ { \"field\": \"title.trigram\", \"suggest_mode\": \"always\", \"min_word_length\": 3 }] } } } } You can specify the following direct generator options. Option Description field\nThe field from which to source suggestions. Required. Can be set for each suggestion or globally.\nsize\nThe maximum number of suggestions to return for each token in the input text.\nsuggest_mode\nThe suggest mode specifies the terms for which suggestions generated on each shard should be included. The suggest mode is applied to suggestions for each shard and is not checked when combining suggestions from different shards. Therefore, if the suggest mode is missing, suggestions will be returned if the term is missing from one shard but exists on another shard. Valid values are: - missing: Return suggestions only for the input text terms that are not in the shard. - popular: Return suggestions only if they occur in the documents more frequently than in the original input text on the shard. - always: Always return suggestions. Default is missing.\nmax_edits\nThe maximum edit distance for suggestions. Valid values are in the [1, 2] range. Default is 2.\nprefix_length\nAn integer that specifies the minimum length the matched prefix must be to start returning suggestions. If the prefix of prefix_length is not matched but the search term is still within the edit distance, no suggestions are returned. Default is 1. Higher values improve spellcheck performance because misspellings don’t tend to occur in the beginning of words.\nmin_word_length\nThe minimum length a suggestion must be in order to be included. Default is 4.\nmax_inspections\nThe multiplication factor for shard_size. The maximum number of candidate suggestions OpenSearch inspects to find suggestions is calculated as shard_size multiplied by max_inspection. May improve accuracy at the cost of decreased performance. Default is 5.\nmin_doc_freq\nThe minimum number or percentage of documents in which a suggestion should appear in order for it to be returned. May improve accuracy by returning only suggestions with high shard-level document frequencies. Valid values are integers that represent the document frequency or floats in the [0, 1] range that represent the percentage of documents. Default is 0 (feature disabled).\nmax_term_freq\nThe maximum number of documents in which a suggestion should appear in order for it to be returned. Valid values are integers that represent the document frequency or floats in the [0, 1] range that represent the percentage of documents. Default is 0.01. Excluding high-frequency terms improves spellcheck performance because high-frequency terms are usually spelled correctly. Uses shard-level document frequencies.\npre_filter\nAn analyzer that is applied to each input text token passed to the generator before a suggestion is generated.\npost_filter\nAn analyzer that is applied to each generated suggestion before it is passed to the phrase scorer.",
    "ancestors": [
      "Search",
      "Searching data"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/searching-data/highlight/",
    "title": "Highlight query matches",
    "content": "Highlighting emphasizes the search term(s) in the results so you can emphasize the query matches.\nTo highlight the search terms, add a highlight parameter outside of the query block: GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"life\" } }, \"size\": 3, \"highlight\": { \"fields\": { \"text_entry\": {} } } } Each document in the results contains a highlight object that shows your search term wrapped in an em tag: { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 805, \"relation\": \"eq\" }, \"max_score\": 7.450247, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"33765\", \"_score\": 7.450247, \"_source\": { \"type\": \"line\", \"line_id\": 33766, \"play_name\": \"Hamlet\", \"speech_number\": 60, \"line_number\": \"2.2.233\", \"speaker\": \"HAMLET\", \"text_entry\": \"my life, except my life.\" }, \"highlight\": { \"text_entry\": [ \"my &lt;em&gt;life&lt;/em&gt;, except my &lt;em&gt;life&lt;/em&gt;.\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"51877\", \"_score\": 6.873042, \"_source\": { \"type\": \"line\", \"line_id\": 51878, \"play_name\": \"King Lear\", \"speech_number\": 18, \"line_number\": \"4.6.52\", \"speaker\": \"EDGAR\", \"text_entry\": \"The treasury of life, when life itself\" }, \"highlight\": { \"text_entry\": [ \"The treasury of &lt;em&gt;life&lt;/em&gt;, when &lt;em&gt;life&lt;/em&gt; itself\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"39245\", \"_score\": 6.6167283, \"_source\": { \"type\": \"line\", \"line_id\": 39246, \"play_name\": \"Henry V\", \"speech_number\": 7, \"line_number\": \"4.7.31\", \"speaker\": \"FLUELLEN\", \"text_entry\": \"mark Alexanders life well, Harry of Monmouths life\" }, \"highlight\": { \"text_entry\": [ \"mark Alexanders &lt;em&gt;life&lt;/em&gt; well, Harry of Monmouths &lt;em&gt;life&lt;/em&gt;\"] } }] } } The highlight function works on the actual field contents. OpenSearch retrieves these contents either from the stored field (the field for which the mapping is to be set to true) or from the _source field if the field is not stored. You can force the retrieval of field contents from the _source field by setting the force_source parameter to true.\nThe highlight parameter highlights the original terms even when using synonyms or stemming for the search itself.\nMethods of obtaining offsets\nTo highlight the search terms, the highlighter needs the start and end character offsets of each term. The offsets mark the term’s position in the original text. The highlighter can obtain the offsets from the following sources: Postings: When documents are indexed, OpenSearch creates an inverted search index—a core data structure used to search for documents. Postings represent the inverted search index and store the mapping of each analyzed term to the list of documents in which it occurs. If you set the index_options parameter to offsets when mapping a text field, OpenSearch adds each term’s start and end character offsets to the inverted index. During highlighting, the highlighter reruns the original query directly on the postings to locate each term. Thus, storing offsets makes highlighting more efficient for large fields because it does not require reanalyzing the text. Storing term offsets requires additional disk space, but uses less disk space than storing term vectors. Text reanalysis: In the absence of both postings and term vectors, the highlighter reanalyzes text in order to highlight it. For every document and every field that needs highlighting, the highlighter creates a small in-memory index and reruns the original query through Lucene’s query execution planner to access low-level match information for the current document. Reanalyzing the text works well in most use cases. However, this method is more memory and time intensive for large fields.\nHighlighter types\nOpenSearch supports three highlighter implementations: plain, unified, and fvh (Fast Vector Highlighter).\nThe following table lists the methods of obtaining the offsets for each highlighter. Highlighter Method of obtaining offsets unified Term vectors if term_vector is set to with_positions_offsets, postings if index_options is set to offsets, text reanalysis otherwise. fvh Term vectors. plain Text reanalysis. Setting the highlighter type\nTo set the highlighter type, specify it in the type field: GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"life\" } }, \"highlight\": { \"fields\": { \"text_entry\": { \"type\": \"plain\" } } } } The unified highlighter\nThe unified highlighter is based on the Lucene Unified Highlighter and is the default highlighter for OpenSearch. It divides the text into sentences and treats those sentences as individual documents, scoring them in terms of similarity using the BM25 algorithm. The unified highlighter supports both exact phrase and multi-term highlighting, including fuzzy, prefix, and regex. If you’re using complex queries to highlight multiple fields in multiple documents, we recommend using the unified highlighter on postings or term_vector fields.\nThe fvh highlighter\nThe fvh highlighter is based on the Lucene Fast Vector Highlighter. To use this highlighter, you need to store term vectors with positions offsets, which increases the index size. The fvh highlighter can combine matched terms from multiple fields into one result. It can also assign weights to matches depending on their positions; thus, you can sort phrase matches above term matches when highlighting a query that boosts phrase matches over term matches. Additionally, you can configure the fvh highlighter to select the boundaries of a returned text fragment, and you can highlight multiple words with different tags.\nThe plain highlighter\nThe plain highlighter is based on the standard Lucene highlighter. It requires the highlighted fields to be stored either individually or in the _source field. The plain highlighter mirrors the query matching logic, in particular word importance and positions in phrase queries. It works for most use cases but may be slow for large fields because it has to reanalyze the text to be highlighted.\nHighlighting options\nThe following table describes the highlighting options you can specify on a global or field level. Field-level settings override global settings. Option Description type\nSpecifies the highlighter to use. Valid values are unified, fvh, and plain. Default is unified.\nfields\nSpecifies the fields to search for text to be highlighted. Supports wildcard expressions. If you use wildcards, only text and keyword fields are highlighted. For example, you can set fields to my_field* to include all text and keyword fields that start with the prefix my_field.\nforce_source\nSpecifies that field values for highlighting should be obtained from the _source field rather than from stored field values. Default is false.\nrequire_field_match\nSpecifies whether to highlight only fields that contain a search query match. Default is true. To highlight all fields, set this option to false.\npre_tags\nSpecifies the HTML start tags for the highlighted text as an array of strings.\npost_tags\nSpecifies the HTML end tags for the highlighted text as an array of strings.\ntags_schema\nIf you set this option to styled, OpenSearch uses the built-in tag schema. In this schema, the pre_tags are &lt;em class=\"hlt1\"&gt;, &lt;em class=\"hlt2\"&gt;, &lt;em class=\"hlt3\"&gt;, &lt;em class=\"hlt4\"&gt;, &lt;em class=\"hlt5\"&gt;, &lt;em class=\"hlt6\"&gt;, &lt;em class=\"hlt7\"&gt;, &lt;em class=\"hlt8\"&gt;, &lt;em class=\"hlt9\"&gt;, and &lt;em class=\"hlt10\"&gt;, and the post_tags is &lt;/em&gt;.\nboundary_chars\nAll boundary characters combined in a string. Default is \".,!? \\t\\n\".\nboundary_scanner\nValid only for the unified and fvh highlighters. Specifies whether to split the highlighted fragments into sentences, words, or characters. Valid values are the following: - sentence: Split highlighted fragments at sentence boundaries, as defined by the BreakIterator. You can specify the BreakIterator’s locale in the boundary_scanner_locale option. - word: Split highlighted fragments at word boundaries, as defined by the BreakIterator. You can specify the BreakIterator’s locale in the boundary_scanner_locale option. - chars: Split highlighted fragments at any character listed in boundary_chars. Valid only for the fvh highlighter.\nboundary_scanner_locale\nProvides a locale for the boundary_scanner. Valid values are language tags (for example, \"en-US\"). Default is Locale.ROOT.\nboundary_max_scan\nControls how far to scan for boundary characters when the boundary_scanner parameter for the fvh highlighter is set to chars. Default is 20.\nencoder\nSpecifies whether the highlighted fragment should be HTML encoded before it is returned. Valid values are default (no encoding) or html (first escape the HTML text and then insert the highlighting tags). For example, if the field text is &lt;h3&gt;Hamlet&lt;/h3&gt; and the encoder is set to html, the highlighted text is \"&amp;lt;h3&amp;gt;&lt;em&gt;Hamlet&lt;/em&gt;&amp;lt;&amp;#x2F;h3&amp;gt;\".\nfragmenter\nSpecifies how to split text into highlighted fragments. Valid only for the plain highlighter. Valid values are the following: - span (default): Splits text into fragments of the same size but tries not to split text between highlighted terms. - simple: Splits text into fragments of the same size.\nfragment_offset\nSpecifies the character offset from which you want to start highlighting. Valid for the fvh highlighter only.\nfragment_size\nThe size of a highlighted fragment, specified as the number of characters. If number_of_fragments is set to 0, fragment_size is ignored. Default is 100.\nnumber_of_fragments\nThe maximum number of returned fragments. If number_of_fragments is set to 0, OpenSearch returns the highlighted contents of the entire field. Default is 5.\norder\nThe sort order for the highlighted fragments. Set order to score to sort fragments by relevance. Each highlighter has a different algorithm for calculating relevance scores. Default is none.\nhighlight_query\nSpecifies that matches for a query other than the search query should be highlighted. The highlight_query option is useful when you use a faster query to get document matches and a slower query (for example, rescore_query) to refine the results. We recommend to include the search query as part of the highlight_query.\nmatched_fields\nCombines matches from different fields to highlight one field. The most common use case for this functionality is highlighting text that is analyzed in different ways and kept in multi-fields. All fields in the matched_fields list must have the term_vector field set to with_positions_offsets. The field in which the matches are combined is the only loaded field, so it is beneficial to set its store option to yes. Valid only for the fvh highlighter.\nno_match_size\nSpecifies the number of characters, starting from the beginning of the field, to return if there are no matching fragments to highlight. Default is 0.\nphrase_limit\nThe number of matching phrases in a document that are considered. Limits the number of phrases to analyze by the fvh highlighter to avoid consuming a lot of memory. If matched_fields are used, phrase_limit specifies the number of phrases for each matched field. A higher phrase_limit leads to increased query time and more memory consumption. Valid only for the fvh highlighter. Default is 256. The unified highlighter’s sentence scanner splits sentences larger than fragment_size at the first word boundary after fragment_size is reached. To return whole sentences without splitting them, set fragment_size to 0.\nChanging the highlighting tags\nDesign your application code to parse the results from the highlight object and perform an action on the search terms, such as changing their color, bolding, italicizing, and so on.\nTo change the default em tags, specify the new tags in the pretag and posttag parameters: GET shakespeare/_search { \"query\": { \"match\": { \"play_name\": \"Henry IV\" } }, \"size\": 3, \"highlight\": { \"pre_tags\": [ \"&lt;strong&gt;\"], \"post_tags\": [ \"&lt;/strong&gt;\"], \"fields\": { \"play_name\": {} } } } The play name is highlighted by the new tags in the response: { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3205, \"relation\": \"eq\" }, \"max_score\": 3.548232, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"0\", \"_score\": 3.548232, \"_source\": { \"type\": \"act\", \"line_id\": 1, \"play_name\": \"Henry IV\", \"speech_number\": \"\", \"line_number\": \"\", \"speaker\": \"\", \"text_entry\": \"ACT I\" }, \"highlight\": { \"play_name\": [ \"&lt;strong&gt;Henry IV&lt;/strong&gt;\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"1\", \"_score\": 3.548232, \"_source\": { \"type\": \"scene\", \"line_id\": 2, \"play_name\": \"Henry IV\", \"speech_number\": \"\", \"line_number\": \"\", \"speaker\": \"\", \"text_entry\": \"SCENE I. London. The palace.\" }, \"highlight\": { \"play_name\": [ \"&lt;strong&gt;Henry IV&lt;/strong&gt;\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"2\", \"_score\": 3.548232, \"_source\": { \"type\": \"line\", \"line_id\": 3, \"play_name\": \"Henry IV\", \"speech_number\": \"\", \"line_number\": \"\", \"speaker\": \"\", \"text_entry\": \"Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others\" }, \"highlight\": { \"play_name\": [ \"&lt;strong&gt;Henry IV&lt;/strong&gt;\"] } }] } } Specifying a highlight query\nBy default, OpenSearch only considers the search query for highlighting. If you use a fast query to get document matches and a slower query like rescore_query to refine the results, it is useful to highlight the refined results. You can do this by adding a highlight_query: GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": { \"query\": \"thats my name\" } } }, \"rescore\": { \"window_size\": 20, \"query\": { \"rescore_query\": { \"match_phrase\": { \"text_entry\": { \"query\": \"thats my name\", \"slop\": 1 } } }, \"rescore_query_weight\": 5 } }, \"_source\": false, \"highlight\": { \"order\": \"score\", \"fields\": { \"text_entry\": { \"highlight_query\": { \"bool\": { \"must\": { \"match\": { \"text_entry\": { \"query\": \"thats my name\" } } }, \"should\": { \"match_phrase\": { \"text_entry\": { \"query\": \"that is my name\", \"slop\": 1, \"boost\": 10.0 } } }, \"minimum_should_match\": 0 } } } } } } Combining matches from different fields to highlight one field\nYou can combine matches from different fields to highlight one field with the fvh highlighter. The most common use case for this functionality is highlighting text that is analyzed in different ways and kept in multi-fields. All fields in the matched_fields list must have the term_vector field set to with_positions_offsets. The field in which the matches are combined is the only loaded field, so it is beneficial to set its store option to yes.\nExample\nCreate a mapping for the shakespeare index where the text_entry field is analyzed with the standard analyzer and has an english subfield that is analyzed with the english analyzer: PUT shakespeare { \"mappings\": { \"properties\": { \"text_entry\": { \"type\": \"text\", \"term_vector\": \"with_positions_offsets\", \"fields\": { \"english\": { \"type\": \"text\", \"analyzer\": \"english\", \"term_vector\": \"with_positions_offsets\" } } } } } } The standard analyzer splits the text_entry fields into individual words. You can confirm this by using the analyze API operation: GET shakespeare/_analyze { \"text\": \"bragging of thine\", \"field\": \"text_entry\" } The response contains the original string split on white space: { \"tokens\": [ { \"token\": \"bragging\", \"start_offset\": 0, \"end_offset\": 8, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"of\", \"start_offset\": 9, \"end_offset\": 11, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"thine\", \"start_offset\": 12, \"end_offset\": 17, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }] } The english analyzer not only splits the string into words but also stems the tokens and removes stopwords. You can confirm this by using the analyze API operation with the text_entry.english field: GET shakespeare/_analyze { \"text\": \"bragging of thine\", \"field\": \"text_entry.english\" } The response contains the stemmed words: { \"tokens\": [ { \"token\": \"brag\", \"start_offset\": 0, \"end_offset\": 8, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"thine\", \"start_offset\": 12, \"end_offset\": 17, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }] } To search for all forms of the word bragging, use the following query: GET shakespeare/_search { \"query\": { \"query_string\": { \"query\": \"text_entry.english:bragging\", \"fields\": [ \"text_entry\"] } }, \"highlight\": { \"order\": \"score\", \"fields\": { \"text_entry\": { \"matched_fields\": [ \"text_entry\", \"text_entry.english\"], \"type\": \"fvh\" } } } } The response highlights all versions of the word “bragging” in the text_entry field: { \"took\": 5, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 26, \"relation\": \"eq\" }, \"max_score\": 10.153671, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"56666\", \"_score\": 10.153671, \"_source\": { \"type\": \"line\", \"line_id\": 56667, \"play_name\": \"macbeth\", \"speech_number\": 34, \"line_number\": \"2.3.118\", \"speaker\": \"MACBETH\", \"text_entry\": \"Is left this vault to brag of.\" }, \"highlight\": { \"text_entry\": [ \"Is left this vault to &lt;em&gt;brag&lt;/em&gt; of.\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"71445\", \"_score\": 9.284528, \"_source\": { \"type\": \"line\", \"line_id\": 71446, \"play_name\": \"Much Ado about nothing\", \"speech_number\": 18, \"line_number\": \"5.1.65\", \"speaker\": \"LEONATO\", \"text_entry\": \"As under privilege of age to brag\" }, \"highlight\": { \"text_entry\": [ \"As under privilege of age to &lt;em&gt;brag&lt;/em&gt;\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"86782\", \"_score\": 9.284528, \"_source\": { \"type\": \"line\", \"line_id\": 86783, \"play_name\": \"Romeo and Juliet\", \"speech_number\": 8, \"line_number\": \"2.6.31\", \"speaker\": \"JULIET\", \"text_entry\": \"Brags of his substance, not of ornament:\" }, \"highlight\": { \"text_entry\": [ \"&lt;em&gt;Brags&lt;/em&gt; of his substance, not of ornament:\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"44531\", \"_score\": 8.552448, \"_source\": { \"type\": \"line\", \"line_id\": 44532, \"play_name\": \"King John\", \"speech_number\": 15, \"line_number\": \"3.1.124\", \"speaker\": \"CONSTANCE\", \"text_entry\": \"A ramping fool, to brag and stamp and swear\" }, \"highlight\": { \"text_entry\": [ \"A ramping fool, to &lt;em&gt;brag&lt;/em&gt; and stamp and swear\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"63208\", \"_score\": 8.552448, \"_source\": { \"type\": \"line\", \"line_id\": 63209, \"play_name\": \"Merchant of Venice\", \"speech_number\": 11, \"line_number\": \"3.4.79\", \"speaker\": \"PORTIA\", \"text_entry\": \"A thousand raw tricks of these bragging Jacks,\" }, \"highlight\": { \"text_entry\": [ \"A thousand raw tricks of these &lt;em&gt;bragging&lt;/em&gt; Jacks,\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"73026\", \"_score\": 8.552448, \"_source\": { \"type\": \"line\", \"line_id\": 73027, \"play_name\": \"Othello\", \"speech_number\": 75, \"line_number\": \"2.1.242\", \"speaker\": \"IAGO\", \"text_entry\": \"but for bragging and telling her fantastical lies:\" }, \"highlight\": { \"text_entry\": [ \"but for &lt;em&gt;bragging&lt;/em&gt; and telling her fantastical lies:\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"85974\", \"_score\": 8.552448, \"_source\": { \"type\": \"line\", \"line_id\": 85975, \"play_name\": \"Romeo and Juliet\", \"speech_number\": 20, \"line_number\": \"1.5.70\", \"speaker\": \"CAPULET\", \"text_entry\": \"And, to say truth, Verona brags of him\" }, \"highlight\": { \"text_entry\": [ \"And, to say truth, Verona &lt;em&gt;brags&lt;/em&gt; of him\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"96800\", \"_score\": 8.552448, \"_source\": { \"type\": \"line\", \"line_id\": 96801, \"play_name\": \"Titus Andronicus\", \"speech_number\": 60, \"line_number\": \"1.1.311\", \"speaker\": \"SATURNINUS\", \"text_entry\": \"Agree these deeds with that proud brag of thine,\" }, \"highlight\": { \"text_entry\": [ \"Agree these deeds with that proud &lt;em&gt;brag&lt;/em&gt; of thine,\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"18189\", \"_score\": 7.9273787, \"_source\": { \"type\": \"line\", \"line_id\": 18190, \"play_name\": \"As you like it\", \"speech_number\": 12, \"line_number\": \"5.2.30\", \"speaker\": \"ROSALIND\", \"text_entry\": \"and Caesars thrasonical brag of I came, saw, and\" }, \"highlight\": { \"text_entry\": [ \"and Caesars thrasonical &lt;em&gt;brag&lt;/em&gt; of I came, saw, and\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"32054\", \"_score\": 7.9273787, \"_source\": { \"type\": \"line\", \"line_id\": 32055, \"play_name\": \"Cymbeline\", \"speech_number\": 52, \"line_number\": \"5.5.211\", \"speaker\": \"IACHIMO\", \"text_entry\": \"And then a mind put int, either our brags\" }, \"highlight\": { \"text_entry\": [ \"And then a mind put int, either our &lt;em&gt;brags&lt;/em&gt;\"] } }] } } To score the original form of the word “bragging” higher, you can boost the text_entry field: GET shakespeare/_search { \"query\": { \"query_string\": { \"query\": \"bragging\", \"fields\": [ \"text_entry^5\", \"text_entry.english\"] } }, \"highlight\": { \"order\": \"score\", \"fields\": { \"text_entry\": { \"matched_fields\": [ \"text_entry\", \"text_entry.english\"], \"type\": \"fvh\" } } } } The response lists documents that contain the word “bragging” first: { \"took\": 17, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 26, \"relation\": \"eq\" }, \"max_score\": 49.746853, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"45739\", \"_score\": 49.746853, \"_source\": { \"type\": \"line\", \"line_id\": 45740, \"play_name\": \"King John\", \"speech_number\": 10, \"line_number\": \"5.1.51\", \"speaker\": \"BASTARD\", \"text_entry\": \"Of bragging horror: so shall inferior eyes,\" }, \"highlight\": { \"text_entry\": [ \"Of &lt;em&gt;bragging&lt;/em&gt; horror: so shall inferior eyes,\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"63208\", \"_score\": 47.077244, \"_source\": { \"type\": \"line\", \"line_id\": 63209, \"play_name\": \"Merchant of Venice\", \"speech_number\": 11, \"line_number\": \"3.4.79\", \"speaker\": \"PORTIA\", \"text_entry\": \"A thousand raw tricks of these bragging Jacks,\" }, \"highlight\": { \"text_entry\": [ \"A thousand raw tricks of these &lt;em&gt;bragging&lt;/em&gt; Jacks,\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"68474\", \"_score\": 47.077244, \"_source\": { \"type\": \"line\", \"line_id\": 68475, \"play_name\": \"A Midsummer nights dream\", \"speech_number\": 101, \"line_number\": \"3.2.427\", \"speaker\": \"PUCK\", \"text_entry\": \"Thou coward, art thou bragging to the stars,\" }, \"highlight\": { \"text_entry\": [ \"Thou coward, art thou &lt;em&gt;bragging&lt;/em&gt; to the stars,\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"73026\", \"_score\": 47.077244, \"_source\": { \"type\": \"line\", \"line_id\": 73027, \"play_name\": \"Othello\", \"speech_number\": 75, \"line_number\": \"2.1.242\", \"speaker\": \"IAGO\", \"text_entry\": \"but for bragging and telling her fantastical lies:\" }, \"highlight\": { \"text_entry\": [ \"but for &lt;em&gt;bragging&lt;/em&gt; and telling her fantastical lies:\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"39816\", \"_score\": 44.679565, \"_source\": { \"type\": \"line\", \"line_id\": 39817, \"play_name\": \"Henry V\", \"speech_number\": 28, \"line_number\": \"5.2.138\", \"speaker\": \"KING HENRY V\", \"text_entry\": \"armour on my back, under the correction of bragging\" }, \"highlight\": { \"text_entry\": [ \"armour on my back, under the correction of &lt;em&gt;bragging&lt;/em&gt;\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"63200\", \"_score\": 44.679565, \"_source\": { \"type\": \"line\", \"line_id\": 63201, \"play_name\": \"Merchant of Venice\", \"speech_number\": 11, \"line_number\": \"3.4.71\", \"speaker\": \"PORTIA\", \"text_entry\": \"Like a fine bragging youth, and tell quaint lies,\" }, \"highlight\": { \"text_entry\": [ \"Like a fine &lt;em&gt;bragging&lt;/em&gt; youth, and tell quaint lies,\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"56666\", \"_score\": 10.153671, \"_source\": { \"type\": \"line\", \"line_id\": 56667, \"play_name\": \"macbeth\", \"speech_number\": 34, \"line_number\": \"2.3.118\", \"speaker\": \"MACBETH\", \"text_entry\": \"Is left this vault to brag of.\" }, \"highlight\": { \"text_entry\": [ \"Is left this vault to &lt;em&gt;brag&lt;/em&gt; of.\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"71445\", \"_score\": 9.284528, \"_source\": { \"type\": \"line\", \"line_id\": 71446, \"play_name\": \"Much Ado about nothing\", \"speech_number\": 18, \"line_number\": \"5.1.65\", \"speaker\": \"LEONATO\", \"text_entry\": \"As under privilege of age to brag\" }, \"highlight\": { \"text_entry\": [ \"As under privilege of age to &lt;em&gt;brag&lt;/em&gt;\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"86782\", \"_score\": 9.284528, \"_source\": { \"type\": \"line\", \"line_id\": 86783, \"play_name\": \"Romeo and Juliet\", \"speech_number\": 8, \"line_number\": \"2.6.31\", \"speaker\": \"JULIET\", \"text_entry\": \"Brags of his substance, not of ornament:\" }, \"highlight\": { \"text_entry\": [ \"&lt;em&gt;Brags&lt;/em&gt; of his substance, not of ornament:\"] } }, { \"_index\": \"shakespeare\", \"_id\": \"44531\", \"_score\": 8.552448, \"_source\": { \"type\": \"line\", \"line_id\": 44532, \"play_name\": \"King John\", \"speech_number\": 15, \"line_number\": \"3.1.124\", \"speaker\": \"CONSTANCE\", \"text_entry\": \"A ramping fool, to brag and stamp and swear\" }, \"highlight\": { \"text_entry\": [ \"A ramping fool, to &lt;em&gt;brag&lt;/em&gt; and stamp and swear\"] } }] } } Query limitations\nNote the following limitations:\nWhen extracting terms to highlight, highlighters don’t reflect the Boolean logic of a query. Therefore, for some complex Boolean queries, such as nested Boolean queries and queries using minimum_should_match, OpenSearch may highlight terms that don’t correspond to query matches.\nThe fvh highlighter does not support span queries.",
    "ancestors": [
      "Search",
      "Searching data"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/searching-data/index/",
    "title": "Searching data",
    "content": "What users expect from search engines has evolved over the years. Just returning relevant results quickly is no longer enough for most users. Now users seek methods that allow them to get even more relevant results, to sort and organize results, and to highlight their queries. OpenSearch includes many features, described in the following table, that enhance the search experience. Feature Description Autocomplete functionality Suggest phrases as the user types. Did-you-mean functionality Check spelling of phrases as the user types. Paginate results Rather than a single, long list, separate search results into pages. Sort results Allow sorting of results by different criteria. Highlight query matches Highlight the search term in the results.",
    "ancestors": [
      "Search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/searching-data/paginate/",
    "title": "Paginate results",
    "content": "Paginate results\nYou can use the following methods to paginate search results in OpenSearch:\nThe from and size parameters The scroll search operation\nThe search_after parameter Point in Time with search_after The from and size parameters\nThe from and size parameters return results one page at a time.\nThe from parameter is the document number from which you want to start showing the results. The size parameter is the number of results that you want to show. Together, they let you return a subset of the search results.\nFor example, if the value of size is 10 and the value of from is 0, you see the first 10 results. If you change the value of from to 10, you see the next 10 results (because the results are zero-indexed). So if you want to see results starting from result 11, from must be 10. GET shakespeare/_search { \"from\": 0, \"size\": 10, \"query\": { \"match\": { \"play_name\": \"Hamlet\" } } } Use the following formula to calculate the from parameter relative to the page number: from = size * (page_number - 1) Each time the user chooses the next page of the results, your application needs to run the same search query with an incremented from value.\nYou can also specify the from and size parameters in the search URI: GET shakespeare/_search?from= 0 &amp;size= 10 If you only specify the size parameter, the from parameter defaults to 0.\nQuerying for pages deep in your results can have a significant performance impact, so OpenSearch limits this approach to 10,000 results.\nThe from and size parameters are stateless, so the results are based on the latest available data.\nThis can cause inconsistent pagination.\nFor example, assume a user stays on the first page of the results and then navigates to the second page. During that time, a new document relevant to the user’s search is indexed and shows up on the first page. In this scenario, the last result on the first page is pushed to the second page, and the user sees duplicate results (that is, the first and second pages both display that last result).\nUse the scroll operation for consistent pagination. The scroll operation keeps a search context open for a certain period of time. Any data changes do not affect the results during that time.\nScroll search\nThe from and size parameters allow you to paginate your search results but with a limit of 10,000 results at a time.\nIf you need to request volumes of data larger than 1 PB from, for example, a machine learning job, use the scroll operation instead. The scroll operation allows you to request an unlimited number of results.\nTo use the scroll operation, add a scroll parameter to the request header with a search context telling OpenSearch for how long you need to keep scrolling. This search context needs to be long enough to process a single batch of results.\nTo set the number of results that you want returned for each batch, use the size parameter: GET shakespeare/_search?scroll= 10 m { \"size\": 10000 } OpenSearch caches the results and returns a scroll ID that you can use to access them in batches: \"_scroll_id\": \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAUWdmpUZDhnRFBUcWFtV21nMmFwUGJEQQ==\" Pass this scroll ID to the scroll operation to obtain the next batch of results: GET _search/scroll { \"scroll\": \"10m\", \"scroll_id\": \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAUWdmpUZDhnRFBUcWFtV21nMmFwUGJEQQ==\" } Using this scroll ID, you get results in batches of 10,000 as long as the search context is still open. Typically, the scroll ID does not change between requests, but it can change, so make sure to always use the latest scroll ID. If you don’t send the next scroll request within the set search context, the scroll operation does not return any results.\nIf you expect billions of results, use a sliced scroll. Slicing allows you to perform multiple scroll operations for the same request but in parallel.\nSet the ID and the maximum number of slices for the scroll: GET shakespeare/_search?scroll= 10 m { \"slice\": { \"id\": 0, \"max\": 10 }, \"query\": { \"match_all\": {} } } With a single scroll ID, you receive 10 results.\nYou can have up to 10 IDs.\nPerform the same command with the ID equal to 1: GET shakespeare/_search?scroll= 10 m { \"slice\": { \"id\": 1, \"max\": 10 }, \"query\": { \"match_all\": {} } } Close the search context when you’re done scrolling, because it continues to consume computing resources until the timeout: DELETE _search/scroll/DXF 1 ZXJ 5 QW 5 kRmV 0 Y 2 gBAAAAAAAAAAcWdmpUZDhnRFBUcWFtV 21 nMmFwUGJEQQ== Sample Response { \"succeeded\": true, \"num_freed\": 1 } Use the following request to close all open scroll contexts: DELETE _search/scroll/_all The scroll operation corresponds to a specific timestamp. It doesn’t consider documents added after that timestamp as potential results.\nBecause open search contexts consume a lot of memory, we suggest you don’t use the scroll operation for frequent user queries that don’t need the search context to be open. Instead, use the sort parameter with the search_after parameter to scroll responses for user queries.\nThe search_after parameter\nThe search_after parameter provides a live cursor that uses the previous page’s results to obtain the next page’s results. It is similar to the scroll operation in that it is meant to scroll many queries in parallel.\nFor example, the following query sorts all lines from the play “Hamlet” by the speech number and then the ID and retrieves the first three results: GET shakespeare/_search { \"size\": 3, \"query\": { \"match\": { \"play_name\": \"Hamlet\" } }, \"sort\": [ { \"speech_number\": \"asc\" }, { \"_id\": \"asc\" }] } The response contains the sort array of values for each document: { \"took\": 7, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4244, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"32435\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 32436, \"play_name\": \"Hamlet\", \"speech_number\": 1, \"line_number\": \"1.1.1\", \"speaker\": \"BERNARDO\", \"text_entry\": \"Whos there?\" }, \"sort\": [ 1, \"32435\"] }, { \"_index\": \"shakespeare\", \"_id\": \"32634\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 32635, \"play_name\": \"Hamlet\", \"speech_number\": 1, \"line_number\": \"1.2.1\", \"speaker\": \"KING CLAUDIUS\", \"text_entry\": \"Though yet of Hamlet our dear brothers death\" }, \"sort\": [ 1, \"32634\"] }, { \"_index\": \"shakespeare\", \"_id\": \"32635\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 32636, \"play_name\": \"Hamlet\", \"speech_number\": 1, \"line_number\": \"1.2.2\", \"speaker\": \"KING CLAUDIUS\", \"text_entry\": \"The memory be green, and that it us befitted\" }, \"sort\": [ 1, \"32635\"] }] } } You can use the last result’s sort values to retrieve the next result by using the search_after parameter: GET shakespeare/_search { \"size\": 10, \"query\": { \"match\": { \"play_name\": \"Hamlet\" } }, \"search_after\": [ 1, \"32635\"], \"sort\": [ { \"speech_number\": \"asc\" }, { \"_id\": \"asc\" }] } Unlike the scroll operation, the search_after parameter is stateless, so the document order may change because of documents being indexed or deleted.\nPoint in Time with search_after Point in Time (PIT) with search_after is the preferred pagination method in OpenSearch, especially for deep pagination. It bypasses the limitations of all other methods because it operates on a dataset that is frozen in time, it is not bound to a query, and it supports consistent pagination going forward and backward. To learn more, see Point in Time.",
    "ancestors": [
      "Search",
      "Searching data"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/searching-data/sort/",
    "title": "Sort results",
    "content": "Sort results\nSorting allows your users to sort results in a way that’s most meaningful to them.\nBy default, full-text queries sort results by the relevance score.\nYou can choose to sort the results by any field value in either ascending or descending order by setting the order parameter to asc or desc.\nFor example, to sort results by descending order of a line_id value, use the following query: GET shakespeare/_search { \"query\": { \"term\": { \"play_name\": { \"value\": \"Henry IV\" } } }, \"sort\": [ { \"line_id\": { \"order\": \"desc\" } }] } The results are sorted by line_id in descending order: { \"took\": 24, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3205, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"3204\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 3205, \"play_name\": \"Henry IV\", \"speech_number\": 8, \"line_number\": \"\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"Exeunt\" }, \"sort\": [ 3205] }, { \"_index\": \"shakespeare\", \"_id\": \"3203\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 3204, \"play_name\": \"Henry IV\", \"speech_number\": 8, \"line_number\": \"5.5.45\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"Let us not leave till all our own be won.\" }, \"sort\": [ 3204] }, { \"_index\": \"shakespeare\", \"_id\": \"3202\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 3203, \"play_name\": \"Henry IV\", \"speech_number\": 8, \"line_number\": \"5.5.44\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"And since this business so fair is done,\" }, \"sort\": [ 3203] }, { \"_index\": \"shakespeare\", \"_id\": \"3201\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 3202, \"play_name\": \"Henry IV\", \"speech_number\": 8, \"line_number\": \"5.5.43\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"Meeting the cheque of such another day:\" }, \"sort\": [ 3202] }, { \"_index\": \"shakespeare\", \"_id\": \"3200\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 3201, \"play_name\": \"Henry IV\", \"speech_number\": 8, \"line_number\": \"5.5.42\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"Rebellion in this land shall lose his sway,\" }, \"sort\": [ 3201] }, { \"_index\": \"shakespeare\", \"_id\": \"3199\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 3200, \"play_name\": \"Henry IV\", \"speech_number\": 8, \"line_number\": \"5.5.41\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"To fight with Glendower and the Earl of March.\" }, \"sort\": [ 3200] }, { \"_index\": \"shakespeare\", \"_id\": \"3198\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 3199, \"play_name\": \"Henry IV\", \"speech_number\": 8, \"line_number\": \"5.5.40\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"Myself and you, son Harry, will towards Wales,\" }, \"sort\": [ 3199] }, { \"_index\": \"shakespeare\", \"_id\": \"3197\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 3198, \"play_name\": \"Henry IV\", \"speech_number\": 8, \"line_number\": \"5.5.39\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"Who, as we hear, are busily in arms:\" }, \"sort\": [ 3198] }, { \"_index\": \"shakespeare\", \"_id\": \"3196\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 3197, \"play_name\": \"Henry IV\", \"speech_number\": 8, \"line_number\": \"5.5.38\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"To meet Northumberland and the prelate Scroop,\" }, \"sort\": [ 3197] }, { \"_index\": \"shakespeare\", \"_id\": \"3195\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 3196, \"play_name\": \"Henry IV\", \"speech_number\": 8, \"line_number\": \"5.5.37\", \"speaker\": \"KING HENRY IV\", \"text_entry\": \"Towards York shall bend you with your dearest speed,\" }, \"sort\": [ 3196] }] } } The sort parameter is an array, so you can specify multiple field values in the order of their priority.\nIf you have two fields with the same value for line_id, OpenSearch uses speech_number, which is the second option for sorting: GET shakespeare/_search { \"query\": { \"term\": { \"play_name\": { \"value\": \"Henry IV\" } } }, \"sort\": [ { \"line_id\": { \"order\": \"desc\" } }, { \"speech_number\": { \"order\": \"desc\" } }] } You can continue to sort by any number of field values to get the results in just the right order. It doesn’t have to be a numerical value—you can also sort by date or timestamp fields: \"sort\": [ { \"date\": { \"order\": \"desc\" } }] A text field that is analyzed cannot be used to sort documents, because the inverted index only contains the individual tokenized terms and not the entire string. So you cannot sort by the play_name, for example.\nTo bypass this limitation, you can use a raw version of the text field mapped as a keyword type. In the following example, play_name.keyword is not analyzed and you have a copy of the full original version for sorting purposes: GET shakespeare/_search { \"query\": { \"term\": { \"play_name\": { \"value\": \"Henry IV\" } } }, \"sort\": [ { \"play_name.keyword\": { \"order\": \"desc\" } }] } The results are sorted by the play_name field in alphabetical order.\nUse sort with the search_after parameter for more efficient scrolling.\nThe results start with the document that comes after the sort values you specify in the search_after array.\nMake sure you have the same number of values in the search_after array as in the sort array, also ordered in the same way.\nIn this case, you are requesting results starting with the document that comes after line_id = 3202 and speech_number = 8: GET shakespeare/_search { \"query\": { \"term\": { \"play_name\": { \"value\": \"Henry IV\" } } }, \"sort\": [ { \"line_id\": { \"order\": \"desc\" } }, { \"speech_number\": { \"order\": \"desc\" } }], \"search_after\": [ \"3202\", \"8\"] } Sort mode\nThe sort mode is applicable to sorting by array or multivalued fields. It specifies what array value should be chosen for sorting the document. For numeric fields that contain an array of numbers, you can sort by the avg, sum, or median modes. To sort by the minimum or maximum values, use the min or max modes that work for both numeric and string data types.\nThe default mode is min for ascending sort order and max for descending sort order.\nThe following example illustrates sorting by an array field using the sort mode.\nConsider an index that holds student grades. Index two documents into the index: PUT students/_doc/ 1 { \"name\": \"John Doe\", \"grades\": [ 70, 90] } PUT students/_doc/ 2 { \"name\": \"Mary Major\", \"grades\": [ 80, 100] } Sort all students by highest grade average using the avg mode: GET students/_search\n{\n\"query\": {\n\"match_all\": {}\n},\n\"sort\": [\n{\"grades\": {\"order\": \"desc\", \"mode\": \"avg\"}}]\n} The response contains students sorted by grades in descending order: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [ { \"_index\": \"students\", \"_id\": \"2\", \"_score\": null, \"_source\": { \"name\": \"Mary Major\", \"grades\": [ 80, 100] }, \"sort\": [ 90] }, { \"_index\": \"students\", \"_id\": \"1\", \"_score\": null, \"_source\": { \"name\": \"John Doe\", \"grades\": [ 70, 90] }, \"sort\": [ 80] }] } } Sorting nested objects\nWhen sorting nested objects, provide the path parameter specifying the path to the field on which to sort.\nFor example, in the index students, map the variable first_sem as nested: PUT students { \"mappings\": { \"properties\": { \"first_sem\": { \"type\": \"nested\" } } } } Index two documents with nested fields: PUT students/_doc/ 1 { \"name\": \"John Doe\", \"first_sem\": { \"grades\": [ 70, 90] } } PUT students/_doc/ 2 { \"name\": \"Mary Major\", \"first_sem\": { \"grades\": [ 80, 100] } } When sorting by grade average, provide the path to the nested field: GET students/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"first_sem.grades\": { \"order\": \"desc\", \"mode\": \"avg\", \"nested\": { \"path\": \"first_sem\" } } }] } Handling missing values\nThe missing parameter specifies the handling of missing values. The built-in valid values are _last (list the documents with the missing value last) and _first (list the documents with the missing value first). The default value is _last. You can also specify a custom value to be used for missing documents as the sort value.\nFor example, you can index a document with an average field and another document without an average field: PUT students/_doc/ 1 { \"name\": \"John Doe\", \"average\": 80 } PUT students/_doc/ 2 { \"name\": \"Mary Major\" } Sort the documents, ordering the document with a missing field first: GET students/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"average\": { \"order\": \"desc\", \"missing\": \"_first\" } }] } The response lists document 2 first: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [ { \"_index\": \"students\", \"_id\": \"2\", \"_score\": null, \"_source\": { \"name\": \"Mary Major\" }, \"sort\": [ 9223372036854775807] }, { \"_index\": \"students\", \"_id\": \"1\", \"_score\": null, \"_source\": { \"name\": \"John Doe\", \"average\": 80 }, \"sort\": [ 80] }] } } Ignoring unmapped fields\nIf a field is not mapped, a search request that sorts by this field fails by default. To avoid this, you can use the unmapped_type parameter, which signals to OpenSearch to ignore the field. For example, if you set unmapped_type to long, the field is treated as if it were mapped as type long. Additionally, all documents in the index that have an unmapped_type field are treated as if they had no value in this field, so they are not sorted by it.\nFor example, consider two indexes. Index a document that contains an average field in the first index: PUT students/_doc/ 1 { \"name\": \"John Doe\", \"average\": 80 } Index a document that does not contain an average field in the second index: PUT students_no_map/_doc/ 2 { \"name\": \"Mary Major\" } Search for all documents in both indexes and sort them by the average field: GET students*/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"average\": { \"order\": \"desc\" } }] } By default, the second index produces an error because the average field is not mapped: { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 2, \"successful\": 1, \"skipped\": 0, \"failed\": 1, \"failures\": [ { \"shard\": 0, \"index\": \"students_no_map\", \"node\": \"cam9NWqVSV-jUIkQ3tRubw\", \"reason\": { \"type\": \"query_shard_exception\", \"reason\": \"No mapping found for [average] in order to sort on\", \"index\": \"students_no_map\", \"index_uuid\": \"JgfRkypKSUSpyU-ZXr9kKA\" } }] }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [ { \"_index\": \"students\", \"_id\": \"1\", \"_score\": null, \"_source\": { \"name\": \"John Doe\", \"average\": 80 }, \"sort\": [ 80] }] } } You can specify the unmapped_type parameter so that the unmapped field is ignored: GET students*/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"average\": { \"order\": \"desc\", \"unmapped_type\": \"long\" } }] } The response contains both documents: { \"took\": 4, \"timed_out\": false, \"_shards\": { \"total\": 2, \"successful\": 2, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [ { \"_index\": \"students\", \"_id\": \"1\", \"_score\": null, \"_source\": { \"name\": \"John Doe\", \"average\": 80 }, \"sort\": [ 80] }, { \"_index\": \"students_no_map\", \"_id\": \"2\", \"_score\": null, \"_source\": { \"name\": \"Mary Major\" }, \"sort\": [ -9223372036854775808] }] } } Tracking scores\nBy default, scores are not computed when sorting on a field. You can set track_scores to true to compute and track scores: GET students/_search { \"query\": { \"match_all\": {} }, \"sort\": [ { \"average\": { \"order\": \"desc\" } }], \"track_scores\": true } Sorting by geo distance\nYou can sort documents by _geo_distance. The following parameters are supported. Parameter Description distance_type\nSpecifies the method of computing the distance. Valid values are arc and plane. The plane method is faster but less accurate for long distances or close to the poles. Default is arc.\nmode\nSpecifies how to handle a field with several geopoints. By default, documents are sorted by the shortest distance when the sort order is ascending and by the longest distance when the sort order is descending. Valid values are min, max, median, and avg.\nunit\nSpecifies the units used to compute sort values. Default is meters ( m).\nignore_unmapped\nSpecifies how to treat an unmapped field. Set ignore_unmapped to true to ignore unmapped fields. Default is false (produce an error when encountering an unmapped field). The _geo_distance parameter does not support missing_values. The distance is always considered to be infinity when a document does not contain the field used for computing distance.\nFor example, index two documents with geopoints: PUT testindex 1 /_doc/ 1 { \"point\": [ 74.00, 40.71] } PUT testindex 1 /_doc/ 2 { \"point\": [ 73.77, -69.63] } Search for all documents and sort them by the distance from the provided point: GET testindex 1 /_search { \"sort\": [ { \"_geo_distance\": { \"point\": [ 59, -54], \"order\": \"asc\", \"unit\": \"km\", \"distance_type\": \"arc\", \"mode\": \"min\", \"ignore_unmapped\": true } }], \"query\": { \"match_all\": {} } } The response contains the sorted documents: { \"took\": 864, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [ { \"_index\": \"testindex1\", \"_id\": \"2\", \"_score\": null, \"_source\": { \"point\": [ 73.77, -69.63] }, \"sort\": [ 1891.2667493895767] }, { \"_index\": \"testindex1\", \"_id\": \"1\", \"_score\": null, \"_source\": { \"point\": [ 74.0, 40.71] }, \"sort\": [ 10628.402240213345] }] } } You can provide coordinates in any format supported by the geopoint field type. For a description of all formats, see the geopoint field type documentation.\nTo pass multiple geopoints to _geo_distance, use an array: GET testindex 1 /_search { \"sort\": [ { \"_geo_distance\": { \"point\": [[ 59, -54], [ 60, -53]], \"order\": \"asc\", \"unit\": \"km\", \"distance_type\": \"arc\", \"mode\": \"min\", \"ignore_unmapped\": true } }], \"query\": { \"match_all\": {} } } For each document, the sorting distance is calculated as the minimum, maximum, or average (as specified by the mode) of the distances from all points provided in the search to all points in the document.\nPerformance considerations\nSorted field values are loaded into memory for sorting. Therefore, for minimum overhead we recommend mapping numeric types to the smallest acceptable types, like short, integer, and float. String types should not have the sorted field analyzed or tokenized.",
    "ancestors": [
      "Search",
      "Searching data"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/cli/",
    "title": "SQL and PPL CLI",
    "content": "The SQL and PPL command line interface (CLI) is a standalone Python application that you can launch with the opensearchsql command.\nTo use the SQL and PPL CLI, install the SQL plugin on your OpenSearch instance, run the CLI using MacOS or Linux, and connect to any valid OpenSearch endpoint. Features\nThe SQL and PPL CLI has the following features:\nMulti-line input\nPPL support\nAutocomplete for SQL syntax and index names\nSyntax highlighting\nFormatted output:\nTabular format\nField names with color\nEnabled horizontal display (by default) and vertical display when output is too wide for your terminal, for better visualization\nPagination for large output\nWorks with or without security enabled\nSupports loading configuration files\nSupports all SQL plugin queries\nInstall\nLaunch your local OpenSearch instance and make sure you have the SQL plugin installed.\nInstall the CLI: pip3 install opensearchsql The SQL CLI only works with Python 3.\nTo launch the CLI, run: opensearchsql https://localhost:9200 --username admin --password admin By default, the opensearchsql command connects to http://localhost:9200.\nConfigure\nWhen you first launch the SQL CLI, a configuration file is automatically created at ~/.config/opensearchsql-cli/config (for MacOS and Linux), the configuration is auto-loaded thereafter.\nYou can configure the following connection properties: endpoint: You do not need to specify an option. Anything that follows the launch command opensearchsql is considered as the endpoint. If you do not provide an endpoint, by default, the SQL CLI connects to http://localhost:9200. -u/-w: Supports username and password for HTTP basic authentication, such as with the Security plugin or fine-grained access control for Amazon OpenSearch Service. --aws-auth: Turns on AWS sigV4 authentication to connect to an Amazon OpenSearch endpoint. Use with the AWS CLI ( aws configure) to retrieve the local AWS configuration to authenticate and connect.\nFor a list of all available configurations, see clirc.\nUsing the CLI\nRun the CLI tool. If your cluster runs with the default security settings, use the following command: opensearchsql --username admin --password admin https://localhost:9200 If your cluster runs without security, run: opensearchsql Run a sample SQL command: SELECT * FROM accounts; By default, you see a maximum output of 200 rows. To show more results, add a LIMIT clause with the desired value.\nTo exit the CLI tool, select Ctrl+D.\nUsing the CLI with PPL\nRun the CLI by specifying the query language: opensearchsql -l ppl &lt;params&gt; Execute a PPL query: source = accounts | fields firstname, lastname Query options\nRun a single query with the following command line options: -q: Follow by a single query -f: Specify JDBC or raw format output -v: Display data vertically -e: Translate SQL to DSL\nCLI options --help: Help page for options -l: Query language option. Available options are sql and ppl. Default is sql -p: Always use pager to display output --clirc: Provide path for the configuration file",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/datatypes/",
    "title": "Data Types",
    "content": "The following table shows the data types supported by the SQL plugin and how each one maps to SQL and OpenSearch data types: OpenSearch SQL Type OpenSearch Type SQL Type boolean\nboolean\nBOOLEAN\nbyte\nbyte\nTINYINT\nshort\nbyte\nSMALLINT\ninteger\ninteger\nINTEGER\nlong\nlong\nBIGINT\nfloat\nfloat\nREAL\nhalf_float\nfloat\nFLOAT\nscaled_float\nfloat\nDOUBLE\ndouble\ndouble\nDOUBLE\nkeyword\nstring\nVARCHAR\ntext\ntext\nVARCHAR\ndate\ntimestamp\nTIMESTAMP\ndate_nanos\ntimestamp\nTIMESTAMP\nip\nip\nVARCHAR\ndate\ntimestamp\nTIMESTAMP\nbinary\nbinary\nVARBINARY\nobject\nstruct\nSTRUCT\nnested\narray\nSTRUCT In addition to this list, the SQL plugin also supports the datetime type, though it doesn’t have a corresponding mapping with OpenSearch or SQL.\nTo use a function without a corresponding mapping, you must explicitly convert the data type to one that does.\nDate and time types\nThe date and time types represent a time period: DATE, TIME, DATETIME, TIMESTAMP, and INTERVAL. By default, the OpenSearch DSL uses the date type as the only date-time related type that contains all information of an absolute time point.\nTo integrate with SQL, each type other than the timestamp type holds part of the time period information. To use date-time functions, see datetime. Some functions might have restrictions for the input argument type.\nDate\nThe date type represents the calendar date regardless of the time zone. A given date value is a 24-hour period, but this period varies in different timezones and might have flexible hours during daylight saving programs. The date type doesn’t contain time information and it only supports a range of 1000-01-01 to 9999-12-31. Type Syntax Range date yyyy-MM-dd 0001-01-01 to 9999-12-31 Time\nThe time type represents the time of a clock regardless of its timezone. The time type doesn’t contain date information. Type Syntax Range time hh:mm:ss[.fraction] 00:00:00.0000000000 to 23:59:59.9999999999 Datetime\nThe datetime type is a combination of date and time. It doesn’t contain timezone information. For an absolute time point that contains date, time, and timezone information, see Timestamp. Type Syntax Range datetime yyyy-MM-dd hh:mm:ss[.fraction] 0001-01-01 00:00:00.0000000000 to 9999-12-31 23:59:59.9999999999 Timestamp\nThe timestamp type is an absolute instance independent of timezone or convention. For example, for a given point of time, if you change the timestamp to a different timezone, its value changes accordingly.\nThe timestamp type is stored differently from the other types. It’s converted from its current timezone to UTC for storage and converted back to its set timezone from UTC when it’s retrieved. Type Syntax Range timestamp yyyy-MM-dd hh:mm:ss[.fraction] 0001-01-01 00:00:01.9999999999 UTC to 9999-12-31 23:59:59.9999999999 Interval\nThe interval type represents a temporal duration or a period. Type Syntax interval INTERVAL expr unit The expr unit is any expression that eventually iterates to a quantity value. It represents a unit for interpreting the quantity, including MICROSECOND, SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, and YEAR. The INTERVAL keyword and the unit specifier are not case sensitive.\nThe interval type has two classes of intervals: year-week intervals and day-time intervals.\nYear-week intervals store years, quarters, months, and weeks.\nDay-time intervals store days, hours, minutes, seconds, and microseconds.\nConvert between date and time types\nApart from the interval type, all date and time types can be converted to each other. The conversion might alter the value or cause some information loss. For example, when extracting the time value from a datetime value, or converting a date value to a datetime value, and so on.\nThe SQL plugin supports the following conversion rules for each of the types: Convert from date Because the date value doesn’t have any time information, conversion to the time type isn’t useful and always returns a zero time value of 00:00:00.\nConverting from date to datetime has a data fill-up due to the lack of time information. It attaches the time 00:00:00 to the original date by default and forms a datetime instance. For example, conversion of 2020-08-17 to a datetime type is 2020-08-17 00:00:00.\nConverting to timestamp type alternates both the time value and the timezone information. It attaches the zero time value 00:00:00 and the session timezone (UTC by default) to the date. For example, conversion of 2020-08-17 to a datetime type with a session timezone UTC is 2020-08-17 00:00:00 UTC. Convert from time You cannot convert the time type to any other date and time types because it doesn’t contain any date information. Convert from datetime Converting datetime to date extracts the date value from the datetime value. For example, conversion of 2020-08-17 14:09:00 to a date type is 2020-08-08.\nConverting datetime to time extracts the time value from the datetime value. For example, conversion of 2020-08-17 14:09:00 to a time type is 14:09:00.\nBecause the datetime type doesn’t contain timezone information, converting to timestamp type fills up the timezone value with the session timezone. For example, conversion of 2020-08-17 14:09:00 (UTC) to a timestamp type is 2020-08-17 14:09:00 UTC. Convert from timestamp Converting from a timestamp type to a date type extracts the date value and converting to a time type extracts the time value. Converting from a timestamp type to datetime type extracts only the datetime value and leaves out the timezone value. For example, conversion of 2020-08-17 14:09:00 UTC to a date type is 2020-08-17, to a time type is 14:09:00, and to a datetime type is 2020-08-17 14:09:00.",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/full-text/",
    "title": "Full-Text Search",
    "content": "Use SQL commands for full-text search. The SQL plugin supports a subset of full-text queries available in OpenSearch.\nTo learn about full-text queries in OpenSearch, see Full-text queries.\nMatch\nUse the MATCH function to search documents that match a string, number, date, or boolean value for a given field.\nSyntax match ( field_expression, query_expression [, option =&lt; option_value &gt;] *) You can specify the following options in any order: analyzer auto_generate_synonyms_phrase fuzziness max_expansions prefix_length fuzzy_transpositions fuzzy_rewrite lenient operator minimum_should_match zero_terms_query boost Refer to the match query documentation for parameter descriptions and supported values.\nExample 1: Search the message field for the text “this is a test”: GET my_index/_search { \"query\": { \"match\": { \"message\": \"this is a test\" } } } SQL query: SELECT message FROM my_index WHERE match ( message, \"this is a test\") PPL query: SOURCE=my_index | WHERE match(message, \"this is a test\") | FIELDS message Example 2: Search the message field with the operator parameter: GET my_index/_search { \"query\": { \"match\": { \"message\": { \"query\": \"this is a test\", \"operator\": \"and\" } } } } SQL query: SELECT message FROM my_index WHERE match ( message, \"this is a test\", operator = 'and') PPL query: SOURCE=my_index | WHERE match(message, \"this is a test\", operator='and') | FIELDS message Example 3: Search the message field with the operator and zero_terms_query parameters: GET my_index/_search { \"query\": { \"match\": { \"message\": { \"query\": \"to be or not to be\", \"operator\": \"and\", \"zero_terms_query\": \"all\" } } } } SQL query: SELECT message FROM my_index WHERE match ( message, \"this is a test\", operator = 'and', zero_terms_query = 'all') PPL query: SOURCE = my_index | WHERE match ( message, \"this is a test\", operator = 'and', zero_terms_query = 'all') | FIELDS message Multi-match\nTo search for text in multiple fields, use MULTI_MATCH function. This function maps to the multi_match query used in search engine, to returns the documents that match a provided text, number, date or boolean value with a given field or fields.\nSyntax\nThe MULTI_MATCH function lets you boost certain fields using ^ character. Boosts are multipliers that weigh matches in one field more heavily than matches in other fields. The syntax allows to specify the fields in double quotes, single quotes, surrounded by backticks, or unquoted. Use star \"*\" to search all fields. Star symbol should be quoted. multi_match ([ field_expression +], query_expression [, option =&lt; option_value &gt;] *) The weight is optional and is specified after the field name. It could be delimited by the caret character – ^ or by whitespace. Please, refer to examples below: multi_match ([ \"Tags\" ^ 2, 'Title' 3. 4, `Body`, Comments ^ 0. 3],...) multi_match ([ \"*\"],...) You can specify the following options for MULTI_MATCH in any order: analyzer auto_generate_synonyms_phrase cutoff_frequency fuzziness fuzzy_transpositions lenient max_expansions minimum_should_match operator prefix_length tie_breaker type slop zero_terms_query boost Please, refer to multi_match query documentation for parameter description and supported values.\nFor example, REST API search for Dale in either the firstname or lastname fields: GET accounts/_search { \"query\": { \"multi_match\": { \"query\": \"Lane Street\", \"fields\": [ \"address\"], } } } could be called from SQL using multi_match function SELECT firstname, lastname FROM accounts WHERE multi_match ([ '*name'], 'Dale') or multi_match PPL function SOURCE = accounts | WHERE multi_match ([ '*name'], 'Dale') | fields firstname, lastname firstname lastname Dale\nAdams Query string\nTo split text based on operators, use the QUERY_STRING function. The QUERY_STRING function supports logical connectives, wildcard, regex, and proximity search.\nThis function maps to the to the query_string query used in search engine, to return the documents that match a provided text, number, date or boolean value with a given field or fields.\nSyntax\nThe QUERY_STRING function has syntax similar to MATCH_QUERY and lets you boost certain fields using ^ character. Boosts are multipliers that weigh matches in one field more heavily than matches in other fields. The syntax allows to specify the fields in double quotes, single quotes, surrounded by backticks, or unquoted. Use star \"*\" to search all fields. Star symbol should be quoted. query_string ([ field_expression +], query_expression [, option =&lt; option_value &gt;] *) The weight is optional and is specified after the field name. It could be delimited by the caret character – ^ or by whitespace. Please, refer to examples below: query_string ([ \"Tags\" ^ 2, 'Title' 3. 4, `Body`, Comments ^ 0. 3],...) query_string ([ \"*\"],...) You can specify the following options for QUERY_STRING in any order: analyzer allow_leading_wildcard analyze_wildcard auto_generate_synonyms_phrase_query boost default_operator enable_position_increments fuzziness fuzzy_rewrite escape fuzzy_max_expansions fuzzy_prefix_length fuzzy_transpositions lenient max_determinized_states minimum_should_match quote_analyzer phrase_slop quote_field_suffix rewrite type tie_breaker time_zone Refer to the query_string query documentation for parameter descriptions and supported values.\nExample of using query_string in SQL and PPL queries:\nThe REST API search request GET accounts/_search { \"query\": { \"query_string\": { \"query\": \"Lane Street\", \"fields\": [ \"address\"], } } } could be called from SQL SELECT account_number, address FROM accounts WHERE query_string ([ 'address'], 'Lane Street', default_operator = 'OR') or from PPL SOURCE = accounts | WHERE query_string ([ 'address'], 'Lane Street', default_operator = 'OR') | fields account_number, address account_number address 1\n880 Holmes Lane\n6\n671 Bristol Street\n13\n789 Madison Street Match phrase\nTo search for exact phrases, use MATCHPHRASE or MATCH_PHRASE functions.\nSyntax matchphrasequery ( field_expression, query_expression) matchphrase ( field_expression, query_expression [, option =&lt; option_value &gt;] *) match_phrase ( field_expression, query_expression [, option =&lt; option_value &gt;] *) The MATCHPHRASE / MATCH_PHRASE functions let you specify the following options in any order: analyzer slop zero_terms_query boost Refer to the match_phrase query documentation for parameter descriptions and supported values.\nExample of using match_phrase in SQL and PPL queries:\nThe REST API search request GET accounts/_search { \"query\": { \"match_phrase\": { \"address\": { \"query\": \"880 Holmes Lane\" } } } } could be called from SQL SELECT account_number, address FROM accounts WHERE match_phrase ( address, '880 Holmes Lane') or PPL SOURCE = accounts | WHERE match_phrase ( address, '880 Holmes Lane') | FIELDS account_number, address account_number address 1\n880 Holmes Lane Simple query string\nThe simple_query_string function maps to the simple_query_string query in OpenSearch. It returns the documents that match a provided text, number, date or boolean value with a given field or fields.\nThe ^ lets you boost certain fields. Boosts are multipliers that weigh matches in one field more heavily than matches in other fields.\nSyntax\nThe syntax allows to specify the fields in double quotes, single quotes, surrounded by backticks, or unquoted. Use star \"*\" to search all fields. Star symbol should be quoted. simple_query_string ([ field_expression +], query_expression [, option =&lt; option_value &gt;] *) The weight is optional and is specified after the field name. It could be delimited by the caret character – ^ or by whitespace. Please, refer to examples below: simple_query_string ([ \"Tags\" ^ 2, 'Title' 3. 4, `Body`, Comments ^ 0. 3],...) simple_query_string ([ \"*\"],...) You can specify the following options for SIMPLE_QUERY_STRING in any order: analyze_wildcard analyzer auto_generate_synonyms_phrase_query boost default_operator flags fuzzy_max_expansions fuzzy_prefix_length fuzzy_transpositions lenient minimum_should_match quote_field_suffix Refer to the simple_query_string query documentation for parameter descriptions and supported values. Example of using simple_query_string in SQL and PPL queries:\nThe REST API search request GET accounts/_search { \"query\": { \"simple_query_string\": { \"query\": \"Lane Street\", \"fields\": [ \"address\"], } } } could be called from SQL SELECT account_number, address FROM accounts WHERE simple_query_string ([ 'address'], 'Lane Street', default_operator = 'OR') or from PPL SOURCE = accounts | WHERE simple_query_string ([ 'address'], 'Lane Street', default_operator = 'OR') | fields account_number, address account_number address 1\n880 Holmes Lane\n6\n671 Bristol Street\n13\n789 Madison Street Match phrase prefix\nTo search for phrases by given prefix, use MATCH_PHRASE_PREFIX function to make a prefix query out of the last term in the query string.\nSyntax match_phrase_prefix ( field_expression, query_expression [, option =&lt; option_value &gt;] *) The MATCH_PHRASE_PREFIX function lets you specify the following options in any order: analyzer slop max_expansions zero_terms_query boost Refer to the match_phrase_prefix query documentation for parameter descriptions and supported values. Example of using match_phrase_prefix in SQL and PPL queries:\nThe REST API search request GET accounts/_search { \"query\": { \"match_phrase_prefix\": { \"author\": { \"query\": \"Alexander Mil\" } } } } could be called from SQL SELECT author, title FROM books WHERE match_phrase_prefix ( author, 'Alexander Mil') or PPL source = books | where match_phrase_prefix ( author, 'Alexander Mil') | fields author, title author title Alan Alexander Milne\nThe House at Pooh Corner\nAlan Alexander Milne\nWinnie-the-Pooh Match boolean prefix\nUse the match_bool_prefix function to search documents that match text only for a given field prefix.\nSyntax match_bool_prefix ( field_expression, query_expression [, option =&lt; option_value &gt;] *) The MATCH_BOOL_PREFIX function lets you specify the following options in any order: minimum_should_match fuzziness prefix_length max_expansions fuzzy_transpositions fuzzy_rewrite boost analyzer operator Refer to the match_bool_prefix query documentation for parameter descriptions and supported values.\nExample of using match_bool_prefix in SQL and PPL queries:\nThe REST API search request GET accounts/_search { \"query\": { \"match_bool_prefix\": { \"address\": { \"query\": \"Bristol Stre\" } } } } could be called from SQL SELECT firstname, address FROM accounts WHERE match_bool_prefix ( address, 'Bristol Stre') or PPL source = accounts | where match_bool_prefix ( address, 'Bristol Stre') | fields firstname, address firstname address Hattie\n671 Bristol Street\nNanette\n789 Madison Street",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/functions/",
    "title": "Functions",
    "content": "You must enable fielddata in the document mapping for most string functions to work properly.\nThe specification shows the return type of the function with a generic type T as the argument.\nFor example, abs(number T) -&gt; T means that the function abs accepts a numerical argument of type T, which could be any subtype of the number type, and it returns the actual type of T as the return type.\nThe SQL plugin supports the following common functions shared across the SQL and PPL languages.\nMathematical Function Specification Example abs abs(number T) -&gt; T SELECT abs(0.5) FROM my-index LIMIT 1 add add(number T, number) -&gt; T SELECT add(1, 5) FROM my-index LIMIT 1 cbrt cbrt(number T) -&gt; T SELECT cbrt(0.5) FROM my-index LIMIT 1 ceil ceil(number T) -&gt; T SELECT ceil(0.5) FROM my-index LIMIT 1 conv conv(string T, int a, int b) -&gt; T SELECT CONV('12', 10, 16), CONV('2C', 16, 10), CONV(12, 10, 2), CONV(1111, 2, 10) FROM my-index LIMIT 1 crc32 crc32(string T) -&gt; T SELECT crc32('MySQL') FROM my-index LIMIT 1 divide divide(number T, number) -&gt; T SELECT divide(1, 0.5) FROM my-index LIMIT 1 e e() -&gt; double SELECT e() FROM my-index LIMIT 1 exp exp(number T) -&gt; T SELECT exp(0.5) FROM my-index LIMIT 1 expm1 expm1(number T) -&gt; T SELECT expm1(0.5) FROM my-index LIMIT 1 floor floor(number T) -&gt; T SELECT floor(0.5) AS Rounded_Down FROM my-index LIMIT 1 ln ln(number T) -&gt; double SELECT ln(10) FROM my-index LIMIT 1 log log(number T) -&gt; double or log(number T, number) -&gt; double SELECT log(10) FROM my-index LIMIT 1 log2 log2(number T) -&gt; double SELECT log2(10) FROM my-index LIMIT 1 log10 log10(number T) -&gt; double SELECT log10(10) FROM my-index LIMIT 1 mod mod(number T, number) -&gt; T SELECT modulus(2, 3) FROM my-index LIMIT 1 multiply multiply(number T, number) -&gt; number SELECT multiply(2, 3) FROM my-index LIMIT 1 pi pi() -&gt; double SELECT pi() FROM my-index LIMIT 1 pow pow(number T) -&gt; T or pow(number T, number) -&gt; T SELECT pow(2, 3) FROM my-index LIMIT 1 power power(number T) -&gt; T or power(number T, number) -&gt; T SELECT power(2, 3) FROM my-index LIMIT 1 rand rand() -&gt; number or rand(number T) -&gt; T SELECT rand(0.5) FROM my-index LIMIT 1 rint rint(number T) -&gt; T SELECT rint(1.5) FROM my-index LIMIT 1 round round(number T) -&gt; T SELECT round(1.5) FROM my-index LIMIT 1 sign sign(number T) -&gt; T SELECT sign(1.5) FROM my-index LIMIT 1 signum signum(number T) -&gt; T SELECT signum(0.5) FROM my-index LIMIT 1 sqrt sqrt(number T) -&gt; T SELECT sqrt(0.5) FROM my-index LIMIT 1 strcmp strcmp(string T, string T) -&gt; T SELECT strcmp('hello', 'hello') FROM my-index LIMIT 1 subtract subtract(number T, number) -&gt; T SELECT subtract(3, 2) FROM my-index LIMIT 1 truncate truncate(number T, number T) -&gt; T SELECT truncate(56.78, 1) FROM my-index LIMIT 1 / number [op] number -&gt; number SELECT 1 / 100 FROM my-index LIMIT 1 % number [op] number -&gt; number SELECT 1 % 100 FROM my-index LIMIT 1 Trigonometric Function Specification Example acos acos(number T) -&gt; double SELECT acos(0.5) FROM my-index LIMIT 1 asin asin(number T) -&gt; double SELECT asin(0.5) FROM my-index LIMIT 1 atan atan(number T) -&gt; double SELECT atan(0.5) FROM my-index LIMIT 1 atan2 atan2(number T, number) -&gt; double SELECT atan2(1, 0.5) FROM my-index LIMIT 1 cos cos(number T) -&gt; double SELECT cos(0.5) FROM my-index LIMIT 1 cosh cosh(number T) -&gt; double SELECT cosh(0.5) FROM my-index LIMIT 1 cot cot(number T) -&gt; double SELECT cot(0.5) FROM my-index LIMIT 1 degrees degrees(number T) -&gt; double SELECT degrees(0.5) FROM my-index LIMIT 1 radians radians(number T) -&gt; double SELECT radians(0.5) FROM my-index LIMIT 1 sin sin(number T) -&gt; double SELECT sin(0.5) FROM my-index LIMIT 1 sinh sinh(number T) -&gt; double SELECT sinh(0.5) FROM my-index LIMIT 1 tan tan(number T) -&gt; double SELECT tan(0.5) FROM my-index LIMIT 1 Date and time Function Specification Example adddate adddate(date, INTERVAL expr unit) -&gt; date SELECT adddate(date('2020-08-26'), INTERVAL 1 hour) FROM my-index LIMIT 1 curdate curdate() -&gt; date SELECT curdate() FROM my-index LIMIT 1 date date(date) -&gt; date SELECT date() FROM my-index LIMIT 1 date_format date_format(date, string) -&gt; string or date_format(date, string, string) -&gt; string SELECT date_format(date, 'Y') FROM my-index LIMIT 1 date_sub date_sub(date, INTERVAL expr unit) -&gt; date SELECT date_sub(date('2008-01-02'), INTERVAL 31 day) FROM my-index LIMIT 1 dayofmonth dayofmonth(date) -&gt; integer SELECT dayofmonth(date) FROM my-index LIMIT 1 dayname dayname(date) -&gt; string SELECT dayname(date('2020-08-26')) FROM my-index LIMIT 1 dayofyear dayofyear(date) -&gt; integer SELECT dayofyear(date('2020-08-26')) FROM my-index LIMIT 1 dayofweek dayofweek(date) -&gt; integer SELECT dayofweek(date('2020-08-26')) FROM my-index LIMIT 1 from_days from_days(N) -&gt; integer SELECT from_days(733687) FROM my-index LIMIT 1 hour hour(time) -&gt; integer SELECT hour((time '01:02:03')) FROM my-index LIMIT 1 maketime maketime(integer, integer, integer) -&gt; date SELECT maketime(1, 2, 3) FROM my-index LIMIT 1 microsecond microsecond(expr) -&gt; integer SELECT microsecond((time '01:02:03.123456')) FROM my-index LIMIT 1 minute minute(expr) -&gt; integer SELECT minute((time '01:02:03')) FROM my-index LIMIT 1 month month(date) -&gt; integer SELECT month(date) FROM my-index monthname monthname(date) -&gt; string SELECT monthname(date) FROM my-index now now() -&gt; date SELECT now() FROM my-index LIMIT 1 quarter quarter(date) -&gt; integer SELECT quarter(date('2020-08-26')) FROM my-index LIMIT 1 second second(time) -&gt; integer SELECT second((time '01:02:03')) FROM my-index LIMIT 1 subdate subdate(date, INTERVAL expr unit) -&gt; date, datetime SELECT subdate(date('2008-01-02'), INTERVAL 31 day) FROM my-index LIMIT 1 time time(expr) -&gt; time SELECT time('13:49:00') FROM my-index LIMIT 1 time_to_sec time_to_sec(time) -&gt; long SELECT time_to_sec(time '22:23:00') FROM my-index LIMIT 1 timestamp timestamp(date) -&gt; date SELECT timestamp(date) FROM my-index LIMIT 1 to_days to_days(date) -&gt; long SELECT to_days(date '2008-10-07') FROM my-index LIMIT 1 week week(date[mode]) -&gt; integer SELECT week(date('2008-02-20')) FROM my-index LIMIT 1 year year(date) -&gt; integer SELECT year(date) FROM my-index LIMIT 1 String Function Specification Example ascii ascii(string T) -&gt; integer SELECT ascii(name.keyword) FROM my-index LIMIT 1 concat concat(str1, str2) -&gt; string SELECT concat('hello', 'world') FROM my-index LIMIT 1 concat_ws concat_ws(separator, string, string…) -&gt; string SELECT concat_ws(\"-\", \"Tutorial\", \"is\", \"fun!\") FROM my-index LIMIT 1 left left(string T, integer) -&gt; T SELECT left('hello', 2) FROM my-index LIMIT 1 length length(string) -&gt; integer SELECT length('hello') FROM my-index LIMIT 1 locate locate(string, string, integer) -&gt; integer or locate(string, string) -&gt; INTEGER SELECT locate('o', 'hello') FROM my-index LIMIT 1, SELECT locate('l', 'hello', 3) FROM my-index LIMIT 1 replace replace(string T, string, string) -&gt; T SELECT replace('hello', 'l', 'x') FROM my-index LIMIT 1 right right(string T, integer) -&gt; T SELECT right('hello', 1) FROM my-index LIMIT 1 rtrim rtrim(string T) -&gt; T SELECT rtrim(name.keyword) FROM my-index LIMIT 1 substring substring(string T, integer, integer) -&gt; T SELECT substring(name.keyword, 2,5) FROM my-index LIMIT 1 trim trim(string T) -&gt; T SELECT trim(' hello') FROM my-index LIMIT 1 upper upper(string T) -&gt; T SELECT upper('helloworld') FROM my-index LIMIT 1 Aggregate Function Specification Example avg avg(number T) -&gt; T SELECT avg(2, 3) FROM my-index LIMIT 1 count count(number T) -&gt; T SELECT count(date) FROM my-index LIMIT 1 min min(number T, number) -&gt; T SELECT min(2, 3) FROM my-index LIMIT 1 show show(string T) -&gt; T SHOW TABLES LIKE my-index Advanced Function Specification Example if if(boolean, es_type, es_type) -&gt; es_type SELECT if(false, 0, 1) FROM my-index LIMIT 1, SELECT if(true, 0, 1) FROM my-index LIMIT 1 ifnull ifnull(es_type, es_type) -&gt; es_type SELECT ifnull('hello', 1) FROM my-index LIMIT 1, SELECT ifnull(null, 1) FROM my-index LIMIT 1 isnull isnull(es_type) -&gt; integer SELECT isnull(null) FROM my-index LIMIT 1, SELECT isnull(1) FROM my-index LIMIT 1 Relevance-based search (full-text search)\nThese functions are only available in the WHERE clause. For their descriptions and usage examples in SQL and PPL, see Full-text search.",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/identifiers/",
    "title": "Identifiers",
    "content": "An identifier is an ID to name your database objects, such as index names, field names, aliases, and so on.\nOpenSearch supports two types of identifiers: regular identifiers and delimited identifiers.\nRegular identifiers\nA regular identifier is a string of characters that starts with an ASCII letter (lower or upper case).\nThe next character can either be a letter, digit, or underscore (_). It can’t be a reserved keyword.\nWhitespace and other special characters are also not allowed.\nOpenSearch supports the following regular identifiers:\nIdentifiers prefixed by a dot. sign. Use to hide an index. For example.opensearch-dashboards.\nIdentifiers prefixed by an @ sign. Use for meta fields generated by Logstash ingestion.\nIdentifiers with hyphen - in the middle. Use for index names with date information.\nIdentifiers with star * present. Use for wildcard match of index patterns.\nFor regular identifiers, you can use the name without any back tick or escape characters.\nIn this example, source, fields, account_number, firstname, and lastname are all identifiers. Out of these, the source field is a reserved identifier. SELECT account_number, firstname, lastname FROM accounts; account_number firstname lastname 1\nAmber\nDuke\n6\nHattie\nBond\n13\nNanette\nBates\n18\nDale\nAdams Delimited identifiers\nA delimited identifier can contain special characters not allowed by a regular identifier.\nYou must enclose delimited identifiers with back ticks (``). Back ticks differentiate the identifier from special characters.\nIf the index name includes a dot (.), for example, log-2021.01.11, use delimited identifiers with back ticks to escape it ` log-2021.01.11 `.\nTypical examples of using delimited identifiers:\nIdentifiers with reserved keywords.\nIdentifiers with a. present. Similarly, - to include date information.\nIdentifiers with other special characters. For example, Unicode characters.\nTo quote an index name with back ticks: source = `accounts` | fields `account_number`; account_number 1\n6\n13\n18 Case sensitivity\nIdentifiers are case sensitive. They must be exactly the same as what’s stored in OpenSearch.\nFor example, if you run source=Accounts, you’ll get an index not found exception because the actual index name is in lower case.",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/index/",
    "title": "SQL and PPL",
    "content": "OpenSearch SQL lets you write queries in SQL rather than the OpenSearch query domain-specific language (DSL). If you’re already familiar with SQL and don’t want to learn the query DSL, this feature is a great option.\nContributing\nTo get involved and help us improve the SQL plugin, see the development guide for instructions on setting up your development environment and building the project.",
    "ancestors": [
      "Search"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/limitation/",
    "title": "Limitations",
    "content": "The SQL plugin has the following limitations:\nAggregation over expression is not supported\nYou can only apply aggregation to fields. Aggregations cannot accept an expression as a parameter. For example, avg(log(age)) is not supported.\nSubquery in the FROM clause\nSubquery in the FROM clause in this format: SELECT outer FROM (SELECT inner) is supported only when the query is merged into one query. For example, the following query is supported: SELECT t. f, t. d FROM ( SELECT FlightNum as f, DestCountry as d FROM opensearch_dashboards_sample_data_flights WHERE OriginCountry = 'US') t But, if the outer query has GROUP BY or ORDER BY, then it’s not supported.\nJOIN does not support aggregations on the joined result\nThe join query does not support aggregations on the joined result.\nFor example, e.g. SELECT depo.name, avg(empo.age) FROM empo JOIN depo WHERE empo.id == depo.id GROUP BY depo.name is not supported.\nPagination only supports basic queries\nThe pagination query enables you to get back paginated responses.\nCurrently, the pagination only supports basic queries. For example, the following query returns the data with cursor id. POST _plugins/_sql/ { \"fetch_size\": 5, \"query\": \"SELECT OriginCountry, DestCountry FROM opensearch_dashboards_sample_data_flights ORDER BY OriginCountry ASC\" } The response in JDBC format with cursor id. { \"schema\": [ { \"name\": \"OriginCountry\", \"type\": \"keyword\" }, { \"name\": \"DestCountry\", \"type\": \"keyword\" }], \"cursor\": \"d:eyJhIjp7fSwicyI6IkRYRjFaWEo1UVc1a1JtVjBZMmdCQUFBQUFBQUFCSllXVTJKVU4yeExiWEJSUkhsNFVrdDVXVEZSYkVKSmR3PT0iLCJjIjpbeyJuYW1lIjoiT3JpZ2luQ291bnRyeSIsInR5cGUiOiJrZXl3b3JkIn0seyJuYW1lIjoiRGVzdENvdW50cnkiLCJ0eXBlIjoia2V5d29yZCJ9XSwiZiI6MSwiaSI6ImtpYmFuYV9zYW1wbGVfZGF0YV9mbGlnaHRzIiwibCI6MTMwNTh9\", \"total\": 13059, \"datarows\": [[ \"AE\", \"CN\"]], \"size\": 1, \"status\": 200 } The query with aggregation and join does not support pagination for now.\nQuery processing engines\nThe SQL plugin has two query processing engines, V1 and V2. Most of the features are supported by both engines, but only the new engine is actively being developed. A query that is first executed on the V2 engine falls back to the V1 engine in case of failure. If a query is supported in V2 but not included in V1, the query will fail with an error response.\nV1 engine limitations\nThe select literal expression without FROM clause is not supported. For example, SELECT 1 is not supported.\nThe WHERE clause does not support expressions. For example, SELECT FlightNum FROM opensearch_dashboards_sample_data_flights where (AvgTicketPrice + 100) &lt;= 1000 is not supported.\nMost relevancy search functions are implemented in the V2 engine only.\nSuch queries are successfully executed by the V2 engine unless they have V1 -specific functions. You will likely never meet these limitations.\nV2 engine limitations\nThe cursor feature is supported by the V1 engine only.\nFor support of cursor / pagination in the V2 engine, track GitHub issue #656.\nThe V2 engine does not track query execution time, so slow queries are not reported.\nThe V2 query engine not only runs queries in the OpenSearch engine but also supports post-processing for complicated queries. Accordingly, the explain output is no longer pure OpenSearch domain-specific language (DSL) but also includes query plan information from the V2 query engine.\nThe V2 engine does not support SCORE_QUERY and WILDCARD_QUERY functions.",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/monitoring/",
    "title": "Monitoring",
    "content": "By a stats endpoint, you are able to collect metrics for the plugin\nwithin the interval. Note that only node level statistics collecting is\nimplemented for now. In other words, you only get the metrics for the\nnode you’re accessing. Cluster level statistics have yet to be\nimplemented.\nNode Stats\nDescription\nThe meaning of fields in the response is as follows: Field name Description request_total\nTotal count of request\nrequest_count\nTotal count of request within the interval\nfailed_request_count_syserr\nCount of failed request due to system error within the interval\nfailed_request_count_cuserr\nCount of failed request due to bad request within the interval\nfailed_request_count_cb\nIndicate if plugin is being circuit broken within the interval Example\nSQL query: &gt; &gt; curl -H 'Content-Type: application/json' -X GET localhost:9200/_plugins/_sql/stats Result set: { \"failed_request_count_cb\": 0, \"failed_request_count_cuserr\": 0, \"circuit_breaker\": 0, \"request_total\": 0, \"request_count\": 0, \"failed_request_count_syserr\": 0 }",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/ppl/functions/",
    "title": "Commands",
    "content": "PPL supports all SQL common functions, including relevance search, but also introduces few more functions (called commands) which are available in PPL only.\ndedup\nThe dedup (data deduplication) command removes duplicate documents defined by a field from the search result.\nSyntax dedup [ int] &lt; field - list &gt; [ keepempty =&lt; bool &gt;] [ consecutive =&lt; bool &gt;] Field Description Type Required Default int Retain the specified number of duplicate events for each combination. The number must be greater than 0. If you do not specify a number, only the first occurring event is kept and all other duplicates are removed from the results. string No\n1 keepempty If true, keep the document if any field in the field list has a null value or a field missing. nested list of objects No\nFalse consecutive If true, remove only consecutive events with duplicate combinations of values. Boolean No\nFalse field-list Specify a comma-delimited field list. At least one field is required. String or comma-separated list of strings\nYes\n- Example 1: Dedup by one field To remove duplicate documents with the same gender: search source = accounts | dedup gender | fields account_number, gender; account_number gender 1\nM\n13\nF Example 2: Keep two duplicate documents To keep two duplicate documents with the same gender: search source = accounts | dedup 2 gender | fields account_number, gender; account_number gender 1\nM\n6\nM\n13\nF Example 3: Keep or ignore an empty field by default To keep two duplicate documents with a null field value: search source = accounts | dedup email keepempty = true | fields account_number, email; account_number email 1\namberduke@pyrami.com\n6\nhattiebond@netagy.com\n13\nnull\n18\ndaleadams@boink.com To remove duplicate documents with the null field value: search source = accounts | dedup email | fields account_number, email; account_number email 1\namberduke@pyrami.com\n6\nhattiebond@netagy.com\n18\ndaleadams@boink.com Example 4: Dedup of consecutive documents To remove duplicates of consecutive documents: search source = accounts | dedup gender consecutive = true | fields account_number, gender; account_number gender 1\nM\n13\nF\n18\nM Limitations\nThe dedup command is not rewritten to OpenSearch DSL, it is only executed on the coordination node.\neval\nThe eval command evaluates an expression and appends its result to the search result.\nSyntax eval &lt; field &gt;=&lt; expression &gt; [ \",\" &lt; field &gt;=&lt; expression &gt;]... Field Description Required field If a field name does not exist, a new field is added. If the field name already exists, it’s overwritten.\nYes expression Specify any supported expression.\nYes Example 1: Create a new field To create a new doubleAge field for each document. doubleAge is the result of age multiplied by 2: search source = accounts | eval doubleAge = age * 2 | fields age, doubleAge; age doubleAge 32\n64\n36\n72\n28\n56\n33\n66 Example 2: Overwrite the existing field\nTo overwrite the age field with age plus 1: search source = accounts | eval age = age + 1 | fields age; age 33\n37\n29\n34 Example 3: Create a new field with a field defined with the eval command To create a new field ddAge. ddAge is the result of doubleAge multiplied by 2, where doubleAge is defined in the eval command: search source = accounts | eval doubleAge = age * 2, ddAge = doubleAge * 2 | fields age, doubleAge, ddAge; age doubleAge ddAge 32\n64\n128\n36\n72\n144\n28\n56\n112\n33\n66\n132 Limitation\nThe eval command is not rewritten to OpenSearch DSL, it is only executed on the coordination node.\nfields\nUse the fields command to keep or remove fields from a search result.\nSyntax fields [ +|-] &lt; field - list &gt; Field Description Required Default index Plus (+) keeps only fields specified in the field list. Minus (-) removes all fields specified in the field list.\nNo\n+ field list Specify a comma-delimited list of fields.\nYes\nNo default Example 1: Select specified fields from result To get account_number, firstname, and lastname fields from a search result: search source = accounts | fields account_number, firstname, lastname; account_number firstname lastname 1\nAmber\nDuke\n6\nHattie\nBond\n13\nNanette\nBates\n18\nDale\nAdams Example 2: Remove specified fields from a search result To remove the account_number field from the search results: search source = accounts | fields account_number, firstname, lastname | fields - account_number; firstname lastname Amber\nDuke\nHattie\nBond\nNanette\nBates\nDale\nAdams parse\nUse the parse command to parse a text field using regular expression and append the result to the search result.\nSyntax parse &lt; field &gt; &lt; regular - expression &gt; Field Description Required field\nA text field.\nYes\nregular-expression\nThe regular expression used to extract new fields from the given test field. If a new field name exists, it will replace the original field.\nYes The regular expression is used to match the whole text field of each document with Java regex engine. Each named capture group in the expression will become a new STRING field. Example 1: Create new field The example shows how to create new field host for each document. host will be the hostname after @ in email field. Parsing a null field will return an empty string. os &gt; source = accounts | parse email '.+@(?&lt;host&gt;.+)' | fields email, host; fetched rows / total rows = 4 / 4 email host amberduke@pyrami.com\npyrami.com\nhattiebond@netagy.com\nnetagy.com\nnull\nnull\ndaleadams@boink.com\nboink.com Example 2: Override the existing field\nThe example shows how to override the existing address field with street number removed. os &gt; source = accounts | parse address ' \\d + (?&lt;address&gt;.+)' | fields address; fetched rows / total rows = 4 / 4 address Holmes Lane\nBristol Street\nMadison Street\nHutchinson Court Example 3: Filter and sort be casted parsed field The example shows how to sort street numbers that are higher than 500 in address field. os &gt; source = accounts | parse address '(?&lt;streetNumber&gt; \\d +) (?&lt;street&gt;.+)' | where cast ( streetNumber as int) &gt; 500 | sort num ( streetNumber) | fields streetNumber, street; fetched rows / total rows = 3 / 3 streetNumber street 671\nBristol Street\n789\nMadison Street\n880\nHolmes Lane Limitations\nA few limitations exist when using the parse command:\nFields defined by parse cannot be parsed again. For example, source=accounts | parse address '\\d+ (?&lt;street&gt;.+)' | parse street '\\w+ (?&lt;road&gt;\\w+)'; will fail to return any expressions.\nFields defined by parse cannot be overridden with other commands. For example, when entering source=accounts | parse address '\\d+ (?&lt;street&gt;.+)' | eval street='1' | where street='1'; where will not match any documents since street cannot be overridden.\nThe text field used by parse cannot be overridden. For example, when entering source=accounts | parse address '\\d+ (?&lt;street&gt;.+)' | eval address='1'; street will not be parse since address is overridden.\nFields defined by parse cannot be filtered/sorted after using them in the stats command. For example, source=accounts | parse email '.+@(?&lt;host&gt;.+)' | stats avg(age) by host | where host=pyrami.com; where will not parse the domain listed.\nrename\nUse the rename command to rename one or more fields in the search result.\nSyntax rename &lt; source - field &gt; AS &lt; target - field &gt; [ \",\" &lt; source - field &gt; AS &lt; target - field &gt;]... Field Description Required source-field The name of the field that you want to rename.\nYes target-field The name you want to rename to.\nYes Example 1: Rename one field Rename the account_number field as an: search source = accounts | rename account_number as an | fields an; an 1\n6\n13\n18 Example 2: Rename multiple fields Rename the account_number field as an and employer as emp: search source = accounts | rename account_number as an, employer as emp | fields an, emp; an emp 1\nPyrami\n6\nNetagy\n13\nQuility\n18\nnull Limitations\nThe rename command is not rewritten to OpenSearch DSL, it is only executed on the coordination node.\nsort\nUse the sort command to sort search results by a specified field.\nSyntax sort [ count] &lt; [ +|-] sort - field &gt;... Field Description Required Default count The maximum number results to return from the sorted result. If count=0, all results are returned.\nNo\n1000 [+|-] Use plus [+] to sort by ascending order and minus [-] to sort by descending order.\nNo\nAscending order sort-field Specify the field that you want to sort by.\nYes\n- Example 1: Sort by one field To sort all documents by the age field in ascending order: search source = accounts | sort age | fields account_number, age; account_number age 13\n28\n1\n32\n18\n33\n6\n36 Example 2: Sort by one field and return all results To sort all documents by the age field in ascending order and specify count as 0 to get back all results: search source = accounts | sort 0 age | fields account_number, age; account_number age 13\n28\n1\n32\n18\n33\n6\n36 Example 3: Sort by one field in descending order To sort all documents by the age field in descending order: search source = accounts | sort - age | fields account_number, age; account_number age 6\n36\n18\n33\n1\n32\n13\n28 Example 4: Specify the number of sorted documents to return To sort all documents by the age field in ascending order and specify count as 2 to get back two results: search source = accounts | sort 2 age | fields account_number, age; account_number age 13\n28\n1\n32 Example 5: Sort by multiple fields To sort all documents by the gender field in ascending order and age field in descending order: search source = accounts | sort + gender, - age | fields account_number, gender, age; account_number gender age 13\nF\n28\n6\nM\n36\n18\nM\n33\n1\nM\n32 stats\nUse the stats command to aggregate from search results.\nThe following table lists the aggregation functions and also indicates how each one handles null or missing values: Function NULL MISSING COUNT Not counted\nNot counted SUM Ignore\nIgnore AVG Ignore\nIgnore MAX Ignore\nIgnore MIN Ignore\nIgnore Syntax stats &lt;aggregation&gt;... [by-clause]... Field Description Required Default aggregation Specify a statistical aggregation function. The argument of this function must be a field.\nYes\n1000 by-clause Specify one or more fields to group the results by. If not specified, the stats command returns only one row, which is the aggregation over the entire result set.\nNo\n- Example 1: Calculate the average value of a field To calculate the average age of all documents: search source = accounts | stats avg ( age); avg(age) 32.25 Example 2: Calculate the average value of a field by group To calculate the average age grouped by gender: search source = accounts | stats avg ( age) by gender; gender avg(age) F\n28.0\nM\n33.666666666666664 Example 3: Calculate the average and sum of a field by group To calculate the average and sum of age grouped by gender: search source = accounts | stats avg ( age), sum ( age) by gender; gender avg(age) sum(age) F\n28\n28\nM\n33.666666666666664\n101 Example 4: Calculate the maximum value of a field To calculate the maximum age: search source = accounts | stats max ( age); max(age) 36 Example 5: Calculate the maximum and minimum value of a field by group To calculate the maximum and minimum age values grouped by gender: search source = accounts | stats max ( age), min ( age) by gender; gender min(age) max(age) F\n28\n28\nM\n32\n36 where\nUse the where command with a bool expression to filter the search result. The where command only returns the result when the bool expression evaluates to true.\nSyntax where &lt; boolean - expression &gt; Field Description Required bool-expression An expression that evaluates to a boolean value.\nNo Example: Filter result set with a condition To get all documents from the accounts index where account_number is 1 or gender is F: search source = accounts | where account_number = 1 or gender = \\ \"F \\\" | fields account_number, gender; account_number gender 1\nM\n13\nF head\nUse the head command to return the first N number of results in a specified search order.\nSyntax head [ N] Field Description Required Default N Specify the number of results to return.\nNo\n10 Example 1: Get the first 10 results To get the first 10 results: search source = accounts | fields firstname, age | head; firstname age Amber\n32\nHattie\n36\nNanette\n28 Example 2: Get the first N results To get the first two results: search source = accounts | fields firstname, age | head 2; firstname age Amber\n32\nHattie\n36 Limitations\nThe head command is not rewritten to OpenSearch DSL, it is only executed on the coordination node.\nrare\nUse the rare command to find the least common values of all fields in a field list.\nA maximum of 10 results are returned for each distinct set of values of the group-by fields.\nSyntax rare &lt; field - list &gt; [ by - clause] Field Description Required field-list Specify a comma-delimited list of field names.\nNo by-clause Specify one or more fields to group the results by.\nNo Example 1: Find the least common values in a field To find the least common values of gender: search source = accounts | rare gender; gender F\nM Example 2: Find the least common values grouped by gender To find the least common age grouped by gender: search source = accounts | rare age by gender; gender age F\n28\nM\n32\nM\n33 Limitations\nThe rare command is not rewritten to OpenSearch DSL, it is only executed on the coordination node.\ntop\nUse the top command to find the most common values of all fields in the field list.\nSyntax top [ N] &lt; field - list &gt; [ by - clause] Field Description Default N Specify the number of results to return.\n10 field-list Specify a comma-delimited list of field names.\n- by-clause Specify one or more fields to group the results by.\n- Example 1: Find the most common values in a field To find the most common genders: search source = accounts | top gender; gender M\nF Example 2: Find the most common value in a field To find the most common gender: search source = accounts | top 1 gender; gender M Example 3: Find the most common values grouped by gender To find the most common age grouped by gender: search source = accounts | top 1 age by gender; gender age F\n28\nM\n32 Limitations\nThe top command is not rewritten to OpenSearch DSL, it is only executed on the coordination node.\nad\nThe ad command applies the Random Cut Forest (RCF) algorithm in the ML Commons plugin on the search result returned by a PPL command. Based on the input, the plugin uses two types of RCF algorithms: fixed in time RCF for processing time-series data and batch RCF for processing non-time-series data.\nSyntax: Fixed In Time RCF For Time-series Data Command ad &lt; shingle_size &gt; &lt; time_decay &gt; &lt; time_field &gt; Field Description Required shingle_size A consecutive sequence of the most recent records. The default value is 8.\nNo time_decay Specifies how much of the recent past to consider when computing an anomaly score. The default value is 0.001.\nNo time_field Specifies the time filed for RCF to use as time-series data. Must be either a long value, such as the timestamp in miliseconds, or a string value in “yyyy-MM-dd HH:mm:ss”.\nYes Syntax: Batch RCF for Non-time-series Data Command ad &lt; shingle_size &gt; &lt; time_decay &gt; Field Description Required shingle_size A consecutive sequence of the most recent records. The default value is 8.\nNo time_decay Specifies how much of the recent past to consider when computing an anomaly score. The default value is 0.001.\nNo Example 1: Detecting events in New York City from taxi ridership data with time-series data The example trains a RCF model and use the model to detect anomalies in the time-series ridership data.\nPPL query: os &gt; source = nyc_taxi | fields value, timestamp | AD time_field = 'timestamp' | where value = 10844. 0 value timestamp score anomaly_grade 10844.0\n1404172800000\n0.0\n0.0 Example 2: Detecting events in New York City from taxi ridership data with non-time-series data PPL query: os &gt; source = nyc_taxi | fields value | AD | where value = 10844. 0 value score anomalous    \n10844.0\n0.0\nfalse kmeans\nThe kmeans command applies the ML Commons plugin’s kmeans algorithm to the provided PPL command’s search results.\nSyntax kmeans &lt; cluster - number &gt; For cluster-number, enter the number of clusters you want to group your data points into. Example: Group Iris data The example shows how to classify three Iris species (Iris setosa, Iris virginica and Iris versicolor) based on the combination of four features measured from each sample: the length and the width of the sepals and petals.\nPPL query: os &gt; source = iris_data | fields sepal_length_in_cm, sepal_width_in_cm, petal_length_in_cm, petal_width_in_cm | kmeans 3 sepal_length_in_cm sepal_width_in_cm petal_length_in_cm petal_width_in_cm ClusterID    \n5.1\n3.5\n1.4\n0.2\n1\n \n5.6\n3.0\n4.1\n1.3\n0\n \n6.7\n2.5\n5.8\n1.8\n2",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "PPL &ndash; Piped Processing Language"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/ppl/index/",
    "title": "PPL &ndash; Piped Processing Language",
    "content": "Piped Processing Language (PPL) is a query language that lets you use pipe ( |) syntax to explore, discover, and query data stored in OpenSearch.\nTo quickly get up and running with PPL, use Query Workbench in OpenSearch Dashboards. To learn more, see Workbench.\nThe PPL syntax consists of commands delimited by the pipe character ( |) where data flows from left to right through each pipeline. search command | command 1 | command 2... You can only use read-only commands like search, where, fields, rename, dedup, stats, sort, eval, head, top, and rare.\nQuick start\nTo get started with PPL, choose Dev Tools in OpenSearch Dashboards and use the bulk operation to index some sample data: PUT accounts/_bulk?refresh { \"index\":{ \"_id\": \"1\" }} { \"account_number\": 1, \"balance\": 39225, \"firstname\": \"Amber\", \"lastname\": \"Duke\", \"age\": 32, \"gender\": \"M\", \"address\": \"880 Holmes Lane\", \"employer\": \"Pyrami\", \"email\": \"amberduke@pyrami.com\", \"city\": \"Brogan\", \"state\": \"IL\" } { \"index\":{ \"_id\": \"6\" }} { \"account_number\": 6, \"balance\": 5686, \"firstname\": \"Hattie\", \"lastname\": \"Bond\", \"age\": 36, \"gender\": \"M\", \"address\": \"671 Bristol Street\", \"employer\": \"Netagy\", \"email\": \"hattiebond@netagy.com\", \"city\": \"Dante\", \"state\": \"TN\" } { \"index\":{ \"_id\": \"13\" }} { \"account_number\": 13, \"balance\": 32838, \"firstname\": \"Nanette\", \"lastname\": \"Bates\", \"age\": 28, \"gender\": \"F\", \"address\": \"789 Madison Street\", \"employer\": \"Quility\", \"city\": \"Nogal\", \"state\": \"VA\" } { \"index\":{ \"_id\": \"18\" }} { \"account_number\": 18, \"balance\": 4180, \"firstname\": \"Dale\", \"lastname\": \"Adams\", \"age\": 33, \"gender\": \"M\", \"address\": \"467 Hutchinson Court\", \"email\": \"daleadams@boink.com\", \"city\": \"Orick\", \"state\": \"MD\" } Go to Query Workbench and select PPL.\nThe following example returns firstname and lastname fields for documents in an accounts index with age greater than 18: search source = accounts | where age &gt; 18 | fields firstname, lastname Sample Response firstname lastname Amber\nDuke\nHattie\nBond\nNanette\nBates\nDale\nAdams",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/ppl/syntax/",
    "title": "Syntax",
    "content": "Every PPL query starts with the search command. It specifies the index to search and retrieve documents from. Subsequent commands can follow in any order.\nCurrently, PPL supports only one search command, which can be omitted to simplify the query.\n{:.note}\nSyntax search source =&lt; index &gt; [ boolean - expression] source =&lt; index &gt; [ boolean - expression] Field Description Required search Specifies search keywords.\nYes index Specifies which index to query from.\nNo bool-expression Specifies an expression that evaluates to a Boolean value.\nNo Examples Example 1: Search through accounts index In the following example, the search command refers to an accounts index as the source and uses fields and where commands for the conditions: search source = accounts | where age &gt; 18 | fields firstname, lastname In the following examples, angle brackets &lt; &gt; enclose required arguments and square brackets [] enclose optional arguments. Example 2: Get all documents To get all documents from the accounts index, specify it as the source: search source = accounts; account_number firstname address balance gender city employer state age email lastname 1\nAmber\n880 Holmes Lane\n39225\nM\nBrogan\nPyrami\nIL\n32\namberduke@pyrami.com\nDuke\n6\nHattie\n671 Bristol Street\n5686\nM\nDante\nNetagy\nTN\n36\nhattiebond@netagy.com\nBond\n13\nNanette\n789 Madison Street\n32838\nF\nNogal\nQuility\nVA\n28\nnull\nBates\n18\nDale\n467 Hutchinson Court\n4180\nM\nOrick\nnull\nMD\n33\ndaleadams@boink.com\nAdams Example 3: Get documents that match a condition To get all documents from the accounts index that either have account_number equal to 1 or have gender as F, use the following query: search source = accounts account_number = 1 or gender = \\ \"F \\\"; account_number firstname address balance gender city employer state age email lastname 1\nAmber\n880 Holmes Lane\n39225\nM\nBrogan\nPyrami\nIL\n32\namberduke@pyrami.com\nDuke\n13\nNanette\n789 Madison Street\n32838\nF\nNogal\nQuility\nVA\n28\nnull\nBates",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "PPL &ndash; Piped Processing Language"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/response-formats/",
    "title": "Response formats",
    "content": "The SQL plugin provides the jdbc, csv, raw, and json response formats that are useful for different purposes. The jdbc format is widely used because it provides the schema information and adds more functionality, such as pagination. Besides the JDBC driver, various clients can benefit from a detailed and well-formatted response.\nJDBC format\nBy default, the SQL plugin returns the response in the standard JDBC format. This format is provided for the JDBC driver and clients that need both the schema and the result set to be well formatted.\nExample request\nThe following query does not specify the response format, so the format is set to jdbc: POST _plugins/_sql { \"query\": \"SELECT firstname, lastname, age FROM accounts ORDER BY age LIMIT 2\" } Example response\nIn the response, the schema contains the field names and types, and the datarows field contains the result set: { \"schema\": [{ \"name\": \"firstname\", \"type\": \"text\" }, { \"name\": \"lastname\", \"type\": \"text\" }, { \"name\": \"age\", \"type\": \"long\" }], \"total\": 4, \"datarows\": [ [ \"Nanette\", \"Bates\", 28], [ \"Amber\", \"Duke\", 32]], \"size\": 2, \"status\": 200 } If an error of any type occurs, OpenSearch returns the error message.\nThe following query searches for a non-existent field unknown: POST /_plugins/_sql { \"query\": \"SELECT unknown FROM accounts\" } The response contains the error message and the cause of the error: { \"error\": { \"reason\": \"Invalid SQL query\", \"details\": \"Field [unknown] cannot be found or used here.\", \"type\": \"SemanticAnalysisException\" }, \"status\": 400 } OpenSearch DSL JSON format\nIf you set the format to json, the original OpenSearch response is returned in JSON format. Because this is the native response from OpenSearch, extra effort is needed to parse and interpret it.\nExample request\nThe following query sets the response format to json: POST _plugins/_sql?format=json { \"query\": \"SELECT firstname, lastname, age FROM accounts ORDER BY age LIMIT 2\" } Example response\nThe response is the original response from OpenSearch: { \"_shards\": { \"total\": 5, \"failed\": 0, \"successful\": 5, \"skipped\": 0 }, \"hits\": { \"hits\": [{ \"_index\": \"accounts\", \"_type\": \"account\", \"_source\": { \"firstname\": \"Nanette\", \"age\": 28, \"lastname\": \"Bates\" }, \"_id\": \"13\", \"sort\": [ 28], \"_score\": null }, { \"_index\": \"accounts\", \"_type\": \"account\", \"_source\": { \"firstname\": \"Amber\", \"age\": 32, \"lastname\": \"Duke\" }, \"_id\": \"1\", \"sort\": [ 32], \"_score\": null }], \"total\": { \"value\": 4, \"relation\": \"eq\" }, \"max_score\": null }, \"took\": 100, \"timed_out\": false } CSV format\nYou can also specify to return results in CSV format.\nExample request POST /_plugins/_sql?format=csv { \"query\": \"SELECT firstname, lastname, age FROM accounts ORDER BY age\" } Example response firstname,lastname,age\nNanette,Bates,28\nAmber,Duke,32\nDale,Adams,33\nHattie,Bond,36 Sanitizing results in CSV format\nBy default, OpenSearch sanitizes header cells (field names) and data cells (field contents) according to the following rules:\nIf a cell starts with +, -, =, or @, the sanitizer inserts a single quote ( ') at the start of the cell.\nIf a cell contains one or more commas (,), the sanitizer surrounds the cell with double quotes ( \").\nExample\nThe following query indexes a document with cells that either start with special characters or contain commas: PUT /userdata/_doc/ 1?refresh= true { \"+firstname\": \"-Hattie\", \"=lastname\": \"@Bond\", \"address\": \"671 Bristol Street, Dente, TN\" } You can use the query below to request results in CSV format: POST /_plugins/_sql?format=csv { \"query\": \"SELECT * FROM userdata\" } In the response, cells that start with special characters are prefixed with '. The cell that has commas is surrounded with quotation marks: '+firstname,'=lastname,address\n'Hattie,'@Bond,\"671 Bristol Street, Dente, TN\" To skip sanitizing, set the sanitize query parameter to false: POST /_plugins/_sql?format=csvandsanitize= false { \"query\": \"SELECT * FROM userdata\" } The response contains the results in the original CSV format: =lastname,address,+firstname\n@Bond,\"671 Bristol Street, Dente, TN\",-Hattie Raw format\nYou can use the raw format to pipe the results to other command line tools for post-processing.\nExample request POST /_plugins/_sql?format=raw { \"query\": \"SELECT firstname, lastname, age FROM accounts ORDER BY age\" } Example response Nanette|Bates|28\nAmber|Duke|32\nDale|Adams|33\nHattie|Bond|36 By default, OpenSearch sanitizes results in raw format according to the following rule:\nIf a data cell contains one or more pipe characters ( |), the sanitizer surrounds the cell with double quotes.\nExample\nThe following query indexes a document with pipe characters ( |) in its fields: PUT /userdata/_doc/ 1?refresh= true { \"+firstname\": \"|Hattie\", \"=lastname\": \"Bond|\", \"|address\": \"671 Bristol Street| Dente| TN\" } You can use the query below to request results in raw format: POST /_plugins/_sql?format=raw { \"query\": \"SELECT * FROM userdata\" } The query returns cells with the | character surrounded by quotation marks: \"|address\"|=lastname|+firstname\n\"671 Bristol Street| Dente| TN\"|\"Bond|\"|\"|Hattie\"",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/settings/",
    "title": "Settings",
    "content": "The SQL plugin adds a few settings to the standard OpenSearch cluster settings. Most are dynamic, so you can change the default behavior of the plugin without restarting your cluster.\nIt is possible to independently disable processing of PPL or SQL queries.\nYou can update these settings like any other cluster setting: PUT _cluster/settings { \"transient\": { \"plugins.sql.enabled\": false } } Alternatively, you can use the following request format: PUT _cluster/settings { \"transient\": { \"plugins\": { \"ppl\": { \"enabled\": \"false\" } } } } Similarly, you can update the settings by sending a request to the _plugins/_query/settings endpoint: PUT _plugins/_query/settings { \"transient\": { \"plugins.sql.enabled\": false } } Alternatively, you can use the following request format: PUT _plugins/_query/settings { \"transient\": { \"plugins\": { \"ppl\": { \"enabled\": \"false\" } } } } Requests to the _plugins/_ppl and _plugins/_sql endpoints include index names in the request body, so they have the same access policy considerations as the bulk, mget, and msearch operations. Setting the rest.action.multi.allow_explicit_index parameter to false disables both the SQL and PPL endpoints. Setting Default Description plugins.sql.enabled True\nChange to false to disable the SQL support in the plugin. plugins.ppl.enabled True\nChange to false to disable the PPL support in the plugin. plugins.sql.slowlog 2 seconds\nConfigures the time limit (in seconds) for slow queries. The plugin logs slow queries as Slow query: elapsed=xxx (ms) in opensearch.log. plugins.sql.cursor.keep_alive 1 minute\nConfigures how long the cursor context is kept open. Cursor contexts are resource resource intensive, so we recommend a low value. plugins.query.memory_limit 85%\nConfigures the heap memory usage limit for the circuit breaker of the query engine. plugins.query.size_limit 200\nSets the default size of index that the query engine fetches from OpenSearch.",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql-ppl-api/",
    "title": "SQL/PPL API",
    "content": "Use the SQL and PPL API to send queries to the SQL plugin. Use the _sql endpoint to send queries in SQL, and the _ppl endpoint to send queries in PPL. For both of these, you can also use the _explain endpoint to translate your query into OpenSearch domain-specific language (DSL) or to troubleshoot errors.\nTable of contents Query API Query parameters Request fields Response fields Explain API Paginating results Example Filtering results Using parameters Query API\nIntroduced 1.0\nSends an SQL/PPL query to the SQL plugin. You can pass the format for the response as a query parameter.\nQuery parameters Parameter Data Type Description format String\nThe format for the response. The _sql endpoint supports jdbc, csv, raw, and json formats. The _ppl endpoint supports jdbc, csv, and raw formats. Default is jdbc.\nsanitize\nBoolean\nSpecifies whether to escape special characters in the results. See Response formats for more information. Default is true. Request fields Field Data Type Description query\nString\nThe query to be executed. Required. filter JSON object\nThe filter for the results. Optional. fetch_size integer\nThe number of results to return in one response. Used for paginating results. Default is 1,000. Optional. Only supported for the jdbc response format. Example request POST /_plugins/_sql { \"query\": \"SELECT * FROM accounts\" } Example response\nThe response contains the schema and the results: { \"schema\": [ { \"name\": \"account_number\", \"type\": \"long\" }, { \"name\": \"firstname\", \"type\": \"text\" }, { \"name\": \"address\", \"type\": \"text\" }, { \"name\": \"balance\", \"type\": \"long\" }, { \"name\": \"gender\", \"type\": \"text\" }, { \"name\": \"city\", \"type\": \"text\" }, { \"name\": \"employer\", \"type\": \"text\" }, { \"name\": \"state\", \"type\": \"text\" }, { \"name\": \"age\", \"type\": \"long\" }, { \"name\": \"email\", \"type\": \"text\" }, { \"name\": \"lastname\", \"type\": \"text\" }], \"datarows\": [ [ 1, \"Amber\", \"880 Holmes Lane\", 39225, \"M\", \"Brogan\", \"Pyrami\", \"IL\", 32, \"amberduke@pyrami.com\", \"Duke\"], [ 6, \"Hattie\", \"671 Bristol Street\", 5686, \"M\", \"Dante\", \"Netagy\", \"TN\", 36, \"hattiebond@netagy.com\", \"Bond\"], [ 13, \"Nanette\", \"789 Madison Street\", 32838, \"F\", \"Nogal\", \"Quility\", \"VA\", 28, \"nanettebates@quility.com\", \"Bates\"], [ 18, \"Dale\", \"467 Hutchinson Court\", 4180, \"M\", \"Orick\", null, \"MD\", 33, \"daleadams@boink.com\", \"Adams\"]], \"total\": 4, \"size\": 4, \"status\": 200 } Response fields Field Data Type Description schema\nArray\nSpecifies the field names and types for all fields.\ndata_rows\n2D array\nAn array of results. Each result represents one matching row (document).\ntotal\nInteger\nThe total number of rows (documents) in the index.\nsize\nInteger\nThe number of results to return in one response.\nstatus\nString\nThe HTTP response status OpenSearch returns after running the query. Explain API\nThe SQL plugin has an explain feature that shows how a query is executed against OpenSearch, which is useful for debugging and development. A POST request to the _plugins/_sql/_explain or _plugins/_ppl/_explain endpoint returns OpenSearch domain-specific language (DSL) in JSON format, explaining the query.\nYou can execute the explain API operation either in command line using curl or in the Dashboards console, like in the example below.\nSample explain request for an SQL query POST _plugins/_sql/_explain { \"query\": \"SELECT firstname, lastname FROM accounts WHERE age &gt; 20\" } Sample SQL query explain response { \"root\": { \"name\": \"ProjectOperator\", \"description\": { \"fields\": \"[firstname, lastname]\" }, \"children\": [ { \"name\": \"OpenSearchIndexScan\", \"description\": { \"request\": \"\"\"OpenSearchQueryRequest(indexName=accounts, sourceBuilder={\" from \":0,\" size \":200,\" timeout \":\" 1 m \",\" query \":{\" range \":{\" age \":{\" from \":20,\" to \":null,\" include_lower \":false,\" include_upper \":true,\" boost \":1.0}}},\" _source \":{\" includes \":[\" firstname \",\" lastname \"],\" excludes \":[]},\" sort \":[{\" _doc \":{\" order \":\" asc \"}}]}, searchDone=false)\"\"\" }, \"children\": [] }] } } Sample explain request for a PPL query POST _plugins/_ppl/_explain { \"query\": \"source=accounts | fields firstname, lastname\" } Sample PPL query explain response { \"root\": { \"name\": \"ProjectOperator\", \"description\": { \"fields\": \"[firstname, lastname]\" }, \"children\": [ { \"name\": \"OpenSearchIndexScan\", \"description\": { \"request\": \"\"\"OpenSearchQueryRequest(indexName=accounts, sourceBuilder={\" from \":0,\" size \":200,\" timeout \":\" 1 m \",\" _source \":{\" includes \":[\" firstname \",\" lastname \"],\" excludes \":[]}}, searchDone=false)\"\"\" }, \"children\": [] }] } } For queries that require post-processing, the explain response includes a query plan in addition to the OpenSearch DSL. For those queries that don’t require post processing, you can see a complete DSL.\nPaginating results\nTo get back a paginated response, use the fetch_size parameter. The value of fetch_size should be greater than 0. The default value is 1,000. A value of 0 will fall back to a non-paginated response.\nThe fetch_size parameter is only supported for the jdbc response format.\nExample\nThe following request contains an SQL query and specifies to return five results at a time: POST _plugins/_sql/ { \"fetch_size\": 5, \"query\": \"SELECT firstname, lastname FROM accounts WHERE age &gt; 20 ORDER BY state ASC\" } The response contains all the fields that a query without fetch_size would contain, and a cursor field that is used to retrieve subsequent pages of results: { \"schema\": [ { \"name\": \"firstname\", \"type\": \"text\" }, { \"name\": \"lastname\", \"type\": \"text\" }], \"cursor\": \"d:eyJhIjp7fSwicyI6IkRYRjFaWEo1UVc1a1JtVjBZMmdCQUFBQUFBQUFBQU1XZWpkdFRFRkZUMlpTZEZkeFdsWnJkRlZoYnpaeVVRPT0iLCJjIjpbeyJuYW1lIjoiZmlyc3RuYW1lIiwidHlwZSI6InRleHQifSx7Im5hbWUiOiJsYXN0bmFtZSIsInR5cGUiOiJ0ZXh0In1dLCJmIjo1LCJpIjoiYWNjb3VudHMiLCJsIjo5NTF9\", \"total\": 956, \"datarows\": [ [ \"Cherry\", \"Carey\"], [ \"Lindsey\", \"Hawkins\"], [ \"Sargent\", \"Powers\"], [ \"Campos\", \"Olsen\"], [ \"Savannah\", \"Kirby\"]], \"size\": 5, \"status\": 200 } To fetch subsequent pages, use the cursor from the previous response: POST /_plugins/_sql { \"cursor\": \"d:eyJhIjp7fSwicyI6IkRYRjFaWEo1UVc1a1JtVjBZMmdCQUFBQUFBQUFBQU1XZWpkdFRFRkZUMlpTZEZkeFdsWnJkRlZoYnpaeVVRPT0iLCJjIjpbeyJuYW1lIjoiZmlyc3RuYW1lIiwidHlwZSI6InRleHQifSx7Im5hbWUiOiJsYXN0bmFtZSIsInR5cGUiOiJ0ZXh0In1dLCJmIjo1LCJpIjoiYWNjb3VudHMiLCJsIjo5NTF9\" } The next response contains only the datarows of the results and a new cursor. { \"cursor\": \"d:eyJhIjp7fSwicyI6IkRYRjFaWEo1UVc1a1JtVjBZMmdCQUFBQUFBQUFBQU1XZWpkdFRFRkZUMlpTZEZkeFdsWnJkRlZoYnpaeVVRPT0iLCJjIjpbeyJuYW1lIjoiZmlyc3RuYW1lIiwidHlwZSI6InRleHQifSx7Im5hbWUiOiJsYXN0bmFtZSIsInR5cGUiOiJ0ZXh0In1dLCJmIjo1LCJpIjoiYWNjb3VudHMabcde12345\", \"datarows\": [ [ \"Abbey\", \"Karen\"], [ \"Chen\", \"Ken\"], [ \"Ani\", \"Jade\"], [ \"Peng\", \"Hu\"], [ \"John\", \"Doe\"]] } The datarows can have more than the fetch_size number of records in case nested fields are flattened.\nThe last page of results has only datarows and no cursor. The cursor context is automatically cleared on the last page.\nTo explicitly clear the cursor context, use the _plugins/_sql/close endpoint operation: POST /_plugins/_sql/close { \"cursor\": \"d:eyJhIjp7fSwicyI6IkRYRjFaWEo1UVc1a1JtVjBZMmdCQUFBQUFBQUFBQU1XZWpkdFRFRkZUMlpTZEZkeFdsWnJkRlZoYnpaeVVRPT0iLCJjIjpbeyJuYW1lIjoiZmlyc3RuYW1lIiwidHlwZSI6InRleHQifSx7Im5hbWUiOiJsYXN0bmFtZSIsInR5cGUiOiJ0ZXh0In1dLCJmIjo1LCJpIjoiYWNjb3VudHMiLCJsIjo5NTF9\" } ' The response is an acknowledgement from OpenSearch: { \"succeeded\": true } Filtering results\nYou can use the filter parameter to add more conditions to the OpenSearch DSL directly.\nThe following SQL query returns the names and account balances of all customers. The results are then filtered to contain only those customers with less than $10,000 balance. POST /_plugins/_sql/ { \"query\": \"SELECT firstname, lastname, balance FROM accounts\", \"filter\": { \"range\": { \"balance\": { \"lt\": 10000 } } } } The response contains the matching results: { \"schema\": [ { \"name\": \"firstname\", \"type\": \"text\" }, { \"name\": \"lastname\", \"type\": \"text\" }, { \"name\": \"balance\", \"type\": \"long\" }], \"total\": 2, \"datarows\": [ [ \"Hattie\", \"Bond\", 5686], [ \"Dale\", \"Adams\", 4180]], \"size\": 2, \"status\": 200 } You can use the Explain API to see how this query is executed against OpenSearch: POST /_plugins/_sql/_explain { \"query\": \"SELECT firstname, lastname, balance FROM accounts\", \"filter\": { \"range\": { \"balance\": { \"lt\": 10000 } } } } ' The response contains the Boolean query in OpenSearch DSL that corresponds to the query above: { \"from\": 0, \"size\": 200, \"query\": { \"bool\": { \"filter\": [{ \"bool\": { \"filter\": [{ \"range\": { \"balance\": { \"from\": null, \"to\": 10000, \"include_lower\": true, \"include_upper\": false, \"boost\": 1.0 } } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"_source\": { \"includes\": [ \"firstname\", \"lastname\", \"balance\"], \"excludes\": [] } } Using parameters\nYou can use the parameters field to pass parameter values to a prepared SQL query.\nThe following explain operation uses an SQL query with an age parameter: POST /_plugins/_sql/_explain { \"query\": \"SELECT * FROM accounts WHERE age =?\", \"parameters\": [{ \"type\": \"integer\", \"value\": 30 }] } The response contains the Boolean query in OpenSearch DSL that corresponds to the SQL query above: { \"from\": 0, \"size\": 200, \"query\": { \"bool\": { \"filter\": [{ \"bool\": { \"must\": [{ \"term\": { \"age\": { \"value\": 30, \"boost\": 1.0 } } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } } }",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql/aggregations/",
    "title": "Aggregate Functions",
    "content": "Aggregate functions operate on subsets defined by the GROUP BY clause. In the absence of a GROUP BY clause, aggregate functions operate on all elements of the result set. You can use aggregate functions in the GROUP BY, SELECT, and HAVING clauses.\nOpenSearch supports the following aggregate functions. Function Description AVG Returns the average of the results. COUNT Returns the number of results. SUM Returns the sum of the results. MIN Returns the minimum of the results. MAX Returns the maximum of the results. VAR_POP or VARIANCE Returns the population variance of the results after discarding nulls. Returns 0 when there is only one row of results. VAR_SAMP Returns the sample variance of the results after discarding nulls. Returns null when there is only one row of results. STD or STDDEV Returns the sample standard deviation of the results. Returns 0 when there is only one row of results. STDDEV_POP Returns the population standard deviation of the results. Returns 0 when there is only one row of results. STDDEV_SAMP Returns the sample standard deviation of the results. Returns null when there is only one row of results. The examples below reference an employees table. You can try out the examples by indexing the following documents into OpenSearch using the bulk index operation: PUT employees/_bulk?refresh { \"index\":{ \"_id\": \"1\" }} { \"employee_id\": 1, \"department\": 1, \"firstname\": \"Amber\", \"lastname\": \"Duke\", \"sales\": 1356, \"sale_date\": \"2020-01-23\" } { \"index\":{ \"_id\": \"2\" }} { \"employee_id\": 1, \"department\": 1, \"firstname\": \"Amber\", \"lastname\": \"Duke\", \"sales\": 39224, \"sale_date\": \"2021-01-06\" } { \"index\":{ \"_id\": \"6\" }} { \"employee_id\": 6, \"department\": 1, \"firstname\": \"Hattie\", \"lastname\": \"Bond\", \"sales\": 5686, \"sale_date\": \"2021-06-07\" } { \"index\":{ \"_id\": \"7\" }} { \"employee_id\": 6, \"department\": 1, \"firstname\": \"Hattie\", \"lastname\": \"Bond\", \"sales\": 12432, \"sale_date\": \"2022-05-18\" } { \"index\":{ \"_id\": \"13\" }} { \"employee_id\": 13, \"department\": 2, \"firstname\": \"Nanette\", \"lastname\": \"Bates\", \"sales\": 32838, \"sale_date\": \"2022-04-11\" } { \"index\":{ \"_id\": \"18\" }} { \"employee_id\": 18, \"department\": 2, \"firstname\": \"Dale\", \"lastname\": \"Adams\", \"sales\": 4180, \"sale_date\": \"2022-11-05\" } GROUP BY\nThe GROUP BY clause defines subsets of a result set. Aggregate functions operate on these subsets and return one result row for each subset.\nYou can use an identifier, ordinal, or expression in the GROUP BY clause.\nUsing an identifier in GROUP BY\nYou can specify the field name (column name) to aggregate on in the GROUP BY clause. For example, the following query returns the department numbers and the total sales for each department: SELECT department, sum ( sales) FROM employees GROUP BY department; department sum(sales) 1\n58700\n2\n37018 Using an ordinal in GROUP BY\nYou can specify the column number to aggregate on in the GROUP BY clause. The column number is determined by the column position in the SELECT clause. For example, the following query is equivalent to the query above. It returns the department numbers and the total sales for each department. It groups the results by the first column of the result set, which is department: SELECT department, sum ( sales) FROM employees GROUP BY 1; department sum(sales) 1\n58700\n2\n37018 Using an expression in GROUP BY\nYou can use an expression in the GROUP BY clause. For example, the following query returns the average sales for each year: SELECT year ( sale_date), avg ( sales) FROM employees GROUP BY year ( sale_date); year(start_date) avg(sales) 2020\n1356.0\n2021\n22455.0\n2022\n16484.0 SELECT\nYou can use aggregate expressions in the SELECT clause either directly or as part of a larger expression. In addition, you can use expressions as arguments of aggregate functions.\nUsing aggregate expressions directly in SELECT\nThe following query returns the average sales for each department: SELECT department, avg ( sales) FROM employees GROUP BY department; department avg(sales) 1\n14675.0\n2\n18509.0 Using aggregate expressions as part of larger expressions in SELECT\nThe following query calculates the average commission for the employees of each department as 5% of the average sales: SELECT department, avg ( sales) * 0. 05 as avg_commission FROM employees GROUP BY department; department avg_commission 1\n733.75\n2\n925.45 Using expressions as arguments to aggregate functions\nThe following query calculates the average commission amount for each department. First it calculates the commission amount for each sales value as 5% of the sales. Then it determines the average of all commission values: SELECT department, avg ( sales * 0. 05) as avg_commission FROM employees GROUP BY department; department avg_commission 1\n733.75\n2\n925.45 COUNT\nThe COUNT function accepts arguments, such as *, or literals, such as 1.\nThe following table describes how various forms of the COUNT function operate. Function type\nDescription COUNT(field) Counts the number of rows where the value of the given field (or expression) is not null. COUNT(*) Counts the total number of rows in a table. COUNT(1) (same as COUNT(*))\nCounts any non-null literal. For example, the following query returns the count of sales for each year: SELECT year ( sale_date), count ( sales) FROM employees GROUP BY year ( sale_date); year(sale_date) count(sales) 2020\n1\n2021\n2\n2022\n3 HAVING\nBoth WHERE and HAVING are used to filter results. The WHERE filter is applied before the GROUP BY phase, so you cannot use aggregate functions in a WHERE clause. However, you can use the WHERE clause to limit the rows to which the aggregate is then applied.\nThe HAVING filter is applied after the GROUP BY phase, so you can use the HAVING clause to limit the groups that are included in the results.\nHAVING with GROUP BY\nYou can use aggregate expressions or their aliases defined in a SELECT clause in a HAVING condition.\nThe following query uses an aggregate expression in the HAVING clause. It returns the number of sales for each employee who made more than one sale: SELECT employee_id, count ( sales) FROM employees GROUP BY employee_id HAVING count ( sales) &gt; 1; employee_id count(sales) 1\n2\n6\n2 The aggregations in a HAVING clause do not have to be the same as the aggregations in a SELECT list. The following query uses the count function in the HAVING clause but the sum function in the SELECT clause. It returns the total sales amount for each employee who made more than one sale: SELECT employee_id, sum ( sales) FROM employees GROUP BY employee_id HAVING count ( sales) &gt; 1; employee_id sum (sales) 1\n40580\n6\n18120 As an extension of the SQL standard, you are not restricted to using only identifiers in the GROUP BY clause. The following query uses an alias in the GROUP BY clause and is equivalent to the previous query: SELECT employee_id as id, sum ( sales) FROM employees GROUP BY id HAVING count ( sales) &gt; 1; id sum (sales) 1\n40580\n6\n18120 You can also use an alias for an aggregate expression in the HAVING clause. The following query returns the total sales for each department where sales exceed $40,000: SELECT department, sum ( sales) as total FROM employees GROUP BY department HAVING total &gt; 40000; department total 1\n58700 If an identifier is ambiguous (for example, present both as a SELECT alias and as an index field), the preference is given to the alias. In the following query the identifier is replaced with the expression aliased in the SELECT clause: SELECT department, sum ( sales) as sales FROM employees GROUP BY department HAVING sales &gt; 40000; department sales 1\n58700 HAVING without GROUP BY\nYou can use a HAVING clause without a GROUP BY clause. In this case, the whole set of data is to be considered one group. The following query will return True if there is more than one value in the department column: SELECT 'True' as more_than_one_department FROM employees HAVING min ( department) &lt; max ( department); more_than_one_department True If all employees in the employee table belonged to the same department, the result would contain zero rows: more_than_one_department  ",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "SQL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql/basic/",
    "title": "Basic Queries",
    "content": "Use the SELECT clause, along with FROM, WHERE, GROUP BY, HAVING, ORDER BY, and LIMIT to search and aggregate data.\nAmong these clauses, SELECT and FROM are required, as they specify which fields to retrieve and which indices to retrieve them from. All other clauses are optional. Use them according to your needs.\nSyntax\nThe complete syntax for searching and aggregating data is as follows: SELECT [ DISTINCT] ( * | expression) [[ AS] alias] [,...] FROM index_name [ WHERE predicates] [ GROUP BY expression [,...] [ HAVING predicates]] [ ORDER BY expression [ IS [ NOT] NULL] [ ASC | DESC] [,...]] [ LIMIT [ offset,] size] Fundamentals\nApart from the predefined keywords of SQL, the most basic elements are literal and identifiers.\nA literal is a numeric, string, date or boolean constant. An identifier is an OpenSearch index or field name.\nWith arithmetic operators and SQL functions, use literals and identifiers to build complex expressions.\nRule expressionAtom: The expression in turn can be combined into a predicate with logical operator. Use a predicate in the WHERE and HAVING clause to filter out data by specific conditions.\nRule expression: Rule predicate: Execution Order\nThese SQL clauses execute in an order different from how they appear: FROM index WHERE predicates GROUP BY expressions HAVING predicates SELECT expressions ORDER BY expressions LIMIT size Select\nSpecify the fields to be retrieved.\nSyntax\nRule selectElements: Rule selectElement: Example 1: Use * to retrieve all fields in an index: SELECT * FROM accounts account_number firstname gender city balance employer state email address lastname age 1\nAmber\nM\nBrogan\n39225\nPyrami\nIL\namberduke@pyrami.com\n880 Holmes Lane\nDuke\n32\n16\nHattie\nM\nDante\n5686\nNetagy\nTN\nhattiebond@netagy.com\n671 Bristol Street\nBond\n36\n13\nNanette\nF\nNogal\n32838\nQuility\nVA\nnanettebates@quility.com\n789 Madison Street\nBates\n28\n18\nDale\nM\nOrick\n4180\n \nMD\ndaleadams@boink.com\n467 Hutchinson Court\nAdams\n33 Example 2: Use field name(s) to retrieve only specific fields: SELECT firstname, lastname FROM accounts firstname lastname Amber\nDuke\nHattie\nBond\nNanette\nBates\nDale\nAdams Example 3: Use field aliases instead of field names. Field aliases are used to make field names more readable: SELECT account_number AS num FROM accounts | num:—\n| 1\n| 6\n| 13\n| 18 Example 4: Use the DISTINCT clause to get back only unique field values. You can specify one or more field names: SELECT DISTINCT age FROM accounts | age:—\n| 28\n| 32\n| 33\n| 36\nFrom\nSpecify the index that you want search.\nYou can specify subqueries within the FROM clause.\nSyntax\nRule tableName: Example 1: Use index aliases to query across indexes. To learn about index aliases, see Index Alias.\nIn this sample query, acc is an alias for the accounts index: SELECT account_number, accounts. age FROM accounts or SELECT account_number, acc. age FROM accounts acc account_number age 1\n32\n6\n36\n13\n28\n18\n33 Example 2: Use index patterns to query indices that match a specific pattern: SELECT account_number FROM account * | account_number:—\n| 1\n| 6\n| 13\n| 18\nWhere\nSpecify a condition to filter the results. Operators Behavior = Equal to. &lt;&gt; Not equal to. &gt; Greater than. &lt; Less than. &gt;= Greater than or equal to. &lt;= Less than or equal to. IN Specify multiple OR operators. BETWEEN Similar to a range query. For more information about range queries, see Range query. LIKE Use for full-text search. For more information about full-text queries, see Full-text queries. IS NULL Check if the field value is NULL. IS NOT NULL Check if the field value is NOT NULL. Combine comparison operators ( =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=) with boolean operators NOT, AND, or OR to build more complex expressions. Example 1: Use comparison operators for numbers, strings, or dates: SELECT account_number FROM accounts WHERE account_number = 1 account_number 1 Example 2: OpenSearch allows for flexible schema， so documents in an index may have different fields. Use IS NULL or IS NOT NULL to retrieve only missing fields or existing fields. We do not differentiate between missing fields and fields explicitly set to NULL: SELECT account_number, employer FROM accounts WHERE employer IS NULL account_number employer 18\n  Example 3: Deletes a document that satisfies the predicates in the WHERE clause: DELETE FROM accounts WHERE age &gt; 30 Group By\nGroup documents with the same field value into buckets. Example 1: Group by fields: SELECT age FROM accounts GROUP BY age id age 0\n28\n1\n32\n2\n33\n3\n36 Example 2: Group by field alias: SELECT account_number AS num FROM accounts GROUP BY num id num 0\n1\n1\n6\n2\n13\n3\n18 Example 4: Use scalar functions in the GROUP BY clause: SELECT ABS ( age) AS a FROM accounts GROUP BY ABS ( age) id a 0\n28.0\n1\n32.0\n2\n33.0\n3\n36.0 Having\nUse the HAVING clause to aggregate inside each bucket based on aggregation functions ( COUNT, AVG, SUM, MIN, and MAX).\nThe HAVING clause filters results from the GROUP BY clause: Example 1: SELECT age, MAX ( balance) FROM accounts GROUP BY age HAVING MIN ( balance) &gt; 10000 id age MAX (balance) 0\n28\n32838\n1\n32\n39225 Order By\nUse the ORDER BY clause to sort results into your desired order. Example 1: Use ORDER BY to sort by ascending or descending order. Besides regular field names, using ordinal, alias, or scalar functions are supported: SELECT account_number FROM accounts ORDER BY account_number DESC account_number 18\n13\n6\n1 Example 2: Specify if documents with missing fields are to be put at the beginning or at the end of the results. The default behavior of OpenSearch is to return nulls or missing fields at the end. To push them before non-nulls, use the IS NOT NULL operator: SELECT employer FROM accounts ORDER BY employer IS NOT NULL employer  \nNetagy\nPyrami\nQuility Limit\nSpecify the maximum number of documents that you want to retrieve. Used to prevent fetching large amounts of data into memory. Example 1: If you pass in a single argument, it’s mapped to the size parameter in OpenSearch and the from parameter is set to 0. SELECT account_number FROM accounts ORDER BY account_number LIMIT 1 account_number 1 Example 2: If you pass in two arguments, the first is mapped to the from parameter and the second to the size parameter in OpenSearch. You can use this for simple pagination for small indices, as it’s inefficient for large indices.\nUse ORDER BY to ensure the same order between pages: SELECT account_number FROM accounts ORDER BY account_number LIMIT 1, 1 account_number 6",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "SQL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql/complex/",
    "title": "Complex Queries",
    "content": "Besides simple SFW ( SELECT-FROM-WHERE) queries, the SQL plugin supports complex queries such as subquery, join, union, and minus. These queries operate on more than one OpenSearch index. To examine how these queries execute behind the scenes, use the explain operation.\nJoins\nOpenSearch SQL supports inner joins, cross joins, and left outer joins.\nConstraints\nJoins have a number of constraints:\nYou can only join two indices.\nYou must use aliases for indices (e.g. people p).\nWithin an ON clause, you can only use AND conditions.\nIn a WHERE statement, don’t combine trees that contain multiple indices. For example, the following statement works: WHERE (a.type1 &gt; 3 OR a.type1 &lt; 0) AND (b.type2 &gt; 4 OR b.type2 &lt; -1) The following statement does not: WHERE (a.type1 &gt; 3 OR b.type2 &lt; 0) AND (a.type1 &gt; 4 OR b.type2 &lt; -1) You can’t use GROUP BY or ORDER BY for results.\nLIMIT with OFFSET (e.g. LIMIT 25 OFFSET 25) is not supported.\nDescription\nThe JOIN clause combines columns from one or more indices using values common to each.\nSyntax\nRule tableSource: Rule joinPart: Example 1: Inner join\nInner join creates a new result set by combining columns of two indices based on your join predicates. It iterates the two indices and compares each document to find the ones that satisfy the join predicates. You can optionally precede the JOIN clause with an INNER keyword.\nThe join predicate(s) is specified by the ON clause.\nSQL query: SELECT a. account_number, a. firstname, a. lastname, e. id, e. name FROM accounts a JOIN employees_nested e ON a. account_number = e. id Explain:\nThe explain output is complicated, because a JOIN clause is associated with two OpenSearch DSL queries that execute in separate query planner frameworks. You can interpret it by examining the Physical Plan and Logical Plan objects. { \"Physical Plan\": { \"Project [ columns=[a.account_number, a.firstname, a.lastname, e.name, e.id]]\": { \"Top [ count=200]\": { \"BlockHashJoin[ conditions=( a.account_number = e.id), type=JOIN, blockSize=[FixedBlockSize with size=10000]]\": { \"Scroll [ employees_nested as e, pageSize=10000]\": { \"request\": { \"size\": 200, \"from\": 0, \"_source\": { \"excludes\": [], \"includes\": [ \"id\", \"name\"] } } }, \"Scroll [ accounts as a, pageSize=10000]\": { \"request\": { \"size\": 200, \"from\": 0, \"_source\": { \"excludes\": [], \"includes\": [ \"account_number\", \"firstname\", \"lastname\"] } } }, \"useTermsFilterOptimization\": false } } } }, \"description\": \"Hash Join algorithm builds hash table based on result of first query, and then probes hash table to find matched rows for each row returned by second query\", \"Logical Plan\": { \"Project [ columns=[a.account_number, a.firstname, a.lastname, e.name, e.id]]\": { \"Top [ count=200]\": { \"Join [ conditions=( a.account_number = e.id) type=JOIN]\": { \"Group\": [ { \"Project [ columns=[a.account_number, a.firstname, a.lastname]]\": { \"TableScan\": { \"tableAlias\": \"a\", \"tableName\": \"accounts\" } } }, { \"Project [ columns=[e.name, e.id]]\": { \"TableScan\": { \"tableAlias\": \"e\", \"tableName\": \"employees_nested\" } } }] } } } } } Result set: a.account_number a.firstname a.lastname e.id e.name 6\nHattie\nBond\n6\nJane Smith Example 2: Cross join\nCross join, also known as cartesian join, combines each document from the first index with each document from the second.\nThe result set is the the cartesian product of documents of both indices.\nThis operation is similar to the inner join without the ON clause that specifies the join condition.\nIt’s risky to perform cross join on two indices of large or even medium size. It might trigger a circuit breaker that terminates the query to avoid running out of memory.\nSQL query: SELECT a. account_number, a. firstname, a. lastname, e. id, e. name FROM accounts a JOIN employees_nested e Result set: a.account_number a.firstname a.lastname e.id e.name 1\nAmber\nDuke\n3\nBob Smith\n1\nAmber\nDuke\n4\nSusan Smith\n1\nAmber\nDuke\n6\nJane Smith\n6\nHattie\nBond\n3\nBob Smith\n6\nHattie\nBond\n4\nSusan Smith\n6\nHattie\nBond\n6\nJane Smith\n13\nNanette\nBates\n3\nBob Smith\n13\nNanette\nBates\n4\nSusan Smith\n13\nNanette\nBates\n6\nJane Smith\n18\nDale\nAdams\n3\nBob Smith\n18\nDale\nAdams\n4\nSusan Smith\n18\nDale\nAdams\n6\nJane Smith Example 3: Left outer join\nUse left outer join to retain rows from the first index if it does not satisfy the join predicate. The keyword OUTER is optional.\nSQL query: SELECT a. account_number, a. firstname, a. lastname, e. id, e. name FROM accounts a LEFT JOIN employees_nested e ON a. account_number = e. id Result set: a.account_number a.firstname a.lastname e.id e.name 1\nAmber\nDuke\nnull\nnull\n6\nHattie\nBond\n6\nJane Smith\n13\nNanette\nBates\nnull\nnull\n18\nDale\nAdams\nnull\nnull Subquery\nA subquery is a complete SELECT statement used within another statement and enclosed in parenthesis.\nFrom the explain output, you can see that some subqueries are actually transformed to an equivalent join query to execute.\nExample 1: Table subquery\nSQL query: SELECT a1. firstname, a1. lastname, a1. balance FROM accounts a1 WHERE a1. account_number IN ( SELECT a2. account_number FROM accounts a2 WHERE a2. balance &gt; 10000) Explain: { \"Physical Plan\": { \"Project [ columns=[a1.balance, a1.firstname, a1.lastname]]\": { \"Top [ count=200]\": { \"BlockHashJoin[ conditions=( a1.account_number = a2.account_number), type=JOIN, blockSize=[FixedBlockSize with size=10000]]\": { \"Scroll [ accounts as a2, pageSize=10000]\": { \"request\": { \"size\": 200, \"query\": { \"bool\": { \"filter\": [ { \"bool\": { \"adjust_pure_negative\": true, \"must\": [ { \"bool\": { \"adjust_pure_negative\": true, \"must\": [ { \"bool\": { \"adjust_pure_negative\": true, \"must_not\": [ { \"bool\": { \"adjust_pure_negative\": true, \"must_not\": [ { \"exists\": { \"field\": \"account_number\", \"boost\": 1 } }], \"boost\": 1 } }], \"boost\": 1 } }, { \"range\": { \"balance\": { \"include_lower\": false, \"include_upper\": true, \"from\": 10000, \"boost\": 1, \"to\": null } } }], \"boost\": 1 } }], \"boost\": 1 } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"from\": 0 } }, \"Scroll [ accounts as a1, pageSize=10000]\": { \"request\": { \"size\": 200, \"from\": 0, \"_source\": { \"excludes\": [], \"includes\": [ \"firstname\", \"lastname\", \"balance\", \"account_number\"] } } }, \"useTermsFilterOptimization\": false } } } }, \"description\": \"Hash Join algorithm builds hash table based on result of first query, and then probes hash table to find matched rows for each row returned by second query\", \"Logical Plan\": { \"Project [ columns=[a1.balance, a1.firstname, a1.lastname]]\": { \"Top [ count=200]\": { \"Join [ conditions=( a1.account_number = a2.account_number) type=JOIN]\": { \"Group\": [ { \"Project [ columns=[a1.balance, a1.firstname, a1.lastname, a1.account_number]]\": { \"TableScan\": { \"tableAlias\": \"a1\", \"tableName\": \"accounts\" } } }, { \"Project [ columns=[a2.account_number]]\": { \"Filter [ conditions=[AND ( AND account_number ISN null, AND balance GT 10000)]]\": { \"TableScan\": { \"tableAlias\": \"a2\", \"tableName\": \"accounts\" } } } }] } } } } } Result set: a1.firstname a1.lastname a1.balance Amber\nDuke\n39225\nNanette\nBates\n32838 Example 2: From subquery\nSQL query: SELECT a. f, a. l, a. a FROM ( SELECT firstname AS f, lastname AS l, age AS a FROM accounts WHERE age &gt; 30) AS a Explain: { \"from\": 0, \"size\": 200, \"query\": { \"bool\": { \"filter\": [ { \"bool\": { \"must\": [ { \"range\": { \"age\": { \"from\": 30, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1.0 } } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"_source\": { \"includes\": [ \"firstname\", \"lastname\", \"age\"], \"excludes\": [] } } Result set: f l a Amber\nDuke\n32\nDale\nAdams\n33\nHattie\nBond\n36",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "SQL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql/delete/",
    "title": "Delete",
    "content": "The DELETE statement deletes documents that satisfy the predicates in the WHERE clause.\nIf you don’t specify the WHERE clause, all documents are deleted.\nSetting\nThe DELETE statement is disabled by default. To enable the DELETE functionality in SQL, you need to update the configuration by sending the following request: PUT _plugins/_query/settings { \"transient\": { \"plugins.sql.delete.enabled\": \"true\" } } Syntax\nRule singleDeleteStatement: Example\nSQL query: DELETE FROM accounts WHERE age &gt; 30 Explain: { \"size\": 1000, \"query\": { \"bool\": { \"must\": [ { \"range\": { \"age\": { \"from\": 30, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1.0 } } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"_source\": false } Result set: { \"schema\": [ { \"name\": \"deleted_rows\", \"type\": \"long\" }], \"total\": 1, \"datarows\": [ [ 3]], \"size\": 1, \"status\": 200 } The datarows field shows the number of documents deleted.",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "SQL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql/functions/",
    "title": "Functions",
    "content": "The SQL language supports all SQL plugin common functions, including relevance search, but also introduces a few function synonyms, which are available in SQL only.\nThese synonyms are provided by the V1 engine. For more information, see Limitations.\nMatch query\nThe MATCHQUERY and MATCH_QUERY functions are synonyms for the MATCH relevance function. They don’t accept additional arguments but provide an alternate syntax.\nSyntax\nTo use matchquery or match_query, pass in your search query and the field name that you want to search against: match_query ( field_expression, query_expression [, option =&lt; option_value &gt;] *) matchquery ( field_expression, query_expression [, option =&lt; option_value &gt;] *) field_expression = match_query ( query_expression [, option =&lt; option_value &gt;] *) field_expression = matchquery ( query_expression [, option =&lt; option_value &gt;] *) You can specify the following options in any order: analyzer boost Example\nYou can use MATCHQUERY to replace MATCH: SELECT account_number, address FROM accounts WHERE MATCHQUERY ( address, 'Holmes') Alternatively, you can use MATCH_QUERY to replace MATCH: SELECT account_number, address FROM accounts WHERE address = MATCH_QUERY ( 'Holmes') The results contain documents in which the address contains “Holmes”: account_number address 1\n880 Holmes Lane Multi-match\nThere are three synonyms for MULTI_MATCH, each with a slightly different syntax. They accept a query string and a fields list with weights. They can also accept additional optional parameters.\nSyntax multimatch ( 'query' = query_expression [, 'fields' = field_expression][, option =&lt; option_value &gt;] *) multi_match ( 'query' = query_expression [, 'fields' = field_expression][, option =&lt; option_value &gt;] *) multimatchquery ( 'query' = query_expression [, 'fields' = field_expression][, option =&lt; option_value &gt;] *) The fields parameter is optional and can contain a single field or a comma-separated list (whitespace characters are not allowed). The weight for each field is optional and is specified after the field name. It should be delimited by the caret character – ^ – without whitespace.\nExample\nThe following queries show the fields parameter of a multi-match query with a single field and a field list: multi_match ( 'fields' = \"Tags^2,Title^3.4,Body,Comments^0.3\",...) multi_match ( 'fields' = \"Title\",...) You can specify the following options in any order: analyzer boost slop type tie_breaker operator Query string\nThe QUERY function is a synonym for QUERY_STRING.\nSyntax query ( 'query' = query_expression [, 'fields' = field_expression][, option =&lt; option_value &gt;] *) The fields parameter is optional and can contain a single field or a comma-separated list (whitespace characters are not allowed). The weight for each field is optional and is specified after the field name. It should be delimited by the caret character – ^ – without whitespace.\nExample\nThe following queries show the fields parameter of a multi-match query with a single field and a field list: query ( 'fields' = \"Tags^2,Title^3.4,Body,Comments^0.3\",...) query ( 'fields' = \"Tags\",...) You can specify the following options in any order: analyzer boost slop default_field Example of using query_string in SQL and PPL queries:\nThe following is a sample REST API search request in OpenSearch DSL. GET accounts/_search { \"query\": { \"query_string\": { \"query\": \"Lane Street\", \"fields\": [ \"address\"], } } } The request above is equivalent to the following query function: SELECT account_number, address FROM accounts WHERE query ( 'address:Lane OR address:Street') The results contain addresses that contain “Lane” or “Street”: account_number address 1\n880 Holmes Lane\n6\n671 Bristol Street\n13\n789 Madison Street Match phrase\nThe MATCHPHRASEQUERY function is a synonym for MATCH_PHRASE.\nSyntax matchphrasequery ( query_expression, field_expression [, option =&lt; option_value &gt;] *) You can specify the following options in any order: analyzer boost slop Score query\nTo return a relevance score along with every matching document, use the SCORE, SCOREQUERY, or SCORE_QUERY functions.\nSyntax\nThe SCORE function expects two arguments. The first argument is the MATCH_QUERY expression. The second argument is an optional floating-point number to boost the score (the default value is 1.0): SCORE ( match_query_expression, score) SCOREQUERY ( match_query_expression, score) SCORE_QUERY ( match_query_expression, score) Example\nThe following example uses the SCORE function to boost the documents’ scores: SELECT account_number, address, _score FROM accounts WHERE SCORE ( MATCH_QUERY ( address, 'Lane'), 0. 5) OR SCORE ( MATCH_QUERY ( address, 'Street'), 100) ORDER BY _score The results contain matches with corresponding scores: account_number address score 1\n880 Holmes Lane\n0.5\n6\n671 Bristol Street\n100\n13\n789 Madison Street\n100 Wildcard query\nTo search documents by a given wildcard, use the WILDCARDQUERY or WILDCARD_QUERY functions.\nSyntax wildcardquery ( field_expression, query_expression [, boost =&lt; value &gt;]) wildcard_query ( field_expression, query_expression [, boost =&lt; value &gt;]) Example\nThe following example uses a wildcard query: SELECT account_number, address FROM accounts WHERE wildcard_query ( address, '*Holmes*'); The results contain documents that match the wildcard expression: account_number address 1\n880 Holmes Lane",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "SQL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql/index/",
    "title": "SQL",
    "content": "Workbench\nThe easiest way to get familiar with the SQL plugin is to use Query Workbench in OpenSearch Dashboards to test various queries. To learn more, see Workbench. SQL and OpenSearch terminology\nHere’s how core SQL concepts map to OpenSearch: SQL OpenSearch Table\nIndex\nRow\nDocument\nColumn\nField REST API\nFor a complete REST API reference for the SQL plugin, see SQL/PPL API.\nTo use the SQL plugin with your own applications, send requests to the _plugins/_sql endpoint: POST _plugins/_sql { \"query\": \"SELECT * FROM my-index LIMIT 50\" } You can query multiple indexes by using a comma-separated list: POST _plugins/_sql { \"query\": \"SELECT * FROM my-index1,myindex2,myindex3 LIMIT 50\" } You can also specify an index pattern with a wildcard expression: POST _plugins/_sql { \"query\": \"SELECT * FROM my-index* LIMIT 50\" } To run the above query in the command line, use the curl command: curl -XPOST https://localhost:9200/_plugins/_sql -u 'admin:admin' -k -H 'Content-Type: application/json' -d '{\"query\": \"SELECT * FROM my-index* LIMIT 50\"}' You can specify the response format as JDBC, standard OpenSearch JSON, CSV, or raw. By default, queries return data in JDBC format. The following query sets the format to JSON: POST _plugins/_sql?format=json { \"query\": \"SELECT * FROM my-index LIMIT 50\" } See the rest of this guide for more information about request parameters, settings, supported operations, and tools.",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql/jdbc/",
    "title": "JDBC Driver",
    "content": "The Java Database Connectivity (JDBC) driver lets you integrate OpenSearch with your favorite business intelligence (BI) applications.\nFor information on downloading and using the JAR file, see the SQL repository on GitHub.\nConnecting to Tableau\nTo connect to Tableau, follow the detailed instructions in the GitHub repository.",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "SQL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql/metadata/",
    "title": "Metadata Queries",
    "content": "To see basic metadata about your indices, use the SHOW and DESCRIBE commands.\nSyntax\nRule showStatement: Rule showFilter: Example 1: See metadata for indices\nTo see metadata for indices that match a specific pattern, use the SHOW command.\nUse the wildcard % to match all indices: SHOW TABLES LIKE % TABLE_CAT TABLE_SCHEM TABLE_NAME TABLE_TYPE REMARKS TYPE_CAT TYPE_SCHEM TYPE_NAME SELF_REFERENCING_COL_NAME REF_GENERATION docker-cluster\nnull\naccounts\nBASE TABLE\nnull\nnull\nnull\nnull\nnull\nnull\ndocker-cluster\nnull\nemployees_nested\nBASE TABLE\nnull\nnull\nnull\nnull\nnull\nnull Example 2: See metadata for a specific index\nTo see metadata for an index name with a prefix of acc: SHOW TABLES LIKE acc % TABLE_CAT TABLE_SCHEM TABLE_NAME TABLE_TYPE REMARKS TYPE_CAT TYPE_SCHEM TYPE_NAME SELF_REFERENCING_COL_NAME REF_GENERATION docker-cluster\nnull\naccounts\nBASE TABLE\nnull\nnull\nnull\nnull\nnull\nnull Example 3: See metadata for fields\nTo see metadata for field names that match a specific pattern, use the DESCRIBE command: DESCRIBE TABLES LIKE accounts TABLE_CAT TABLE_SCHEM TABLE_NAME COLUMN_NAME DATA_TYPE TYPE_NAME COLUMN_SIZE BUFFER_LENGTH DECIMAL_DIGITS NUM_PREC_RADIX NULLABLE REMARKS COLUMN_DEF SQL_DATA_TYPE SQL_DATETIME_SUB CHAR_OCTET_LENGTH ORDINAL_POSITION IS_NULLABLE SCOPE_CATALOG SCOPE_SCHEMA SCOPE_TABLE SOURCE_DATA_TYPE IS_AUTOINCREMENT IS_GENERATEDCOLUMN docker-cluster\nnull\naccounts\naccount_number\nnull\nlong\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n1\n \nnull\nnull\nnull\nnull\nNO\n \ndocker-cluster\nnull\naccounts\nfirstname\nnull\ntext\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n2\n \nnull\nnull\nnull\nnull\nNO\n \ndocker-cluster\nnull\naccounts\naddress\nnull\ntext\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n3\n \nnull\nnull\nnull\nnull\nNO\n \ndocker-cluster\nnull\naccounts\nbalance\nnull\nlong\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n4\n \nnull\nnull\nnull\nnull\nNO\n \ndocker-cluster\nnull\naccounts\ngender\nnull\ntext\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n5\n \nnull\nnull\nnull\nnull\nNO\n \ndocker-cluster\nnull\naccounts\ncity\nnull\ntext\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n6\n \nnull\nnull\nnull\nnull\nNO\n \ndocker-cluster\nnull\naccounts\nemployer\nnull\ntext\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n7\n \nnull\nnull\nnull\nnull\nNO\n \ndocker-cluster\nnull\naccounts\nstate\nnull\ntext\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n8\n \nnull\nnull\nnull\nnull\nNO\n \ndocker-cluster\nnull\naccounts\nage\nnull\nlong\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n9\n \nnull\nnull\nnull\nnull\nNO\n \ndocker-cluster\nnull\naccounts\nemail\nnull\ntext\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n10\n \nnull\nnull\nnull\nnull\nNO\n \ndocker-cluster\nnull\naccounts\nlastname\nnull\ntext\nnull\nnull\nnull\n10\n2\nnull\nnull\nnull\nnull\nnull\n11\n \nnull\nnull\nnull\nnull\nNO\n ",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "SQL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql/odbc/",
    "title": "ODBC Driver",
    "content": "The Open Database Connectivity (ODBC) driver is a read-only ODBC driver for Windows and macOS that lets you connect business intelligence (BI) and data visualization applications like Microsoft Excel and Power BI to the SQL plugin.\nFor information on downloading and using the driver, see the SQL repository on GitHub.\nSpecifications\nThe ODBC driver is compatible with ODBC version 3.51.\nSupported OS versions\nThe following operating systems are supported: Operating System Version Windows\nWindows 10, Windows 11\nmacOS\nCatalina 10.15.4, Mojave 10.14.6, Big Sur 11.6.7, Monterey 12.4 Concepts Term Definition DSN A DSN (Data Source Name) is used to store driver information in the system. By storing the information in the system, the information does not need to be specified each time the driver connects..tdc file\nThe TDC file contains configuration information that Tableau applies to any connection matching the database vendor name and driver name defined in the file. This configuration allows you to fine-tune parts of your ODBC data connection and turn on/off certain features not supported by the data source. Install driver\nTo install the driver, download the bundled distribution installer from here or by build from the source.\nWindows\nOpen the downloaded OpenSearch SQL ODBC Driver-&lt;version&gt;-Windows.msi installer.\nThe installer is unsigned and shows a security dialog. Choose More info and Run anyway.\nChoose Next to proceed with the installation.\nAccept the agreement, and choose Next.\nThe installer comes bundled with documentation and useful resource files to connect to various BI tools (for example, a.tdc file for Tableau). You can choose to keep or remove these resources. Choose Next.\nChoose Install and Finish.\nThe following connection information is set up as part of the default DSN: Host: localhost\nPort: 9200\nAuth: NONE To customize the DSN, use ODBC Data Source Administrator which is pre-installed on Windows 10.\nmacOS\nBefore installing the ODBC Driver on macOS, install the iODBC Driver Manager.\nOpen the downloaded OpenSearch SQL ODBC Driver-&lt;version&gt;-Darwin.pkg installer.\nThe installer is unsigned and shows a security dialog. Right-click on the installer and choose Open.\nChoose Continue several times to proceed with the installation.\nChoose the Destination to install the driver files.\nThe installer comes bundled with documentation and useful resources files to connect to various BI tools (for example, a.tdc file for Tableau). You can choose to keep or remove these resources. Choose Continue.\nChoose Install and Close.\nCurrently, the DSN is not set up as part of the installation and needs to be configured manually. First, open iODBC Administrator: sudo /Applications/iODBC/iODBC\\ Administrator64.app/Contents/MacOS/iODBC\\ Administrator64 This command gives the application permissions to save the driver and DSN configurations.\nChoose ODBC Drivers tab.\nChoose Add a Driver and fill in the following details: Description of the Driver: Enter the driver name that you used for the ODBC connection (for example, OpenSearch SQL ODBC Driver). Driver File Name: Enter the path to the driver file (default: &lt;driver-install-dir&gt;/bin/libopensearchsqlodbc.dylib). Setup File Name: Enter the path to the setup file (default: &lt;driver-install-dir&gt;/bin/libopensearchsqlodbc.dylib).\nChoose the user driver.\nChoose OK to save the options.\nChoose the User DSN tab.\nSelect Add.\nChoose the driver that you added above.\nFor Data Source Name (DSN), enter the name of the DSN used to store connection options (for example, OpenSearch SQL ODBC DSN).\nFor Comment, add an optional comment.\nAdd key-value pairs by using the + button. We recommend the following options for a default local OpenSearch installation: Host: localhost - OpenSearch server endpoint Port: 9200 - The server port Auth: NONE - The authentication mode Username: (blank) - The username used for BASIC auth Password: (blank) - The password used for BASIC auth ResponseTimeout: 10 - The number of seconds to wait for a response from the server UseSSL: 0 - Do not use SSL for connections\nChoose OK to save the DSN configuration.\nChoose OK to exit the iODBC Administrator.\nCustomizing the ODBC driver\nThe driver is in the form of a library file: opensearchsqlodbc.dll for Windows and libopensearchsqlodbc.dylib for macOS.\nIf you’re using with ODBC compatible BI tools, refer to your BI tool documentation for configuring a new ODBC driver.\nTypically, all that’s required is to make the BI tool aware of the location of the driver library file and then use it to set up the database (i.e., OpenSearch) connection.\nConnection strings and other settings\nThe ODBC driver uses an ODBC connection string.\nThe connection strings are semicolon-delimited strings that specify the set of options that you can use for a connection.\nTypically, a connection string will either:\nSpecify a Data Source Name (DSN) that contains a pre-configured set of options ( DSN=xxx;User=xxx;Password=xxx;).\nOr, configure options explicitly using the string ( Host=xxx;Port=xxx;LogLevel=ES_DEBUG;...).\nYou can configure the following driver options using a DSN or connection string:\nAll option names are case-insensitive.\nBasic options Option Description Type Default DSN Data source name that you used for configuring the connection. string - Host / Server Hostname or IP address for the target cluster. string - Port Port number on which the OpenSearch cluster’s REST interface is listening. string - Authentication Options Option Description Type Default Auth Authentication mechanism to use. BASIC (basic HTTP), AWS_SIGV4 (AWS auth), or NONE NONE User / UID [ Auth=BASIC] Username for the connection. string - Password / PWD [ Auth=BASIC] Password for the connection. string - Region [ Auth=AWS_SIGV4] Region used for signing requests. AWS region (for example, us-west-1) - Advanced options Option Description Type Default UseSSL Whether to establish the connection over SSL/TLS. boolean (0 or 1) false (0) HostnameVerification Indicates whether certificate hostname verification should be performed for an SSL/TLS connection. boolean (0 or 1) true (1) ResponseTimeout The maximum time to wait for responses from the host, in seconds. integer 10 Logging options Option Description Type Default LogLevel Severity level for driver logs. LOG_OFF, LOG_FATAL, LOG_ERROR, LOG_INFO, LOG_DEBUG, LOG_TRACE, or LOG_ALL LOG_WARNING LogOutput Location for storing driver logs. string WIN: C:\\, MAC: /tmp You need administrative privileges to change the logging options.\nConnecting to Tableau\nPre-requisites:\nMake sure the DSN is already set up.\nMake sure OpenSearch is running on host and port as configured in DSN.\nMake sure the.tdc is copied to &lt;user_home_directory&gt;/Documents/My Tableau Repository/Datasources in both macOS and Windows.\nStart Tableau. Under the Connect section, go to To a Server and choose Other Databases (ODBC).\nIn the DSN drop-down, select the OpenSearch DSN you set up in the previous set of steps. The options you added will be automatically filled in under the Connection Attributes.\nSelect Sign In. After a few seconds, Tableau connects to your OpenSearch server. Once connected, you will be directed to the Datasource window. The Database will be already be populated with the name of the OpenSearch cluster.\nTo list all the indices, click the search icon under Table.\nStart experimenting with data by dragging the table to the connection area. Choose Update Now or Automatically Update to populate the table data.\nSee more detailed instructions in the GitHub repository.\nTroubleshooting Problem Unable to connect to server. Workaround This is most likely due to OpenSearch server not running on host and post configured in DSN.\nConfirm host and post are correct and OpenSearch server is running with OpenSearch SQL plugin.\nAlso make sure.tdc that was downloaded with the installer is copied correctly to &lt;user_home_directory&gt;/Documents/My Tableau Repository/Datasources directory.\nConnecting to Microsoft Power BI\nFollow the installation instructions and the configuration instructions published in the GitHub repository.",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "SQL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/sql/partiql/",
    "title": "JSON Support",
    "content": "SQL plugin supports JSON by following PartiQL specification, a SQL-compatible query language that lets you query semi-structured and nested data for any data format. The SQL plugin only supports a subset of the PartiQL specification.\nQuerying nested collection\nPartiQL extends SQL to allow you to query and unnest nested collections. In OpenSearch, this is very useful to query a JSON index with nested objects or fields.\nTo follow along, use the bulk operation to index some sample data: POST employees_nested/_bulk?refresh { \"index\":{ \"_id\": \"1\" }} { \"id\": 3, \"name\": \"Bob Smith\", \"title\": null, \"projects\":[{ \"name\": \"SQL Spectrum querying\", \"started_year\": 1990 },{ \"name\": \"SQL security\", \"started_year\": 1999 },{ \"name\": \"OpenSearch security\", \"started_year\": 2015 }]} { \"index\":{ \"_id\": \"2\" }} { \"id\": 4, \"name\": \"Susan Smith\", \"title\": \"Dev Mgr\", \"projects\":[]} { \"index\":{ \"_id\": \"3\" }} { \"id\": 6, \"name\": \"Jane Smith\", \"title\": \"Software Eng 2\", \"projects\":[{ \"name\": \"SQL security\", \"started_year\": 1998 },{ \"name\": \"Hello security\", \"started_year\": 2015, \"address\":[{ \"city\": \"Dallas\", \"state\": \"TX\" }]}]} Example 1: Unnesting a nested collection\nThis example finds the nested document ( projects) with a field value ( name) that satisfies the predicate (contains security). Because each parent document can have more than one nested documents, the nested document that matches is flattened. In other words, the final result is the cartesian product between the parent and nested documents. SELECT e. name AS employeeName, p. name AS projectName FROM employees_nested AS e, e. projects AS p WHERE p. name LIKE '%security%' Explain: { \"from\": 0, \"size\": 200, \"query\": { \"bool\": { \"filter\": [ { \"bool\": { \"must\": [ { \"nested\": { \"query\": { \"wildcard\": { \"projects.name\": { \"wildcard\": \"*security*\", \"boost\": 1.0 } } }, \"path\": \"projects\", \"ignore_unmapped\": false, \"score_mode\": \"none\", \"boost\": 1.0, \"inner_hits\": { \"ignore_unmapped\": false, \"from\": 0, \"size\": 3, \"version\": false, \"seq_no_primary_term\": false, \"explain\": false, \"track_scores\": false, \"_source\": { \"includes\": [ \"projects.name\"], \"excludes\": [] } } } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"_source\": { \"includes\": [ \"name\"], \"excludes\": [] } } Result set: employeeName projectName Bob Smith\nOpenSearch Security\nBob Smith\nSQL security\nJane Smith\nHello security\nJane Smith\nSQL security Example 2: Unnesting in existential subquery\nTo unnest a nested collection in a subquery to check if it satisfies a condition: SELECT e. name AS employeeName FROM employees_nested AS e WHERE EXISTS ( SELECT * FROM e. projects AS p WHERE p. name LIKE '%security%') Explain: { \"from\": 0, \"size\": 200, \"query\": { \"bool\": { \"filter\": [ { \"bool\": { \"must\": [ { \"nested\": { \"query\": { \"bool\": { \"must\": [ { \"bool\": { \"must\": [ { \"bool\": { \"must_not\": [ { \"bool\": { \"must_not\": [ { \"exists\": { \"field\": \"projects\", \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, { \"wildcard\": { \"projects.name\": { \"wildcard\": \"*security*\", \"boost\": 1.0 } } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"path\": \"projects\", \"ignore_unmapped\": false, \"score_mode\": \"none\", \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"_source\": { \"includes\": [ \"name\"], \"excludes\": [] } } Result set: employeeName Bob Smith\nJane Smith",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "SQL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/troubleshoot/",
    "title": "Troubleshooting",
    "content": "The SQL plugin is stateless, so troubleshooting is mostly focused on why a particular query fails.\nThe most common error is the dreaded null pointer exception, which can occur during parsing errors or when using the wrong HTTP method (POST vs. GET and vice versa). The POST method and HTTP request body offer the most consistent results: POST _plugins/_sql { \"query\": \"SELECT * FROM my-index WHERE ['name.firstname']='saanvi' LIMIT 5\" } If a query isn’t behaving the way you expect, use the _explain API to see the translated query, which you can then troubleshoot. For most operations, _explain returns OpenSearch query DSL. For UNION, MINUS, and JOIN, it returns something more akin to a SQL execution plan.\nExample request POST _plugins/_sql/_explain { \"query\": \"SELECT * FROM my-index LIMIT 50\" } Example response { \"from\": 0, \"size\": 50 } Index mapping verification exception\nIf you see the following verification exception: { \"error\": { \"reason\": \"There was internal problem at backend\", \"details\": \"When using multiple indices, the mappings must be identical.\", \"type\": \"VerificationException\" }, \"status\": 503 } Make sure the index in your query is not an index pattern and is not an index pattern and doesn’t have multiple types.\nIf these steps don’t work, submit a Github issue here.",
    "ancestors": [
      "Search",
      "SQL and PPL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/search-plugins/sql/workbench/",
    "title": "Query Workbench",
    "content": "Use the Query Workbench to easily run on-demand SQL queries, translate SQL into its REST equivalent, and view and save results as text, JSON, JDBC, or CSV.\nQuick start\nTo get started with SQL Workbench, choose Dev Tools in OpenSearch Dashboards and use the bulk operation to index some sample data: PUT accounts/_bulk?refresh { \"index\":{ \"_id\": \"1\" }} { \"account_number\": 1, \"balance\": 39225, \"firstname\": \"Amber\", \"lastname\": \"Duke\", \"age\": 32, \"gender\": \"M\", \"address\": \"880 Holmes Lane\", \"employer\": \"Pyrami\", \"email\": \"amberduke@pyrami.com\", \"city\": \"Brogan\", \"state\": \"IL\" } { \"index\":{ \"_id\": \"6\" }} { \"account_number\": 6, \"balance\": 5686, \"firstname\": \"Hattie\", \"lastname\": \"Bond\", \"age\": 36, \"gender\": \"M\", \"address\": \"671 Bristol Street\", \"employer\": \"Netagy\", \"email\": \"hattiebond@netagy.com\", \"city\": \"Dante\", \"state\": \"TN\" } { \"index\":{ \"_id\": \"13\" }} { \"account_number\": 13, \"balance\": 32838, \"firstname\": \"Nanette\", \"lastname\": \"Bates\", \"age\": 28, \"gender\": \"F\", \"address\": \"789 Madison Street\", \"employer\": \"Quility\", \"email\": \"nanettebates@quility.com\", \"city\": \"Nogal\", \"state\": \"VA\" } { \"index\":{ \"_id\": \"18\" }} { \"account_number\": 18, \"balance\": 4180, \"firstname\": \"Dale\", \"lastname\": \"Adams\", \"age\": 33, \"gender\": \"M\", \"address\": \"467 Hutchinson Court\", \"email\": \"daleadams@boink.com\", \"city\": \"Orick\", \"state\": \"MD\" } Then return to SQL Workbench.\nList indices\nTo list all your indices: SHOW TABLES LIKE % TABLE_NAME accounts Read data\nAfter you index a document, retrieve it using the following SQL expression: SELECT * FROM accounts WHERE _id = 1 account_number firstname gender city balance employer state email address lastname age 1\nAmber\nM\nBrogan\n39225\nPyrami\nIL\namberduke@pyrami.com\n880 Holmes Lane\nDuke\n32 Delete data\nTo delete a document from an index, use the DELETE clause: DELETE FROM accounts WHERE _id = 0 deleted_rows 1",
    "ancestors": [
      "Search",
      "SQL and PPL",
      "SQL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/ml-commons-plugin/algorithms/",
    "title": "Supported Algorithms",
    "content": "ML Commons supports various algorithms to help train and predict machine learning (ML) models or test data-driven predictions without a model. This page outlines the algorithms supported by the ML Commons plugin and the API operations they support.\nCommon limitation\nExcept for the Localization algorithm, all of the following algorithms can only support retrieving 10,000 documents from an index as an input.\nK-means\nK-means is a simple and popular unsupervised clustering ML algorithm built on top of Tribuo library. K-means will randomly choose centroids, then calculate iteratively to optimize the position of the centroids until each observation belongs to the cluster with the nearest mean.\nParameters Parameter Type Description Default Value centroids\ninteger\nThe number of clusters in which to group the generated data 2 iterations\ninteger\nThe number of iterations to perform against the data until a mean generates 10 distance_type\nenum, such as EUCLIDEAN, COSINE, or L1 The type of measurement from which to measure the distance between centroids EUCLIDEAN APIs Train Predict Train and predict Example\nThe following example uses the Iris Data index to train k-means synchronously. POST /_plugins/_ml/_train/kmeans { \"parameters\": { \"centroids\": 3, \"iterations\": 10, \"distance_type\": \"COSINE\" }, \"input_query\": { \"_source\": [ \"petal_length_in_cm\", \"petal_width_in_cm\"], \"size\": 10000 }, \"input_index\": [ \"iris_data\"] } Limitations\nThe training process supports multi-threads, but the number of threads should be less than half of the number of CPUs.\nLinear regression\nLinear regression maps the linear relationship between inputs and outputs. In ML Commons, the linear regression algorithm is adopted from the public machine learning library Tribuo, which offers multidimensional linear regression models. The model supports the linear optimizer in training, including popular approaches like Linear Decay, SQRT_DECAY, ADA, ADAM, and RMS_DROP.\nParameters Parameter Type Description Default Value learningRate\nDouble\nThe initial step size used in an iterative optimization algorithm.\n0.01\nmomentumFactor\nDouble\nThe extra weight factors that accelerate the rate at which the weight is adjusted. This helps move the minimization routine out of local minima.\n0\nepsilon\nDouble\nThe value for stabilizing gradient inversion.\n1.00E-06\nbeta1\nDouble\nThe exponential decay rates for the moment estimates.\n0.9\nbeta2\nDouble\nThe exponential decay rates for the moment estimates.\n0.99\ndecayRate\nDouble\nThe Root Mean Squared Propagation (RMSProp).\n0.9\nmomentumType\nMomentumType\nThe defined Stochastic Gradient Descent (SGD) momentum type that helps accelerate gradient vectors in the right directions, leading to a fast convergence.\nSTANDARD\noptimizerType\nOptimizerType\nThe optimizer used in the model.\nSIMPLE_SGD APIs Train Predict Example\nThe following example creates a new prediction based on the previously trained linear regression model. Request POST _plugins/_ml/_predict/LINEAR_REGRESSION/ROZs -38 Br 5 eVE 0 lTsoD 9 { \"parameters\": { \"target\": \"price\" }, \"input_data\": { \"column_metas\": [ { \"name\": \"A\", \"column_type\": \"DOUBLE\" }, { \"name\": \"B\", \"column_type\": \"DOUBLE\" }], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 3 }, { \"column_type\": \"DOUBLE\", \"value\": 5 }] }] } } Response { \"status\": \"COMPLETED\", \"prediction_result\": { \"column_metas\": [ { \"name\": \"price\", \"column_type\": \"DOUBLE\" }], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 17.25701855310131 }] }] } } Limitations\nML Commons only supports the linear Stochastic gradient trainer or optimizer, which cannot effectively map the non-linear relationships in trained data. When used with complicated datasets, the linear Stochastic trainer might cause some convergence problems and inaccurate results.\nRCF Random Cut Forest (RCF) is a probabilistic data structure used primarily for unsupervised anomaly detection. Its use also extends to density estimation and forecasting. OpenSearch leverages RCF for anomaly detection. ML Commons supports two new variants of RCF for different use cases:\nBatch RCF: Detects anomalies in non-time series data.\nFixed in time (FIT) RCF: Detects anomalies in time series data.\nParameters\nBatch RCF Parameter Type Description Default Value number_of_trees\ninteger\nThe number of trees in the forest.\n30\nsample_size\ninteger\nThe same size used by the stream samplers in the forest.\n256\noutput_after\ninteger\nThe number of points required by stream samplers before results return.\n32\ntraining_data_size\ninteger\nThe size of your training data.\nDataset size\nanomaly_score_threshold\ndouble\nThe threshold of the anomaly score.\n1.0 Fit RCF\nAll parameters are optional except time_field. Parameter Type Description Default Value number_of_trees\ninteger\nThe number of trees in the forest.\n30\nshingle_size\ninteger\nA shingle, or a consecutive sequence of the most recent records.\n8\nsample_size\ninteger\nThe sample size used by stream samplers in the forest.\n256\noutput_after\ninteger\nThe number of points required by stream samplers before results return.\n32\ntime_decay\ndouble\nThe decay factor used by stream samplers in the forest.\n0.0001\nanomaly_rate\ndouble\nThe anomaly rate.\n0.005\ntime_field\nstring\n( Required) The time field for RCF to use as time series data.\nN/A\ndate_format\nstring\nThe date and time format for the time_field field.\n“yyyy-MM-ddHH:mm:ss”\ntime_zone\nstring\nThe time zone for the time_field field.\n“UTC” APIs Train Predict Train and predict Limitations\nFor FIT RCF, you can train the model with historical data and store the trained model in your index. The model will be deserialized and predict new data points when using the Predict API. However, the model in the index will not be refreshed with new data, because the model is fixed in time.\nRCF Summarize\nRCF Summarize is a clustering algorithm based on the Clustering Using Representatives (CURE) algorithm. Compared to k-means, which uses random iterations to cluster, RCF Summarize uses a hierarchical clustering technique. The algorithm starts, with a set of randomly selected centroids larger than the centroids’ ground truth distribution. During iteration, centroid pairs too close to each other automatically merge. Therefore, the number of centroids ( max_k) converge to a rational number of clusters that fits ground truth, as opposed to a fixed k number of clusters.\nParameters Parameter Type Description Default Value max_k\ninteger\nThe max allowed number of centroids.\n2\ndistance_type\nenum, such as EUCLIDEAN, L1, L2, or LInfinity The type of measurement used to measure the distance between centroids.\nEUCLIDEAN APIs Train Predict Train and predict Example: Train and predict\nThe following example estimates cluster centers and provides cluster labels for each sample in a given data frame. POST _plugins/_ml/_train_predict/RCF_SUMMARIZE { \"parameters\": { \"centroids\": 3, \"max_k\": 15, \"distance_type\": \"L2\" }, \"input_data\": { \"column_metas\": [ { \"name\": \"d0\", \"column_type\": \"DOUBLE\" }, { \"name\": \"d1\", \"column_type\": \"DOUBLE\" }], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 6.2 }, { \"column_type\": \"DOUBLE\", \"value\": 3.4 }] }] } } Response The rows parameter within the prediction result has been modified for length. In your response, expect more rows and columns to be contained within the response body. { \"status\": \"COMPLETED\", \"prediction_result\": { \"column_metas\": [ { \"name\": \"ClusterID\", \"column_type\": \"INTEGER\" }], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 0 }] }] } } Localization\nThe Localization algorithm finds subset-level information for aggregate data (for example, aggregated over time) that demonstrates the activity of interest, such as spikes, drops, changes, or anomalies. Localization can be applied in different scenarios, such as data exploration or root cause analysis, to expose the contributors driving the activity of interest in the aggregate data.\nParameters\nAll parameters are required except filter_query and anomaly_start. Parameter Type Description Default Value index_name\nString\nThe data collection to analyze.\nN/A\nattribute_field_names\nList The fields for entity keys.\nN/A\naggregations\nList The fields and aggregation for values.\nN/A\ntime_field_name\nString\nThe timestamp field.\nnull\nstart_time\nLong\nThe beginning of the time range.\n0\nend_time\nLong\nThe end of the time range.\n0\nmin_time_interval\nLong\nThe minimum time interval/scale for analysis.\n0\nnum_outputs\ninteger\nThe maximum number of values from localization/slicing.\n0\nfilter_query\nLong\n(Optional) Reduces the collection of data for analysis.\nOptional.empty()\nanomaly_star\nQueryBuilder\n(Optional) The time after which the data will be analyzed.\nOptional.empty() Example: Execute localization\nThe following example executes Localization against an RCA index. Request POST /_plugins/_ml/_execute/anomaly_localization { \"index_name\": \"rca-index\", \"attribute_field_names\": [ \"attribute\"], \"aggregations\": [ { \"sum\": { \"sum\": { \"field\": \"value\" } } }], \"time_field_name\": \"timestamp\", \"start_time\": 1620630000000, \"end_time\": 1621234800000, \"min_time_interval\": 86400000, \"num_outputs\": 10 } Response The API responds with the sum of the contribution and base values per aggregation, every time the algorithm executes in the specified time interval. { \"results\": [ { \"name\": \"sum\", \"result\": { \"buckets\": [ { \"start_time\": 1620630000000, \"end_time\": 1620716400000, \"overall_aggregate_value\": 65.0 }, { \"start_time\": 1620716400000, \"end_time\": 1620802800000, \"overall_aggregate_value\": 75.0, \"entities\": [ { \"key\": [ \"attr0\"], \"contribution_value\": 1.0, \"base_value\": 2.0, \"new_value\": 3.0 }, { \"key\": [ \"attr1\"], \"contribution_value\": 1.0, \"base_value\": 3.0, \"new_value\": 4.0 }, {... }, { \"key\": [ \"attr8\"], \"contribution_value\": 6.0, \"base_value\": 10.0, \"new_value\": 16.0 }, { \"key\": [ \"attr9\"], \"contribution_value\": 6.0, \"base_value\": 11.0, \"new_value\": 17.0 }] }] } }] } Limitations\nThe Localization algorithm can only be executed directly. Therefore, it cannot be used with the ML Commons Train and Predict APIs.\nLogistic regression\nA classification algorithm, logistic regression models the probability of a discrete outcome given an input variable. In ML Commons, these classifications include both binary and multi-class. The most common is the binary classification, which takes two values, such as “true/false” or “yes/no”, and predicts the outcome based on the values specified. Alternatively, a multi-class output can categorize different inputs based on type. This makes logistic regression most useful for situations where you are trying to determine how your inputs fit best into a specified category.\nParameters Parameter Type Description Default Value learningRate\nDouble\nThe initial step size used in an iterative optimization algorithm.\n1\nmomentumFactor\nDouble\nThe extra weight factors that accelerate the rate at which the weight is adjusted. This helps move the minimization routine out of local minima.\n0\nepsilon\nDouble\nThe value for stabilizing gradient inversion.\n0.1\nbeta1\nDouble\nThe exponential decay rates for the moment estimates.\n0.9\nbeta2\nDouble\nThe exponential decay rates for the moment estimates.\n0.99\ndecayRate\nDouble\nThe Root Mean Squared Propagation (RMSProp).\n0.9\nmomentumType\nMomentumType\nThe Stochastic Gradient Descent (SGD) momentum that helps accelerate gradient vectors in the right direction, leading to faster convergence between vectors.\nSTANDARD\noptimizerType\nOptimizerType\nThe optimizer used in the model.\nAdaGrad\ntarget\nString\nThe target field.\nnull\nobjectiveType\nObjectiveType\nThe objective function type.\nLogMulticlass\nepochs\nInteger\nThe number of iterations.\n5\nbatchSize\nInteger\nThe size of minbatches.\n1\nloggingInterval\nInteger\nThe interval of logs lost after many iterations. The interval is 1 if the algorithm contains no logs.\n1000 APIs Train Predict Example: Train/Predict with Iris data\nThe following example creates an index in OpenSearch with the Iris dataset, then trains the data using logistic regression. Lastly, it uses the trained model to predict Iris types separated by row.\nCreate an Iris index\nBefore using this request, make sure that you have downloaded Iris data. PUT /iris_data { \"mappings\": { \"properties\": { \"sepal_length_in_cm\": { \"type\": \"double\" }, \"sepal_width_in_cm\": { \"type\": \"double\" }, \"petal_length_in_cm\": { \"type\": \"double\" }, \"petal_width_in_cm\": { \"type\": \"double\" }, \"class\": { \"type\": \"keyword\" } } } } Ingest data from IRIS_data.txt POST _bulk { \"index\": { \"_index\": \"iris_data\" } } { \"sepal_length_in_cm\":5.1, \"sepal_width_in_cm\":3.5, \"petal_length_in_cm\":1.4, \"petal_width_in_cm\":0.2, \"class\": \"Iris-setosa\" } { \"index\": { \"_index\": \"iris_data\" } } { \"sepal_length_in_cm\":4.9, \"sepal_width_in_cm\":3.0, \"petal_length_in_cm\":1.4, \"petal_width_in_cm\":0.2, \"class\": \"Iris-setosa\" }...... Train the logistic regression model\nThis example uses a multi-class logistic regression categorization methodology. Here, the inputs of sepal and petal length and width are used to train the model to categorize centroids based on the class, as indicated by the target parameter. Request { \"parameters\": { \"target\": \"class\" }, \"input_query\": { \"query\": { \"match_all\": {} }, \"_source\": [ \"sepal_length_in_cm\", \"sepal_width_in_cm\", \"petal_length_in_cm\", \"petal_width_in_cm\", \"class\"], \"size\": 200 }, \"input_index\": [ \"iris_data\"] } Response The model_id will be used to predict the class of the Iris. { \"model_id\": \"TOgsf4IByBqD7FK_FQGc\", \"status\": \"COMPLETED\" } Predict results\nUsing the model_id of the trained Iris dataset, logistic regression will predict the class of the Iris based on the input data. POST _plugins/_ml/_predict/logistic_regression/SsfQaoIBEoC4g4joZiyD { \"parameters\": { \"target\": \"class\" }, \"input_data\": { \"column_metas\": [ { \"name\": \"sepal_length_in_cm\", \"column_type\": \"DOUBLE\" }, { \"name\": \"sepal_width_in_cm\", \"column_type\": \"DOUBLE\" }, { \"name\": \"petal_length_in_cm\", \"column_type\": \"DOUBLE\" }, { \"name\": \"petal_width_in_cm\", \"column_type\": \"DOUBLE\" }], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 6.2 }, { \"column_type\": \"DOUBLE\", \"value\": 3.4 }, { \"column_type\": \"DOUBLE\", \"value\": 5.4 }, { \"column_type\": \"DOUBLE\", \"value\": 2.3 }] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 5.9 }, { \"column_type\": \"DOUBLE\", \"value\": 3.0 }, { \"column_type\": \"DOUBLE\", \"value\": 5.1 }, { \"column_type\": \"DOUBLE\", \"value\": 1.8 }] }] } } Response { \"status\": \"COMPLETED\", \"prediction_result\": { \"column_metas\": [ { \"name\": \"result\", \"column_type\": \"STRING\" }], \"rows\": [ { \"values\": [ { \"column_type\": \"STRING\", \"value\": \"Iris-virginica\" }] }, { \"values\": [ { \"column_type\": \"STRING\", \"value\": \"Iris-virginica\" }] }] } } Limitations\nConvergence metrics are not built into Tribuo’s trainers. Therefore, ML Commons cannot indicate the convergence status through the ML Commons API.\nMetrics correlation\nThe metrics correlation feature is an experimental feature released in OpenSearch 2.7. It can’t be used in a production environment. To leave feedback on improving the feature, create an issue in the ML Commons repository.\nThe metrics correlation algorithm finds events in a set of metrics data. The algorithm defines events as a window in time in which multiple metrics simultaneously display anomalous behavior. When given a set of metrics, the algorithm counts the number of events that occurred, when each event occurred, and determines which metrics were involved in each event.\nTo enable the metrics correlation algorithm, update the following cluster setting: PUT /_cluster/settings\n{\n\"persistent\": {\n\"plugins.ml_commons.enable_inhouse_python_model\": true\n}\n} Parameters\nTo use the metrics correlation algorithm, include the following parameters. Parameter Type Description Default value metrics\nArray\nA list of metrics within the time series that can be correlated to anomalous behavior\nN/A Input\nThe metrics correlation input is an $M$ x $T$ array of metrics data, where M is the number of metrics and T is the length of each individual sequence of metric values.\nWhen inputting metrics into the algorithm, assume the following:\nFor each metric, the input sequence has the same length, $T$.\nAll input metrics should have the same corresponding set of timestamps.\nThe total number of data points are $M$ * $T$ &lt;= 10000.\nExample: Simple metrics correlation\nThe following example inputs the number of metrics ($M$) as 3 and the number of timesteps ($T$) as 128: POST /_plugins/_ml/_execute/METRICS_CORRELATION\n{\"metrics\": [[-1.1635416, -1.5003631, 0.46138194, 0.5308311, -0.83149344, -3.7009873, -3.5463789, 0.22571462, -5.0380244, 0.76588845, 1.236113, 1.8460795, 1.7576948, 0.44893077, 0.7363948, 0.70440894, 0.89451003, 4.2006273, 0.3697659, 2.2458954, -2.302939, -1.7706926, 1.7445002, -1.5246059, 0.07985192, -2.7756078, 1.0002468, 1.5977372, 2.9152713, 1.4172368, -0.26551363, -2.2883027, 1.5882446, 2.0145164, 3.4862874, -1.2486862, -2.4811826, -0.17609037, -2.1095612, -1.2184235, 0.63118523, -1.8909532, 2.039797, -0.5317177, -2.2922578, -2.0179775, -0.07992507, -0.12554549, -0.2553092, 1.1450123, -0.4640453, -2.190223, -4.671612, -1.5076426, 1.635445, -1.1394824, -0.7503817, 0.98424894, -0.38896716, 1.0328646, 1.9543738, -0.5236269, 0.14298044, 3.2963762, 8.1641035, 5.717064, 7.4869685, 2.5987444, 11.018798, 9.151356, 5.7354255, 6.862203, 3.0524514, 4.431755, 5.1481285, 7.9548607, 7.4519925, 6.09533, 7.634116, 8.898271, 3.898491, 9.447067, 8.197385, 5.8284273, 5.804283, 7.7688456, 10.574343, 7.5679493, 7.1888094, 7.1107903, 8.454468, 8.066334, 8.83665, 7.11204, 4.4898267, 8.614764, 6.336754, 11.577503, 3.3998494, 9.501525, 13.17289, 6.1116023, 5.143777, 2.7813284, 3.7917604, 7.1683135, 7.627272, 7.290255, 3.1299121, 7.089733, 9.140584, 8.844729, 9.403275, 10.220029, 8.039719, 8.85549, 4.034555, 4.412663, 7.54451, 7.2116737, 4.6346946, 7.0044127, 9.7557, 10.982841, 5.897937, 6.870126, 3.5638695, 5.7872133], [1.3037996, 2.7976995, -0.12042701, 1.3688855, 1.6955005, -2.2575269, 0.080582514, 3.011721, -0.4320283, 3.2440786, -1.0321085, 1.2346085, -2.3152106, -0.9783513, 0.6837618, 1.5320586, -1.6148578, -0.94538075, 0.55978125, -4.7430468, 3.466028, 2.3792691, 1.3269067, -0.35359794, -1.5547276, 0.5202475, 1.0269136, -1.7531714, 0.43987304, -0.18845831, 2.3086758, 2.519588, 2.0116413, 0.019745048, -0.010070452, 2.496933, 1.1557871, 0.08433053, 1.375894, -1.2135965, -1.2588277, -0.31454003, 0.045949124, -1.7518936, -2.3533764, -2.0125146, 0.10255043, 1.1782314, 2.4579153, -0.8780899, -4.1442213, 3.8300152, 2.772975, 2.6803262, 0.9867382, 0.77618766, 0.46541777, 3.8959959, -2.1713195, 0.10609512, -0.26438138, -2.145317, 3.6734529, 1.4830295, -5.3445525, -10.6427765, -8.300354, -1.9608921, -6.6779685, -10.019544, -8.341513, -9.607174, -7.2441607, -3.411102, -6.180552, -8.318714, -6.060591, -7.790343, -5.9695, -7.9429936, -3.775652, -5.2827606, -3.7168224, -6.729588, -9.761094, -7.4683576, -7.2595067, -6.6790915, -9.832726, -8.352172, -6.936336, -8.252518, -6.787475, -9.091013, -11.465944, -6.712504, -8.987438, -6.946672, -8.877166, -6.7854185, -3.6417139, -6.1036086, -5.360772, -4.0435786, -4.5864973, -6.971063, -10.522461, -6.3692527, -4.387658, -9.723745, -4.7020173, -5.097396, -9.903703, -4.882414, -4.1999683, -6.7829437, -6.2555966, -8.121125, -5.334131, -9.174302, -3.9752126, -4.179469, -8.335524, -9.359406, -6.4938803, -6.794677, -8.382997, -9.879416], [1.8792984, -3.1561708, -0.8443318, -1.998743, -0.6319316, 2.4614046, -0.44511616, 0.82785237, 1.7911717, -1.8172283, 0.46574894, -1.8691323, 3.9586513, 0.8078605, 0.9049874, 5.4086914, -0.7425967, -0.20115769, -1.197923, 2.741789, 0.85432875, -1.1688408, -1.7771784, 1.615249, -4.1103697, 0.4721327, -2.75669, -0.38393462, -3.1137516, -2.2572582, 0.9580673, -3.7139492, -0.68303126, 1.6007807, 0.6313973, -2.5115106, 0.703251, 2.4844077, -1.7405633, -3.007687, 2.372802, 2.4684637, 0.6443977, -3.1433117, 0.05976736, -1.9809214, 3.514713, 2.1880944, 1.242541, 1.8236228, 0.8642841, -0.17313614, 1.7042321, 0.8298376, 4.2443194, 0.13983983, 1.1940852, 2.5076652, 39.285202, 82.73858, 44.707516, -4.267148, 0.25930226, 0.20799652, -3.7213502, 1.475217, -1.2394199, -0.0034497892, 1.1413965, 55.18923, -2.2969518, -4.1400924, -2.4707043, 43.193188, -0.19258368, 3.471275, 1.1374166, 1.2147579, 4.13017, -2.0576499, 2.1529694, -0.28360432, 0.8477302, -0.63012695, 1.2569811, 1.943168, 0.17070436, 3.2358394, -2.3737662, 0.77060974, 4.99065, 3.1079204, 3.6347675, 0.6801177, -2.2205186, 1.0961101, -2.4445753, -2.0919478, -2.895031, 2.5458927, 0.38599384, 1.0492333, -0.081834644, -7.4079595, -2.1785216, -0.7277175, -2.7413428, -3.2083786, 3.2958643, -1.1839997, 5.4849496, 2.0259023, 5.607272, -1.0125756, 3.721461, 2.5715313, 0.7741753, -0.55034757, 0.7526307, -2.6758716, -2.964664, -0.57379586, -0.28817406, -3.2334063, -0.22387607, -2.0793931, -6.4562697, 0.80134094]]} Response The API returns the following information: event_window: The event interval event_pattern: The intensity score across the time window and the overall severity of the event suspected_metrics: The set of metrics involved\nIn the following example response, each item corresponds to an event discovered in the metrics data. The algorithm finds one event in the input data of the request, as indicated by the output in event_pattern having a length of 1. event_window shows that the event occurred between time point $t$ = 52 and $t$ = 72. Lastly, suspected_metrics shows that the event involved all three metrics. { \"function_name\": \"METRICS_CORRELATION\", \"output\": { \"inference_results\": [ { \"event_window\": [ 52, 72], \"event_pattern\": [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.99625e-05, 0.0001052875, 0.0002605894, 0.00064648513, 0.0014303402, 0.002980127, 0.005871893, 0.010885878, 0.01904726, 0.031481907, 0.04920215, 0.07283493, 0.10219432, 0.1361888, 0.17257516, 0.20853643, 0.24082609, 0.26901975, 0.28376183, 0.29364157, 0.29541212, 0.2832976, 0.29041746, 0.2574534, 0.2610143, 0.22938538, 0.19999361, 0.18074994, 0.15539801, 0.13064545, 0.10544432, 0.081248805, 0.05965102, 0.041305058, 0.027082501, 0.01676033, 0.009760197, 0.005362286, 0.0027713624, 0.0013381141, 0.0006126331, 0.0002634901, 0.000106459476, 4.0407333e-05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"suspected_metrics\": [ 0, 1, 2] }] } }",
    "ancestors": [
      "Machine learning"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/ml-commons-plugin/api/",
    "title": "API",
    "content": "Table of contents Train model Request Response Getting model information Registering a model Request fields Example Response Deploying a model Example: Deploying to all available ML nodes Example: Deploying to a specific node Response Undeploying a model Example: Undeploying model from all ML nodes Response: Undeploying a model from all ML nodes Example: Undeploying specific models from specific nodes Response: Undeploying specific models from specific nodes Response: Undeploying all models from specific nodes Example: Undeploying specific models from all nodes Response: Undeploying specific models from all nodes Searching for a model Example: Querying all models Example: Querying models with algorithm “FIT_RCF” Response Deleting a model Profile Path parameters Request fields Example: Returning all tasks and models on a specific node Response: Returning all tasks and models on a specific node Predict Request Response Train and predict Example: Train and predict with indexed data Example: Train and predict with data directly Response Getting task information Searching for a task Example: Search task which “function_name” is “KMEANS” Response Deleting a task Stats Example: Get all stats Response Execute Example: Execute localization The Machine Learning (ML) commons API lets you train ML algorithms synchronously and asynchronously, make predictions with that trained model, and train and predict with the same data set.\nIn order to train tasks through the API, three inputs are required.\nAlgorithm name: Must be one of a FunctionName. This determines what algorithm the ML Engine runs. To add a new function, see How To Add a New Function.\nModel hyper parameters: Adjust these parameters to make the model train better.\nInput data: The data input that trains the ML model, or applies the ML models to predictions. You can input data in two ways, query against your index or use data frame.\nTrain model\nThe train operation trains a model based on a selected algorithm. Training can occur both synchronously and asynchronously.\nRequest\nThe following examples use the kmeans algorithm to train index data. Train with kmeans synchronously POST /_plugins/_ml/_train/kmeans { \"parameters\": { \"centroids\": 3, \"iterations\": 10, \"distance_type\": \"COSINE\" }, \"input_query\": { \"_source\": [ \"petal_length_in_cm\", \"petal_width_in_cm\"], \"size\": 10000 }, \"input_index\": [ \"iris_data\"] } Train with kmeans asynchronously POST /_plugins/_ml/_train/kmeans?async= true { \"parameters\": { \"centroids\": 3, \"iterations\": 10, \"distance_type\": \"COSINE\" }, \"input_query\": { \"_source\": [ \"petal_length_in_cm\", \"petal_width_in_cm\"], \"size\": 10000 }, \"input_index\": [ \"iris_data\"] } Response Synchronously For synchronous responses, the API returns the model_id, which can be used to get or delete a model. { \"model_id\": \"lblVmX8BO5w8y8RaYYvN\", \"status\": \"COMPLETED\" } Asynchronously For asynchronous responses, the API returns the task_id, which can be used to get or delete a task. { \"task_id\": \"lrlamX8BO5w8y8Ra2otd\", \"status\": \"CREATED\" } Getting model information\nYou can retrieve information on your model using the model_id. GET /_plugins/_ml/models/&lt;model-id&gt; The API returns information on the model, the algorithm used, and the content found within the model. { \"name\": \"KMEANS\", \"algorithm\": \"KMEANS\", \"version\": 1, \"content\": \"\" } Registering a model\nUse the register operation to register a custom model to a model index. ML Commons splits the model into smaller chunks and saves those chunks in the model’s index. POST /_plugins/_ml/models/_register Request fields\nAll request fields are required. Field Data type Description name string\nThe name of the model. version integer\nThe version number of the model. model_format string\nThe portable format of the model file. Currently only supports TORCH_SCRIPT. model_config json object\nThe model’s configuration, including the model_type, embedding_dimension, and framework_type. all_config is an optional JSON string which contains all model configurations. url string\nThe URL which contains the model. Example\nThe following example request registers a version 1.0.0 of an NLP sentence transformation model named all-MiniLM-L6-v2. POST /_plugins/_ml/models/_register { \"name\": \"all-MiniLM-L6-v2\", \"version\": \"1.0.0\", \"description\": \"test model\", \"model_format\": \"TORCH_SCRIPT\", \"model_config\": { \"model_type\": \"bert\", \"embedding_dimension\": 384, \"framework_type\": \"sentence_transformers\", }, \"url\": \"https://github.com/opensearch-project/ml-commons/raw/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip?raw=true\" } Response\nOpenSearch responds with the task_id and task status. { \"task_id\": \"ew8I44MBhyWuIwnfvDIH\", \"status\": \"CREATED\" } To see the status of your model registration, enter the task_id in the [task API] … { \"model_id\": \"WWQI44MBbzI2oUKAvNUt\", \"task_type\": \"UPLOAD_MODEL\", \"function_name\": \"TEXT_EMBEDDING\", \"state\": \"REGISTERED\", \"worker_node\": \"KzONM8c8T4Od-NoUANQNGg\", \"create_time\": 1665961344003, \"last_update_time\": 1665961373047, \"is_async\": true } Deploying a model\nThe deploy model operation reads the model’s chunks from the model index and then creates an instance of the model to cache into memory. This operation requires the model_id. POST /_plugins/_ml/models/&lt;model_id&gt;/_deploy Example: Deploying to all available ML nodes\nIn this example request, OpenSearch deploys the model to any available OpenSearch ML node: POST /_plugins/_ml/models/WWQI 44 MBbzI 2 oUKAvNUt/_deploy Example: Deploying to a specific node\nIf you want to reserve the memory of other ML nodes within your cluster, you can deploy your model to a specific node(s) by specifying the node_ids in the request body: POST /_plugins/_ml/models/WWQI 44 MBbzI 2 oUKAvNUt/_deploy { \"node_ids\": [ \"4PLK7KJWReyX0oWKnBA8nA\"] } Response { \"task_id\": \"hA8P44MBhyWuIwnfvTKP\", \"status\": \"DEPLOYING\" } Undeploying a model\nTo undeploy a model from memory, use the undeploy operation: POST /_plugins/_ml/models/&lt;model_id&gt;/_undeploy Example: Undeploying model from all ML nodes POST /_plugins/_ml/models/MGqJhYMBbbh 0 ushjm 8 p_/_undeploy Response: Undeploying a model from all ML nodes { \"s5JwjZRqTY6nOT0EvFwVdA\": { \"stats\": { \"MGqJhYMBbbh0ushjm8p_\": \"UNDEPLOYED\" } } } Example: Undeploying specific models from specific nodes POST /_plugins/_ml/models/_undeploy { \"node_ids\": [ \"sv7-3CbwQW-4PiIsDOfLxQ\"], \"model_ids\": [ \"KDo2ZYQB-v9VEDwdjkZ4\"] } Response: Undeploying specific models from specific nodes { \"sv7-3CbwQW-4PiIsDOfLxQ\": { \"stats\": { \"KDo2ZYQB-v9VEDwdjkZ4\": \"UNDEPLOYED\" } } } Response: Undeploying all models from specific nodes { \"sv7-3CbwQW-4PiIsDOfLxQ\": { \"stats\": { \"KDo2ZYQB-v9VEDwdjkZ4\": \"UNDEPLOYED\", \"-8o8ZYQBvrLMaN0vtwzN\": \"UNDEPLOYED\" } } } Example: Undeploying specific models from all nodes { \"model_ids\": [ \"KDo2ZYQB-v9VEDwdjkZ4\"] } Response: Undeploying specific models from all nodes { \"sv7-3CbwQW-4PiIsDOfLxQ\": { \"stats\": { \"KDo2ZYQB-v9VEDwdjkZ4\": \"UNDEPLOYED\" } } } Searching for a model\nUse this command to search models you’ve already created. POST /_plugins/_ml/models/_search { query } Example: Querying all models POST /_plugins/_ml/models/_search { \"query\": { \"match_all\": {} }, \"size\": 1000 } Example: Querying models with algorithm “FIT_RCF” POST /_plugins/_ml/models/_search { \"query\": { \"term\": { \"algorithm\": { \"value\": \"FIT_RCF\" } } } } Response { \"took\": 8, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 2.4159138, \"hits\": [ { \"_index\": \".plugins-ml-model\", \"_id\": \"-QkKJX8BvytMh9aUeuLD\", \"_version\": 1, \"_seq_no\": 12, \"_primary_term\": 15, \"_score\": 2.4159138, \"_source\": { \"name\": \"FIT_RCF\", \"version\": 1, \"content\": \"xxx\", \"algorithm\": \"FIT_RCF\" } }, { \"_index\": \".plugins-ml-model\", \"_id\": \"OxkvHn8BNJ65KnIpck8x\", \"_version\": 1, \"_seq_no\": 2, \"_primary_term\": 8, \"_score\": 2.4159138, \"_source\": { \"name\": \"FIT_RCF\", \"version\": 1, \"content\": \"xxx\", \"algorithm\": \"FIT_RCF\" } }] } } Deleting a model\nDeletes a model based on the model_id. DELETE /_plugins/_ml/models/&lt;model_id&gt; The API returns the following: { \"_index\": \".plugins-ml-model\", \"_id\": \"MzcIJX8BA7mbufL6DOwl\", \"_version\": 2, \"result\": \"deleted\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 27, \"_primary_term\": 18 } Profile\nThe profile operation returns runtime information on ML tasks and models. The profile operation can help debug issues with models at runtime. GET /_plugins/_ml/profile GET /_plugins/_ml/profile/models GET /_plugins/_ml/profile/tasks Path parameters Parameter Data type Description model_id\nstring\nReturns runtime data for a specific model. You can string together multiple model_id s to return multiple model profiles.\ntasks\nstring\nReturns runtime data for a specific task. You can string together multiple task_id s to return multiple task profiles. Request fields\nAll profile body request fields are optional. Field Data type Description node_ids\nstring\nReturns all tasks and profiles from a specific node.\nmodel_ids\nstring\nReturns runtime data for a specific model. You can string together multiple model_id s to return multiple model profiles.\ntask_ids\nstring\nReturns runtime data for a specific task. You can string together multiple task_id s to return multiple task profiles.\nreturn_all_tasks\nboolean\nDetermines whether or not a request returns all tasks. When set to false task profiles are left out of the response.\nreturn_all_models\nboolean\nDetermines whether or not a profile request returns all models. When set to false model profiles are left out of the response. Example: Returning all tasks and models on a specific node GET /_plugins/_ml/profile { \"node_ids\": [ \"KzONM8c8T4Od-NoUANQNGg\"], \"return_all_tasks\": true, \"return_all_models\": true } Response: Returning all tasks and models on a specific node { \"nodes\": { \"qTduw0FJTrmGrqMrxH0dcA\": { # node id \"models\": { \"WWQI44MBbzI2oUKAvNUt\": { # model id \"worker_nodes\": [ # routing table \"KzONM8c8T4Od-NoUANQNGg\"] } } },... \"KzONM8c8T4Od-NoUANQNGg\": { # node id \"models\": { \"WWQI44MBbzI2oUKAvNUt\": { # model id \"model_state\": \"DEPLOYED\", # model status \"predictor\": \"org.opensearch.ml.engine.algorithms.text_embedding.TextEmbeddingModel@592814c9\", \"worker_nodes\": [ # routing table \"KzONM8c8T4Od-NoUANQNGg\"], \"predict_request_stats\": { # predict request stats on this node \"count\": 2, # total predict requests on this node \"max\": 89.978681, # max latency in milliseconds \"min\": 5.402, \"average\": 47.6903405, \"p50\": 47.6903405, \"p90\": 81.5210129, \"p99\": 89.13291418999998 } } } },... } Predict\nML Commons can predict new data with your trained model either from indexed data or a data frame. To use the Predict API, the model_id is required. POST /_plugins/_ml/_predict/&lt;algorithm_name&gt;/&lt;model_id&gt; Request POST /_plugins/_ml/_predict/kmeans/&lt;model-id&gt; { \"input_query\": { \"_source\": [ \"petal_length_in_cm\", \"petal_width_in_cm\"], \"size\": 10000 }, \"input_index\": [ \"iris_data\"] } Response { \"status\": \"COMPLETED\", \"prediction_result\": { \"column_metas\": [ { \"name\": \"ClusterID\", \"column_type\": \"INTEGER\" }], \"rows\": [ { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 1 }] }, { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 1 }] }, { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 0 }] }, { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 0 }] }, { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 0 }] }, { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 0 }] }] } Train and predict\nUse to train and then immediately predict against the same training data set. Can only be used with unsupervised learning models and the following algorithms:\nBATCH_RCF\nFIT_RCF\nkmeans\nExample: Train and predict with indexed data POST /_plugins/_ml/_train_predict/kmeans { \"parameters\": { \"centroids\": 2, \"iterations\": 10, \"distance_type\": \"COSINE\" }, \"input_query\": { \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"k1\": { \"gte\": 0 } } }] } }, \"size\": 10 }, \"input_index\": [ \"test_data\"] } Example: Train and predict with data directly POST /_plugins/_ml/_train_predict/kmeans { \"parameters\": { \"centroids\": 2, \"iterations\": 1, \"distance_type\": \"EUCLIDEAN\" }, \"input_data\": { \"column_metas\": [ { \"name\": \"k1\", \"column_type\": \"DOUBLE\" }, { \"name\": \"k2\", \"column_type\": \"DOUBLE\" }], \"rows\": [ { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 1.00 }, { \"column_type\": \"DOUBLE\", \"value\": 2.00 }] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 1.00 }, { \"column_type\": \"DOUBLE\", \"value\": 4.00 }] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 1.00 }, { \"column_type\": \"DOUBLE\", \"value\": 0.00 }] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 10.00 }, { \"column_type\": \"DOUBLE\", \"value\": 2.00 }] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 10.00 }, { \"column_type\": \"DOUBLE\", \"value\": 4.00 }] }, { \"values\": [ { \"column_type\": \"DOUBLE\", \"value\": 10.00 }, { \"column_type\": \"DOUBLE\", \"value\": 0.00 }] }] } } Response { \"status\": \"COMPLETED\", \"prediction_result\": { \"column_metas\": [ { \"name\": \"ClusterID\", \"column_type\": \"INTEGER\" }], \"rows\": [ { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 1 }] }, { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 1 }] }, { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 1 }] }, { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 0 }] }, { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 0 }] }, { \"values\": [ { \"column_type\": \"INTEGER\", \"value\": 0 }] }] } } Getting task information\nYou can retrieve information about a task using the task_id. GET /_plugins/_ml/tasks/&lt;task_id&gt; The response includes information about the task. { \"model_id\": \"l7lamX8BO5w8y8Ra2oty\", \"task_type\": \"TRAINING\", \"function_name\": \"KMEANS\", \"state\": \"COMPLETED\", \"input_type\": \"SEARCH_QUERY\", \"worker_node\": \"54xOe0w8Qjyze00UuLDfdA\", \"create_time\": 1647545342556, \"last_update_time\": 1647545342587, \"is_async\": true } Searching for a task\nSearch tasks based on parameters indicated in the request body. GET /_plugins/_ml/tasks/_search { query body } Example: Search task which “function_name” is “KMEANS” GET /_plugins/_ml/tasks/_search { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"function_name\": \"KMEANS\" } }] } } } Response { \"took\": 12, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 0.0, \"hits\": [ { \"_index\": \".plugins-ml-task\", \"_id\": \"_wnLJ38BvytMh9aUi-Ia\", \"_version\": 4, \"_seq_no\": 29, \"_primary_term\": 4, \"_score\": 0.0, \"_source\": { \"last_update_time\": 1645640125267, \"create_time\": 1645640125209, \"is_async\": true, \"function_name\": \"KMEANS\", \"input_type\": \"SEARCH_QUERY\", \"worker_node\": \"jjqFrlW7QWmni1tRnb_7Dg\", \"state\": \"COMPLETED\", \"model_id\": \"AAnLJ38BvytMh9aUi-M2\", \"task_type\": \"TRAINING\" } }, { \"_index\": \".plugins-ml-task\", \"_id\": \"wwRRLX8BydmmU1x6I-AI\", \"_version\": 3, \"_seq_no\": 38, \"_primary_term\": 7, \"_score\": 0.0, \"_source\": { \"last_update_time\": 1645732766656, \"create_time\": 1645732766472, \"is_async\": true, \"function_name\": \"KMEANS\", \"input_type\": \"SEARCH_QUERY\", \"worker_node\": \"A_IiqoloTDK01uZvCjREaA\", \"state\": \"COMPLETED\", \"model_id\": \"xARRLX8BydmmU1x6I-CG\", \"task_type\": \"TRAINING\" } }] } } Deleting a task\nDelete a task based on the task_id.\nML Commons does not check the task status when running the Delete request. There is a risk that a currently running task could be deleted before the task completes. To check the status of a task, run GET /_plugins/_ml/tasks/&lt;task_id&gt; before task deletion. DELETE /_plugins/_ml/tasks/ { task_id } The API returns the following: { \"_index\": \".plugins-ml-task\", \"_id\": \"xQRYLX8BydmmU1x6nuD3\", \"_version\": 4, \"result\": \"deleted\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 42, \"_primary_term\": 7 } Stats\nGet statistics related to the number of tasks.\nTo receive all stats, use: GET /_plugins/_ml/stats To receive stats for a specific node, use: GET /_plugins/_ml/&lt;nodeId&gt;/stats/ To receive stats for a specific node and return a specified stat, use: GET /_plugins/_ml/&lt;nodeId&gt;/stats/&lt;stat&gt; To receive information on a specific stat from all nodes, use: GET /_plugins/_ml/stats/&lt;stat&gt; Example: Get all stats GET /_plugins/_ml/stats Response { \"zbduvgCCSOeu6cfbQhTpnQ\": { \"ml_executing_task_count\": 0 }, \"54xOe0w8Qjyze00UuLDfdA\": { \"ml_executing_task_count\": 0 }, \"UJiykI7bTKiCpR-rqLYHyw\": { \"ml_executing_task_count\": 0 }, \"zj2_NgIbTP-StNlGZJlxdg\": { \"ml_executing_task_count\": 0 }, \"jjqFrlW7QWmni1tRnb_7Dg\": { \"ml_executing_task_count\": 0 }, \"3pSSjl5PSVqzv5-hBdFqyA\": { \"ml_executing_task_count\": 0 }, \"A_IiqoloTDK01uZvCjREaA\": { \"ml_executing_task_count\": 0 } } Execute\nSome algorithms, such as Localization, don’t require trained models. You can run no-model-based algorithms using the execute API. POST _plugins/_ml/_execute/&lt;algorithm_name&gt; Example: Execute localization\nThe following example uses the Localization algorithm to find subset-level information for aggregate data (for example, aggregated over time) that demonstrates the activity of interest, such as spikes, drops, changes, or anomalies. POST /_plugins/_ml/_execute/anomaly_localization { \"index_name\": \"rca-index\", \"attribute_field_names\": [ \"attribute\"], \"aggregations\": [ { \"sum\": { \"sum\": { \"field\": \"value\" } } }], \"time_field_name\": \"timestamp\", \"start_time\": 1620630000000, \"end_time\": 1621234800000, \"min_time_interval\": 86400000, \"num_outputs\": 10 } Upon execution, the API returns the following: \"results\": [ { \"name\": \"sum\", \"result\": { \"buckets\": [ { \"start_time\": 1620630000000, \"end_time\": 1620716400000, \"overall_aggregate_value\": 65.0 }, { \"start_time\": 1620716400000, \"end_time\": 1620802800000, \"overall_aggregate_value\": 75.0, \"entities\": [ { \"key\": [ \"attr0\"], \"contribution_value\": 1.0, \"base_value\": 2.0, \"new_value\": 3.0 }, { \"key\": [ \"attr1\"], \"contribution_value\": 1.0, \"base_value\": 3.0, \"new_value\": 4.0 }, { \"key\": [ \"attr2\"], \"contribution_value\": 1.0, \"base_value\": 4.0, \"new_value\": 5.0 }, { \"key\": [ \"attr3\"], \"contribution_value\": 1.0, \"base_value\": 5.0, \"new_value\": 6.0 }, { \"key\": [ \"attr4\"], \"contribution_value\": 1.0, \"base_value\": 6.0, \"new_value\": 7.0 }, { \"key\": [ \"attr5\"], \"contribution_value\": 1.0, \"base_value\": 7.0, \"new_value\": 8.0 }, { \"key\": [ \"attr6\"], \"contribution_value\": 1.0, \"base_value\": 8.0, \"new_value\": 9.0 }, { \"key\": [ \"attr7\"], \"contribution_value\": 1.0, \"base_value\": 9.0, \"new_value\": 10.0 }, { \"key\": [ \"attr8\"], \"contribution_value\": 1.0, \"base_value\": 10.0, \"new_value\": 11.0 }, { \"key\": [ \"attr9\"], \"contribution_value\": 1.0, \"base_value\": 11.0, \"new_value\": 12.0 }] },...] } }] }",
    "ancestors": [
      "Machine learning"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/ml-commons-plugin/cluster-settings/",
    "title": "ML Commons cluster settings",
    "content": "To enhance and customize your OpenSearch cluster for machine learning (ML), you can add and modify several configuration settings for the ML Commons plugin in your ‘opensearch.yml’ file.\nRun tasks and models on ML nodes only\nIf true, ML Commons tasks and models run machine learning (ML) tasks on ML nodes only. If false, tasks and models run on ML nodes first. If no ML nodes exist, tasks and models run on data nodes. We recommend that you do not set this value to “false” on production clusters.\nSetting plugins.ml_commons.only_run_on_ml_node: true Values\nDefault value: true Value range: true or false Dispatch tasks to ML node round_robin dispatches ML tasks to ML nodes using round robin routing. least_load gathers runtime information from all ML nodes, like JVM heap memory usage and running tasks, and then dispatches the tasks to the ML node with the lowest load.\nSetting plugins.ml_commons.task_dispatch_policy: round_robin Values\nDefault value: round_robin Value range: round_robin or least_load Set number of ML tasks per node\nSets the number of ML tasks that can run on each ML node. When set to 0, no ML tasks run on any nodes.\nSetting plugins.ml_commons.max_ml_task_per_node: 10 Values\nDefault value: 10 Value range: [0, 10,000]\nSet number of ML models per node\nSets the number of ML models that can be deployed to each ML node. When set to 0, no ML models can deploy on any node.\nSetting plugins.ml_commons.max_model_on_node: 10 Values\nDefault value: 10 Value range: [0, 10,000]\nSet sync job intervals\nWhen returning runtime information with the Profile API, ML Commons will run a regular job to sync newly deployed or undeployed models on each node. When set to 0, ML Commons immediately stops sync-up jobs.\nSetting plugins.ml_commons.sync_up_job_interval_in_seconds: 3 Values\nDefault value: 3 Value range: [0, 86,400]\nPredict monitoring requests\nControls how many predict requests are monitored on one node. If set to 0, OpenSearch clears all monitoring predict requests in cache and does not monitor for new predict requests.\nSetting plugins.ml_commons.monitoring_request_count: 100 Value range\nDefault value: 100 Value range: [0, 10,000,000]\nUpload model tasks per node\nControls how many upload model tasks can run in parallel on one node. If set to 0, you cannot upload models to any node.\nSetting plugins.ml_commons.max_upload_model_tasks_per_node: 10 Values\nDefault value: 10 Value range: [0, 10]\nLoad model tasks per node\nControls how many load model tasks can run in parallel on one node. If set to 0, you cannot load models to any node.\nSetting plugins.ml_commons.max_load_model_tasks_per_node: 10 Values\nDefault value: 10 Value range: [0, 10]\nAdd trusted URL\nThe default value allows you to upload a model file from any http/https/ftp/local file. You can change this value to restrict trusted model URLs.\nSetting\nThe default URL value for this trusted URL setting is not secure. To ensure the security, please use you own regex string to the trusted repository that contains your models, for example https://github.com/opensearch-project/ml-commons/blob/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/*. plugins.ml_commons.trusted_url_regex: &lt;model-repository-url&gt; Values\nDefault value: \"^(https?|ftp|file)://[-a-zA-Z0-9+&amp;@#/%?=~_|!:,.;]*[-a-zA-Z0-9+&amp;@#/%=~_|]\" Value range: Java regular expression (regex) string\nAssign task timeout\nAssigns how long in seconds an ML task will live. After the timeout, the task will fail.\nSetting plugins.ml_commons.ml_task_timeout_in_seconds: 600 Values\nDefault value: 600\nValue range: [1, 86,400]\nSet native memory threshold\nSets a circuit breaker that checks all system memory usage before running an ML task. If the native memory exceeds the threshold, OpenSearch throws an exception and stops running any ML task.\nValues are based on the percentage of memory available. When set to 0, no ML tasks will run. When set to 100, the circuit breaker closes and no threshold exists.\nSetting plugins.ml_commons.native_memory_threshold: 90 Values\nDefault value: 90\nValue range: [0, 100]\nAllow custom deployment plans\nWhen enabled, this setting grants users the ability to deploy models to specific ML nodes according to that user’s permissions.\nSetting plugins.ml_commons.allow_custom_deployment_plan: false Values\nDefault value: false\nValue range: [false, true]\nEnable auto redeploy\nThis setting automatically redeploys deployed or partially deployed models upon cluster failure. If all ML nodes inside a cluster crash, the model switches to the DEPLOYED_FAILED state, and the model must be deployed manually.\nSetting plugins.ml_commons.model_auto_redeploy.enable: false Values\nDefault value: false\nValue range: [false, true]\nSet retires for auto redeploy\nThis setting sets the limit for the number of times a deployed or partially deployed model will try and redeploy when ML nodes in a cluster fail or new ML nodes join the cluster.\nSetting plugins.ml_commons.model_auto_redeploy.lifetime_retry_times: 3 Values\nDefault value: 3\nValue range: [0, 100]\nSet auto redeploy success ratio\nThis setting sets the ratio of success for the auto-redeployment of a model based on the available ML nodes in a cluster. For example, if ML nodes crash inside a cluster, the auto redeploy protocol adds another node or retires a crashed node. If the ratio is 0.7 and 70% of all ML nodes successfully redeploy the model on auto-redeploy activation, the redeployment is a success. If the model redeploys on fewer than 70% of available ML nodes, the auto-redeploy retries until the redeployment succeeds or OpenSearch reaches the maximum number of retries.\nSetting plugins.ml_commons.model_auto_redeploy_success_ratio: 0.8 Values\nDefault value: 0.8\nValue range: [0, 1]",
    "ancestors": [
      "Machine learning"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/ml-commons-plugin/gpu-acceleration/",
    "title": "GPU acceleration",
    "content": "GPU acceleration is an experimental feature. For updates on the progress of GPU acceleration, or if you want to leave feedback that could help improve the feature, join the discussion in the OpenSearch forum.\nWhen running a natural language processing (NLP) model in your OpenSearch cluster with a machine learning (ML) node, you can achieve better performance on the ML node using graphics processing unit (GPU) acceleration. GPUs can work in tandem with the CPU of your cluster to speed up the model upload and training.\nSupported GPUs\nCurrently, ML nodes following GPU instances: NVIDIA instances with CUDA 11.6 AWS Inferentia If you need GPU power, you can provision GPU instances through Amazon Elastic Compute Cloud (Amazon EC2). For more information on how to provision a GPU instance, see Recommended GPU Instances.\nSupported images\nYou can use GPU acceleration with both Docker images with CUDA 11.6 and Amazon Machine Images (AMIs).\nPyTorch\nGPU-accelerated ML nodes require PyTorch 1.12.1 work with ML models.\nSetting up a GPU-accelerated ML node\nDepending on the GPU, you can provision a GPU-accelerated ML node manually or by using automated initialization scripts.\nPreparing an NVIDIA ML node\nNVIDIA uses CUDA to increase node performance. In order to take advantage of CUDA, you need to make sure that your drivers include the nvidia-uvm kernel inside the /dev directory. To check for the kernel, enter ls -al /dev | grep nvidia-uvm.\nIf the nvidia-uvm kernel does not exist, run nvidia-uvm-init.sh: #!/bin/bash\n## Script to initialize nvidia device nodes.\n## https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-verifications\n/sbin/modprobe nvidia\nif [ \"$?\" -eq 0]; then\n# Count the number of NVIDIA controllers found.\nNVDEVS=`lspci | grep -i NVIDIA`\nN3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l`\nNVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l`\nN=`expr $N3D + $NVGA - 1`\nfor i in `seq 0 $N`; do\nmknod -m 666 /dev/nvidia$i c 195 $i\ndone\nmknod -m 666 /dev/nvidiactl c 195 255\nelse\nexit 1\nfi\n/sbin/modprobe nvidia-uvm\nif [ \"$?\" -eq 0]; then\n# Find out the major device number used by the nvidia-uvm driver\nD=`grep nvidia-uvm /proc/devices | awk '{print $1}'`\nmknod -m 666 /dev/nvidia-uvm c $D 0\nmknod -m 666 /dev/nvidia-uvm-tools c $D 0\nelse\nexit 1\nfi After verifying that nvidia-uvm exists under /dev, you can start OpenSearch inside your cluster.\nPreparing AWS Inferentia ML node\nDepending on the Linux operating system running on AWS Inferentia, you can use the following commands and scripts to provision an ML node and run OpenSearch inside your cluster.\nTo start, download and install OpenSearch on your cluster.\nThen export OpenSearch and set up your environment variables. This example exports OpenSearch into the directory opensearch-2.5.0, so OPENSEARCH_HOME = opensearch-2.5.0: echo \"export OPENSEARCH_HOME=~/opensearch-2.5.0\" | tee -a ~/.bash_profile\necho \"export PYTORCH_VERSION=1.12.1\" | tee -a ~/.bash_profile\nsource ~/.bash_profile Next, create a shell script file called prepare_torch_neuron.sh. You can copy and customize one of the following examples based on your Linux operating system: Ubuntu 20.04 Amazon Linux 2 After you’ve run the scripts, exit your current terminal and open a new terminal to start OpenSearch.\nGPU acceleration has only been tested on Ubuntu 20.04 and Amazon Linux 2. However, you can use other Linux operating systems.\nUbuntu 20.04. /etc/os-release\nsudo tee /etc/apt/sources.list.d/neuron.list &gt; /dev/null &lt;&lt;EOF\ndeb https://apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main\nEOF\nwget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -\n# Update OS packages\nsudo apt-get update -y\n################################################################################################################\n# To install or update to Neuron versions 1.19.1 and newer from previous releases:\n# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver\n################################################################################################################\n# Install OS headers\nsudo apt-get install linux-headers-$(uname -r) -y\n# Install Neuron Driver\nsudo apt-get install aws-neuronx-dkms -y\n####################################################################################\n# Warning: If Linux kernel is updated as a result of OS package update\n# Neuron driver (aws-neuron-dkms) should be re-installed after reboot\n####################################################################################\n# Install Neuron Tools\nsudo apt-get install aws-neuronx-tools -y\n######################################################\n# Only for Ubuntu 20 - Install Python3.7\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt-get install python3.7\n######################################################\n# Install Python venv and activate Python virtual environment to install\n# Neuron pip packages.\ncd ~\nsudo apt-get install -y python3.7-venv g++\npython3.7 -m venv pytorch_venv\nsource pytorch_venv/bin/activate\npip install -U pip\n# Set pip repository to point to the Neuron repository\npip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n#Install Neuron PyTorch\npip install torch-neuron torchvision\n# If you need to trace the neuron model, install torch neuron with this command\n# pip install torch-neuron neuron-cc[tensorflow] \"protobuf==3.20.1\" torchvision\n# If you need to trace neuron model, install the transformers for tracing the Huggingface model.\n# pip install transformers\n# Copy torch neuron lib to OpenSearch\nPYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/\nmkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron\nexport PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so\necho \"export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so\" | tee -a ~/.bash_profile\n# Increase JVm stack size to &gt;=2MB\necho \"-Xss2m\" | tee -a $OPENSEARCH_HOME/config/jvm.options\n# Increase max file descriptors to 65535\necho \"$(whoami) - nofile 65535\" | sudo tee -a /etc/security/limits.conf\n# max virtual memory areas vm.max_map_count to 262144\nsudo sysctl -w vm.max_map_count=262144 Amazon Linux 2 # Configure Linux for Neuron repository updates\nsudo tee /etc/yum.repos.d/neuron.repo &gt; /dev/null &lt;&lt;EOF\n[neuron]\nname=Neuron YUM Repository\nbaseurl=https://yum.repos.neuron.amazonaws.com\nenabled=1\nmetadata_expire=0\nEOF\nsudo rpm --import https://yum.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\n# Update OS packages\nsudo yum update -y\n################################################################################################################\n# To install or update to Neuron versions 1.19.1 and newer from previous releases:\n# - DO NOT skip 'aws-neuron-dkms' install or upgrade step, you MUST install or upgrade to latest Neuron driver\n################################################################################################################\n# Install OS headers\nsudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) -y\n# Install Neuron Driver\n####################################################################################\n# Warning: If Linux kernel is updated as a result of OS package update\n# Neuron driver (aws-neuron-dkms) should be re-installed after reboot\n####################################################################################\nsudo yum install aws-neuronx-dkms -y\n# Install Neuron Tools\nsudo yum install aws-neuronx-tools -y\n# Install Python venv and activate Python virtual environment to install\n# Neuron pip packages.\ncd ~\nsudo yum install -y python3.7-venv gcc-c++\npython3.7 -m venv pytorch_venv\nsource pytorch_venv/bin/activate\npip install -U pip\n# Set Pip repository to point to the Neuron repository\npip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n# Install Neuron PyTorch\npip install torch-neuron torchvision\n# If you need to trace the neuron model, install torch neuron with this command\n# pip install torch-neuron neuron-cc[tensorflow] \"protobuf&lt;4\" torchvision\n# If you need to run the trace neuron model, install transformers for tracing Huggingface model.\n# pip install transformers\n# Copy torch neuron lib to OpenSearch\nPYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/\nmkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron\nexport PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so\necho \"export PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so\" | tee -a ~/.bash_profile\n# Increase JVm stack size to &gt;=2MB\necho \"-Xss2m\" | tee -a $OPENSEARCH_HOME/config/jvm.options\n# Increase max file descriptors to 65535\necho \"$(whoami) - nofile 65535\" | sudo tee -a /etc/security/limits.conf\n# max virtual memory areas vm.max_map_count to 262144\nsudo sysctl -w vm.max_map_count=262144 When the script completes running, open a new terminal for the settings to take effect. Then, start OpenSearch.\nOpenSearch should now be running inside your GPU-accelerated cluster. However, if any errors occur during provisioning, you can install the GPU accelerator drivers manually.\nPrepare ML node manually\nIf the previous two scripts do not provision your GPU-accelerated node properly, you can install the drivers for AWS Inferentia manually:\nDeploy an AWS accelerator instance based on your chosen Linux operating system. For instructions, see Deploy on AWS accelerator instance.\nCopy the Neuron library into OpenSearch. The following command uses a directory named opensearch-2.5.0: OPENSEARCH_HOME=~/opensearch-2.5.0 Set the PYTORCH_EXTRA_LIBRARY_PATH path. In this example, we create a pytorch virtual environment in the OPENSEARCH_HOME folder: PYTORCH_NEURON_LIB_PATH=~/pytorch_venv/lib/python3.7/site-packages/torch_neuron/lib/\nmkdir -p $OPENSEARCH_HOME/lib/torch_neuron; cp -r $PYTORCH_NEURON_LIB_PATH/ $OPENSEARCH_HOME/lib/torch_neuron\nexport PYTORCH_EXTRA_LIBRARY_PATH=$OPENSEARCH_HOME/lib/torch_neuron/lib/libtorchneuron.so (Optional) To monitor the GPU usage of your accelerator instance, install Neuron tools, which allows models to be used inside your instance: # Install Neuron Tools\nsudo apt-get install aws-neuronx-tools -y # Add Neuron tools your PATH\nexport PATH=/opt/aws/neuron/bin:$PATH # Test Neuron tools\nneuron-top To make sure you have enough memory to upload a model, increase the JVM stack size to &gt;+2MB: echo \"-Xss2m\" | sudo tee -a $OPENSEARCH_HOME/config/jvm.options Start OpenSearch.\nTroubleshooting\nDue to the amount of data required to work with ML models, you might encounter the following max file descriptors or vm.max_map_count errors when trying to run OpenSearch in a your cluster: [1]: max file descriptors [8192] for opensearch process is too low, increase to at least [65535]\n[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] To troubleshoot the max file descriptors error, run the following command: echo \"$(whoami) - nofile 65535\" | sudo tee -a /etc/security/limits.conf To fix the vm.max_map_count error, run this command to increase the count to 262114: sudo sysctl -w vm.max_map_count=262144 Next steps\nIf you want to try a GPU-accelerated cluster using AWS Inferentia with a pretrained HuggingFace model, see Compiling and Deploying HuggingFace Pretrained BERT.",
    "ancestors": [
      "Machine learning",
      "Model-serving framework"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/ml-commons-plugin/index/",
    "title": "About ML Commons",
    "content": "ML Commons for OpenSearch eases the development of machine learning features by providing a set of common machine learning (ML) algorithms through transport and REST API calls. Those calls choose the right nodes and resources for each ML request and monitors ML tasks to ensure uptime. This allows you to leverage existing open-source ML algorithms and reduce the effort required to develop new ML features.\nInteraction with the ML Commons plugin occurs through either the REST API or ad and kmeans Piped Processing Language (PPL) commands.\nModels trained through the ML Commons plugin support model-based algorithms such as kmeans. After you’ve trained a model enough so that it meets your precision requirements, you can apply the model to predict new data safely.\nShould you not want to use a model, you can use the Train and Predict API to test your model without having to evaluate the model’s performance.\nPermissions\nThere are two reserved user roles that can use of the ML Commons plugin. ml_full_access: Full access to all ML features, including starting new ML tasks and reading or deleting models. ml_readonly_access: Can only read ML tasks, trained models and statistics relevant to the model’s cluster. Cannot start nor delete ML tasks or models.\nML node\nTo prevent your cluster from failing when running ML tasks, you configure a node with the ml node role. When configuring without the data node role, ML nodes will not store any shards and will calculate resource requirements at runtime. To use an ML node, create a node in your opensearch.yml file. Give your node a custom name and define the node role as ml: node.name: ml-node node.roles: [ ml]",
    "ancestors": [
      "Machine learning"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/ml-commons-plugin/ml-dashboard/",
    "title": "Managing ML models in OpenSearch Dashboards",
    "content": "Released in OpenSearch 2.6, the machine learning (ML) functionality in OpenSearch Dashboards is experimental and can’t be used in a production environment. For updates or to leave feedback, see the OpenSearch Forum discussion.\nAdministrators of machine learning (ML) clusters can use OpenSearch Dashboards to manage and check the status of ML models running inside a cluster. This can help ML developers provision nodes to ensure their models run efficiently.\nAs of OpenSearch 2.6, you can only upload models using the API. For more information about how to upload a model to your cluster, see Upload model to OpenSearch.\nEnabling ML in Dashboards\nIn OpenSearch 2.6, ML functionality is disabled by default. To enable it, you need to edit the configuration in opensearch_dashboards.yml and then restart your cluster.\nTo enable the feature:\nIn your OpenSearch cluster, navigate to your Dashboards home directory; for example, in Docker, /usr/share/opensearch-dashboards.\nOpen your local copy of the Dashboards configuration file opensearch_dashboards.yml. If you don’t have a copy, get one from GitHub: opensearch_dashboards.yml.\nAdd the setting ml_commons_dashboards.enabled: to opensearch_dashboards.yml. Then, set it to ml_commons_dashboards.enabled: true and save the configuration file.\nRestart the Dashboards container.\nVerify that the feature configuration settings were created and configured properly by launching OpenSearch Dashboards. The Machine Learning section should appear under OpenSearch plugins.\nAccessing ML functionality in Dashboards\nTo access ML functionality in OpenSearch Dashboards,select OpenSearch plugins &gt; Machine Learning. In the Machine Learning section, you now have access to the Deployed models dashboard.\nDeployed models dashboard\nThe deployed models dashboard gives admins the ability to check the status of any models stored inside your OpenSearch cluster. The dashboard includes the following information about the model: Name: The name of the model given upon upload. Status: The number of nodes for which the model responds.\nWhen all nodes are responsive, the status is Green.\nWhen some nodes are responsive,the status is Yellow.\nWhen all nodes are unresponsive,the status is Red. Model ID: The model ID. Action: What actions you can take with the model.\nAs of OpenSearch 2.6, the only action available is View Status Details, shown in the following image. When selected, the Status Details panel appears.\nThe panel provides the following details inside the panel: Model ID Model status by node: The number of nodes for which the model is responsive.\nA list of nodes gives you a view of each node the model is running on, including each node’s Node ID and status, as shown in the following image. This is useful if you want to use the node’s Node ID to determine why a node is unresponsive. Next steps\nFor more information about how to manage ML models in OpenSearch, see Model-serving framework.",
    "ancestors": [
      "Machine learning"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/ml-commons-plugin/model-serving-framework/",
    "title": "Model-serving framework",
    "content": "The model-serving framework is an experimental feature. For updates on the progress of the model-serving framework, or if you want to leave feedback that could help improve the feature, join the discussion in the Model-serving framework forum.\nML Commons allows you to serve custom models and use those models to make inferences. For those who want to run their PyTorch deep learning model inside an OpenSearch cluster, you can upload and run that model with the ML Commons REST API.\nThis page outlines the steps required to upload a custom model and run it with the ML Commons plugin.\nPrerequisites\nTo upload a custom model to OpenSearch, you need to prepare it outside of your OpenSearch cluster. You can use a pretrained model, like one from Huggingface, or train a new model in accordance with your needs.\nModel support\nAs of OpenSearch 2.6, the model-serving framework supports text embedding models.\nModel format\nTo use a model in OpenSearch, you’ll need to export the model into a portable format. As of Version 2.5, OpenSearch only supports the TorchScript and ONNX formats.\nFurthermore, files must be saved as zip files before upload. Therefore, to ensure that ML Commons can upload your model, compress your TorchScript file before uploading. You can download an example file here.\nModel size\nMost deep learning models are more than 100 MB, making it difficult to fit them into a single document. OpenSearch splits the model file into smaller chunks to be stored in a model index. When allocating machine learning (ML) or data nodes for your OpenSearch cluster, make sure you correctly size your ML nodes so that you have enough memory when making ML inferences.\nGPU acceleration\nTo achieve better performance within the model-serving framework, you can take advantage of GPU acceleration on your ML node. For more information, see GPU acceleration.\nUpload model to OpenSearch\nUse the URL upload operation for models that already exist on another server, such as GitHub or S3. POST /_plugins/_ml/models/_upload The URL upload method requires the following request fields. Field Data type Description name String\nThe name of the model. version String\nThe version number of the model. Since OpenSearch does not enforce a specific version schema for models, you can choose any number or format that makes sense for your models. model_format String\nThe portable format of the model file. Currently only supports TORCH_SCRIPT. model_config JSON object\nThe model’s configuration, including the model_type, embedding_dimension, and framework_type. url string\nThe URL where the model is located. The model_config object Field Data type Description model_type String\nThe model type, such as bert. For a Huggingface model, the model type is specified in config.json. For an example, see the all-MiniLM-L6-v2 Huggingface model config.json. embedding_dimension Integer\nThe dimension of the model-generated dense vector. For a Huggingface model, the dimension is specified in the model card. For example, in the all-MiniLM-L6-v2 Huggingface model card, the statement 384 dimensional dense vector space specifies 384 as the embedding dimension. framework_type String\nThe framework the model is using. Currently, we support sentence_transformers and huggingface_transformers frameworks. The sentence_transformers model outputs text embeddings directly, so ML Commons does not perform any post processing. For huggingface_transformers, ML Commons performs post processing by applying mean pooling to get text embeddings. See the example all-MiniLM-L6-v2 Huggingface model for more details. all_config (Optional) String\nThis field is used for reference purposes. You can specify all model configurations in this field. For example, if you are using a Huggingface model, you can minify the config.json file to one line and save its contents in the all_config field. Once the model is uploaded, you can use the get model API operation to get all model configurations stored in this field. You can further customize a pre-trained sentence transformer model’s post-processing logic with the following optional fields in the model_config object. Field Data type Description pooling_mode String\nThe post-process model output, either mean, mean_sqrt_len, max, weightedmean, or cls. normalize_result Boolean\nWhen set to true, normalizes the model output in order to scale to a standard range for the model. Example request\nThe following example request uploads version 1.0.0 of a natural language processing (NLP) sentence transformation model named all-MiniLM-L6-v2: POST /_plugins/_ml/models/_upload { \"name\": \"all-MiniLM-L6-v2\", \"version\": \"1.0.0\", \"description\": \"test model\", \"model_format\": \"TORCH_SCRIPT\", \"model_config\": { \"model_type\": \"bert\", \"embedding_dimension\": 384, \"framework_type\": \"sentence_transformers\" }, \"url\": \"https://github.com/opensearch-project/ml-commons/raw/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip?raw=true\" } Example response\nOpenSearch responds with the task_id and task status: { \"task_id\": \"ew8I44MBhyWuIwnfvDIH\", \"status\": \"CREATED\" } To see the status of your model upload, pass the task_id into the task API.\nLoad the model\nThe load model operation reads the model’s chunks from the model index and then creates an instance of the model to load into memory. The bigger the model, the more chunks the model is split into. The more chunks a model index contains, the longer it takes for the model to load into memory.\nGet the model_id To load a model, you need the model_id. To find the model_id, take the task_id from the model’s upload operations API response and use the GET _ml/tasks API.\nThis example request uses the task_id from the upload example. GET /_plugins/_ml/tasks/ew 8 I 44 MBhyWuIwnfvDIH OpenSearch responds with the model_id: { \"model_id\": \"WWQI44MBbzI2oUKAvNUt\", \"task_type\": \"UPLOAD_MODEL\", \"function_name\": \"TEXT_EMBEDDING\", \"state\": \"COMPLETED\", \"worker_node\": \"KzONM8c8T4Od-NoUANQNGg\", \"create_time\": 3455961564003, \"last_update_time\": 3216361373241, \"is_async\": true } Load the model from the model index\nWith the model_id, you can now load the model from the model’s index in order to deploy the model to ML nodes. The load API reads model chunks from the model index, creates an instance of that model, and saves the model instance in the ML node’s cache.\nAdd the model_id to the load API: POST /_plugins/_ml/models/&lt;model_id&gt;/_load By default, the ML Commons setting plugins.ml_commons.only_run_on_ml_node is set to false. When false, models load on ML nodes first. If no ML nodes exist, models load on data nodes. When running ML models in production, set plugins.ml_commons.only_run_on_ml_node to true so that models only load on ML nodes.\nExample request: Load into any available ML node\nIn this example request, OpenSearch loads the model into all available OpenSearch node: POST /_plugins/_ml/models/WWQI 44 MBbzI 2 oUKAvNUt/_load Example request: Load into a specific node\nIf you want to reserve the memory of other ML nodes within your cluster, you can load your model into a specific node(s) by specifying each node’s ID in the request body: POST /_plugins/_ml/models/WWQI 44 MBbzI 2 oUKAvNUt/_load { \"node_ids\": [ \"4PLK7KJWReyX0oWKnBA8nA\"] } Example response\nAll models load asynchronously. Therefore, the load API responds with a new task_id based on the load and responds with a new status for the task. { \"task_id\": \"hA8P44MBhyWuIwnfvTKP\", \"status\": \"CREATED\" } Check the model load status\nWith your task_id from the load response, you can use the GET _ml/tasks API to see the load status of your model. Before a loaded model can be used for inferences, the load task’s state must be COMPLETED.\nExample request GET /_plugins/_ml/tasks/hA 8 P 44 MBhyWuIwnfvTKP Example response { \"model_id\": \"WWQI44MBbzI2oUKAvNUt\", \"task_type\": \"LOAD_MODEL\", \"function_name\": \"TEXT_EMBEDDING\", \"state\": \"COMPLETED\", \"worker_node\": \"KzONM8c8T4Od-NoUANQNGg\", \"create_time\": 1665961803150, \"last_update_time\": 1665961815959, \"is_async\": true } Use the loaded model for inferences\nAfter the model has been loaded, you can enter the model_id into the predict API to perform inferences. POST /_plugins/_ml/models/&lt;model_id&gt;/_predict Example request POST /_plugins/_ml/_predict/text_embedding/WWQI 44 MBbzI 2 oUKAvNUt { \"text_docs\":[ \"today is sunny\"], \"return_number\": true, \"target_response\": [ \"sentence_embedding\"] } Example response { \"inference_results\": [ { \"output\": [ { \"name\": \"sentence_embedding\", \"data_type\": \"FLOAT32\", \"shape\": [ 384], \"data\": [ -0.023315024, 0.08975691, 0.078479774,...] }] }] } Unload the model\nIf you’re done making predictions with your model, use the unload operation to remove the model from your memory cache. The model will remain accessible in the model index. POST /_plugins/_ml/models/&lt;model_id&gt;/_unload Example request POST /_plugins/_ml/models/MGqJhYMBbbh 0 ushjm 8 p_/_unload Example response { \"s5JwjZRqTY6nOT0EvFwVdA\": { \"stats\": { \"MGqJhYMBbbh0ushjm8p_\": \"deleted\" } } }",
    "ancestors": [
      "Machine learning"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/ml-commons-plugin/pretrained-models/",
    "title": "Pretrained models",
    "content": "The model-serving framework is an experimental feature. For updates on the progress of the model-serving framework, or if you want to leave feedback that could help improve the feature, join the discussion in the Model-serving framework forum.\nThe model-serving framework supports a variety of open-source pretrained models that can assist with a range of machine learning (ML) search and analytics use cases.\nUploading pretrained models\nTo use a pretrained model in your OpenSearch cluster:\nSelect the model you want to upload. For a list of pretrained models, see supported pretrained models.\nUpload the model using the upload API. Because a pretrained model originates from the ML Commons model repository, you only need to provide the name, version, and model_format in the upload API request. POST /_plugins/_ml/models/_upload\n{\n\"name\": \"huggingface/sentence-transformers/all-MiniLM-L12-v2\",\n\"version\": \"1.0.1\",\n\"model_format\": \"TORCH_SCRIPT\"\n} For more information on how to upload and use ML models, see Model-serving framework.\nSupported pretrained models\nThe model-serving framework supports the following models, categorized by type. All models are traced from Hugging Face. Although models with the same type will have similar use cases, each model has a different model size and performs differently depending on your cluster. For a comparison of the performances of some pretrained models, see the sbert documentation.\nSentence transformers\nSentence transformer models map sentences and paragraphs across a dimensional dense vector space. The number of vectors depends on the model. Use these models for use cases such as clustering and semantic search.\nThe following table provides a list of sentence transformer models and artifact links to download them. As of OpenSearch 2.6, all artifacts are set to version 1.0.1. Model name Vector dimensions Auto-truncation Torchscript artifact ONNX artifact sentence-transformers/all-distilroberta-v1 768-dimensional dense vector space.\nYes\n- model_url - config_url - model_url - config_url sentence-transformers/all-MiniLM-L6-v2 384-dimensional dense vector space.\nYes\n- model_url - config_url - model_url - config_url sentence-transformers/all-MiniLM-L12-v2 384-dimensional dense vector space.\nYes\n- model_url - config_url - model_url - config_url sentence-transformers/all-mpnet-base-v2 768-dimensional dense vector space.\nYes\n- model_url - config_url - model_url - config_url sentence-transformers/msmarco-distilbert-base-tas-b 768-dimensional dense vector space. Optimized for semantic search.\nNo\n- model_url - config_url - model_url - config_url sentence-transformers/multi-qa-MiniLM-L6-cos-v1 384 dimensional dense vector space. Designed for semantic search and trained on 215 million question/answer pairs.\nYes\n- model_url - config_url - model_url - config_url sentence-transformers/multi-qa-mpnet-base-dot-v1 384 dimensional dense vector space.\nYes\n- model_url - config_url - model_url - config_url sentence-transformers/paraphrase-MiniLM-L3-v2 384-dimensional dense vector space.\nYes\n- model_url - config_url - model_url - config_url sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 384-dimensional dense vector space.\nYes\n- model_url - config_url - model_url - config_url",
    "ancestors": [
      "Machine learning",
      "Model-serving framework"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/monitoring-your-cluster/job-scheduler/index/",
    "title": "Job Scheduler",
    "content": "The OpenSearch Job Scheduler plugin provides a framework that can be used to build schedules for common tasks performed on your cluster. You can use Job Scheduler’s Service Provider Interface (SPI) to define schedules for cluster management tasks such as taking snapshots, managing your data’s lifecycle, and running periodic jobs. Job Scheduler has a sweeper that listens for updated events on the OpenSearch cluster and a scheduler that manages when jobs run.\nYou can install the Job Scheduler plugin by following the standard OpenSearch plugin installation process. The sample-extension-plugin example provided in the Job Scheduler GitHub repository provides a complete example of utilizing Job Scheduler when building a plugin. To define schedules, you build a plugin that implements the interfaces provided in the Job Scheduler library. You can schedule jobs by specifying an interval, or you can use a Unix cron expression such as 0 12 * *?, which runs at noon every day, to define a more flexible schedule.\nBuilding a plugin for Job Scheduler\nOpenSearch plugin developers can extend the Job Scheduler plugin to schedule jobs to perform on the cluster. Jobs you can schedule include running aggregation queries against raw data, saving the aggregated data to a new index every hour, or continuing to monitor the shard allocation by calling the OpenSearch API and then posting the output to a webhook.\nFor examples of building a plugin that uses the Job Scheduler plugin, see the Job Scheduler README.\nDefining an endpoint\nYou can configure your plugin’s API endpoint by referencing the example SampleExtensionRestHandler.java file. Set the endpoint URL that your plugin will expose with WATCH_INDEX_URI: public class SampleExtensionRestHandler extends BaseRestHandler { public static final String WATCH_INDEX_URI = \"/_plugins/scheduler_sample/watch\"; You can define the job configuration by extending ScheduledJobParameter. You can also define the fields used by your plugin, like indexToWatch, as shown in the example SampleJobParameter file. This job configuration will be saved as a document in an index you define, as shown in this example.\nConfiguring parameters\nYou can configure your plugin’s parameters by referencing the example SampleJobParameter.java file and modifying it to fit your needs: /**\n* A sample job parameter.\n* &lt;p&gt;\n* It adds an additional \"indexToWatch\" field to {@link ScheduledJobParameter}, which stores the index\n* the job runner will watch.\n*/ public class SampleJobParameter implements ScheduledJobParameter { public static final String NAME_FIELD = \"name\"; public static final String ENABLED_FILED = \"enabled\"; public static final String LAST_UPDATE_TIME_FIELD = \"last_update_time\"; public static final String LAST_UPDATE_TIME_FIELD_READABLE = \"last_update_time_field\"; public static final String SCHEDULE_FIELD = \"schedule\"; public static final String ENABLED_TIME_FILED = \"enabled_time\"; public static final String ENABLED_TIME_FILED_READABLE = \"enabled_time_field\"; public static final String INDEX_NAME_FIELD = \"index_name_to_watch\"; public static final String LOCK_DURATION_SECONDS = \"lock_duration_seconds\"; public static final String JITTER = \"jitter\"; private String jobName; private Instant lastUpdateTime; private Instant enabledTime; private boolean isEnabled; private Schedule schedule; private String indexToWatch; private Long lockDurationSeconds; private Double jitter; Next, configure the request parameters you would like your plugin to use with Job Scheduler. These will be based on the variables you declare when configuring your plugin. The following example shows the request parameters you set when building your plugin: public SampleJobParameter ( String id, String name, String indexToWatch, Schedule schedule, Long lockDurationSeconds, Double jitter) { this. jobName = name; this. indexToWatch = indexToWatch; this. schedule = schedule; Instant now = Instant. now (); this. isEnabled = true; this. enabledTime = now; this. lastUpdateTime = now; this. lockDurationSeconds = lockDurationSeconds; this. jitter = jitter; } @Override public String getName () { return this. jobName; } @Override public Instant getLastUpdateTime () { return this. lastUpdateTime; } @Override public Instant getEnabledTime () { return this. enabledTime; } @Override public Schedule getSchedule () { return this. schedule; } @Override public boolean isEnabled () { return this. isEnabled; } @Override public Long getLockDurationSeconds () { return this. lockDurationSeconds; } @Override public Double getJitter () { return jitter; } The following table describes the request parameters configured in the previous example. All the request parameters shown are required. Field Data type Description getName\nString\nReturns the name of the job.\ngetLastUpdateTime\nTime unit\nReturns the time that the job was last run.\ngetEnabledTime\nTime unit\nReturns the time that the job was enabled.\ngetSchedule\nUnix cron\nReturns the job schedule formatted in Unix cron syntax.\nisEnabled\nBoolean\nIndicates whether or not the job is enabled.\ngetLockDurationSeconds\nInteger\nReturns the duration of time for which the job is locked.\ngetJitter\nInteger\nReturns the defined jitter value. The logic used by your job should be defined by a class extended from ScheduledJobRunner in the SampleJobParameter.java sample file, such as SampleJobRunner. While the job is running, there is a locking mechanism you can use to prevent other nodes from running the same job. First, acquire the lock. Then make sure to release the lock before the job finishes.\nFor more information, see the Job Scheduler sample extension directory in the Job Scheduler GitHub repo.",
    "ancestors": [
      "Monitoring your cluster"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/monitoring-your-cluster/logs/",
    "title": "Logs",
    "content": "The OpenSearch logs include valuable information for monitoring cluster operations and troubleshooting issues. The location of the logs differs based on the installation type:\nOn Docker, OpenSearch writes most logs to the console and stores the remainder in opensearch/logs/. The tarball installation also uses opensearch/logs/.\nOn most Linux installations, OpenSearch writes logs to /var/log/opensearch/.\nLogs are available as.log (plain text) and.json files. Permissions for the OpenSearch logs are -rw-r--r-- by default, meaning that any user account on the node can read them. You can change this behavior for each log type in log4j2.properties using the filePermissions option. For example, you might add appender.rolling.filePermissions = rw-r----- to change permissions for the JSON server log. For details, see the Log4j 2 documentation.\nApplication logs\nFor its application logs, OpenSearch uses Apache Log4j 2 and its built-in log levels (from least to most severe) of TRACE, DEBUG, INFO, WARN, ERROR, and FATAL. The default OpenSearch log level is INFO.\nRather than changing the default log level ( logger.level), you change the log level for individual OpenSearch modules: PUT /_cluster/settings { \"persistent\": { \"logger.org.opensearch.index.reindex\": \"DEBUG\" } } The easiest way to identify modules is not from the logs, which abbreviate the path (for example, o.o.i.r), but from the OpenSearch source code.\nAfter this sample change, OpenSearch emits much more detailed logs during reindex operations: [2019-10-18T16:52:51,184][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: starting\n[2019-10-18T16:52:51,186][DEBUG][o.o.i.r.TransportReindexAction] [node1] executing initial scroll against [some-index]\n[2019-10-18T16:52:51,291][DEBUG][o.o.i.r.TransportReindexAction] [node1] scroll returned [3] documents with a scroll id of [DXF1Z==]\n[2019-10-18T16:52:51,292][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: got scroll response with [3] hits\n[2019-10-18T16:52:51,294][DEBUG][o.o.i.r.WorkerBulkByScrollTaskState] [node1] [1626]: preparing bulk request for [0s]\n[2019-10-18T16:52:51,297][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: preparing bulk request\n[2019-10-18T16:52:51,299][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: sending [3] entry, [222b] bulk request\n[2019-10-18T16:52:51,310][INFO][o.e.c.m.MetaDataMappingService] [node1] [some-new-index/R-j3adc6QTmEAEb-eAie9g] create_mapping [_doc]\n[2019-10-18T16:52:51,383][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: got scroll response with [0] hits\n[2019-10-18T16:52:51,384][DEBUG][o.o.i.r.WorkerBulkByScrollTaskState] [node1] [1626]: preparing bulk request for [0s]\n[2019-10-18T16:52:51,385][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: preparing bulk request\n[2019-10-18T16:52:51,386][DEBUG][o.o.i.r.TransportReindexAction] [node1] [1626]: finishing without any catastrophic failures\n[2019-10-18T16:52:51,395][DEBUG][o.o.i.r.TransportReindexAction] [node1] Freed [1] contexts The DEBUG and TRACE levels are extremely verbose. If you enable either one to troubleshoot a problem, disable it after you finish.\nThere are other ways to change log levels:\nAdd lines to opensearch.yml: logger.org.opensearch.index.reindex: debug Modifying opensearch.yml makes the most sense if you want to reuse your logging configuration across multiple clusters or debug startup issues with a single node.\nModify log4j2.properties: # Define a new logger with unique ID of reindex\nlogger.reindex.name = org.opensearch.index.reindex\n# Set the log level for that ID\nlogger.reindex.level = debug This approach is extremely flexible, but requires familiarity with the Log4j 2 property file syntax. In general, the other options offer a simpler configuration experience.\nIf you examine the default log4j2.properties file in the configuration directory, you can see a few OpenSearch-specific variables: appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n\nappender.rolling_old.fileName = ${sys:os.logs.base_path}${sys:file.separator}${sys:os.logs.cluster_name}.log ${sys:os.logs.base_path} is the directory for logs (for example, /var/log/opensearch/). ${sys:os.logs.cluster_name} is the name of the cluster. [%node_name] is the name of the node.\nSlow logs\nOpenSearch has two slow logs, logs that help you identify performance issues: the search slow log and the indexing slow log.\nThese logs rely on thresholds to define what qualifies as a “slow” search or indexing operation. For example, you might decide that a query is slow if it takes more than 15 seconds to complete. Unlike application logs, which you configure for modules, you configure slow logs for indices. By default, both logs are disabled (all thresholds are set to -1): GET &lt;some-index&gt;/_settings?include_defaults= true { \"indexing\": { \"slowlog\": { \"reformat\": \"true\", \"threshold\": { \"index\": { \"warn\": \"-1\", \"trace\": \"-1\", \"debug\": \"-1\", \"info\": \"-1\" } }, \"source\": \"1000\", \"level\": \"TRACE\" } }, \"search\": { \"slowlog\": { \"level\": \"TRACE\", \"threshold\": { \"fetch\": { \"warn\": \"-1\", \"trace\": \"-1\", \"debug\": \"-1\", \"info\": \"-1\" }, \"query\": { \"warn\": \"-1\", \"trace\": \"-1\", \"debug\": \"-1\", \"info\": \"-1\" } } } } } To enable these logs, increase one or more thresholds: PUT &lt;some-index&gt;/_settings { \"indexing\": { \"slowlog\": { \"threshold\": { \"index\": { \"warn\": \"15s\", \"trace\": \"750ms\", \"debug\": \"3s\", \"info\": \"10s\" } }, \"source\": \"500\", \"level\": \"INFO\" } } } In this example, OpenSearch logs indexing operations that take 15 seconds or longer at the WARN level and operations that take between 10 and 14. x seconds at the INFO level. If you set a threshold to 0 seconds, OpenSearch logs all operations, which can be useful for testing whether slow logs are indeed enabled. reformat specifies whether to log the document _source field as a single line ( true) or let it span multiple lines ( false). source is the number of characters of the document _source field to log. level is the minimum log level to include.\nA line from opensearch_index_indexing_slowlog.log might look like this: node1 | [2019-10-24T19:48:51,012][WARN][i.i.s.index] [node1] [some-index/i86iF5kyTyy-PS8zrdDeAA] took[3.4ms], took_millis[3], type[_doc], id[1], routing[], source[{\"title\":\"Your Name\", \"Director\":\"Makoto Shinkai\"}] Slow logs can consume considerable disk space if you set thresholds or levels too low. Consider enabling them temporarily for troubleshooting or performance tuning. To disable slow logs, return all thresholds to -1.\nTask logs\nOpenSearch can log CPU time and memory utilization for the top N memory expensive search tasks when task resource consumers are enabled. By default, task resource consumers will log the top 10 search tasks at 60 second intervals. These values can be configured in opensearch.yml.\nTask logging is enabled dynamically through the cluster settings API: PUT _cluster/settings { \"persistent\": { \"task_resource_consumers.enabled\": \"true\" } } Enabling task resource consumers can have an impact on search latency.\nOnce enabled, logs will be written to logs/opensearch_task_detailslog.json and logs/opensearch_task_detailslog.log.\nTo configure the logging interval and the number of search tasks logged, add the following lines to opensearch.yml: # Number of expensive search tasks to log cluster.task.consumers.top_n.size:100 # Logging interval cluster.task.consumers.top_n.frequency:30s Deprecation logs\nDeprecation logs record when clients make deprecated API calls to your cluster. These logs can help you identify and fix issues prior to upgrading to a new major version. By default, OpenSearch logs deprecated API calls at the WARN level, which works well for almost all use cases. If desired, configure logger.deprecation.level using _cluster/settings, opensearch.yml, or log4j2.properties.",
    "ancestors": [
      "Monitoring your cluster"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/monitoring-your-cluster/pa/api/",
    "title": "API",
    "content": "Introduced 1.0\nPerformance Analyzer uses a single HTTP method and URI for most requests: GET &lt;endpoint&gt;:9600/_plugins/_performanceanalyzer/metrics Note the use of port 9600. Provide parameters for metrics, aggregations, dimensions, and nodes (optional):?metrics=&lt;metrics&gt;&amp;agg=&lt;aggregations&gt;&amp;dim=&lt;dimensions&gt;&amp;nodes=all\" For a full list of metrics, see Metrics reference. Performance Analyzer updates its data every five seconds. If you create a custom client, we recommend using that same interval for calls to the API.\nExample request GET localhost:9600/_plugins/_performanceanalyzer/metrics?metrics=Latency,CPU_Utilization&amp;agg=avg,max&amp;dim=ShardID&amp;nodes=all Example response { \"keHlhQbbTpm1BYicficEQg\": { \"timestamp\": 1554940530000, \"data\": { \"fields\": [{ \"name\": \"ShardID\", \"type\": \"VARCHAR\" }, { \"name\": \"Latency\", \"type\": \"DOUBLE\" }, { \"name\": \"CPU_Utilization\", \"type\": \"DOUBLE\" }], \"records\": [ [ null, null, 0.012552206029147535], [ \"1\", 4.8, 0.0009780939762972104]] } }, \"bHdpbMJZTs-TKtZro2SmYA\": { \"timestamp\": 1554940530000, \"data\": { \"fields\": [{ \"name\": \"ShardID\", \"type\": \"VARCHAR\" }, { \"name\": \"Latency\", \"type\": \"DOUBLE\" }, { \"name\": \"CPU_Utilization\", \"type\": \"DOUBLE\" }], \"records\": [ [ null, 18.2, 0.011966493817311527], [ \"1\", 14.8, 0.0007670829370071493]] } } } In this case, each top-level object represents a node. The API returns names and data types for the metrics and dimensions that you specified, along with values from five seconds ago and current values (if different). Null values represent inactivity during that time period.\nPerformance Analyzer has one additional URI that returns the unit for each metric.\nExample request GET localhost:9600/_plugins/_performanceanalyzer/metrics/units Example response { \"Disk_Utilization\": \"%\", \"Cache_Request_Hit\": \"count\", \"HTTP_RequestDocs\": \"count\", \"Net_TCP_Lost\": \"segments/flow\", \"Refresh_Time\": \"ms\", \"GC_Collection_Event\": \"count\", \"Merge_Time\": \"ms\", \"Sched_CtxRate\": \"count/s\", \"Cache_Request_Size\": \"B\", \"ThreadPool_QueueSize\": \"count\", \"Sched_Runtime\": \"s/ctxswitch\", \"Disk_ServiceRate\": \"MB/s\", \"Heap_AllocRate\": \"B/s\", \"Heap_Max\": \"B\", \"Sched_Waittime\": \"s/ctxswitch\", \"ShardBulkDocs\": \"count\", \"Thread_Blocked_Time\": \"s/event\", \"VersionMap_Memory\": \"B\", \"Master_Task_Queue_Time\": \"ms\", \"Merge_CurrentEvent\": \"count\", \"Indexing_Buffer\": \"B\", \"Bitset_Memory\": \"B\", \"Net_PacketDropRate4\": \"packets/s\", \"Heap_Committed\": \"B\", \"Net_PacketDropRate6\": \"packets/s\", \"Thread_Blocked_Event\": \"count\", \"GC_Collection_Time\": \"ms\", \"Cache_Query_Miss\": \"count\", \"IO_TotThroughput\": \"B/s\", \"Latency\": \"ms\", \"Net_PacketRate6\": \"packets/s\", \"Cache_Query_Hit\": \"count\", \"IO_ReadSyscallRate\": \"count/s\", \"Net_PacketRate4\": \"packets/s\", \"Cache_Request_Miss\": \"count\", \"CB_ConfiguredSize\": \"B\", \"CB_TrippedEvents\": \"count\", \"ThreadPool_RejectedReqs\": \"count\", \"Disk_WaitTime\": \"ms\", \"Net_TCP_TxQ\": \"segments/flow\", \"Master_Task_Run_Time\": \"ms\", \"IO_WriteSyscallRate\": \"count/s\", \"IO_WriteThroughput\": \"B/s\", \"Flush_Event\": \"count\", \"Net_TCP_RxQ\": \"segments/flow\", \"Refresh_Event\": \"count\", \"Flush_Time\": \"ms\", \"Heap_Init\": \"B\", \"CPU_Utilization\": \"cores\", \"HTTP_TotalRequests\": \"count\", \"ThreadPool_ActiveThreads\": \"count\", \"Cache_Query_Size\": \"B\", \"Paging_MinfltRate\": \"count/s\", \"Merge_Event\": \"count\", \"Net_TCP_SendCWND\": \"B/flow\", \"Cache_Request_Eviction\": \"count\", \"Segments_Total\": \"count\", \"Heap_Used\": \"B\", \"Cache_FieldData_Eviction\": \"count\", \"IO_TotalSyscallRate\": \"count/s\", \"CB_EstimatedSize\": \"B\", \"Net_Throughput\": \"B/s\", \"Paging_RSS\": \"pages\", \"Indexing_ThrottleTime\": \"ms\", \"IndexWriter_Memory\": \"B\", \"Master_PendingQueueSize\": \"count\", \"Net_TCP_SSThresh\": \"B/flow\", \"Cache_FieldData_Size\": \"B\", \"Paging_MajfltRate\": \"count/s\", \"ThreadPool_TotalThreads\": \"count\", \"IO_ReadThroughput\": \"B/s\", \"ShardEvents\": \"count\", \"Net_TCP_NumFlows\": \"count\" }",
    "ancestors": [
      "Monitoring your cluster",
      "Performance Analyzer"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/monitoring-your-cluster/pa/dashboards/",
    "title": "Create PerfTop Dashboards",
    "content": "Dashboards are defined in JSON and composed of three main elements: tables, line graphs, and bar graphs. You define a grid of rows and columns and then place elements within that grid, with each element spanning as many rows and columns as you specify.\nThe best way to get started with building custom dashboards is to duplicate and modify one of the existing JSON files in the dashboards directory.\nTable of contents Summary of elements Position elements Add queries Add options All elements Tables Bars Lines Summary of elements\nTables show metrics per dimension. For example, if your metric is CPU_Utilization and your dimension ShardID, a PerfTop table shows a row for each shard on each node.\nBar graphs are aggregated for the cluster, unless you add nodeName to the dashboard. See the options for all elements.\nLine graphs are aggregated for each node. Each line represents a node.\nPosition elements\nPerfTop positions elements within a grid. For example, consider this 12 * 12 grid. The upper-left of the grid represents row 0, column 0, so the starting positions for the three boxes are:\nOrange: row 0, column 0\nPurple: row 2, column 2\nGreen: row 1, column 6\nThese boxes span a number of rows and columns. In this case:\nOrange: 2 rows, 4 columns\nPurple: 1 row, 4 columns\nGreen: 3 rows, 2 columns\nIn JSON form, we have the following: { \"gridOptions\": { \"rows\": 12, \"cols\": 12 }, \"graphs\": { \"tables\": [{ \"options\": { \"gridPosition\": { \"row\": 0, \"col\": 0, \"rowSpan\": 2, \"colSpan\": 4 } } }, { \"options\": { \"gridPosition\": { \"row\": 2, \"col\": 2, \"rowSpan\": 1, \"colSpan\": 4 } } }, { \"options\": { \"gridPosition\": { \"row\": 1, \"col\": 6, \"rowSpan\": 3, \"colSpan\": 2 } } }] } } At this point, however, all the JSON does is define the size and position of three tables. To fill elements with data, you specify a query.\nAdd queries\nQueries use the same elements as the REST API, just in JSON form: { \"queryParams\": { \"metrics\": \"estimated,limitConfigured\", \"aggregates\": \"avg,avg\", \"dimensions\": \"type\", \"sortBy\": \"estimated\" } } For details on available metrics, see Metrics reference.\nAdd options\nOptions include labels, colors, and a refresh interval. Different elements types have different options.\nDashboards support the 16 ANSI colors: black, red, green, yellow, blue, magenta, cyan, and white. For the “bright” variants of these colors, use the numbers 8–15. If your terminal supports 256 colors, you can also use hex codes (e.g. #6D40ED).\nAll elements Option Type Description label String or integer\nThe text in the upper-left corner of the box. labelColor String or integer\nThe color of the label. refreshInterval Integer\nThe number of milliseconds between calls to the Performance Analyzer API for new data. Minimum value is 5000. dimensionFilters String array\nThe dimension value to diplay for the graph. For example, if you query for metric=Net_Throughput&amp;agg=sum&amp;dim=Direction and the possible dimension values are in and out, you can define dimensionFilters: [\"in\"] to only display the metric data for in dimension nodeName String\nIf non-null, lets you restrict elements to individual nodes. You can specify the node name directly in the dashboard file, but the better approach is to use \"nodeName\": \"#nodeName\" in the dashboard and include the --nodename &lt;node_name&gt; argument when starting PerfTop. Tables Option Type Description bg String or integer\nThe background color. fg String or integer\nThe text color. selectedFg String or integer\nThe text color for focused text. selectedBg String or integer\nThe background color for focused text. columnSpacing Integer\nThe amount of space (measured in characters) between columns. keys Boolean\nHas no impact at this time. Bars Option Type Description barWidth Integer\nThe width of each bar (measured in characters) in the graph. xOffset Integer\nThe amount of space (measured in characters) between the y-axis and the first bar in the graph. maxHeight Integer\nThe maximum height of each bar (measured in characters) in the graph. Lines Option Type Description showNthLabel Integer\nWhich of the xAxis labels to show. For example, \"showNthLabel\": 2 shows every other label. showLegend Boolean\nWhether or not to display a legend for the line graph. legend.width Integer\nThe width of the legend (measured in characters) in the graph. xAxis String array\nArray of labels for the x-axis. For example, [\"0:00\", \"0:10\", \"0:20\", \"0:30\", \"0:40\", \"0:50\"]. colors String array\nArray of line colors to choose from. For example, [\"magenta\", \"cyan\"]. If you don’t provide this value, PerfTop chooses random colors for each line.",
    "ancestors": [
      "Monitoring your cluster",
      "Performance Analyzer"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/monitoring-your-cluster/pa/index/",
    "title": "Performance Analyzer",
    "content": "Performance analyzer is an agent and REST API that allows you to query numerous performance metrics for your cluster, including aggregations of those metrics.\nThe performance analyzer plugin is installed by default in OpenSearch version 2.0 and higher.\nPerformance analyzer installation and configuration\nThe following sections provide the steps for installing and configuring the performance analyzer plugin.\nInstall performance analyzer\nThe performance analyzer plugin is included in the installation for Docker and tarball. If you need to install the performance analyzer plugin manually, download the plugin from Maven and install the plugin using the standard plugins install process. Performance analyzer will run on each node in a cluster.\nTo start the performance analyzer root cause analysis (RCA) agent on a tarball installation, run the following command: OPENSEARCH_HOME = ~/opensearch-2.2.1 OPENSEARCH_JAVA_HOME = ~/opensearch-2.2.1/jdk OPENSEARCH_PATH_CONF = ~/opensearch-2.2.1/bin./performance-analyzer-agent-cli The following command enables the performance analyzer plugin and performance analyzer RCA agent: curl -XPOST localhost:9200/_plugins/_performanceanalyzer/cluster/config -H 'Content-Type: application/json' -d '{\"enabled\": true}' To shut down the performance analyzer RCA agent, run the following command: kill $( ps aux | grep -i 'PerformanceAnalyzerApp' | grep -v grep | awk '{print $2}') To disable the performance analyzer plugin, run the following command: curl -XPOST localhost:9200/_plugins/_performanceanalyzer/cluster/config -H 'Content-Type: application/json' -d '{\"enabled\": false}' To uninstall the performance analyzer plugin, run the following command: bin/opensearch-plugin remove opensearch-performance-analyzer Configure performance analyzer\nTo configure the performance analyzer plugin, you will need to edit the performance-analyzer.properties configuration file in the config/opensearch-performance-analyzer/ directory. Make sure to uncomment the line #webservice-bind-host and set it to 0.0.0.0. You can reference the following example configuration. # ======================== OpenSearch performance analyzer plugin config ========================= # NOTE: this is an example for Linux. Please modify the config accordingly if you are using it under other OS. # WebService bind host; default to all interfaces webservice-bind-host = 0.0.0.0 # Metrics data location metrics-location = /dev/shm/performanceanalyzer/ # Metrics deletion interval (minutes) for metrics data. # Interval should be between 1 to 60. metrics-deletion-interval = 1 # If set to true, the system cleans up the files behind it. So at any point, we should expect only 2 # metrics-db-file-prefix-path files. If set to false, no files are cleaned up. This can be useful, if you are archiving # the files and wouldn't like for them to be cleaned up. cleanup-metrics-db-files = true # WebService exposed by App's port webservice-listener-port = 9600 # Metric DB File Prefix Path location metrics-db-file-prefix-path = /tmp/metricsdb_\nhttps-enabled = false #Setup the correct path for certificates #certificate-file-path = specify_path #private-key-file-path = specify_path # Plugin Stats Metadata file name, expected to be in the same location plugin-stats-metadata = plugin-stats-metadata # Agent Stats Metadata file name, expected to be in the same location agent-stats-metadata = agent-stats-metadata To start the performance analyzer RCA agent, run the following command. OPENSEARCH_HOME = ~/opensearch-2.2.1 OPENSEARCH_JAVA_HOME = ~/opensearch-2.2.1/jdk OPENSEARCH_PATH_CONF = ~/opensearch-2.2.1/bin./performance-analyzer-agent-cli Storage\nPerformance analyzer uses /dev/shm for temporary storage. During heavy workloads on a cluster, performance analyzer can use up to 1 GB of space.\nDocker, however, has a default /dev/shm size of 64 MB. To change this value, you can use the docker run --shm-size 1gb flag or a similar setting in Docker Compose.\nIf you’re not using Docker, check the size of /dev/shm using df -h. The default value is probably plenty, but if you need to change its size, add the following line to /etc/fstab: tmpfs /dev/shm tmpfs defaults,noexec,nosuid,size = 1G 0 0 Then remount the file system: mount -o remount /dev/shm Security\nPerformance analyzer supports encryption in transit for requests. It currently does not support client or server authentication for requests. To enable encryption in transit, edit performance-analyzer.properties in your $OPENSEARCH_HOME directory. vi $OPENSEARCH_HOME /config/opensearch-performance-analyzer/performance-analyzer.properties Change the following lines to configure encryption in transit. Note that certificate-file-path must be a certificate for the server, not a root certificate authority (CA). https-enabled = true #Setup the correct path for certificates certificate-file-path = specify_path\nprivate-key-file-path = specify_path Enable performance analyzer for RPM/YUM installations\nIf you installed OpenSearch from an RPM distribution, you can start and stop performance analyzer with systemctl. # Start OpenSearch Performance Analyzer sudo systemctl start opensearch-performance-analyzer.service # Stop OpenSearch Performance Analyzer sudo systemctl stop opensearch-performance-analyzer.service Example API query and response\nThe following is an example API query: GET localhost:9600/_plugins/_performanceanalyzer/metrics/units The following is an example response: { \"Disk_Utilization\": \"%\", \"Cache_Request_Hit\": \"count\", \"Refresh_Time\": \"ms\", \"ThreadPool_QueueLatency\": \"count\", \"Merge_Time\": \"ms\", \"ClusterApplierService_Latency\": \"ms\", \"PublishClusterState_Latency\": \"ms\", \"Cache_Request_Size\": \"B\", \"LeaderCheck_Failure\": \"count\", \"ThreadPool_QueueSize\": \"count\", \"Sched_Runtime\": \"s/ctxswitch\", \"Disk_ServiceRate\": \"MB/s\", \"Heap_AllocRate\": \"B/s\", \"Indexing_Pressure_Current_Limits\": \"B\", \"Sched_Waittime\": \"s/ctxswitch\", \"ShardBulkDocs\": \"count\", \"Thread_Blocked_Time\": \"s/event\", \"VersionMap_Memory\": \"B\", \"Master_Task_Queue_Time\": \"ms\", \"IO_TotThroughput\": \"B/s\", \"Indexing_Pressure_Current_Bytes\": \"B\", \"Indexing_Pressure_Last_Successful_Timestamp\": \"ms\", \"Net_PacketRate6\": \"packets/s\", \"Cache_Query_Hit\": \"count\", \"IO_ReadSyscallRate\": \"count/s\", \"Net_PacketRate4\": \"packets/s\", \"Cache_Request_Miss\": \"count\", \"ThreadPool_RejectedReqs\": \"count\", \"Net_TCP_TxQ\": \"segments/flow\", \"Master_Task_Run_Time\": \"ms\", \"IO_WriteSyscallRate\": \"count/s\", \"IO_WriteThroughput\": \"B/s\", \"Refresh_Event\": \"count\", \"Flush_Time\": \"ms\", \"Heap_Init\": \"B\", \"Indexing_Pressure_Rejection_Count\": \"count\", \"CPU_Utilization\": \"cores\", \"Cache_Query_Size\": \"B\", \"Merge_Event\": \"count\", \"Cache_FieldData_Eviction\": \"count\", \"IO_TotalSyscallRate\": \"count/s\", \"Net_Throughput\": \"B/s\", \"Paging_RSS\": \"pages\", \"AdmissionControl_ThresholdValue\": \"count\", \"Indexing_Pressure_Average_Window_Throughput\": \"count/s\", \"Cache_MaxSize\": \"B\", \"IndexWriter_Memory\": \"B\", \"Net_TCP_SSThresh\": \"B/flow\", \"IO_ReadThroughput\": \"B/s\", \"LeaderCheck_Latency\": \"ms\", \"FollowerCheck_Failure\": \"count\", \"HTTP_RequestDocs\": \"count\", \"Net_TCP_Lost\": \"segments/flow\", \"GC_Collection_Event\": \"count\", \"Sched_CtxRate\": \"count/s\", \"AdmissionControl_RejectionCount\": \"count\", \"Heap_Max\": \"B\", \"ClusterApplierService_Failure\": \"count\", \"PublishClusterState_Failure\": \"count\", \"Merge_CurrentEvent\": \"count\", \"Indexing_Buffer\": \"B\", \"Bitset_Memory\": \"B\", \"Net_PacketDropRate4\": \"packets/s\", \"Heap_Committed\": \"B\", \"Net_PacketDropRate6\": \"packets/s\", \"Thread_Blocked_Event\": \"count\", \"GC_Collection_Time\": \"ms\", \"Cache_Query_Miss\": \"count\", \"Latency\": \"ms\", \"Shard_State\": \"count\", \"Thread_Waited_Event\": \"count\", \"CB_ConfiguredSize\": \"B\", \"ThreadPool_QueueCapacity\": \"count\", \"CB_TrippedEvents\": \"count\", \"Disk_WaitTime\": \"ms\", \"Data_RetryingPendingTasksCount\": \"count\", \"AdmissionControl_CurrentValue\": \"count\", \"Flush_Event\": \"count\", \"Net_TCP_RxQ\": \"segments/flow\", \"Shard_Size_In_Bytes\": \"B\", \"Thread_Waited_Time\": \"s/event\", \"HTTP_TotalRequests\": \"count\", \"ThreadPool_ActiveThreads\": \"count\", \"Paging_MinfltRate\": \"count/s\", \"Net_TCP_SendCWND\": \"B/flow\", \"Cache_Request_Eviction\": \"count\", \"Segments_Total\": \"count\", \"FollowerCheck_Latency\": \"ms\", \"Heap_Used\": \"B\", \"Master_ThrottledPendingTasksCount\": \"count\", \"CB_EstimatedSize\": \"B\", \"Indexing_ThrottleTime\": \"ms\", \"Master_PendingQueueSize\": \"count\", \"Cache_FieldData_Size\": \"B\", \"Paging_MajfltRate\": \"count/s\", \"ThreadPool_TotalThreads\": \"count\", \"ShardEvents\": \"count\", \"Net_TCP_NumFlows\": \"count\", \"Election_Term\": \"count\" } Root cause analysis\nThe root cause analysis (RCA) framework uses the information from performance analyzer to inform administrators of the root cause of performance and availability issues that their clusters might be experiencing.\nEnable the RCA framework\nTo enable the RCA framework, run the following command: curl -XPOST http://localhost:9200/_plugins/_performanceanalyzer/rca/cluster/config -H 'Content-Type: application/json' -d '{\"enabled\": true}' If you encounter the curl: (52) Empty reply from server response, run the following command to enable RCA: curl -XPOST https://localhost:9200/_plugins/_performanceanalyzer/rca/cluster/config -H 'Content-Type: application/json' -d '{\"enabled\": true}' -u 'admin:admin' -k Example API query and response\nTo request all available RCAs, run the following command: GET localhost:9600/_plugins/_performanceanalyzer/rca To request a specific RCA, run the following command: GET localhost:9600/_plugins/_performanceanalyzer/rca?name = HighHeapUsageClusterRCA The following is an example response: { \"HighHeapUsageClusterRCA\": [{ \"RCA_name\": \"HighHeapUsageClusterRCA\", \"state\": \"unhealthy\", \"timestamp\": 1587426650942, \"HotClusterSummary\": [{ \"number_of_nodes\": 2, \"number_of_unhealthy_nodes\": 1, \"HotNodeSummary\": [{ \"host_address\": \"192.168.144.2\", \"node_id\": \"JtlEoRowSI6iNpzpjlbp_Q\", \"HotResourceSummary\": [{ \"resource_type\": \"old gen\", \"threshold\": 0.65, \"value\": 0.81827232588145373, \"avg\": NaN, \"max\": NaN, \"min\": NaN, \"unit_type\": \"heap usage in percentage\", \"time_period_seconds\": 600, \"TopConsumerSummary\": [{ \"name\": \"CACHE_FIELDDATA_SIZE\", \"value\": 590702564 }, { \"name\": \"CACHE_REQUEST_SIZE\", \"value\": 28375 }, { \"name\": \"CACHE_QUERY_SIZE\", \"value\": 12687 }], }] }] }] }] } Performance analyzer and RCA API references\nRelated links\nFurther documentation on the use of performance analyzer and RCA can be found at the following links: Performance analyzer API guide. RCA. RCA API guide. RFC: Root cause analysis.",
    "ancestors": [
      "Monitoring your cluster"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/monitoring-your-cluster/pa/rca/api/",
    "title": "API",
    "content": "Example request # Request all available RCAs\nGET localhost:9600/_plugins/_performanceanalyzer/rca\n# Request a specific RCA\nGET localhost:9600/_plugins/_performanceanalyzer/rca?name=HighHeapUsageClusterRca Example response { \"HighHeapUsageClusterRca\": [{ \"rca_name\": \"HighHeapUsageClusterRca\", \"state\": \"unhealthy\", \"timestamp\": 1587426650942, \"HotClusterSummary\": [{ \"number_of_nodes\": 2, \"number_of_unhealthy_nodes\": 1, \"HotNodeSummary\": [{ \"host_address\": \"192.168.144.2\", \"node_id\": \"JtlEoRowSI6iNpzpjlbp_Q\", \"HotResourceSummary\": [{ \"resource_type\": \"old gen\", \"threshold\": 0.65, \"value\": 0.81827232588145373, \"avg\": NaN, \"max\": NaN, \"min\": NaN, \"unit_type\": \"heap usage in percentage\", \"time_period_seconds\": 600, \"TopConsumerSummary\": [{ \"name\": \"CACHE_FIELDDATA_SIZE\", \"value\": 590702564 }, { \"name\": \"CACHE_REQUEST_SIZE\", \"value\": 28375 }, { \"name\": \"CACHE_QUERY_SIZE\", \"value\": 12687 }], }] }] }] }] }",
    "ancestors": [
      "Monitoring your cluster",
      "Performance Analyzer",
      "Root Cause Analysis"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/monitoring-your-cluster/pa/rca/index/",
    "title": "Root Cause Analysis",
    "content": "The OpenSearch Performance Analyzer plugin (PA) captures OpenSearch and JVM activity, plus their lower-level resource usage (e.g. disk, network, CPU, and memory). Based on this instrumentation, Performance Analyzer computes and exposes diagnostic metrics so that administrators can measure and understand the bottlenecks in their OpenSearch clusters.\nThe Root Cause Analysis framework (RCA) uses the information from PA to alert administrators about the root cause of performance and availability issues that their clusters might be experiencing.\nIn broad strokes, the framework helps you access data streams from OpenSearch nodes running Performance Analyzer. You write snippets of Java to choose the streams that matter to you and evaluate the streams’ PA metrics against certain thresholds. As RCA runs, you can access the state of each analysis using the REST API.\nTo learn more about Root Cause Analysis, see its repository on GitHub.",
    "ancestors": [
      "Monitoring your cluster",
      "Performance Analyzer"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/monitoring-your-cluster/pa/rca/reference/",
    "title": "RCA Reference",
    "content": "You can find a reference of available RCAs and their purposes on GitHub.",
    "ancestors": [
      "Monitoring your cluster",
      "Performance Analyzer",
      "Root Cause Analysis"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/monitoring-your-cluster/pa/rca/shard-hotspot/",
    "title": "Hot shard identification",
    "content": "Hot shard identification root cause analysis (RCA) lets you identify a hot shard within an index. A hot shard is an outlier that consumes more resources than other shards and may lead to poor indexing and search performance. The hot shard identification RCA monitors the following metrics:\nCPU utilization\nHeap allocation rate\nShards may become hot because of the nature of your workload. When you use a _routing parameter or a custom document ID, a specific shard or several shards within the cluster receive frequent updates, consuming more CPU and heap resources than other shards.\nThe hot shard identification RCA compares the CPU utilization and heap allocation rates against their threshold values. If the usage for either metric is greater than the threshold, the shard is considered to be hot.\nFor more information about the hot shard identification RCA implementation, see Hot Shard RCA.\nExample request\nThe following query requests hot shard identification: GET _plugins/_performanceanalyzer/rca?name = HotShardClusterRca copy Example Response\nThe response contains a list of unhealthy shards: \"HotShardClusterRca\": [{ \"rca_name\": \"HotShardClusterRca\", \"timestamp\": 1680721367563, \"state\": \"unhealthy\", \"HotClusterSummary\": [ { \"number_of_nodes\": 3, \"number_of_unhealthy_nodes\": 1, \"HotNodeSummary\": [ { \"node_id\": \"7kosAbpASsqBoHmHkVXxmw\", \"host_address\": \"192.168.80.4\", \"HotResourceSummary\": [ { \"resource_type\": \"cpu usage\", \"resource_metric\": \"cpu usage(num of cores)\", \"threshold\": 0.027397981341796683, \"value\": 0.034449630200405396, \"time_period_seconds\": 60, \"meta_data\": \"ssZw1WRUSHS5DZCW73BOJQ index9 4\" }, { \"resource_type\": \"heap\", \"resource_metric\": \"heap alloc rate(heap alloc rate in bytes per second)\", \"threshold\": 7605441.367010161, \"value\": 10872119.748328414, \"time_period_seconds\": 60, \"meta_data\": \"ssZw1WRUSHS5DZCW73BOJQ index9 4\" }, { \"resource_type\": \"heap\", \"resource_metric\": \"heap alloc rate(heap alloc rate in bytes per second)\", \"threshold\": 7605441.367010161, \"value\": 8019622.354388569, \"time_period_seconds\": 60, \"meta_data\": \"QRF4rBM7SNCDr1g3KU6HyA index9 0\" }] }] }] }] Response fields\nThe following table lists the response fields. Field Type Description rca_name\nString\nThe name of the RCA. In this case, “HotShardClusterRca”.\ntimestamp\nInteger\nThe timestamp of the RCA.\nstate\nObject\nThe state of the cluster determined by the RCA. The state can be healthy, unhealthy, or unknown.\nHotClusterSummary.HotNodeSummary.number_of_nodes\nInteger\nThe number of nodes in the cluster.\nHotClusterSummary.HotNodeSummary.number_of_unhealthy_nodes\nInteger\nThe number of nodes found to be in an unhealthy state.\nHotClusterSummary.HotNodeSummary.HotResourceSummary.resource_type\nObject\nThe type of resource causing the unhealthy state, either “cpu usage” or “heap”.\nHotClusterSummary.HotNodeSummary.HotResourceSummary.resource_metric\nString\nThe definition of the resource_type. Either “cpu usage(num of cores)” or “heap alloc rate(heap alloc rate in bytes per second)”.\nHotClusterSummary.HotNodeSummary.HotResourceSummary.threshold\nFloat\nThe value that determines whether a resource is contended.\nHotClusterSummary.HotNodeSummary.HotResourceSummary.value\nFloat\nThe current value of the resource.\nHotClusterSummary.HotNodeSummary.HotResourceSummary.time_period_seconds\nTime\nThe amount of time that a shard was monitored before its state was declared to be healthy or unhealthy.\nHotClusterSummary.HotNodeSummary.HotResourceSummary.meta_data\nString\nThe metadata associated with the resource_type. In the preceding example response, meta_data is QRF4rBM7SNCDr1g3KU6HyA index9 0. The meta_data string consists of three fields:\nNode name: QRF4rBM7SNCDr1g3KU6HyA Index name: index9 Shard ID: 0 This means that shard 0 of index index9 on node QRF4rBM7SNCDr1g3KU6HyA is hot.",
    "ancestors": [
      "Monitoring your cluster",
      "Performance Analyzer",
      "Root Cause Analysis"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/monitoring-your-cluster/pa/reference/",
    "title": "Metrics Reference",
    "content": "This page contains all Performance Analyzer metrics. All metrics support the avg, sum, min, and max aggregations, although certain metrics measure only one thing, making the choice of aggregation irrelevant.\nFor information on dimensions, see the dimensions reference.\nThis list is extensive. We recommend using Ctrl/Cmd + F to find what you’re looking for. Metric Dimensions Description CPU_Utilization\nShardID, IndexName, Operation, ShardRole\nCPU usage ratio. CPU time (in milliseconds) used by the associated thread(s) in the past five seconds, divided by 5000 milliseconds.\nPaging_MajfltRate\nThe number of major faults per second in the past five seconds. A major fault requires the process to load a memory page from disk.\nPaging_MinfltRate\nThe number of minor faults per second in the past five seconds. A minor fault does not requires the process to load a memory page from disk.\nPaging_RSS\nThe number of pages the process has in real memory---the pages that count towards text, data, or stack space. This number does not include pages that have not been demand-loaded in or swapped out.\nSched_Runtime\nTime (seconds) spent executing on the CPU per context switch.\nSched_Waittime\nTime (seconds) spent waiting on a run queue per context switch.\nSched_CtxRate\nNumber of times run on the CPU per second in the past five seconds.\nHeap_AllocRate\nAn approximation of the heap memory allocated, in bytes, per second in the past five seconds\nIO_ReadThroughput\nNumber of bytes read per second in the last five seconds.\nIO_WriteThroughput\nNumber of bytes written per second in the last five seconds.\nIO_TotThroughput\nNumber of bytes read or written per second in the last five seconds.\nIO_ReadSyscallRate\nRead system calls per second in the last five seconds.\nIO_WriteSyscallRate\nWrite system calls per second in the last five seconds.\nIO_TotalSyscallRate\nRead and write system calls per second in the last five seconds.\nThread_Blocked_Time\nAverage time (seconds) that the associated thread(s) blocked to enter or reenter a monitor.\nThread_Blocked_Event\nThe total number of times that the associated thread(s) blocked to enter or reenter a monitor (i.e. the number of times a thread has been in the blocked state).\nThread_Waited_Time\nAverage time (seconds) that the associated thread(s) waited to enter or reenter a monitor in WAITING or TIMED_WAITING state.\nThread_Waited_Event\nThe total number of times that the associated thread(s) waited to enter or reenter a monitor (i.e. the number of times a thread has been in the WAITING or TIMED_WAITING state).\nShardEvents\nThe total number of events executed on a shard in the past five seconds.\nShardBulkDocs\nThe total number of documents indexed in the past five seconds.\nIndexing_ThrottleTime\nShardID, IndexName\nTime (milliseconds) that the index has been under merge throttling control in the past five seconds.\nCache_Query_Hit\nThe number of successful lookups in the query cache in the past five seconds.\nCache_Query_Miss\nThe number of lookups in the query cache that failed to retrieve a `DocIdSet` in the past five seconds. `DocIdSet` is a set of document IDs in Lucene.\nCache_Query_Size\nQuery cache memory size in bytes.\nCache_FieldData_Eviction\nThe number of times OpenSearch has evicted data from the fielddata heap space (occurs when the heap space is full) in the past five seconds.\nCache_FieldData_Size\nFielddata memory size in bytes.\nCache_Request_Hit\nThe number of successful lookups in the shard request cache in the past five seconds.\nCache_Request_Miss\nThe number of lookups in the request cache that failed to retrieve the results of search requests in the past five seconds.\nCache_Request_Eviction\nThe number of times OpenSearch evicts data from shard request cache (occurs when the request cache is full) in the past five seconds.\nCache_Request_Size\nShard request cache memory size in bytes.\nRefresh_Event\nThe total number of refreshes executed in the past five seconds.\nRefresh_Time\nThe total time (milliseconds) spent executing refreshes in the past five seconds\nFlush_Event\nThe total number of flushes executed in the past five seconds.\nFlush_Time\nThe total time (milliseconds) spent executing flushes in the past five seconds.\nMerge_Event\nThe total number of merges executed in the past five seconds.\nMerge_Time\nThe total time (milliseconds) spent executing merges in the past five seconds.\nMerge_CurrentEvent\nThe current number of merges executing.\nIndexing_Buffer\nIndex buffer memory size in bytes.\nSegments_Total\nThe number of segments.\nIndexWriter_Memory\nEstimated memory usage by the index writer in bytes.\nBitset_Memory\nEstimated memory usage for the cached bit sets in bytes.\nVersionMap_Memory\nEstimated memory usage of the version map in bytes.\nShard_Size_In_Bytes\nEstimated disk usage of the shard in bytes.\nIndexing_Pressure_Current_Limits\nShardID, IndexName, IndexingStage\nTotal heap size (in bytes) that is available for utilization by a shard of an index in a particular indexing stage (Coordinating, Primary or Replica).\nIndexing_Pressure_Current_Bytes\nTotal heap size (in bytes) occupied by a shard of an index in a particular indexing stage (Coordinating, Primary or Replica).\nIndexing_Pressure_Last_Successful_Timestamp\nTimestamp of a request that was successful for a shard of an index in a particular indexing stage (Coordinating, Primary or Replica).\nIndexing_Pressure_Rejection_Count\nTotal rejections performed by OpenSearch for a shard of an index in a particular indexing stage (Coordinating, Primary or Replica).\nIndexing_Pressure_Average_Window_Throughput\nAverage throughput of the last n requests (The value of n is determined by `shard_indexing_pressure.secondary_parameter.throughput.request_size_window` setting) for a shard of an index in a particular indexing stage (Coordinating, Primary or Replica).\nLatency\nOperation, Exception, Indices, HTTPRespCode, ShardID, IndexName, ShardRole\nLatency (milliseconds) of a request.\nGC_Collection_Event\nMemType\nThe number of garbage collections that have occurred in the past five seconds.\nGC_Collection_Time\nThe approximate accumulated time (milliseconds) of all garbage collections that have occurred in the past five seconds.\nHeap_Committed\nThe amount of memory (bytes) that is committed for the JVM to use.\nHeap_Init\nThe amount of memory (bytes) that the JVM initially requests from the operating system for memory management.\nHeap_Max\nThe maximum amount of memory (bytes) that can be used for memory management.\nHeap_Used\nThe amount of used memory in bytes.\nDisk_Utilization\nDiskName\nDisk utilization rate: percentage of disk time spent reading and writing by the OpenSearch process in the past five seconds.\nDisk_WaitTime\nAverage duration (milliseconds) of read and write operations in the past five seconds.\nDisk_ServiceRate\nService rate: MB read or written per second in the past five seconds. This metric assumes that each disk sector stores 512 bytes.\nNet_TCP_NumFlows\nDestAddr\nNumber of samples collected. Performance Analyzer collects one sample every five seconds.\nNet_TCP_TxQ\nAverage number of TCP packets in the send buffer.\nNet_TCP_RxQ\nAverage number of TCP packets in the receive buffer.\nNet_TCP_Lost\nAverage number of unrecovered recurring timeouts. This number is reset when the recovery finishes or `SND.UNA` is advanced. `SND.UNA` is the sequence number of the first byte of data that has been sent, but not yet acknowledged.\nNet_TCP_SendCWND\nAverage size (bytes) of the sending congestion window.\nNet_TCP_SSThresh\nAverage size (bytes) of the slow start size threshold.\nNet_PacketRate4\nDirection\nThe total number of IPv4 datagrams transmitted/received from/by interfaces per second, including those transmitted or received in error.\nNet_PacketDropRate4\nThe total number of IPv4 datagrams transmitted or received in error per second.\nNet_PacketRate6\nThe total number of IPv6 datagrams transmitted or received from or by interfaces per second, including those transmitted or received in error.\nNet_PacketDropRate6\nThe total number of IPv6 datagrams transmitted or received in error per second.\nNet_Throughput\nThe number of bits transmitted or received per second by all network interfaces.\nThreadPool_QueueSize\nThreadPoolType\nThe size of the task queue.\nThreadPool_RejectedReqs\nThe number of rejected executions.\nThreadPool_TotalThreads\nThe current number of threads in the pool.\nThreadPool_ActiveThreads\nThe approximate number of threads that are actively executing tasks.\nThreadPool_QueueLatency\nThe latency of the task queue.\nThreadPool_QueueCapacity\nThe current capacity of the task queue.\nMaster_PendingQueueSize\nMaster_PendingTaskType\nThe current number of pending tasks in the cluster state update thread. Each node has a cluster state update thread that submits cluster state update tasks (create index, update mapping, allocate shard, fail shard, etc.).\nHTTP_RequestDocs\nOperation, Exception, Indices, HTTPRespCode\nThe number of items in the request (only for `_bulk` request type).\nHTTP_TotalRequests\nThe number of finished requests in the past five seconds.\nCB_EstimatedSize\nCBType\nThe current number of estimated bytes.\nCB_TrippedEvents\nThe number of times the circuit breaker has tripped.\nCB_ConfiguredSize\nThe limit (bytes) for how much memory operations can use.\nMaster_Task_Queue_Time\nMasterTaskInsertOrder, MasterTaskPriority, MasterTaskType, MasterTaskMetadata\nThe time (milliseconds) that a master task spent in the queue.\nMaster_Task_Run_Time\nThe time (milliseconds) that a master task has been executed.\nCache_MaxSize\nCacheType\nThe max size of the cache in bytes.\nAdmissionControl_RejectionCount (WIP)\nControllerName\nTotal rejections performed by a Controller of Admission Control.\nAdmissionControl_CurrentValue (WIP)\nCurrent value for Controller of Admission Control.\nAdmissionControl_ThresholdValue (WIP)\nThreshold value for Controller of Admission Control.\nData_RetryingPendingTasksCount (WIP)\nNodeID\nNumber of throttled pending tasks on which data node is actively performing retries. It will be an absolute metric at that point of time.\nMaster_ThrottledPendingTasksCount (WIP)\nSum of total pending tasks which got throttled by node (master node). It is a cumulative metric so look at the max aggregation.\nElection_Term (WIP)\nN/A\nMonotonically increasing number with every master election.\nPublishClusterState_Latency (WIP)\nThe time taken by quorum of nodes to publish new cluster state. This metric is available for current master.\nPublishClusterState_Failure (WIP)\nThe number of times publish new cluster state action failed on master node.\nClusterApplierService_Latency (WIP)\nThe time taken by each node to apply cluster state sent by master.\nClusterApplierService_Failure (WIP)\nThe number of times apply cluster state action failed on each node.\nShard_State (WIP)\nIndexName, NodeName, ShardType, ShardID\nThe state of each shard - whether it is STARTED, UNASSIGNED, RELOCATING etc.\nLeaderCheck_Latency (WIP)\nWIP\nWIP\nFollowerCheck_Failure (WIP)\nLeaderCheck_Failure (WIP)\nFollowerCheck_Latency (WIP) Dimensions reference Dimension Return values ShardID\nID for the shard (e.g. 1).\nIndexName\nName of the index (e.g. my-index).\nOperation\nType of operation (e.g. shardbulk).\nShardRole primary, replica Exception\nOpenSearch exceptions (e.g. org.opensearch.index_not_found_exception).\nIndices\nThe list of indices in the request URI.\nHTTPRespCode\nResponse code from OpenSearch (e.g. 200).\nMemType totYoungGC, totFullGC, Survivor, PermGen, OldGen, Eden, NonHeap, Heap DiskName\nName of the disk (e.g. sda1).\nDestAddr\nDestination address (e.g. 010015AC).\nDirection in, out ThreadPoolType\nThe OpenSearch thread pools (e.g. index, search, snapshot).\nCBType accounting, fielddata, in_flight_requests, parent, request MasterTaskInsertOrder\nThe order in which the task was inserted (e.g. 3691).\nMasterTaskPriority\nPriority of the task (e.g. URGENT). OpenSearch executes higher priority tasks before lower priority ones, regardless of insert_order.\nMasterTaskType shard-started, create-index, delete-index, refresh-mapping, put-mapping, CleanupSnapshotRestoreState, Update snapshot state MasterTaskMetadata\nMetadata for the task (if any).\nCacheType Field_Data_Cache, Shard_Request_Cache, Node_Query_Cache",
    "ancestors": [
      "Monitoring your cluster",
      "Performance Analyzer"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/ad/api/",
    "title": "Anomaly detection API",
    "content": "Use these anomaly detection operations to programmatically create and manage detectors.\nTable of contents Create anomaly detector Validate detector Get detector Update detector Delete detector Preview detector Start detector job Stop detector job Search detector Search detector tasks Search detector result Search top anomalies Get detector stats Profile detector Delete detector results Create monitor Create anomaly detector\nIntroduced 1.0\nCreates an anomaly detector.\nThis command creates a single-entity detector named test-detector that finds anomalies based on the sum of the value field and stores the result in a custom opensearch-ad-plugin-result-test index:\nRequest POST _plugins/_anomaly_detection/detectors { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"result_index\": \"opensearch-ad-plugin-result-test\" } Example response { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 1, \"_seq_no\": 5, \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"_primary_term\": 1 } To create a high cardinality detector by specifying a category field:\nRequest POST _plugins/_anomaly_detection/detectors { \"name\": \"test-hc-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"category_field\": [ \"ip\"] } Example response { \"_id\": \"b0HRTXwBwf_U8gjUw43R\", \"_version\": 1, \"_seq_no\": 6, \"anomaly_detector\": { \"name\": \"test-hc-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"bkHRTXwBwf_U8gjUw43K\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"last_update_time\": 1633393165265, \"category_field\": [ \"ip\"], \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"MULTI_ENTITY\" }, \"_primary_term\": 1 } You can specify a maximum of two category fields: \"category_field\": [ \"ip\"] \"category_field\": [ \"ip\", \"error_type\"] You can specify the following options. Options Description Type Required name The name of the detector. string Yes description A description of the detector. string No time_field The name of the time field. string Yes indices A list of indices to use as the data source. list Yes feature_attributes Specify a feature_name, set the enabled parameter to true, and specify an aggregation query. list Yes filter_query Provide an optional filter query for your feature. object No detection_interval The time interval for your anomaly detector. object Yes window_delay Add extra processing time for data collection. object No category_field Categorizes or slices data with a dimension. Similar to GROUP BY in SQL. list No Validate detector\nIntroduced 1.2\nReturns whether the detector configuration has any issues that might prevent OpenSearch from creating the detector.\nYou can use the validate detector API operation to identify issues in your detector configuration before creating the detector.\nThe request body consists of the detector configuration and follows the same format as the request body of the create detector API.\nYou have the following validation options:\nOnly validate against the detector configuration and find any issues that would completely block detector creation: POST _plugins/_anomaly_detection/detectors/_validate\nPOST _plugins/_anomaly_detection/detectors/_validate/detector Validate against the source data to see how likely the detector would complete model training. POST _plugins/_anomaly_detection/detectors/_validate/model Responses from this API operation return either blocking issues as detector type responses or a response indicating a field that could be revised to increase likelihood of model training completing successfully. Model type issues don’t need to be fixed for detector creation to succeed, but the detector would likely not train successfully if they aren’t addressed.\nRequest POST _plugins/_anomaly_detection/detectors/_validate POST _plugins/_anomaly_detection/detectors/_validate/detector { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } } } If the validate detector API doesn’t find any issue in the detector configuration, it returns an empty response:\nExample response {} If the validate detector API finds an issue, it returns a message explaining what’s wrong with the configuration. In this example, the feature query aggregates over a field that doesn’t exist in the data source:\nExample response { \"detector\": { \"feature_attributes\": { \"message\": \"Feature has invalid query returning empty aggregated data: average_total_rev\", \"sub_issues\": { \"average_total_rev\": \"Feature has invalid query returning empty aggregated data\" } } } } The following request validates against the source data to see if model training might succeed. In this example, the data is ingested at a rate of every 5 minutes, and detector interval is set to 1 minute. POST _plugins/_anomaly_detection/detectors/_validate/model { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } } } If the validate detector API finds areas of improvement with your configuration, it returns a response with suggestions about how you can change your configuration to improve model training.\nSample Responses\nIn this example, the validate detector API returns a response indicating that changing the detector interval length to at least four minutes can increase the chances of successful model training. { \"model\": { \"detection_interval\": { \"message\": \"The selected detector interval might collect sparse data. Consider changing interval length to: 4\", \"suggested_value\": { \"period\": { \"interval\": 4, \"unit\": \"Minutes\" } } } } } Another response might indicate that you can change filter_query (data filter) because the currently filtered data is too sparse for the model to train correctly, which can happen because the index is also ingesting data that falls outside the chosen filter. Using another filter_query can make your data more dense. { \"model\": { \"filter_query\": { \"message\": \"Data is too sparse after data filter is applied. Consider changing the data filter\" } } } Get detector\nIntroduced 1.0\nReturns all information about a detector based on the detector_id.\nRequest GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt; Example response { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 5, \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" } } A “job” is something that you schedule to run periodically, so it’s only applicable for real-time anomaly detection and not historical analysis that you run just one time.\nWhen you start a real-time detector, the anomaly detection plugin creates a job or if the job already exists updates it.\nWhen you start or a restart a real-time detector, the plugin creates a new real-time task that records run-time information like detector configuration snapshot, real-time job states (initializing/running/stopped), init progress, and so on.\nA single detector can only have one real-time job (job ID is the same as detector ID), but it can have multiple real-time tasks because each restart of a real-time job creates a new real-time task. You can limit the number of real-time tasks with the plugins.anomaly_detection.max_old_ad_task_docs_per_detector setting.\nHistorical analysis doesn’t have an associated job. When you start or rerun historical analysis for a detector, the anomaly detection plugin creates a new historical batch task that tracks the historical analysis runtime information like state, coordinating/worker node, task progress, and so on. You can limit the historical task number with the plugins.anomaly_detection.max_old_ad_task_docs_per_detector setting.\nUse job=true to get real-time analysis task information.\nRequest GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;?job= true Example response { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 5, \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"anomaly_detector_job\": { \"name\": \"VEHKTXwBwf_U8gjUXY2s\", \"schedule\": { \"interval\": { \"start_time\": 1633393656357, \"period\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"enabled\": true, \"enabled_time\": 1633393656357, \"last_update_time\": 1633393656357, \"lock_duration_seconds\": 60, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } } } Use task=true to get information for both real-time and historical analysis task information.\nRequest GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;?task= true Example response { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 5, \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"realtime_detection_task\": { \"task_id\": \"nkTZTXwBjd8s6RK4QlMq\", \"last_update_time\": 1633393776375, \"started_by\": \"admin\", \"error\": \"\", \"state\": \"RUNNING\", \"detector_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"task_progress\": 0, \"init_progress\": 1, \"execution_start_time\": 1633393656362, \"is_latest\": true, \"task_type\": \"REALTIME_SINGLE_ENTITY\", \"coordinating_node\": \"SWD7ihu9TaaW1zKwFZNVNg\", \"detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"estimated_minutes_left\": 0, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } }, \"historical_analysis_task\": { \"task_id\": \"99DaTXwB6HknB84StRN1\", \"last_update_time\": 1633393797040, \"started_by\": \"admin\", \"state\": \"RUNNING\", \"detector_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"task_progress\": 0.89285713, \"init_progress\": 1, \"current_piece\": 1633328940000, \"execution_start_time\": 1633393751412, \"is_latest\": true, \"task_type\": \"HISTORICAL_SINGLE_ENTITY\", \"coordinating_node\": \"SWD7ihu9TaaW1zKwFZNVNg\", \"worker_node\": \"2Z4q22BySEyzakYt_A0A2A\", \"detector\": { \"name\": \"test-detector\", \"description\": \"Test detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"U0HKTXwBwf_U8gjUXY2m\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"last_update_time\": 1633392680364, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"detection_date_range\": { \"start_time\": 1632788951329, \"end_time\": 1633393751329 }, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } } } Update detector\nIntroduced 1.0\nUpdates a detector with any changes, including the description or adding or removing of features.\nTo update a detector, you need to first stop both real-time detection and historical analysis.\nYou can’t update a category field.\nRequest PUT _plugins/_anomaly_detection/detectors/&lt;detectorId&gt; { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } } } Example response { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 2, \"_seq_no\": 7, \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"3kHiTXwBwf_U8gjUlY15\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"last_update_time\": 1633394267522, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"_primary_term\": 1 } Delete detector\nIntroduced 1.0\nDeletes a detector based on the detector_id.\nTo delete a detector, you need to first stop both real-time detection and historical analysis.\nRequest DELETE _plugins/_anomaly_detection/detectors/&lt;detectorId&gt; Example response { \"_index\": \".opensearch-anomaly-detectors\", \"_id\": \"70TxTXwBjd8s6RK4j1Pj\", \"_version\": 2, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 9, \"_primary_term\": 1 } Preview detector\nIntroduced 1.0\nPasses a date range to the anomaly detector to return any anomalies within that date range.\nTo preview a single-entity detector:\nRequest POST _plugins/_anomaly_detection/detectors/_preview { \"period_start\": 1633048868000, \"period_end\": 1633394468000, \"detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } } } } Example response { \"anomaly_result\": [ { \"detector_id\": null, \"data_start_time\": 1633049280000, \"data_end_time\": 1633049340000, \"schema_version\": 0, \"feature_data\": [ { \"feature_id\": \"8EHmTXwBwf_U8gjU0Y0u\", \"feature_name\": \"test\", \"data\": 0 }], \"anomaly_grade\": 0, \"confidence\": 0 },...], \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"8EHmTXwBwf_U8gjU0Y0u\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"detector_type\": \"SINGLE_ENTITY\" } } If you specify a category field, each result is associated with an entity:\nRequest POST _plugins/_anomaly_detection/detectors/_preview { \"period_start\": 1633048868000, \"period_end\": 1633394468000, \"detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"feature_attributes\": [ { \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"gt\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"category_field\": [ \"error_type\"] } } Example response { \"anomaly_result\": [ { \"detector_id\": null, \"data_start_time\": 1633049280000, \"data_end_time\": 1633049340000, \"schema_version\": 0, \"feature_data\": [ { \"feature_id\": \"tkTpTXwBjd8s6RK4DlOZ\", \"feature_name\": \"test\", \"data\": 0 }], \"anomaly_grade\": 0, \"confidence\": 0, \"entity\": [ { \"name\": \"error_type\", \"value\": \"error1\" }] },...], \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"tkTpTXwBjd8s6RK4DlOZ\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"category_field\": [ \"error_type\"], \"detector_type\": \"MULTI_ENTITY\" } } You can preview a detector with the detector ID: POST _plugins/_anomaly_detection/detectors/_preview { \"detector_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"period_start\": 1633048868000, \"period_end\": 1633394468000 } Or: POST _opendistro/_anomaly_detection/detectors/VEHKTXwBwf_U 8 gjUXY 2 s/_preview { \"period_start\": 1633048868000, \"period_end\": 1633394468000 } Example response { \"anomaly_result\": [ { \"detector_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"data_start_time\": 1633049280000, \"data_end_time\": 1633049340000, \"schema_version\": 0, \"feature_data\": [ { \"feature_id\": \"3kHiTXwBwf_U8gjUlY15\", \"feature_name\": \"test\", \"data\": 0 }], \"anomaly_grade\": 0, \"confidence\": 0, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } },...], \"anomaly_detector\": { \"name\": \"test-detector\", \"description\": \"Test update detector\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log*\"], \"filter_query\": { \"bool\": { \"filter\": [ { \"range\": { \"value\": { \"from\": 1, \"to\": null, \"include_lower\": false, \"include_upper\": true, \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"3kHiTXwBwf_U8gjUlY15\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"last_update_time\": 1633394267522, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" } } Start detector job\nIntroduced 1.0\nStarts a real-time or historical anomaly detector job.\nTo start a real-time detector job:\nRequest POST _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_start Example response { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 3, \"_seq_no\": 6, \"_primary_term\": 1 } The _id represents the real-time job ID, which is the same as the detector ID.\nTo start historical analysis: POST _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_start { \"start_time\": 1633048868000, \"end_time\": 1633394468000 } Example response { \"_id\": \"f9DsTXwB6HknB84SoRTY\", \"_version\": 1, \"_seq_no\": 958, \"_primary_term\": 1 } The _id represents the historical batch task ID, which is a random universally unique identifier (UUID).\nStop detector job\nIntroduced 1.0\nStops a real-time or historical anomaly detector job.\nTo stop a real-time detector job:\nRequest POST _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_stop Example response { \"_id\": \"VEHKTXwBwf_U8gjUXY2s\", \"_version\": 0, \"_seq_no\": 0, \"_primary_term\": 0 } To stop historical analysis:\nIntroduced 1.1 POST _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_stop?historical= true Example response { \"_id\": \"f9DsTXwB6HknB84SoRTY\", \"_version\": 0, \"_seq_no\": 0, \"_primary_term\": 0 } Search detector\nIntroduced 1.0\nReturns all anomaly detectors for a search query.\nTo search detectors using the server_log* index:\nRequest GET _plugins/_anomaly_detection/detectors/_search POST _plugins/_anomaly_detection/detectors/_search { \"query\": { \"wildcard\": { \"indices\": { \"value\": \"server_log*\" } } } } Example response { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \".opensearch-anomaly-detectors\", \"_id\": \"Zi5zTXwBwf_U8gjUTfJG\", \"_version\": 1, \"_seq_no\": 1, \"_primary_term\": 1, \"_score\": 1, \"_source\": { \"name\": \"test\", \"description\": \"test\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log\"], \"filter_query\": { \"match_all\": { \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 5, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"feature_name\": \"test_feature\", \"feature_enabled\": true, \"aggregation_query\": { \"test_feature\": { \"sum\": { \"field\": \"value\" } } } }], \"last_update_time\": 1633386974533, \"category_field\": [ \"error_type\"], \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"MULTI_ENTITY\" } },...] } } Search detector tasks\nIntroduced 1.1\nSearches detector tasks.\nTo search for the latest detector level historical analysis task for a high cardinality detector\nRequest GET _plugins/_anomaly_detection/detectors/tasks/_search POST _plugins/_anomaly_detection/detectors/tasks/_search { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\" } }, { \"term\": { \"task_type\": \"HISTORICAL_HC_DETECTOR\" } }, { \"term\": { \"is_latest\": \"true\" } }] } } } Example response { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 0, \"hits\": [ { \"_index\": \".opensearch-anomaly-detection-state\", \"_id\": \"fm-RTXwBYwCbWecgB753\", \"_version\": 34, \"_seq_no\": 928, \"_primary_term\": 1, \"_score\": 0, \"_source\": { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\", \"error\": \"\", \"detection_date_range\": { \"start_time\": 1630794960000, \"end_time\": 1633386960000 }, \"task_progress\": 1, \"last_update_time\": 1633389090738, \"execution_start_time\": 1633388922742, \"state\": \"FINISHED\", \"coordinating_node\": \"2Z4q22BySEyzakYt_A0A2A\", \"task_type\": \"HISTORICAL_HC_DETECTOR\", \"execution_end_time\": 1633389090738, \"started_by\": \"admin\", \"init_progress\": 0, \"is_latest\": true, \"detector\": { \"category_field\": [ \"error_type\"], \"description\": \"test\", \"ui_metadata\": { \"features\": { \"test_feature\": { \"aggregationBy\": \"sum\", \"aggregationOf\": \"value\", \"featureType\": \"simple_aggs\" } }, \"filters\": [] }, \"feature_attributes\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"feature_enabled\": true, \"feature_name\": \"test_feature\", \"aggregation_query\": { \"test_feature\": { \"sum\": { \"field\": \"value\" } } } }], \"schema_version\": 0, \"time_field\": \"timestamp\", \"last_update_time\": 1633386974533, \"indices\": [ \"server_log\"], \"window_delay\": { \"period\": { \"unit\": \"Minutes\", \"interval\": 1 } }, \"detection_interval\": { \"period\": { \"unit\": \"Minutes\", \"interval\": 5 } }, \"name\": \"testhc\", \"filter_query\": { \"match_all\": { \"boost\": 1 } }, \"shingle_size\": 8, \"user\": { \"backend_roles\": [ \"admin\"], \"custom_attribute_names\": [], \"roles\": [ \"own_index\", \"all_access\"], \"name\": \"admin\", \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"MULTI_ENTITY\" }, \"user\": { \"backend_roles\": [ \"admin\"], \"custom_attribute_names\": [], \"roles\": [ \"own_index\", \"all_access\"], \"name\": \"admin\", \"user_requested_tenant\": \"__user__\" } } }] } } To search for the latest entity-level tasks for the historical analysis of a high cardinality detector:\nRequest GET _plugins/_anomaly_detection/detectors/tasks/_search POST _plugins/_anomaly_detection/detectors/tasks/_search { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\" } }, { \"term\": { \"task_type\": \"HISTORICAL_HC_ENTITY\" } }, { \"term\": { \"is_latest\": \"true\" } }] } }, \"sort\": [ { \"execution_start_time\": { \"order\": \"desc\" } }], \"size\": 100 } To search and aggregate states for all entity-level historical tasks:\nThe parent_task_id is the same as the task ID that you can get with the profile detector API: GET _plugins/_anomaly_detection/detectors/&lt;detector_ID&gt;/_profile/ad_task.\nRequest GET _plugins/_anomaly_detection/detectors/tasks/_search POST _plugins/_anomaly_detection/detectors/tasks/_search { \"size\": 0, \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": { \"value\": \"Zi5zTXwBwf_U8gjUTfJG\", \"boost\": 1 } } }, { \"term\": { \"parent_task_id\": { \"value\": \"fm-RTXwBYwCbWecgB753\", \"boost\": 1 } } }, { \"terms\": { \"task_type\": [ \"HISTORICAL_HC_ENTITY\"], \"boost\": 1 } }] } }, \"aggs\": { \"test\": { \"terms\": { \"field\": \"state\", \"size\": 100 } } } } Example response { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 32, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"test\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"FINISHED\", \"doc_count\": 32 }] } } } Search detector result\nIntroduced 1.0\nReturns all results for a search query.\nYou have the following search options:\nTo search only the default result index, simply use the search API: POST _plugins/_anomaly_detection/detectors/results/_search/ To search both the custom result index and default result index, you can either add the custom result index to the search API: POST _plugins/_anomaly_detection/detectors/results/_search/&lt;custom_result_index&gt; Or, add the custom result index and set the only_query_custom_result_index parameter to false: POST _plugins/_anomaly_detection/detectors/results/_search/&lt;custom_result_index&gt;?only_query_custom_result_index= false To search only the custom result index, add the custom result index to the search API and set the only_query_custom_result_index parameter to true: POST _plugins/_anomaly_detection/detectors/results/_search/&lt;custom_result_index&gt;?only_query_custom_result_index= true The following example searches anomaly results for grade greater than 0 for real-time analysis:\nRequest GET _plugins/_anomaly_detection/detectors/results/_search/opensearch-ad-plugin-result-test POST _plugins/_anomaly_detection/detectors/results/_search/opensearch-ad-plugin-result-test { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": \"EWy02nwBm38sXcF2AiFJ\" } }, { \"range\": { \"anomaly_grade\": { \"gt\": 0 } } }], \"must_not\": [ { \"exists\": { \"field\": \"task_id\" } }] } } } If you specify the custom result index like in this example, the search results API searches both the default result indices and custom result indices.\nIf you don’t specify the custom result index and you just use the _plugins/_anomaly_detection/detectors/results/_search URL, the anomaly detection plugin searches only the default result indices.\nReal-time detection doesn’t persist the task ID in the anomaly result, so the task ID will be null.\nFor information about the response body fields, see Anomaly result mapping.\nExample response { \"took\": 4, \"timed_out\": false, \"_shards\": { \"total\": 3, \"successful\": 3, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 90, \"relation\": \"eq\" }, \"max_score\": 0, \"hits\": [ { \"_index\": \".opensearch-anomaly-results-history-2021.10.04-1\", \"_id\": \"686KTXwB6HknB84SMr6G\", \"_version\": 1, \"_seq_no\": 103622, \"_primary_term\": 1, \"_score\": 0, \"_source\": { \"detector_id\": \"EWy02nwBm38sXcF2AiFJ\", \"confidence\": 0.918886275269358, \"model_id\": \"EWy02nwBm38sXcF2AiFJ_entity_error16\", \"schema_version\": 4, \"anomaly_score\": 1.1093755891885446, \"execution_start_time\": 1633388475001, \"data_end_time\": 1633388414989, \"data_start_time\": 1633388114989, \"feature_data\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"feature_name\": \"test_feature\", \"data\": 0.532 }], \"relevant_attribution\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"data\": 1.0 }], \"expected_values\": [ { \"likelihood\": 1, \"value_list\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"data\": 2 }] }], \"execution_end_time\": 1633388475014, \"user\": { \"backend_roles\": [ \"admin\"], \"custom_attribute_names\": [], \"roles\": [ \"own_index\", \"all_access\"], \"name\": \"admin\", \"user_requested_tenant\": \"__user__\" }, \"anomaly_grade\": 0.031023547546561225, \"entity\": [ { \"name\": \"error_type\", \"value\": \"error16\" }] } },...] } } You can run historical analysis as many times as you like. So, multiple tasks might exist for the same detector.\nYou can search for the latest historical batch task first and then search the historical batch task results.\nTo search anomaly results for grade greater than 0 for historical analysis with the task_id:\nRequest GET _plugins/_anomaly_detection/detectors/results/_search POST _plugins/_anomaly_detection/detectors/results/_search { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\" } }, { \"range\": { \"anomaly_grade\": { \"gt\": 0 } } }, { \"term\": { \"task_id\": \"fm-RTXwBYwCbWecgB753\" } }] } } } Example response { \"took\": 915, \"timed_out\": false, \"_shards\": { \"total\": 3, \"successful\": 3, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4115, \"relation\": \"eq\" }, \"max_score\": 0, \"hits\": [ { \"_index\": \".opensearch-anomaly-results-history-2021.10.04-1\", \"_id\": \"VRyRTXwBDx7vzPBV8jYC\", \"_version\": 1, \"_seq_no\": 149657, \"_primary_term\": 1, \"_score\": 0, \"_source\": { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\", \"confidence\": 0.9642989263957601, \"task_id\": \"fm-RTXwBYwCbWecgB753\", \"model_id\": \"Zi5zTXwBwf_U8gjUTfJG_entity_error24\", \"schema_version\": 4, \"anomaly_score\": 1.2260712437521946, \"execution_start_time\": 1633388982692, \"data_end_time\": 1631721300000, \"data_start_time\": 1631721000000, \"feature_data\": [ { \"feature_id\": \"ZS5zTXwBwf_U8gjUTfIn\", \"feature_name\": \"test_feature\", \"data\": 10 }], \"execution_end_time\": 1633388982709, \"user\": { \"backend_roles\": [ \"admin\"], \"custom_attribute_names\": [], \"roles\": [ \"own_index\", \"all_access\"], \"name\": \"admin\", \"user_requested_tenant\": \"__user__\" }, \"anomaly_grade\": 0.14249628345655782, \"entity\": [ { \"name\": \"error_type\", \"value\": \"error1\" }] } },...] } } Search top anomalies\nIntroduced 1.2\nReturns the top anomaly results for a high-cardinality detector, bucketed by categorical field values.\nYou can pass a historical boolean parameter to specify whether you want to analyze real-time or historical results.\nRequest GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/results/_topAnomalies?historical= false { \"size\": 3, \"category_field\": [ \"ip\"], \"order\": \"severity\", \"task_id\": \"example-task-id\", \"start_time_ms\": 123456789000, \"end_time_ms\": 987654321000 } Example response { \"buckets\": [ { \"key\": { \"ip\": \"1.2.3.4\" }, \"doc_count\": 10, \"max_anomaly_grade\": 0.8 }, { \"key\": { \"ip\": \"5.6.7.8\" }, \"doc_count\": 12, \"max_anomaly_grade\": 0.6 }, { \"key\": { \"ip\": \"9.10.11.12\" }, \"doc_count\": 3, \"max_anomaly_grade\": 0.5 }] } You can specify the following options. Options Description Type Required size Specify the number of top buckets that you want to see. Default is 10. The maximum number is 10,000. integer No category_field Specify the set of category fields that you want to aggregate on. Defaults to all category fields for the detector. list No order Specify severity (anomaly grade) or occurrence (number of anomalies). Default is severity. string No task_id Specify a historical task ID to see results only from that specific task. Use only when historical=true, otherwise the anomaly detection plugin ignores this parameter. string No start_time_ms Specify the time to start analyzing results, in Epoch milliseconds. long Yes end_time_ms Specify the time to end analyzing results, in Epoch milliseconds. long Yes Get detector stats\nIntroduced 1.0\nProvides information about how the plugin is performing.\nTo get all stats:\nRequest GET _plugins/_anomaly_detection/stats Example response { \"anomaly_detectors_index_status\": \"green\", \"anomaly_detection_state_status\": \"green\", \"single_entity_detector_count\": 2, \"detector_count\": 5, \"multi_entity_detector_count\": 3, \"anomaly_detection_job_index_status\": \"green\", \"models_checkpoint_index_status\": \"green\", \"anomaly_results_index_status\": \"green\", \"nodes\": { \"2Z4q22BySEyzakYt_A0A2A\": { \"ad_execute_request_count\": 95, \"models\": [ { \"detector_id\": \"WTBnTXwBjd8s6RK4b1Sz\", \"model_type\": \"rcf\", \"last_used_time\": 1633398197185, \"model_id\": \"WTBnTXwBjd8s6RK4b1Sz_model_rcf_0\", \"last_checkpoint_time\": 1633396573679 },...], \"ad_canceled_batch_task_count\": 0, \"ad_hc_execute_request_count\": 75, \"ad_hc_execute_failure_count\": 0, \"model_count\": 28, \"ad_execute_failure_count\": 1, \"ad_batch_task_failure_count\": 0, \"ad_total_batch_task_execution_count\": 27, \"ad_executing_batch_task_count\": 3 }, \"SWD7ihu9TaaW1zKwFZNVNg\": { \"ad_execute_request_count\": 12, \"models\": [ { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\", \"model_type\": \"entity\", \"last_used_time\": 1633398375008, \"model_id\": \"Zi5zTXwBwf_U8gjUTfJG_entity_error13\", \"last_checkpoint_time\": 1633392973682, \"entity\": [ { \"name\": \"error_type\", \"value\": \"error13\" }] },...], \"ad_canceled_batch_task_count\": 1, \"ad_hc_execute_request_count\": 0, \"ad_hc_execute_failure_count\": 0, \"model_count\": 15, \"ad_execute_failure_count\": 2, \"ad_batch_task_failure_count\": 0, \"ad_total_batch_task_execution_count\": 27, \"ad_executing_batch_task_count\": 4 }, \"TQDUXEzyTJyV0H6_T4hYUw\": { \"ad_execute_request_count\": 0, \"models\": [ { \"detector_id\": \"Zi5zTXwBwf_U8gjUTfJG\", \"model_type\": \"entity\", \"last_used_time\": 1633398375004, \"model_id\": \"Zi5zTXwBwf_U8gjUTfJG_entity_error24\", \"last_checkpoint_time\": 1633388177359, \"entity\": [ { \"name\": \"error_type\", \"value\": \"error24\" }] },...], \"ad_canceled_batch_task_count\": 0, \"ad_hc_execute_request_count\": 0, \"ad_hc_execute_failure_count\": 0, \"model_count\": 22, \"ad_execute_failure_count\": 0, \"ad_batch_task_failure_count\": 0, \"ad_total_batch_task_execution_count\": 28, \"ad_executing_batch_task_count\": 3 } } } The model_count parameter shows the total number of models running on each node’s memory.\nFor historical analysis, you see the values for the following fields: ad_total_batch_task_execution_count ad_executing_batch_task_count ad_canceled_batch_task_count ad_batch_task_failure_count If haven’t run any historical analysis, these values show up as 0.\nTo get all stats for a specific node:\nRequest GET _plugins/_anomaly_detection/&lt;nodeId&gt;/stats To get specific stats for a node:\nRequest GET _plugins/_anomaly_detection/&lt;nodeId&gt;/stats/&lt;stat&gt; For example, to get the ad_execute_request_count value for node SWD7ihu9TaaW1zKwFZNVNg: GET _plugins/_anomaly_detection/SWD 7 ihu 9 TaaW 1 zKwFZNVNg/stats/ad_execute_request_count Example response { \"nodes\": { \"SWD7ihu9TaaW1zKwFZNVNg\": { \"ad_execute_request_count\": 12 } } } To get a specific type of stats:\nRequest GET _plugins/_anomaly_detection/stats/&lt;stat&gt; For example: GET _plugins/_anomaly_detection/stats/ad_executing_batch_task_count Example response { \"nodes\": { \"2Z4q22BySEyzakYt_A0A2A\": { \"ad_executing_batch_task_count\": 3 }, \"SWD7ihu9TaaW1zKwFZNVNg\": { \"ad_executing_batch_task_count\": 3 }, \"TQDUXEzyTJyV0H6_T4hYUw\": { \"ad_executing_batch_task_count\": 4 } } } Profile detector\nIntroduced 1.0\nReturns information related to the current state of the detector and memory usage, including current errors and shingle size, to help troubleshoot the detector.\nThis command helps locate logs by identifying the nodes that run the anomaly detector job for each detector.\nIt also helps track the initialization percentage, the required shingles, and the estimated time left.\nRequest GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile/ GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile?_all= true GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile/&lt;type&gt; GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile/&lt;type 1 &gt;,&lt;type 2 &gt; Sample Responses GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile { \"state\": \"DISABLED\", \"error\": \"Stopped detector: AD models memory usage exceeds our limit.\" } GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile?_all= true &amp;pretty { \"state\": \"RUNNING\", \"error\": \"\", \"models\": [ { \"model_id\": \"3Dh6TXwBwf_U8gjURE0F_entity_KSLSh0Wv05RQXiBAQHTEZg\", \"entity\": [ { \"name\": \"ip\", \"value\": \"192.168.1.1\" }, { \"name\": \"error_type\", \"value\": \"error8\" }], \"model_size_in_bytes\": 403491, \"node_id\": \"2Z4q22BySEyzakYt_A0A2A\" },...], \"total_size_in_bytes\": 12911712, \"init_progress\": { \"percentage\": \"100%\" }, \"total_entities\": 33, \"active_entities\": 32, \"ad_task\": { \"ad_task\": { \"task_id\": \"D3I5TnwBYwCbWecg7lN9\", \"last_update_time\": 1633399993685, \"started_by\": \"admin\", \"state\": \"RUNNING\", \"detector_id\": \"3Dh6TXwBwf_U8gjURE0F\", \"task_progress\": 0, \"init_progress\": 0, \"execution_start_time\": 1633399991933, \"is_latest\": true, \"task_type\": \"HISTORICAL_HC_DETECTOR\", \"coordinating_node\": \"2Z4q22BySEyzakYt_A0A2A\", \"detector\": { \"name\": \"testhc-mc\", \"description\": \"test\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log\"], \"filter_query\": { \"match_all\": { \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 5, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"2zh6TXwBwf_U8gjUQ039\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"ui_metadata\": { \"features\": { \"test\": { \"aggregationBy\": \"sum\", \"aggregationOf\": \"value\", \"featureType\": \"simple_aggs\" } }, \"filters\": [] }, \"last_update_time\": 1633387430916, \"category_field\": [ \"ip\", \"error_type\"], \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"MULTI_ENTITY\" }, \"detection_date_range\": { \"start_time\": 1632793800000, \"end_time\": 1633398600000 }, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } }, \"node_id\": \"2Z4q22BySEyzakYt_A0A2A\", \"task_id\": \"D3I5TnwBYwCbWecg7lN9\", \"task_type\": \"HISTORICAL_HC_DETECTOR\", \"detector_task_slots\": 10, \"total_entities_count\": 32, \"pending_entities_count\": 22, \"running_entities_count\": 10, \"running_entities\": [ \"\"\"[{\" name \":\" ip \",\" value \":\" 192.168. 1.1 \"},{\" name \":\" error_type \",\" value \":\" error 9 \"}]\"\"\",...], \"entity_task_profiles\": [ { \"shingle_size\": 8, \"rcf_total_updates\": 1994, \"threshold_model_trained\": true, \"threshold_model_training_data_size\": 0, \"model_size_in_bytes\": 1593240, \"node_id\": \"2Z4q22BySEyzakYt_A0A2A\", \"entity\": [ { \"name\": \"ip\", \"value\": \"192.168.1.1\" }, { \"name\": \"error_type\", \"value\": \"error7\" }], \"task_id\": \"E3I5TnwBYwCbWecg9FMm\", \"task_type\": \"HISTORICAL_HC_ENTITY\" },...] }, \"model_count\": 32 } GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile/total_size_in_bytes { \"total_size_in_bytes\": 13369344 } You can see the ad_task field only for historical analysis.\nThe model_count parameter shows the total number of models that a detector runs on each node’s memory. This is useful if you have several models running on your cluster and want to know the count.\nIf you configured the category field, you can see the number of unique values in the field and all active entities with models running in memory.\nYou can use this data to estimate how much memory is required for anomaly detection so you can decide how to size your cluster. For example, if a detector has one million entities and only 10 of them are active in memory, you need to scale your cluster up or out.\nFor a single-entity detector:\nExample response { \"state\": \"INIT\", \"total_size_in_bytes\": 0, \"init_progress\": { \"percentage\": \"0%\", \"needed_shingles\": 128 }, \"ad_task\": { \"ad_task\": { \"task_id\": \"cfUNOXwBFLNqSEcxAlde\", \"last_update_time\": 1633044731640, \"started_by\": \"admin\", \"state\": \"RUNNING\", \"detector_id\": \"qL4NOXwB__6eNorTAKtJ\", \"task_progress\": 0.49603173, \"init_progress\": 1, \"current_piece\": 1632739800000, \"execution_start_time\": 1633044726365, \"is_latest\": true, \"task_type\": \"HISTORICAL_SINGLE_ENTITY\", \"coordinating_node\": \"bCtWtxWPThq0BIn5P5I4Xw\", \"worker_node\": \"dIyavWhmSYWGz65b4u-lpQ\", \"detector\": { \"name\": \"detector1\", \"description\": \"test\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log\"], \"filter_query\": { \"match_all\": { \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 5, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"p74NOXwB__6eNorTAKss\", \"feature_name\": \"test-feature\", \"feature_enabled\": true, \"aggregation_query\": { \"test_feature\": { \"sum\": { \"field\": \"value\" } } } }], \"ui_metadata\": { \"features\": { \"test-feature\": { \"aggregationBy\": \"sum\", \"aggregationOf\": \"value\", \"featureType\": \"simple_aggs\" } }, \"filters\": [] }, \"last_update_time\": 1633044725832, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"SINGLE_ENTITY\" }, \"detection_date_range\": { \"start_time\": 1632439925885, \"end_time\": 1633044725885 }, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } }, \"shingle_size\": 8, \"rcf_total_updates\": 1994, \"threshold_model_trained\": true, \"threshold_model_training_data_size\": 0, \"model_size_in_bytes\": 1593240, \"node_id\": \"dIyavWhmSYWGz65b4u-lpQ\", \"detector_task_slots\": 1 } } The total_entities parameter shows you the total number of entities including the number of category fields for a detector.\nGetting the total count of entities is an expensive operation for real-time analysis of a detector with more than one category field. By default, for a real-time detection profile, a detector counts the number of entities up to a value of 10,000. For historical analysis, the anomaly detection plugin only detects the top 1,000 entities by default and caches the top entities in memory, so it doesn’t cost much to get the total count of entities for historical analysis.\nThe profile operation also provides information about each entity, such as the entity’s last_sample_timestamp and last_active_timestamp. last_sample_timestamp shows the last document in the input data source index containing the entity, while last_active_timestamp shows the timestamp when the entity’s model was last seen in the model cache.\nIf there are no anomaly results for an entity, either the entity doesn’t have any sample data or resources such as memory and disk IO are constrained relative to the number of entities.\nRequest GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile?_all= true { \"entity\": [ { \"name\": \"host\", \"value\": \"i-00f28ec1eb8997686\" }] } Sample Responses { \"is_active\": true, \"last_active_timestamp\": 1604026394879, \"last_sample_timestamp\": 1604026394879, \"init_progress\": { \"percentage\": \"100%\" }, \"model\": { \"model_id\": \"TFUdd3UBBwIAGQeRh5IS_entity_i-00f28ec1eb8997686\", \"model_size_in_bytes\": 712480, \"node_id\": \"MQ-bTBW3Q2uU_2zX3pyEQg\" }, \"state\": \"RUNNING\" } To get profile information for only historical analysis, specify ad_task.\nSpecifying _all is an expensive operation for multi-category high cardinality detectors.\nRequest GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile?_all GET _plugins/_anomaly_detection/detectors/&lt;detectorId&gt;/_profile/ad_task Sample Responses { \"ad_task\": { \"ad_task\": { \"task_id\": \"CHI0TnwBYwCbWecgqgRA\", \"last_update_time\": 1633399648413, \"started_by\": \"admin\", \"state\": \"RUNNING\", \"detector_id\": \"3Dh6TXwBwf_U8gjURE0F\", \"task_progress\": 0, \"init_progress\": 0, \"execution_start_time\": 1633399646784, \"is_latest\": true, \"task_type\": \"HISTORICAL_HC_DETECTOR\", \"coordinating_node\": \"2Z4q22BySEyzakYt_A0A2A\", \"detector\": { \"name\": \"testhc-mc\", \"description\": \"test\", \"time_field\": \"timestamp\", \"indices\": [ \"server_log\"], \"filter_query\": { \"match_all\": { \"boost\": 1 } }, \"detection_interval\": { \"period\": { \"interval\": 5, \"unit\": \"Minutes\" } }, \"window_delay\": { \"period\": { \"interval\": 1, \"unit\": \"Minutes\" } }, \"shingle_size\": 8, \"schema_version\": 0, \"feature_attributes\": [ { \"feature_id\": \"2zh6TXwBwf_U8gjUQ039\", \"feature_name\": \"test\", \"feature_enabled\": true, \"aggregation_query\": { \"test\": { \"sum\": { \"field\": \"value\" } } } }], \"ui_metadata\": { \"features\": { \"test\": { \"aggregationBy\": \"sum\", \"aggregationOf\": \"value\", \"featureType\": \"simple_aggs\" } }, \"filters\": [] }, \"last_update_time\": 1633387430916, \"category_field\": [ \"ip\", \"error_type\"], \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" }, \"detector_type\": \"MULTI_ENTITY\" }, \"detection_date_range\": { \"start_time\": 1632793800000, \"end_time\": 1633398600000 }, \"user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"own_index\", \"all_access\"], \"custom_attribute_names\": [], \"user_requested_tenant\": \"__user__\" } }, \"node_id\": \"2Z4q22BySEyzakYt_A0A2A\", \"task_id\": \"CHI0TnwBYwCbWecgqgRA\", \"task_type\": \"HISTORICAL_HC_DETECTOR\", \"detector_task_slots\": 10, \"total_entities_count\": 32, \"pending_entities_count\": 22, \"running_entities_count\": 10, \"running_entities\": [ \"\"\"[{\" name \":\" ip \",\" value \":\" 192.168. 1.1 \"},{\" name \":\" error_type \",\" value \":\" error 9 \"}]\"\"\",...], \"entity_task_profiles\": [ { \"shingle_size\": 8, \"rcf_total_updates\": 994, \"threshold_model_trained\": true, \"threshold_model_training_data_size\": 0, \"model_size_in_bytes\": 1593240, \"node_id\": \"2Z4q22BySEyzakYt_A0A2A\", \"entity\": [ { \"name\": \"ip\", \"value\": \"192.168.1.1\" }, { \"name\": \"error_type\", \"value\": \"error6\" }], \"task_id\": \"9XI0TnwBYwCbWecgsAd6\", \"task_type\": \"HISTORICAL_HC_ENTITY\" },...] } } Delete detector results\nIntroduced 1.1\nDeletes the results of a detector based on a query.\nThe delete detector results API only deletes anomaly result documents in the default result index. It doesn’t support deleting anomaly result documents stored in any custom result indices.\nYou need to manually delete anomaly result documents that you don’t need from custom result indices.\nRequest DELETE _plugins/_anomaly_detection/detectors/results { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"detector_id\": { \"value\": \"rlDtOHwBD5tpxlbyW7Nt\" } } }, { \"term\": { \"task_id\": { \"value\": \"TM3tOHwBCi2h__AOXlyQ\" } } }, { \"range\": { \"data_start_time\": { \"lte\": 1632441600000 } } }] } } } Example response { \"took\": 48, \"timed_out\": false, \"total\": 28, \"updated\": 0, \"created\": 0, \"deleted\": 28, \"batches\": 1, \"version_conflicts\": 0, \"noops\": 0, \"retries\": { \"bulk\": 0, \"search\": 0 }, \"throttled_millis\": 0, \"requests_per_second\": -1, \"throttled_until_millis\": 0, \"failures\": [] } Create monitor\nIntroduced 1.0\nCreate a monitor to set up alerts for the detector.\nRequest POST _plugins/_alerting/monitors { \"type\": \"monitor\", \"name\": \"test-monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 20, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"search\": { \"indices\": [ \".opensearch-anomaly-results*\"], \"query\": { \"size\": 1, \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"data_end_time\": { \"from\": \"||-20m\", \"to\": \"\", \"include_lower\": true, \"include_upper\": true, \"boost\": 1 } } }, { \"term\": { \"detector_id\": { \"value\": \"m4ccEnIBTXsGi3mvMt9p\", \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"sort\": [ { \"anomaly_grade\": { \"order\": \"desc\" } }, { \"confidence\": { \"order\": \"desc\" } }], \"aggregations\": { \"max_anomaly_grade\": { \"max\": { \"field\": \"anomaly_grade\" } } } } } }], \"triggers\": [ { \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return ctx.results[0].aggregations.max_anomaly_grade.value!= null &amp;&amp; ctx.results[0].aggregations.max_anomaly_grade.value &gt; 0.7 &amp;&amp; ctx.results[0].hits.hits[0]._source.confidence &gt; 0.7\", \"lang\": \"painless\" } }, \"actions\": [ { \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is my message body.\" }, \"throttle_enabled\": false, \"subject_template\": { \"source\": \"TheSubject\" } }] }] } Example response { \"_id\": \"OClTEnIBmSf7y6LP11Jz\", \"_version\": 1, \"_seq_no\": 10, \"_primary_term\": 1, \"monitor\": { \"type\": \"monitor\", \"schema_version\": 1, \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1589445384043, \"schedule\": { \"period\": { \"interval\": 20, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"search\": { \"indices\": [ \".opensearch-anomaly-results*\"], \"query\": { \"size\": 1, \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"data_end_time\": { \"from\": \"||-20m\", \"to\": \"\", \"include_lower\": true, \"include_upper\": true, \"boost\": 1 } } }, { \"term\": { \"detector_id\": { \"value\": \"m4ccEnIBTXsGi3mvMt9p\", \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"sort\": [ { \"anomaly_grade\": { \"order\": \"desc\" } }, { \"confidence\": { \"order\": \"desc\" } }], \"aggregations\": { \"max_anomaly_grade\": { \"max\": { \"field\": \"anomaly_grade\" } } } } } }], \"triggers\": [ { \"id\": \"NilTEnIBmSf7y6LP11Jr\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return ctx.results[0].aggregations.max_anomaly_grade.value!= null &amp;&amp; ctx.results[0].aggregations.max_anomaly_grade.value &gt; 0.7 &amp;&amp; ctx.results[0].hits.hits[0]._source.confidence &gt; 0.7\", \"lang\": \"painless\" } }, \"actions\": [ { \"id\": \"NylTEnIBmSf7y6LP11Jr\", \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" }, \"throttle_enabled\": false, \"subject_template\": { \"source\": \"TheSubject\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1589445384043 } }",
    "ancestors": [
      "Observability",
      "Anomaly detection"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/ad/index/",
    "title": "Anomaly detection",
    "content": "An anomaly in OpenSearch is any unusual behavior change in your time-series data. Anomalies can provide valuable insights into your data. For example, for IT infrastructure data, an anomaly in the memory usage metric might help you uncover early signs of a system failure.\nIt can be challenging to discover anomalies using conventional methods such as creating visualizations and dashboards. You could configure an alert based on a static threshold, but this requires prior domain knowledge and isn’t adaptive to data that exhibits organic growth or seasonal behavior.\nAnomaly detection automatically detects anomalies in your OpenSearch data in near real-time using the Random Cut Forest (RCF) algorithm. RCF is an unsupervised machine learning algorithm that models a sketch of your incoming data stream to compute an anomaly grade and confidence score value for each incoming data point. These values are used to differentiate an anomaly from normal variations. For more information about how RCF works, see Random Cut Forests.\nYou can pair the anomaly detection plugin with the alerting plugin to notify you as soon as an anomaly is detected.\nTo get started, choose Anomaly Detection in OpenSearch Dashboards.\nTo first test with sample streaming data, you can try out one of the preconfigured detectors with one of the sample datasets.\nStep 1: Define a detector\nA detector is an individual anomaly detection task. You can define multiple detectors, and all the detectors can run simultaneously, with each analyzing data from different sources.\nChoose Create detector.\nAdd in the detector details.\nEnter a name and brief description. Make sure the name is unique and descriptive enough to help you to identify the purpose of the detector.\nSpecify the data source.\nFor Data source, choose the index you want to use as the data source. You can optionally use index patterns to choose multiple indices.\n(Optional) For Data filter, filter the index you chose as the data source. From the Data filter menu, choose Add data filter, and then design your filter query by selecting Field, Operator, and Value, or choose Use query DSL and add your own JSON filter query.\nSpecify a timestamp.\nSelect the Timestamp field in your index.\nDefine operation settings.\nFor Operation settings, define the Detector interval, which is the time interval at which the detector collects data.\nThe detector aggregates the data in this interval, then feeds the aggregated result into the anomaly detection model.\nThe shorter you set this interval, the fewer data points the detector aggregates.\nThe anomaly detection model uses a shingling process, a technique that uses consecutive data points to create a sample for the model. This process needs a certain number of aggregated data points from contiguous intervals.\nWe recommend setting the detector interval based on your actual data. If it’s too long it might delay the results, and if it’s too short it might miss some data. It also won’t have a sufficient number of consecutive data points for the shingle process.\n(Optional) To add extra processing time for data collection, specify a Window delay value.\nThis value tells the detector that the data is not ingested into OpenSearch in real time but with a certain delay. Set the window delay to shift the detector interval to account for this delay.\nFor example, say the detector interval is 10 minutes and data is ingested into your cluster with a general delay of 1 minute. Assume the detector runs at 2:00. The detector attempts to get the last 10 minutes of data from 1:50 to 2:00, but because of the 1-minute delay, it only gets 9 minutes of data and misses the data from 1:59 to 2:00. Setting the window delay to 1 minute shifts the interval window to 1:49 - 1:59, so the detector accounts for all 10 minutes of the detector interval time.\nSpecify custom result index.\nIf you want to store the anomaly detection results in your own index, choose Enable custom result index and specify the custom index to store the result. The anomaly detection plugin adds an opensearch-ad-plugin-result- prefix to the index name that you input. For example, if you input abc as the result index name, the final index name is opensearch-ad-plugin-result-abc.\nYou can use the dash “-” sign to separate the namespace to manage custom result index permissions. For example, if you use opensearch-ad-plugin-result-financial-us-group1 as the result index, you can create a permission role based on the pattern opensearch-ad-plugin-result-financial-us-* to represent the “financial” department at a granular level for the “us” area.\nIf the custom index you specify doesn’t already exist, the anomaly detection plugin creates this index when you create the detector and start your real-time or historical analysis.\nIf the custom index already exists, the plugin checks if the index mapping of the custom index matches the anomaly result file. You need to make sure the custom index has valid mapping as shown here: anomaly-results.json.\nTo use the custom result index option, you need the following permissions: indices:admin/create - If the custom index already exists, you don’t need this. indices:data/write/index - You need the write permission for the anomaly detection plugin to write results into the custom index for a single-entity detector. indices:data/read/search - You need the search permission because the anomaly detection plugin needs to search custom result indices to show results on the anomaly detection UI. indices:data/write/delete - Because the detector might generate a large number of anomaly results, you need the delete permission to delete old data and save disk space. indices:data/write/bulk* - You need the bulk* permission because the anomaly detection plugin uses the bulk API to write results into the custom index.\nManaging the custom result index:\nThe anomaly detection dashboard queries all detectors’ results from all custom result indices. Having too many custom result indices might impact the performance of the anomaly detection plugin.\nYou can use Index State Management to rollover old result indices. You can also manually delete or archive any old result indices. We recommend reusing a custom result index for multiple detectors.\nChoose Next.\nAfter you define the detector, the next step is to configure the model.\nStep 2: Configure the model\nAdd features to your detector\nA feature is the field in your index that you want to check for anomalies. A detector can discover anomalies across one or more features. You must choose an aggregation method for each feature: average(), count(), sum(), min(), or max(). The aggregation method determines what constitutes an anomaly.\nFor example, if you choose min(), the detector focuses on finding anomalies based on the minimum values of your feature. If you choose average(), the detector finds anomalies based on the average values of your feature.\nA multi-feature model correlates anomalies across all its features. The curse of dimensionality makes it less likely for multi-feature models to identify smaller anomalies as compared to a single-feature model. Adding more features might negatively impact the precision and recall of a model. A higher proportion of noise in your data might further amplify this negative impact. Selecting the optimal feature set is usually an iterative process. By default, the maximum number of features for a detector is 5. You can adjust this limit with the plugins.anomaly_detection.max_anomaly_features setting.\nTo configure an anomaly detection model based on an aggregation method, follow these steps:\nOn the Configure Model page, enter the Feature name and check Enable feature.\nFor Find anomalies based on, select Field Value.\nFor aggregation method, select either average(), count(), sum(), min(), or max().\nFor Field, select from the available options.\nTo configure an anomaly detection model based on a JSON aggregation query, follow these steps:\nOn the Configure Model page, enter the Feature name and check Enable feature.\nFor Find anomalies based on, select Custom expression. You will see the JSON editor window open up.\nEnter your JSON aggregation query in the editor.\nFor acceptable JSON query syntax, see OpenSearch Query DSL (Optional) Set category fields for high cardinality\nYou can categorize anomalies based on a keyword or IP field type.\nThe category field categorizes or slices the source time series with a dimension like IP addresses, product IDs, country codes, and so on. This helps to see a granular view of anomalies within each entity of the category field to isolate and debug issues.\nTo set a category field, choose Enable a category field and select a field. You can’t change the category fields after you create the detector.\nOnly a certain number of unique entities are supported in the category field. Use the following equation to calculate the recommended total number of entities supported in a cluster: (data nodes * heap size * anomaly detection maximum memory percentage) / (entity model size of a detector) To get the entity model size of a detector, use the profile detector API. You can adjust the maximum memory percentage with the plugins.anomaly_detection.model_max_size_percent setting.\nThis formula provides a good starting point, but make sure to test with a representative workload.\nFor example, for a cluster with three data nodes, each with 8 GB of JVM heap size, a maximum memory percentage of 10% (default), and the entity model size of the detector as 1MB: the total number of unique entities supported is (8.096 * 10^9 * 0.1 / 1 MB) * 3 = 2429.\nIf the actual total number of unique entities higher than this number that you calculate (in this case: 2429), the anomaly detector makes its best effort to model the extra entities. The detector prioritizes entities that occur more often and are more recent.\n(Advanced settings) Set a shingle size\nSet the number of aggregation intervals from your data stream to consider in a detection window. It’s best to choose this value based on your actual data to see which one leads to the best results for your use case.\nThe anomaly detector expects the shingle size to be in the range of 1 and 60. The default shingle size is 8. We recommend that you don’t choose 1 unless you have two or more features. Smaller values might increase recall but also false positives. Larger values might be useful for ignoring noise in a signal.\nPreview sample anomalies\nPreview sample anomalies and adjust the feature settings if needed.\nFor sample previews, the anomaly detection plugin selects a small number of data samples—for example, one data point every 30 minutes—and uses interpolation to estimate the remaining data points to approximate the actual feature data. It loads this sample dataset into the detector. The detector uses this sample dataset to generate a sample preview of anomaly results.\nExamine the sample preview and use it to fine-tune your feature configurations (for example, enable or disable features) to get more accurate results.\nChoose Preview sample anomalies.\nIf you don’t see any sample anomaly result, check the detector interval and make sure you have more than 400 data points for some entities during the preview date range.\nChoose Next.\nStep 3: Set up detector jobs\nTo start a real-time detector to find anomalies in your data in near real-time, check Start real-time detector automatically (recommended).\nAlternatively, if you want to perform historical analysis and find patterns in long historical data windows (weeks or months), check Run historical analysis detection and select a date range (at least 128 detection intervals).\nAnalyzing historical data helps you get familiar with the anomaly detection plugin. You can also evaluate the performance of a detector with historical data to further fine-tune it.\nWe recommend experimenting with historical analysis with different feature sets and checking the precision before moving on to real-time detectors.\nStep 4: Review and create\nReview your detector settings and model configurations to make sure that they’re valid and then select Create detector. If you see any validation errors, edit the settings to fix the errors and then return back to this page.\nStep 5: Observe the results\nChoose the Real-time results or Historical analysis tab. For real-time results, you need to wait for some time to see the anomaly results. If the detector interval is 10 minutes, the detector might take more than an hour to start, because its waiting for sufficient data to generate anomalies.\nA shorter interval means the model passes the shingle process more quickly and starts to generate the anomaly results sooner.\nUse the profile detector operation to make sure you have sufficient data points.\nIf you see the detector pending in “initialization” for longer than a day, aggregate your existing data using the detector interval to check for any missing data points. If you find a lot of missing data points from the aggregated data, consider increasing the detector interval.\nChoose and drag over the anomaly line chart to zoom in and see a more detailed view of an anomaly.\nAnalyze anomalies with the following visualizations: Live anomalies (for real-time results) displays live anomaly results for the last 60 intervals. For example, if the interval is 10, it shows results for the last 600 minutes. The chart refreshes every 30 seconds. Anomaly overview (for real-time results) / Anomaly history (for historical analysis in the Historical analysis tab) plots the anomaly grade with the corresponding measure of confidence. This pane includes:\nThe number of anomaly occurrences based on the given data-time range.\nThe Average anomaly grade, a number between 0 and 1 that indicates how anomalous a data point is. An anomaly grade of 0 represents “not an anomaly,” and a non-zero value represents the relative severity of the anomaly. Confidence estimate of the probability that the reported anomaly grade matches the expected anomaly grade. Confidence increases as the model observes more data and learns the data behavior and trends. Note that confidence is distinct from model accuracy. Last anomaly occurrence is the time at which the last anomaly occurred.\nUnderneath Anomaly overview / Anomaly history are: Feature breakdown plots the features based on the aggregation method. You can vary the date-time range of the detector. Selecting a point on the feature line chart shows the Feature output, the number of times a field appears in your index, and the Expected value, a predicted value for the feature output. Where there is no anomaly, the output and expected values are equal. Anomaly occurrences shows the Start time, End time, Data confidence, and Anomaly grade for each detected anomaly.\nSelecting a point on the anomaly line chart shows Feature Contribution, the percentage of a feature that contributes to the anomaly If you set the category field, you see an additional Heat map chart. The heat map correlates results for anomalous entities. This chart is empty until you select an anomalous entity. You also see the anomaly and feature line chart for the time period of the anomaly ( anomaly_grade &gt; 0).\nIf you have set multiple category fields, you can select a subset of fields to filter and sort the fields by. Selecting a subset of fields lets you see the top values of one field that share a common value with another field.\nFor example, if you have a detector with the category fields ip and endpoint, you can select endpoint in the View by dropdown menu. Then, select a specific cell to overlay the top 20 values of ip on the charts. The anomaly detection plugin selects the top ip by default. You can see a maximum of 5 individual time-series values at the same time.\nStep 6: Set up alerts\nUnder Real-time results, choose Set up alerts and configure a monitor to notify you when anomalies are detected. For steps to create a monitor and set up notifications based on your anomaly detector, see Monitors.\nIf you stop or delete a detector, make sure to delete any monitors associated with it.\nStep 7: Adjust the model\nTo see all the configuration settings for a detector, choose the Detector configuration tab.\nTo make any changes to the detector configuration, or fine tune the time interval to minimize any false positives, go to the Detector configuration section and choose Edit.\nYou need to stop real-time and historical analysis to change its configuration. Confirm that you want to stop the detector and proceed.\nTo enable or disable features, in the Features section, choose Edit and adjust the feature settings as needed. After you make your changes, choose Save and start detector.\nStep 8: Manage your detectors\nTo start, stop, or delete a detector, go to the Detectors page.\nChoose the detector name.\nChoose Actions and select Start real-time detectors, Stop real-time detectors, or Delete detectors.",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/ad/result-mapping/",
    "title": "Anomaly result mapping",
    "content": "If you enabled custom result index, the anomaly detection plugin stores the results in your own index.\nIf the anomaly detector doesn’t detect an anomaly, the result has the following format: { \"detector_id\": \"kzcZ43wBgEQAbjDnhzGF\", \"schema_version\": 5, \"data_start_time\": 1635898161367, \"data_end_time\": 1635898221367, \"feature_data\": [ { \"feature_id\": \"processing_bytes_max\", \"feature_name\": \"processing bytes max\", \"data\": 2322 }, { \"feature_id\": \"processing_bytes_avg\", \"feature_name\": \"processing bytes avg\", \"data\": 1718.6666666666667 }, { \"feature_id\": \"processing_bytes_min\", \"feature_name\": \"processing bytes min\", \"data\": 1375 }, { \"feature_id\": \"processing_bytes_sum\", \"feature_name\": \"processing bytes sum\", \"data\": 5156 }, { \"feature_id\": \"processing_time_max\", \"feature_name\": \"processing time max\", \"data\": 31198 }], \"execution_start_time\": 1635898231577, \"execution_end_time\": 1635898231622, \"anomaly_score\": 1.8124904404395776, \"anomaly_grade\": 0, \"confidence\": 0.9802940756605277, \"entity\": [ { \"name\": \"process_name\", \"value\": \"process_3\" }], \"model_id\": \"kzcZ43wBgEQAbjDnhzGF_entity_process_3\", \"threshold\": 1.2368549346675202 } Response body fields Field Description detector_id A unique ID for identifying a detector. schema_version The mapping version of the result index. data_start_time The start of the detection range of the aggregated data. data_end_time The end of the detection range of the aggregated data. feature_data An array of the aggregated data points between the data_start_time and data_end_time. execution_start_time The actual start time of the detector for a specific run that produces the anomaly result. This start time includes the window delay parameter that you can set to delay data collection. Window delay is the difference between the execution_start_time and data_start_time. execution_end_time The actual end time of the detector for a specific run that produces the anomaly result. anomaly_score Indicates relative severity of an anomaly. The higher the score, the more anomalous a data point is. anomaly_grade A normalized version of the anomaly_score on a scale between 0 and 1. confidence The probability of the accuracy of the anomaly_score. The closer this number is to 1, the higher the accuracy. During the probation period of a running detector, the confidence is low (&lt; 0.9) because of its exposure to limited data. entity An entity is a combination of specific category fields’ values. It includes the name and value of the category field. In the previous example, process_name is the category field and one of the processes such as process_3 is the field’s value. The entity field is only present for a high-cardinality detector (where you’ve selected a category field). model_id A unique ID that identifies a model. If a detector is a single-stream detector (with no category field), it has only one model. If a detector is a high-cardinality detector (with one or more category fields), it might have multiple models, one for each entity. threshold One of the criteria for a detector to classify a data point as an anomaly is that its anomaly_score must surpass a dynamic threshold. This field records the current threshold. If an anomaly detector detects an anomaly, the result has the following format: { \"detector_id\": \"fylE53wBc9MCt6q12tKp\", \"schema_version\": 0, \"data_start_time\": 1635927900000, \"data_end_time\": 1635927960000, \"feature_data\": [ { \"feature_id\": \"processing_bytes_max\", \"feature_name\": \"processing bytes max\", \"data\": 2291 }, { \"feature_id\": \"processing_bytes_avg\", \"feature_name\": \"processing bytes avg\", \"data\": 1677.3333333333333 }, { \"feature_id\": \"processing_bytes_min\", \"feature_name\": \"processing bytes min\", \"data\": 1054 }, { \"feature_id\": \"processing_bytes_sum\", \"feature_name\": \"processing bytes sum\", \"data\": 5032 }, { \"feature_id\": \"processing_time_max\", \"feature_name\": \"processing time max\", \"data\": 11422 }], \"anomaly_score\": 1.1986675882872033, \"anomaly_grade\": 0.26806225550178464, \"confidence\": 0.9607519742565531, \"entity\": [ { \"name\": \"process_name\", \"value\": \"process_3\" }], \"approx_anomaly_start_time\": 1635927900000, \"relevant_attribution\": [ { \"feature_id\": \"processing_bytes_max\", \"data\": 0.03628638020431366 }, { \"feature_id\": \"processing_bytes_avg\", \"data\": 0.03384479053991436 }, { \"feature_id\": \"processing_bytes_min\", \"data\": 0.058812549572819096 }, { \"feature_id\": \"processing_bytes_sum\", \"data\": 0.10154576265526988 }, { \"feature_id\": \"processing_time_max\", \"data\": 0.7695105170276828 }], \"expected_values\": [ { \"likelihood\": 1, \"value_list\": [ { \"feature_id\": \"processing_bytes_max\", \"data\": 2291 }, { \"feature_id\": \"processing_bytes_avg\", \"data\": 1677.3333333333333 }, { \"feature_id\": \"processing_bytes_min\", \"data\": 1054 }, { \"feature_id\": \"processing_bytes_sum\", \"data\": 6062 }, { \"feature_id\": \"processing_time_max\", \"data\": 23379 }] }], \"threshold\": 1.0993584705913992, \"execution_end_time\": 1635898427895, \"execution_start_time\": 1635898427803 } You can see the following additional fields: Field Description relevant_attribution Represents the contribution of each input variable. The sum of the attributions is normalized to 1. expected_values The expected value for each feature. At times, the detector might detect an anomaly late.\nLet’s say the detector sees a random mix of the triples {1, 2, 3} and {2, 4, 5} that correspond to slow weeks and busy weeks, respectively. For example 1, 2, 3, 1, 2, 3, 2, 4, 5, 1, 2, 3, 2, 4, 5, … and so on.\nIf the detector comes across a pattern {2, 2, X} and it’s yet to see X, the detector infers that the pattern is anomalous, but it can’t determine at this point which of the 2’s is the cause. If X = 3, then the detector knows it’s the first 2 in that unfinished triple, and if X = 5, then it’s the second 2. If it’s the first 2, then the detector detects the anomaly late.\nIf a detector detects an anomaly late, the result has the following additional fields: Field Description past_values The actual input that triggered an anomaly. If past_values is null, the attributions or expected values are from the current input. If past_values is not null, the attributions or expected values are from a past input (for example, the previous two steps of the data [1,2,3]). approx_anomaly_start_time The approximate time of the actual input that triggers an anomaly. This field helps you understand when a detector flags an anomaly. Both single-stream and high-cardinality detectors don’t query previous anomaly results because these queries are expensive operations. The cost is especially high for high-cardinality detectors that might have a lot of entities. If the data is not continuous, the accuracy of this field is low and the actual time that the detector detects an anomaly can be earlier. { \"detector_id\": \"kzcZ43wBgEQAbjDnhzGF\", \"confidence\": 0.9746820962328963, \"relevant_attribution\": [ { \"feature_id\": \"deny_max1\", \"data\": 0.07339452532666227 }, { \"feature_id\": \"deny_avg\", \"data\": 0.04934972719948845 }, { \"feature_id\": \"deny_min\", \"data\": 0.01803003656061806 }, { \"feature_id\": \"deny_sum\", \"data\": 0.14804918212089874 }, { \"feature_id\": \"accept_max5\", \"data\": 0.7111765287923325 }], \"task_id\": \"9Dck43wBgEQAbjDn4zEe\", \"threshold\": 1, \"model_id\": \"kzcZ43wBgEQAbjDnhzGF_entity_app_0\", \"schema_version\": 5, \"anomaly_score\": 1.141419389056506, \"execution_start_time\": 1635898427803, \"past_values\": [ { \"feature_id\": \"processing_bytes_max\", \"data\": 905 }, { \"feature_id\": \"processing_bytes_avg\", \"data\": 479 }, { \"feature_id\": \"processing_bytes_min\", \"data\": 128 }, { \"feature_id\": \"processing_bytes_sum\", \"data\": 1437 }, { \"feature_id\": \"processing_time_max\", \"data\": 8440 }], \"data_end_time\": 1635883920000, \"data_start_time\": 1635883860000, \"feature_data\": [ { \"feature_id\": \"processing_bytes_max\", \"feature_name\": \"processing bytes max\", \"data\": 1360 }, { \"feature_id\": \"processing_bytes_avg\", \"feature_name\": \"processing bytes avg\", \"data\": 990 }, { \"feature_id\": \"processing_bytes_min\", \"feature_name\": \"processing bytes min\", \"data\": 608 }, { \"feature_id\": \"processing_bytes_sum\", \"feature_name\": \"processing bytes sum\", \"data\": 2970 }, { \"feature_id\": \"processing_time_max\", \"feature_name\": \"processing time max\", \"data\": 9670 }], \"expected_values\": [ { \"likelihood\": 1, \"value_list\": [ { \"feature_id\": \"processing_bytes_max\", \"data\": 905 }, { \"feature_id\": \"processing_bytes_avg\", \"data\": 479 }, { \"feature_id\": \"processing_bytes_min\", \"data\": 128 }, { \"feature_id\": \"processing_bytes_sum\", \"data\": 4847 }, { \"feature_id\": \"processing_time_max\", \"data\": 15713 }] }], \"execution_end_time\": 1635898427895, \"anomaly_grade\": 0.5514172746375128, \"entity\": [ { \"name\": \"process_name\", \"value\": \"process_3\" }], \"approx_anomaly_start_time\": 1635883620000 }",
    "ancestors": [
      "Observability",
      "Anomaly detection"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/ad/security/",
    "title": "Anomaly detection security",
    "content": "You can use the Security plugin with anomaly detection in OpenSearch to limit non-admin users to specific actions. For example, you might want some users to only be able to create, update, or delete detectors, while others to only view detectors.\nAll anomaly detection indices are protected as system indices. Only a super admin user or an admin user with a TLS certificate can access system indices. For more information, see System indices.\nSecurity for anomaly detection works the same as security for alerting.\nBasic permissions\nAs an admin user, you can use the Security plugin to assign specific permissions to users based on which APIs they need access to. For a list of supported APIs, see Anomaly detection API.\nThe Security plugin has two built-in roles that cover most anomaly detection use cases: anomaly_full_access and anomaly_read_access. For descriptions of each, see Predefined roles.\nIf these roles don’t meet your needs, mix and match individual anomaly detection permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the cluster:admin/opensearch/ad/detector/delete permission lets you delete detectors.\nA note on alerts and fine-grained access control\nWhen a trigger generates an alert, the detector and monitor configurations, the alert itself, and any notification that is sent to a channel may include metadata describing the index being queried. By design, the plugin must extract the data and store it as metadata outside of the index. Document-level security (DLS) and field-level security (FLS) access controls are designed to protect the data in the index. But once the data is stored outside the index as metadata, users with access to the detector and monitor configurations, alerts, and their notifications will be able to view this metadata and possibly infer the contents and quality of data in the index, which would otherwise be concealed by DLS and FLS access control.\nTo reduce the chances of unintended users viewing metadata that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See Limit access by backend role for details.\n(Advanced) Limit access by backend role\nUse backend roles to configure fine-grained access to individual detectors based on roles. For example, users of different departments in an organization can view detectors owned by their own department.\nFirst, make sure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider, but if you use the internal user database, you can use the REST API to add them manually.\nNext, enable the following setting: PUT _cluster/settings { \"transient\": { \"plugins.anomaly_detection.filter_by_backend_roles\": \"true\" } } Now when users view anomaly detection resources in OpenSearch Dashboards (or make REST API calls), they only see detectors created by users who share at least one backend role.\nFor example, consider two users: alice and bob. alice has an analyst backend role: PUT _plugins/_security/api/internalusers/alice { \"password\": \"alice\", \"backend_roles\": [ \"analyst\"], \"attributes\": {} } bob has a human-resources backend role: PUT _plugins/_security/api/internalusers/bob { \"password\": \"bob\", \"backend_roles\": [ \"human-resources\"], \"attributes\": {} } Both alice and bob have full access to anomaly detection: PUT _plugins/_security/api/rolesmapping/anomaly_full_access { \"backend_roles\": [], \"hosts\": [], \"users\": [ \"alice\", \"bob\"] } Because they have different backend roles, alice and bob cannot view each other’s detectors or their results.",
    "ancestors": [
      "Observability",
      "Anomaly detection"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/ad/settings/",
    "title": "Settings",
    "content": "The anomaly detection plugin adds several settings to the standard OpenSearch cluster settings.\nThe settings are dynamic, so you can change the default behavior of the plugin without restarting your cluster.\nYou can mark settings as persistent or transient.\nFor example, to update the retention period of the result index: PUT _cluster/settings { \"transient\": { \"plugins.anomaly_detection.ad_result_history_retention_period\": \"5m\" } } Setting Default Description plugins.anomaly_detection.enabled\nTrue\nWhether the anomaly detection plugin is enabled or not. If disabled, all detectors immediately stop running.\nplugins.anomaly_detection.max_anomaly_detectors\n1,000\nThe maximum number of non-high cardinality detectors (no category field) users can create.\nplugins.anomaly_detection.max_multi_entity_anomaly_detectors\n10\nThe maximum number of high cardinality detectors (with category field) in a cluster.\nplugins.anomaly_detection.max_anomaly_features\n5\nThe maximum number of features for a detector.\nplugins.anomaly_detection.ad_result_history_rollover_period\n12h\nHow often the rollover condition is checked. If true, the anomaly detection plugin rolls over the result index to a new index.\nplugins.anomaly_detection.ad_result_history_max_docs_per_shard\n1,350,000,000\nThe maximum number of documents in a single shard of the result index. The anomaly detection plugin only counts the refreshed documents in the primary shards.\nplugins.anomaly_detection.max_entities_per_query\n1,000,000\nThe maximum unique values per detection interval for high cardinality detectors. By default, if the category field(s) have more than the configured unique values in a detector interval, the anomaly detection plugin orders them by the natural ordering of categorical values (for example, entity ab comes before bc) and then selects the top values.\nplugins.anomaly_detection.max_entities_for_preview\n5\nThe maximum unique category field values displayed with the preview operation for high cardinality detectors. By default, if the category field(s) have more than the configured unique values in a detector interval, the anomaly detection plugin orders them by the natural ordering of categorical values (for example, entity ab comes before bc) and then selects the top values.\nplugins.anomaly_detection.max_primary_shards\n10\nThe maximum number of primary shards an anomaly detection index can have.\nplugins.anomaly_detection.filter_by_backend_roles\nFalse\nWhen you enable the Security plugin and set this to true, the anomaly detection plugin filters results based on the user’s backend role(s).\nplugins.anomaly_detection.max_batch_task_per_node\n10\nStarting a historical analysis triggers a batch task. This setting is the number of batch tasks that you can run per data node. You can tune this setting from 1 to 1,000. If the data nodes can’t support all batch tasks and you’re not sure if the data nodes are capable of running more historical analysis, add more data nodes instead of changing this setting to a higher value. Increasing this value might bring more load on each data node.\nplugins.anomaly_detection.max_old_ad_task_docs_per_detector\n1\nYou can run historical analysis for the same detector many times. For each run, the anomaly detection plugin creates a new task. This setting is the number of previous tasks the plugin keeps. Set this value to at least 1 to track its last run. You can keep a maximum of 1,000 old tasks to avoid overwhelming the cluster.\nplugins.anomaly_detection.batch_task_piece_size\n1,000\nThe date range for a historical task is split into smaller pieces and the anomaly detection plugin runs the task piece by piece. Each piece contains 1,000 detection intervals by default. For example, if detector interval is 1 minute and one piece is 1,000 minutes, the feature data is queried every 1,000 minutes. You can change this setting from 1 to 10,000.\nplugins.anomaly_detection.batch_task_piece_interval_seconds\n5\nAdd a time interval between two pieces of the same historical analysis task. This interval prevents the task from consuming too much of the available resources and starving other operations like search and bulk index. You can change this setting from 1 to 600 seconds.\nplugins.anomaly_detection.max_top_entities_for_historical_analysis\n1,000\nThe maximum number of top entities that you run for a high cardinality detector historical analysis. The range is from 1 to 10,000.\nplugins.anomaly_detection.max_running_entities_per_detector_for_historical_analysis\n10\nThe number of entity tasks that you can run in parallel for a high cardinality detector analysis. The task slots available on your cluster also impact how many entities run in parallel. If a cluster has 3 data nodes, each data node has 10 task slots by default. Say you already have two high cardinality detectors and each of them run 10 entities. If you start a single-entity detector that takes 1 task slot, the number of task slots available is 10 * 3 - 10 * 2 - 1 = 9. If you now start a new high cardinality detector, the detector can only run 9 entities in parallel and not 10. You can tune this value from 1 to 1,000 based on your cluster’s capability. If you set a higher value, the anomaly detection plugin runs historical analysis faster but also consumes more resources.\nplugins.anomaly_detection.max_cached_deleted_tasks\n1,000\nYou can rerun historical analysis for a single detector as many times as you like. The anomaly detection plugin only keeps a limited number of old tasks, by default 1 old task. If you run historical analysis three times for a detector, the oldest task is deleted. Because historical analysis generates a number of anomaly results in a short span of time, it’s necessary to clean up anomaly results for a deleted task. With this field, you can configure how many deleted tasks you can cache at most. The plugin cleans up a task’s results when it’s deleted. If the plugin fails to do this cleanup, it adds the task’s results into a cache and an hourly cron job performs the cleanup. You can use this setting to limit how many old tasks are put into cache to avoid a DDoS attack. After an hour, if still you find an old task result in the cache, use the delete detector results API to delete the task result manually. You can tune this setting from 1 to 10,000.\nplugins.anomaly_detection.delete_anomaly_result_when_delete_detector\nFalse\nWhether the anomaly detection plugin deletes the anomaly result when you delete a detector. If you want to save some disk space, especially if you’ve high cardinality detectors generating a lot of results, set this field to true. Alternatively, you can use the delete detector results API to manually delete the results.\nplugins.anomaly_detection.dedicated_cache_size\n10\nIf the real-time analysis of a high cardinality detector starts successfully, the anomaly detection plugin guarantees keeping 10 (dynamically adjustable via this setting) entities’ models in memory per node. If the number of entities exceeds this limit, the plugin puts the extra entities’ models in a memory space shared by all detectors. The actual number of entities varies based on the memory that you’ve available and the frequencies of the entities. If you’d like the plugin to guarantee keeping more entities’ models in memory and if you’re cluster has sufficient memory, you can increase this setting value.\nplugins.anomaly_detection.max_concurrent_preview\n2\nThe maximum number of concurrent previews. You can use this setting to limit resource usage.\nplugins.anomaly_detection.model_max_size_percent\n0.1\nThe upper bound of the memory percentage for a model.\nplugins.anomaly_detection.door_keeper_in_cache.enabled\nFalse\nWhen set to true, OpenSearch places a bloom filter in front of an inactive entity cache to filter out items that are not likely to appear more than once.\nplugins.anomaly_detection.hcad_cold_start_interpolation.enabled\nFalse\nWhen set to true, enables interpolation in high-cardinality anomaly detection (HCAD) cold start.",
    "ancestors": [
      "Observability",
      "Anomaly detection"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/alerting/api/",
    "title": "API",
    "content": "Use the Alerting API to programmatically create, update, and manage monitors and alerts.\nTable of contents Create a query-level monitor Create a bucket-level monitor Document-level monitors Search for monitor findings Create a document-level monitor Limitations Update monitor Get monitor Monitor stats Delete monitor Search monitors Run monitor Get alerts Acknowledge alert Create destination Update destination Get destination Get destinations Delete destination Create email account Update email account Get email account Delete email account Search email account Create email group Update email group Get email group Delete email group Search email group Create a query-level monitor\nIntroduced 1.0\nQuery-level monitors run the query and check whether or not the results should trigger an alert. Query-level monitors can only trigger one alert at a time. For more information about query-level monitors and bucket-level monitors, see Create monitors.\nSample Request POST _plugins/_alerting/monitors { \"type\": \"monitor\", \"name\": \"test-monitor\", \"monitor_type\": \"query_level_monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"movies\"], \"query\": { \"size\": 0, \"aggregations\": {}, \"query\": { \"bool\": { \"filter\": { \"range\": { \"@timestamp\": { \"gte\": \"{{period_end}}||-1h\", \"lte\": \"{{period_end}}\", \"format\": \"epoch_millis\" } } } } } } } }], \"triggers\": [{ \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"ctx.results[0].hits.total.value &gt; 0\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is my message body.\" }, \"throttle_enabled\": true, \"throttle\": { \"value\": 27, \"unit\": \"MINUTES\" }, \"subject_template\": { \"source\": \"TheSubject\" } }] }] } If you use a custom webhook for your destination and need to embed JSON in the message body, be sure to escape your quotes: { \"message_template\": { \"source\": \"{ \\\" text \\\": \\\" Monitor {{ctx.monitor.name}} just entered alert status. Please investigate the issue. - Trigger: {{ctx.trigger.name}} - Severity: {{ctx.trigger.severity}} - Period start: {{ctx.periodStart}} - Period end: {{ctx.periodEnd}} \\\" }\" } } Optionally, to specify a backend role, you can add the rbac_roles parameter and backend role names to the bottom of your create monitor request.\nExample request\nThe following request creates a query-level monitor and provides two backend roles, role1 and role2. The section at the bottom of the request shows the line that specifies the roles with this syntax: \"rbac_roles\": [\"role1\", \"role2\"]. POST _plugins/_alerting/monitors { \"type\": \"monitor\", \"name\": \"test-monitor\", \"monitor_type\": \"query_level_monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"movies\"], \"query\": { \"size\": 0, \"aggregations\": {}, \"query\": { \"bool\": { \"filter\": { \"range\": { \"@timestamp\": { \"gte\": \"||-1h\", \"lte\": \"\", \"format\": \"epoch_millis\" } } } } } } } }], \"triggers\": [{ \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"ctx.results[0].hits.total.value &gt; 0\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is my message body.\" }, \"throttle_enabled\": true, \"throttle\": { \"value\": 27, \"unit\": \"MINUTES\" }, \"subject_template\": { \"source\": \"TheSubject\" } }] }], \"rbac_roles\": [ \"role1\", \"role2\"] } To learn more about using backend roles to limit access, see (Advanced) Limit access by backend role.\nExample response { \"_id\": \"vd5k2GsBlQ5JUWWFxhsP\", \"_version\": 1, \"_seq_no\": 7, \"_primary_term\": 1, \"monitor\": { \"type\": \"monitor\", \"schema_version\": 1, \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1562703611363, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"movies\"], \"query\": { \"size\": 0, \"query\": { \"bool\": { \"filter\": [{ \"range\": { \"@timestamp\": { \"from\": \"{{period_end}}||-1h\", \"to\": \"{{period_end}}\", \"include_lower\": true, \"include_upper\": true, \"format\": \"epoch_millis\", \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"aggregations\": {} } } }], \"triggers\": [{ \"id\": \"ud5k2GsBlQ5JUWWFxRvi\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"ctx.results[0].hits.total.value &gt; 0\", \"lang\": \"painless\" } }, \"actions\": [{ \"id\": \"ut5k2GsBlQ5JUWWFxRvj\", \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" }, \"throttle_enabled\": false, \"subject_template\": { \"source\": \"Subject\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1562703611363 } } If you want to specify a timezone, you can do so by including a cron expression with a timezone name in the schedule section of your request.\nThe following example creates a monitor that runs at 12:10 PM Pacific Time on the 1st day of every month.\nRequest { \"type\": \"monitor\", \"name\": \"test-monitor\", \"monitor_type\": \"query_level_monitor\", \"enabled\": true, \"schedule\": { \"cron\": { \"expression\": \"10 12 1 * *\", \"timezone\": \"America/Los_Angeles\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"movies\"], \"query\": { \"size\": 0, \"aggregations\": {}, \"query\": { \"bool\": { \"filter\": { \"range\": { \"@timestamp\": { \"gte\": \"{{period_end}}||-1h\", \"lte\": \"{{period_end}}\", \"format\": \"epoch_millis\" } } } } } } } }], \"triggers\": [{ \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"ctx.results[0].hits.total.value &gt; 0\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"ld7912sBlQ5JUWWFThoW\", \"message_template\": { \"source\": \"This is a message body.\" }, \"throttle_enabled\": true, \"throttle\": { \"value\": 27, \"unit\": \"MINUTES\" }, \"subject_template\": { \"source\": \"Subject\" } }] }] } For a full list of timezone names, refer to Wikipedia. The alerting plugin uses the Java TimeZone class to convert a ZoneId to a valid timezone.\nCreate a bucket-level monitor\nBucket-level monitors categorize results into buckets separated by fields. The monitor then runs your script with each bucket’s results and evaluates whether to trigger an alert. For more information about bucket-level and query-level monitors, see Create monitors. POST _plugins/_alerting/monitors { \"type\": \"monitor\", \"name\": \"Demo bucket-level monitor\", \"monitor_type\": \"bucket_level_monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"search\": { \"indices\": [ \"movies\"], \"query\": { \"size\": 0, \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"order_date\": { \"from\": \"{{period_end}}||-1h\", \"to\": \"{{period_end}}\", \"include_lower\": true, \"include_upper\": true, \"format\": \"epoch_millis\" } } }] } }, \"aggregations\": { \"composite_agg\": { \"composite\": { \"sources\": [ { \"user\": { \"terms\": { \"field\": \"user\" } } }] }, \"aggregations\": { \"avg_products_base_price\": { \"avg\": { \"field\": \"products.base_price\" } } } } } } } }], \"triggers\": [ { \"bucket_level_trigger\": { \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"buckets_path\": { \"_count\": \"_count\", \"avg_products_base_price\": \"avg_products_base_price\" }, \"parent_bucket_path\": \"composite_agg\", \"script\": { \"source\": \"params._count &gt; 50 || params.avg_products_base_price &lt; 35\", \"lang\": \"painless\" } }, \"actions\": [ { \"name\": \"test-action\", \"destination_id\": \"E4o5hnsB6KjPKmHtpfCA\", \"message_template\": { \"source\": \"\"\"Monitor {{ctx.monitor.name}} just entered alert status. Please investigate the issue. - Trigger: {{ctx.trigger.name}} - Severity: {{ctx.trigger.severity}} - Period start: {{ctx.periodStart}} - Period end: {{ctx.periodEnd}} - Deduped Alerts: {{ctx.dedupedAlerts}} * {{id}}: {{bucket_keys}} {{ctx.dedupedAlerts}} - New Alerts: {{ctx.newAlerts}} * {{id}}: {{bucket_keys}} {{ctx.newAlerts}} - Completed Alerts: {{ctx.completedAlerts}} * {{id}}: {{bucket_keys}} {{ctx.completedAlerts}}\"\"\", \"lang\": \"mustache\" }, \"throttle_enabled\": false, \"throttle\": { \"value\": 10, \"unit\": \"MINUTES\" }, \"action_execution_policy\": { \"action_execution_scope\": { \"per_alert\": { \"actionable_alerts\": [ \"DEDUPED\", \"NEW\"] } } }, \"subject_template\": { \"source\": \"The Subject\", \"lang\": \"mustache\" } }] } }] } Example response { \"_id\": \"Dfxr63sBwex6DxEhHV5N\", \"_version\": 1, \"_seq_no\": 3, \"_primary_term\": 1, \"monitor\": { \"type\": \"monitor\", \"schema_version\": 4, \"name\": \"Demo a bucket-level monitor\", \"monitor_type\": \"bucket_level_monitor\", \"user\": { \"name\": \"\", \"backend_roles\": [], \"roles\": [], \"custom_attribute_names\": [], \"user_requested_tenant\": null }, \"enabled\": true, \"enabled_time\": 1631742270785, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"search\": { \"indices\": [ \"opensearch_dashboards_sample_data_flights\"], \"query\": { \"size\": 0, \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"order_date\": { \"from\": \"{{period_end}}||-1h\", \"to\": \"{{period_end}}\", \"include_lower\": true, \"include_upper\": true, \"format\": \"epoch_millis\", \"boost\": 1.0 } } }], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"aggregations\": { \"composite_agg\": { \"composite\": { \"size\": 10, \"sources\": [ { \"user\": { \"terms\": { \"field\": \"user\", \"missing_bucket\": false, \"order\": \"asc\" } } }] }, \"aggregations\": { \"avg_products_base_price\": { \"avg\": { \"field\": \"products.base_price\" } } } } } } } }], \"triggers\": [ { \"bucket_level_trigger\": { \"id\": \"C_xr63sBwex6DxEhHV5B\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"buckets_path\": { \"_count\": \"_count\", \"avg_products_base_price\": \"avg_products_base_price\" }, \"parent_bucket_path\": \"composite_agg\", \"script\": { \"source\": \"params._count &gt; 50 || params.avg_products_base_price &lt; 35\", \"lang\": \"painless\" }, \"gap_policy\": \"skip\" }, \"actions\": [ { \"id\": \"DPxr63sBwex6DxEhHV5B\", \"name\": \"test-action\", \"destination_id\": \"E4o5hnsB6KjPKmHtpfCA\", \"message_template\": { \"source\": \"Monitor {{ctx.monitor.name}} just entered alert status. Please investigate the issue. - Trigger: {{ctx.trigger.name}} - Severity: {{ctx.trigger.severity}} - Period start: {{ctx.periodStart}} - Period end: {{ctx.periodEnd}} - Deduped Alerts: {{ctx.dedupedAlerts}} * {{id}}: {{bucket_keys}} {{ctx.dedupedAlerts}} - New Alerts: {{ctx.newAlerts}} * {{id}}: {{bucket_keys}} {{ctx.newAlerts}} - Completed Alerts: {{ctx.completedAlerts}} * {{id}}: {{bucket_keys}} {{ctx.completedAlerts}}\", \"lang\": \"mustache\" }, \"throttle_enabled\": false, \"subject_template\": { \"source\": \"The Subject\", \"lang\": \"mustache\" }, \"throttle\": { \"value\": 10, \"unit\": \"MINUTES\" }, \"action_execution_policy\": { \"action_execution_scope\": { \"per_alert\": { \"actionable_alerts\": [ \"DEDUPED\", \"NEW\"] } } } }] } }], \"last_update_time\": 1631742270785 } } Document-level monitors\nIntroduced 2.0\nDocument-level monitors check whether individual documents in an index match trigger conditions. If so, the monitor generates an alert notification. When you run a query with a document-level monitor, the results are returned for each document that matches the trigger condition. You can create trigger conditions based on query names, query IDs, or tags that combine multiple queries.\nTo learn more about per document monitors that function similarly to the document-level monitor API, see Monitor types.\nSearch for monitor findings\nYou can use the Alerting search API operation to search the findings index.opensearch-alerting-finding* for available document findings with a GET request. By default, a GET request without path parameters returns all available findings. To learn more about monitor findings, see Document findings.\nTo retrieve any available findings, send a GET request without any path parameters as follows: GET /_plugins/_alerting/findings/_search? To retrieve metadata for an individual document finding entry, you can search for the finding by its findingId as follows: GET /_plugins/_alerting/findings/_search?findingId=gKQhj 8 WJit 3 BxjGfiOXC The response returns the number of individual finding entries in the total_findings field.\nTo get more specific results in a findings search, you can use any of the optional path parameters that are defined in the following table: Path parameter Description Usage findingId The identifier for the finding entry.\nThe finding ID is returned in the initial query response. sortString This field specifies which string the Alerting plugin uses to sort the findings.\nThe default value is id. sortOrder The order to sort the list of findings, either ascending or descending.\nUse sortOrder=asc to indicate ascending, or sortOrder=desc for descending sort order. size An optional limit for the maximum number of results returned in the response.\nThere is no minimum or maximum values. startIndex The pagination indicator.\nDefault is 0. searchString The finding attribute you want returned in the search.\nTo search in a specific index, specify the index name in the request path. For example, to search findings in the indexABC index, use `searchString=indexABC’. Create a document-level monitor\nYou can create a document-level monitor with a POST request that provides the monitor details in the request body.\nAt a minimum, you need to provide the following details: specify the queries or combinations by tag with the inputs field, a valid trigger condition, and provide the notification message in the action field.\nThe following table shows the syntax to use for each trigger option: Trigger options Definition Syntax Tag\nCreates alerts for documents that match a multiple query with this tag applied. If you group multiple queries by a single tag, then you can set it to trigger an alert if the results are returned by this tag name. query[tag=&lt;tag-name&gt;] Query by name\nCreates alerts for documents matched or returned by the named query. query[name=&lt;query-name&gt;] Query by ID\nCreates alerts for documents that were returned by the identified query. query[id=&lt;query-id&gt;] Sample Request\nThe following sample shows how to create a document-level monitor: POST _plugins/_alerting/monitors { \"type\": \"monitor\", \"monitor_type\": \"doc_level_monitor\", \"name\": \"Example document-level monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [ { \"doc_level_input\": { \"description\": \"Example document-level monitor for audit logs\", \"indices\": [ \"audit-logs\"], \"queries\": [ { \"id\": \"nKQnFYABit3BxjGfiOXC\", \"name\": \"sigma-123\", \"query\": \"region: \\\" us-west-2 \\\" \", \"tags\": [ \"tag1\"] }, { \"id\": \"gKQnABEJit3BxjGfiOXC\", \"name\": \"sigma-456\", \"query\": \"region: \\\" us-east-1 \\\" \", \"tags\": [ \"tag2\"] }, { \"id\": \"h4J2ABEFNW3vxjGfiOXC\", \"name\": \"sigma-789\", \"query\": \"message: \\\" This is a SEPARATE error from IAD region \\\" \", \"tags\": [ \"tag3\"] }] } }], \"triggers\": [ { \"document_level_trigger\": { \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"(query[name=sigma-123] || query[tag=tag3]) &amp;&amp; query[name=sigma-789]\", \"lang\": \"painless\" } }, \"actions\": [ { \"name\": \"test-action\", \"destination_id\": \"E4o5hnsB6KjPKmHtpfCA\", \"message_template\": { \"source\": \"\"\"Monitor just entered alert status. Please investigate the issue. Related Finding Ids: {{ctx.alerts.0.finding_ids}}, Related Document Ids: {{ctx.alerts.0.related_doc_ids}}\"\"\", \"lang\": \"mustache\" }, \"action_execution_policy\": { \"action_execution_scope\": { \"per_alert\": { \"actionable_alerts\": [] } } }, \"subject_template\": { \"source\": \"The Subject\", \"lang\": \"mustache\" } }] }}] } Limitations\nIf you run a document-level query while the index is getting reindexed, the API response will not return the reindexed results. To get updates, wait until the reindexing process completes, then rerun the query.\nUpdate monitor\nIntroduced 1.0\nWhen updating a monitor, you can optionally include seq_no and primary_term as URL parameters. If these numbers don’t match the existing monitor or the monitor doesn’t exist, the alerting plugin throws an error. OpenSearch increments the version number and the sequence number automatically (see the example response).\nRequest PUT _plugins/_alerting/monitors/&lt;monitor_id&gt; { \"type\": \"monitor\", \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1551466220455, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"*\"], \"query\": { \"query\": { \"match_all\": { \"boost\": 1 } } } } }], \"triggers\": [{ \"id\": \"StaeOmkBC25HCRGmL_y-\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return true\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"RtaaOmkBC25HCRGm0fxi\", \"subject_template\": { \"source\": \"My Message Subject\", \"lang\": \"mustache\" }, \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1551466639295 } PUT _plugins/_alerting/monitors/&lt;monitor_id&gt;?if_seq_no= 3 &amp;if_primary_term= 1 { \"type\": \"monitor\", \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1551466220455, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"*\"], \"query\": { \"query\": { \"match_all\": { \"boost\": 1 } } } } }], \"triggers\": [{ \"id\": \"StaeOmkBC25HCRGmL_y-\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return true\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"RtaaOmkBC25HCRGm0fxi\", \"subject_template\": { \"source\": \"My Message Subject\", \"lang\": \"mustache\" }, \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1551466639295 } Example response { \"_id\": \"Q9aXOmkBC25HCRGmzfw-\", \"_version\": 4, \"_seq_no\": 4, \"_primary_term\": 1, \"monitor\": { \"type\": \"monitor\", \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1551466220455, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"*\"], \"query\": { \"query\": { \"match_all\": { \"boost\": 1 } } } } }], \"triggers\": [{ \"id\": \"StaeOmkBC25HCRGmL_y-\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return true\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"RtaaOmkBC25HCRGm0fxi\", \"subject_template\": { \"source\": \"My Message Subject\", \"lang\": \"mustache\" }, \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1551466761596 } } Get monitor\nIntroduced 1.0\nRequest GET _plugins/_alerting/monitors/&lt;monitor_id&gt; Example response { \"_id\": \"Q9aXOmkBC25HCRGmzfw-\", \"_version\": 3, \"_seq_no\": 3, \"_primary_term\": 1, \"monitor\": { \"type\": \"monitor\", \"name\": \"test-monitor\", \"enabled\": true, \"enabled_time\": 1551466220455, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"*\"], \"query\": { \"query\": { \"match_all\": { \"boost\": 1 } } } } }], \"triggers\": [{ \"id\": \"StaeOmkBC25HCRGmL_y-\", \"name\": \"test-trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"return true\", \"lang\": \"painless\" } }, \"actions\": [{ \"name\": \"test-action\", \"destination_id\": \"RtaaOmkBC25HCRGm0fxi\", \"subject_template\": { \"source\": \"My Message Subject\", \"lang\": \"mustache\" }, \"message_template\": { \"source\": \"This is my message body.\", \"lang\": \"mustache\" } }] }], \"last_update_time\": 1551466639295 } } Monitor stats\nIntroduced 1.0\nReturns statistics about the alerting feature. Use _plugins/_alerting/stats to find node IDs and metrics. Then you can drill down using those values.\nRequest GET _plugins/_alerting/stats GET _plugins/_alerting/stats/&lt;metric&gt; GET _plugins/_alerting/&lt;node-id&gt;/stats GET _plugins/_alerting/&lt;node-id&gt;/stats/&lt;metric&gt; Example response { \"_nodes\": { \"total\": 9, \"successful\": 9, \"failed\": 0 }, \"cluster_name\": \"475300751431:alerting65-dont-delete\", \"plugins.scheduled_jobs.enabled\": true, \"scheduled_job_index_exists\": true, \"scheduled_job_index_status\": \"green\", \"nodes_on_schedule\": 9, \"nodes_not_on_schedule\": 0, \"nodes\": { \"qWcbKbb-TVyyI-Q7VSeOqA\": { \"name\": \"qWcbKbb\", \"schedule_status\": \"green\", \"roles\": [ \"MASTER\"], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 207017, \"full_sweep_on_time\": true }, \"jobs_info\": {} }, \"Do-DX9ZcS06Y9w1XbSJo1A\": { \"name\": \"Do-DX9Z\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\"], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 230516, \"full_sweep_on_time\": true }, \"jobs_info\": {} }, \"n5phkBiYQfS5I0FDzcqjZQ\": { \"name\": \"n5phkBi\", \"schedule_status\": \"green\", \"roles\": [ \"MASTER\"], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 228406, \"full_sweep_on_time\": true }, \"jobs_info\": {} }, \"Tazzo8cQSY-g3vOjgYYLzA\": { \"name\": \"Tazzo8c\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\"], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 211722, \"full_sweep_on_time\": true }, \"jobs_info\": { \"i-wsFmkB8NzS6aXjQSk0\": { \"last_execution_time\": 1550864912882, \"running_on_time\": true } } }, \"Nyf7F8brTOSJuFPXw6CnpA\": { \"name\": \"Nyf7F8b\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\"], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 223300, \"full_sweep_on_time\": true }, \"jobs_info\": { \"NbpoFmkBeSe-hD59AKgE\": { \"last_execution_time\": 1550864928354, \"running_on_time\": true }, \"-LlLFmkBeSe-hD59Ydtb\": { \"last_execution_time\": 1550864732727, \"running_on_time\": true }, \"pBFxFmkBNXkgNmTBaFj1\": { \"last_execution_time\": 1550863325024, \"running_on_time\": true }, \"hfasEmkBNXkgNmTBrvIW\": { \"last_execution_time\": 1550862000001, \"running_on_time\": true } } }, \"oOdJDIBVT5qbbO3d8VLeEw\": { \"name\": \"oOdJDIB\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\"], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 227570, \"full_sweep_on_time\": true }, \"jobs_info\": { \"4hKRFmkBNXkgNmTBKjYX\": { \"last_execution_time\": 1550864806101, \"running_on_time\": true } } }, \"NRDG6JYgR8m0GOZYQ9QGjQ\": { \"name\": \"NRDG6JY\", \"schedule_status\": \"green\", \"roles\": [ \"MASTER\"], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 227652, \"full_sweep_on_time\": true }, \"jobs_info\": {} }, \"URMrXRz3Tm-CB72hlsl93Q\": { \"name\": \"URMrXRz\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\"], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 231048, \"full_sweep_on_time\": true }, \"jobs_info\": { \"m7uKFmkBeSe-hD59jplP\": { \"running_on_time\": true } } }, \"eXgt1k9oTRCLmx2HBGElUw\": { \"name\": \"eXgt1k9\", \"schedule_status\": \"green\", \"roles\": [ \"DATA\", \"INGEST\"], \"job_scheduling_metrics\": { \"last_full_sweep_time_millis\": 229234, \"full_sweep_on_time\": true }, \"jobs_info\": { \"wWkFFmkBc2NG-PeLntxk\": { \"running_on_time\": true }, \"3usNFmkB8NzS6aXjO1Gs\": { \"last_execution_time\": 1550863959848, \"running_on_time\": true } } } } } Delete monitor\nIntroduced 1.0\nRequest DELETE _plugins/_alerting/monitors/&lt;monitor_id&gt; Example response { \"_index\": \".opensearch-scheduled-jobs\", \"_id\": \"OYAHOmgBl3cmwnqZl_yH\", \"_version\": 2, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 11, \"_primary_term\": 1 } Search monitors\nIntroduced 1.0\nRequest GET _plugins/_alerting/monitors/_search { \"query\": { \"match\": { \"monitor.name\": \"my-monitor-name\" } } } Example response { \"took\": 17, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 1, \"max_score\": 0.6931472, \"hits\": [{ \"_index\": \".opensearch-scheduled-jobs\", \"_type\": \"_doc\", \"_id\": \"eGQi7GcBRS7-AJEqfAnr\", \"_score\": 0.6931472, \"_source\": { \"type\": \"monitor\", \"name\": \"my-monitor-name\", \"enabled\": true, \"enabled_time\": 1545854942426, \"schedule\": { \"period\": { \"interval\": 1, \"unit\": \"MINUTES\" } }, \"inputs\": [{ \"search\": { \"indices\": [ \"*\"], \"query\": { \"size\": 0, \"query\": { \"bool\": { \"filter\": [{ \"range\": { \"@timestamp\": { \"from\": \"{{period_end}}||-1h\", \"to\": \"{{period_end}}\", \"include_lower\": true, \"include_upper\": true, \"format\": \"epoch_millis\", \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"aggregations\": {} } } }], \"triggers\": [{ \"id\": \"Sooi7GcB53a0ewuj_6MH\", \"name\": \"Over\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"_ctx.results[0].hits.total &gt; 400000\", \"lang\": \"painless\" } }, \"actions\": [] }], \"last_update_time\": 1545854975758 } }] } } Run monitor\nIntroduced 1.0\nYou can add the optional?dryrun=true parameter to the URL to show the results of a run without actions sending any message.\nRequest POST _plugins/_alerting/monitors/&lt;monitor_id&gt;/_execute Example response { \"monitor_name\": \"logs\", \"period_start\": 1547161872322, \"period_end\": 1547161932322, \"error\": null, \"trigger_results\": { \"Sooi7GcB53a0ewuj_6MH\": { \"name\": \"Over\", \"triggered\": true, \"error\": null, \"action_results\": {} } } } Get alerts\nIntroduced 1.0\nReturns an array of all alerts.\nRequest GET _plugins/_alerting/monitors/alerts Response { \"alerts\": [ { \"id\": \"eQURa3gBKo1jAh6qUo49\", \"version\": 300, \"monitor_id\": \"awUMa3gBKo1jAh6qu47E\", \"schema_version\": 2, \"monitor_version\": 2, \"monitor_name\": \"Example_monitor_name\", \"monitor_user\": { \"name\": \"admin\", \"backend_roles\": [ \"admin\"], \"roles\": [ \"all_access\", \"own_index\"], \"custom_attribute_names\": [], \"user_requested_tenant\": null }, \"trigger_id\": \"bQUQa3gBKo1jAh6qnY6G\", \"trigger_name\": \"Example_trigger_name\", \"state\": \"ACTIVE\", \"error_message\": null, \"alert_history\": [ { \"timestamp\": 1617314504873, \"message\": \"Example error emssage\" }, { \"timestamp\": 1617312543925, \"message\": \"Example error message\" }], \"severity\": \"1\", \"action_execution_results\": [ { \"action_id\": \"bgUQa3gBKo1jAh6qnY6G\", \"last_execution_time\": 1617317979908, \"throttled_count\": 0 }], \"start_time\": 1616704000492, \"last_notification_time\": 1617317979908, \"end_time\": null, \"acknowledged_time\": null }], \"totalAlerts\": 1 } Acknowledge alert\nIntroduced 1.0 After getting your alerts, you can acknowledge any number of active alerts in one call. If the alert is already in an ERROR, COMPLETED, or ACKNOWLEDGED state, it appears in the failed array.\nRequest POST _plugins/_alerting/monitors/&lt;monitor-id&gt;/_acknowledge/alerts { \"alerts\": [ \"eQURa3gBKo1jAh6qUo49\"] } Example response { \"success\": [ \"eQURa3gBKo1jAh6qUo49\"], \"failed\": [] } Create destination\nIntroduced 1.0\nRequests POST _plugins/_alerting/destinations { \"name\": \"my-destination\", \"type\": \"slack\", \"slack\": { \"url\": \"http://www.example.com\" } } POST _plugins/_alerting/destinations { \"type\": \"custom_webhook\", \"name\": \"my-custom-destination\", \"custom_webhook\": { \"path\": \"incomingwebhooks/123456-123456-XXXXXX\", \"header_params\": { \"Content-Type\": \"application/json\" }, \"scheme\": \"HTTPS\", \"port\": 443, \"query_params\": { \"token\": \"R2x1UlN4ZHF8MXxxVFJpelJNVDgzdGNwXXXXXXXXX\" }, \"host\": \"hooks.chime.aws\" } } POST _plugins/_alerting/destinations { \"type\": \"email\", \"name\": \"my-email-destination\", \"email\": { \"email_account_id\": \"YjY7mXMBx015759_IcfW\", \"recipients\": [ { \"type\": \"email_group\", \"email_group_id\": \"YzY-mXMBx015759_dscs\" }, { \"type\": \"email\", \"email\": \"example@email.com\" }] } } // The email_account_id and email_group_id will be the document IDs of the email_account and email_group you have created. Example response { \"_id\": \"nO-yFmkB8NzS6aXjJdiI\", \"_version\": 1, \"_seq_no\": 3, \"_primary_term\": 1, \"destination\": { \"type\": \"slack\", \"name\": \"my-destination\", \"last_update_time\": 1550863967624, \"slack\": { \"url\": \"http://www.example.com\" } } } Update destination\nIntroduced 1.0\nWhen updating a destination, you can optionally include seq_no and primary_term as URL parameters. If these numbers don’t match the existing destination or the destination doesn’t exist, the alerting plugin throws an error. OpenSearch increments the version number and the sequence number automatically (see the example response).\nRequest PUT _plugins/_alerting/destinations/&lt;destination-id&gt; { \"name\": \"my-updated-destination\", \"type\": \"slack\", \"slack\": { \"url\": \"http://www.example.com\" } } PUT _plugins/_alerting/destinations/&lt;destination-id&gt;?if_seq_no= 3 &amp;if_primary_term= 1 { \"name\": \"my-updated-destination\", \"type\": \"slack\", \"slack\": { \"url\": \"http://www.example.com\" } } Example response { \"_id\": \"pe-1FmkB8NzS6aXjqvVY\", \"_version\": 2, \"_seq_no\": 4, \"_primary_term\": 1, \"destination\": { \"type\": \"slack\", \"name\": \"my-updated-destination\", \"last_update_time\": 1550864289375, \"slack\": { \"url\": \"http://www.example.com\" } } } Get destination\nIntroduced 1.0\nRetrieve one destination.\nRequests GET _plugins/_alerting/destinations/&lt;destination-id&gt; Example response { \"totalDestinations\": 1, \"destinations\": [{ \"id\": \"1a2a3a4a5a6a7a\", \"type\": \"slack\", \"name\": \"sample-destination\", \"user\": { \"name\": \"psantos\", \"backend_roles\": [ \"human-resources\"], \"roles\": [ \"alerting_full_access\", \"hr-role\"], \"custom_attribute_names\": [] }, \"schema_version\": 3, \"seq_no\": 0, \"primary_term\": 6, \"last_update_time\": 1603943261722, \"slack\": { \"url\": \"https://example.com\" } }] } Get destinations\nIntroduced 1.0\nRetrieve all destinations.\nRequests GET _plugins/_alerting/destinations Example response { \"totalDestinations\": 1, \"destinations\": [{ \"id\": \"1a2a3a4a5a6a7a\", \"type\": \"slack\", \"name\": \"sample-destination\", \"user\": { \"name\": \"psantos\", \"backend_roles\": [ \"human-resources\"], \"roles\": [ \"alerting_full_access\", \"hr-role\"], \"custom_attribute_names\": [] }, \"schema_version\": 3, \"seq_no\": 0, \"primary_term\": 6, \"last_update_time\": 1603943261722, \"slack\": { \"url\": \"https://example.com\" } }] } Delete destination\nIntroduced 1.0\nRequest DELETE _plugins/_alerting/destinations/&lt;destination-id&gt; Example response { \"_index\": \".opendistro-alerting-config\", \"_type\": \"_doc\", \"_id\": \"Zu-zFmkB8NzS6aXjLeBI\", \"_version\": 2, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 8, \"_primary_term\": 1 } Create email account\nIntroduced 1.0\nRequest POST _plugins/_alerting/destinations/email_accounts { \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" } Example response { \"_id\": \"email_account_id\", \"_version\": 1, \"_seq_no\": 7, \"_primary_term\": 2, \"email_account\": { \"schema_version\": 2, \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" } } Update email account\nIntroduced 1.0\nWhen updating an email account, you can optionally include seq_no and primary_term as URL parameters. If these numbers don’t match the existing email account or the email account doesn’t exist, the alerting plugin throws an error. OpenSearch increments the version number and the sequence number automatically (see the example response).\nRequest PUT _plugins/_alerting/destinations/email_accounts/&lt;email_account_id&gt; { \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" } PUT _plugins/_alerting/destinations/email_accounts/&lt;email_account_id&gt;?if_seq_no= 18 &amp;if_primary_term= 2 { \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" } Example response { \"_id\": \"email_account_id\", \"_version\": 3, \"_seq_no\": 19, \"_primary_term\": 2, \"email_account\": { \"schema_version\": 2, \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" } } Get email account\nIntroduced 1.0\nRequest GET _plugins/_alerting/destinations/email_accounts/&lt;email_account_id&gt; { \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" } Example response { \"_id\": \"email_account_id\", \"_version\": 2, \"_seq_no\": 8, \"_primary_term\": 2, \"email_account\": { \"schema_version\": 2, \"name\": \"test_account\", \"email\": \"test@email.com\", \"host\": \"smtp.test.com\", \"port\": 465, \"method\": \"ssl\" } } Delete email account\nIntroduced 1.0\nRequest DELETE _plugins/_alerting/destinations/email_accounts/&lt;email_account_id&gt; Example response { \"_index\": \".opendistro-alerting-config\", \"_type\": \"_doc\", \"_id\": \"email_account_id\", \"_version\": 1, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 12, \"_primary_term\": 2 } Search email account\nIntroduced 1.0\nRequest POST _plugins/_alerting/destinations/email_accounts/_search { \"from\": 0, \"size\": 20, \"sort\": { \"email_account.name.keyword\": \"desc\" }, \"query\": { \"bool\": { \"must\": { \"match_all\": {} } } } } Example response { \"took\": 8, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [ { \"_index\": \".opendistro-alerting-config\", \"_type\": \"_doc\", \"_id\": \"email_account_id\", \"_seq_no\": 8, \"_primary_term\": 2, \"_score\": null, \"_source\": { \"schema_version\": 2, \"name\": \"example_account\", \"email\": \"example@email.com\", \"host\": \"smtp.email.com\", \"port\": 465, \"method\": \"ssl\" }, \"sort\": [ \"example_account\"] },...] } } Create email group\nIntroduced 1.0\nRequest POST _plugins/_alerting/destinations/email_groups { \"name\": \"example_email_group\", \"emails\": [{ \"email\": \"example@email.com\" }] } Example response { \"_id\": \"email_group_id\", \"_version\": 1, \"_seq_no\": 9, \"_primary_term\": 2, \"email_group\": { \"schema_version\": 2, \"name\": \"example_email_group\", \"emails\": [ { \"email\": \"example@email.com\" }] } } Update email group\nIntroduced 1.0\nWhen updating an email group, you can optionally include seq_no and primary_term as URL parameters. If these numbers don’t match the existing email group or the email group doesn’t exist, the alerting plugin throws an error. OpenSearch increments the version number and the sequence number automatically (see the example response).\nRequest PUT _plugins/_alerting/destinations/email_groups/&lt;email_group_id&gt; { \"name\": \"example_email_group\", \"emails\": [{ \"email\": \"example@email.com\" }] } PUT _plugins/_alerting/destinations/email_groups/&lt;email_group_id&gt;?if_seq_no= 16 &amp;if_primary_term= 2 { \"name\": \"example_email_group\", \"emails\": [{ \"email\": \"example@email.com\" }] } Example response { \"_id\": \"email_group_id\", \"_version\": 4, \"_seq_no\": 17, \"_primary_term\": 2, \"email_group\": { \"schema_version\": 2, \"name\": \"example_email_group\", \"emails\": [ { \"email\": \"example@email.com\" }] } } Get email group\nIntroduced 1.0\nRequest GET _plugins/_alerting/destinations/email_groups/&lt;email_group_id&gt; { \"name\": \"example_email_group\", \"emails\": [{ \"email\": \"example@email.com\" }] } Example response { \"_id\": \"email_group_id\", \"_version\": 4, \"_seq_no\": 17, \"_primary_term\": 2, \"email_group\": { \"schema_version\": 2, \"name\": \"example_email_group\", \"emails\": [ { \"email\": \"example@email.com\" }] } } Delete email group\nIntroduced 1.0\nRequest DELETE _plugins/_alerting/destinations/email_groups/&lt;email_group_id&gt; Example response { \"_index\": \".opendistro-alerting-config\", \"_type\": \"_doc\", \"_id\": \"email_group_id\", \"_version\": 1, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 11, \"_primary_term\": 2 } Search email group\nIntroduced 1.0\nRequest POST _plugins/_alerting/destinations/email_groups/_search { \"from\": 0, \"size\": 20, \"sort\": { \"email_group.name.keyword\": \"desc\" }, \"query\": { \"bool\": { \"must\": { \"match_all\": {} } } } } Example response { \"took\": 7, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 5, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [ { \"_index\": \".opendistro-alerting-config\", \"_type\": \"_doc\", \"_id\": \"email_group_id\", \"_seq_no\": 10, \"_primary_term\": 2, \"_score\": null, \"_source\": { \"schema_version\": 2, \"name\": \"example_email_group\", \"emails\": [ { \"email\": \"example@email.com\" }] }, \"sort\": [ \"example_email_group\"] },...] } }",
    "ancestors": [
      "Observability",
      "Alerting"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/alerting/cron/",
    "title": "Cron",
    "content": "Monitors can run at a variety of fixed intervals (e.g. hourly, daily, etc.), but you can also define custom cron expressions for when they should run. Monitors use the Unix cron syntax and support five fields: Field Valid values Minute\n0-59\nHour\n0-23\nDay of month\n1-31\nMonth\n1-12\nDay of week\n0-7 (0 and 7 are both Sunday) or SUN, MON, TUE, WED, THU, FRI, SAT For example, the following expression translates to “every Monday through Friday at 11:30 AM”: 30 11 * * 1-5 Features Feature Description * Wildcard. Specifies all valid values., List. Use to specify several values (e.g. 1,15,30). - Range. Use to specify a range of values (e.g. 1-15). / Step. Use after a wildcard or range to specify the “step” between values. For example, 0-11/2 is equivalent to 0,2,4,6,8,10. Note that you can specify the day using two fields: day of month and day of week. For most situations, we recommend that you use just one of these fields and leave the other as *.\nIf you use a non-wildcard value in both fields, the monitor runs when either field matches the time. For example, 15 2 1,15 * 1 causes the monitor to run at 2:15 AM on the 1st of the month, the 15th of the month, and every Monday.\nSample expressions\nEvery other day at 1:45 PM: 45 13 1-31/2 * * Every 10 minutes on Saturday and Sunday: 0/10 * * * 6-7 Every three hours on the first day of every other month: 0 0-23/3 1 1-12/2 * API\nFor an example of how to use a custom cron expression in an API call, see the create monitor API operation.",
    "ancestors": [
      "Observability",
      "Alerting"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/alerting/index/",
    "title": "Alerting",
    "content": "OpenSearch Dashboards\nYou can use the Alerting plugin in OpenSearch Dashboards to monitor your data and create alert notifications that trigger when conditions occur in one or more indexes.\nYou create a monitor with trigger conditions that generate various alert notifications through the message channel you select as a destination. Notifications can be sent to email, Slack, or Amazon Chime.\nThe monitor you create notifies you when data from one or more OpenSearch indexes meets certain conditions. For example, you might want to notify a Slack channel if your application logs more than five HTTP 503 errors in one hour, or you might want to page a developer if no new documents have been indexed in the past 20 minutes.\nTo get started, choose Alerting in OpenSearch Dashboards. Figure 1: Alerting plugin in OpenSearch Dashboards",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/alerting/monitors/",
    "title": "Monitors",
    "content": "Table of contents Monitor types Key terms Per document monitors Document findings Create destinations Email as a destination Create a monitor Create triggers Visual editor Extraction query Anomaly detector Available variables Add actions Questions about destinations Work with alerts Create cluster metrics monitor Supported APIs Restrict API fields Painless triggers Limitations Monitor types\nThe OpenSearch Dashboard Alerting plugin provides four monitor types: per query – This monitor runs a query and generates alert notifications based on criteria that matches. per bucket – This monitor runs a query that evaluates trigger criteria based on aggregated values in the dataset. per cluster metrics – This monitor runs API requests on the cluster to monitor its health. per document – This monitor runs a query (or multiple queries combined by a tag) that returns individual documents that match the alert notification trigger condition.\nKey terms Term Definition Monitor\nA job that runs on a defined schedule and queries OpenSearch indexes. The results of these queries are then used as input for one or more triggers.\nTrigger\nConditions that, if met, generate alerts.\nTag\nA label that can be applied to multiple queries to combine them with the logical OR operation in a per document monitor. You cannot use tags with other monitor types.\nAlert\nAn event associated with a trigger. When an alert is created, the trigger performs actions, which can include sending a notification.\nAction\nThe information that you want the monitor to send out after being triggered. Actions have a destination, a message subject, and a message body.\nDestination\nA reusable location for an action. Supported locations are Amazon Chime, Email, Slack, or custom webhook.\nFinding\nAn entry for an individual document found by a per document monitor query that contains the document ID, index name, and timestamp. Findings are stored in the Findings index:.opensearch-alerting-finding*.\nChannel\nA notification channel to use in an action. See notifications for more information. Per document monitors\nIntroduced 2.0\nPer document monitors allow you to define up to 10 queries that compare the selected field with your desired value. You can define supported field data types using the following operators: is is not is greater than is greater than equal is less than is less than equal You query each trigger using up to 10 tags, adding the tag as a single trigger condition instead of specifying a single query. The Alerting plugin processes the trigger conditions from all queries as a logical OR operation, so if any of the query conditions are met, it triggers an alert. Next, the Alerting plugin tells the Notifications plugin to send the notification to a channel.\nThe Alerting plugin also creates a list of document findings that contains metadata about which document matches each query. Security analytics can use the document findings data to keep track of and analyze the query data separately from the alert processes.\nThe Alerting API provides a document-level monitor that programmatically accomplishes the same function as the per document monitor in the OpenSearch Dashboards. To learn more, see Document-level monitors.\nDocument findings\nWhen a per document monitor executes a query that matches a document in an index, a finding is created. OpenSearch provides a Findings index:.opensearch-alerting-finding* that contains findings data for all per document monitor queries. You can search the findings index with the Alerting API search operation. To learn more, see Search for monitor findings.\nThe following metadata is provided for each document finding entry: Document – The document ID and index name. For example: Re5akdirhj3fl | test-logs-index. Query – The query name that matched the document. Time found – The timestamp that indicates when the document was found during the runtime.\nIt is possible to configure an alert notification for each finding, however we don’t recommend this unless rules are well defined to prevent a huge volume of findings in a high ingestion cluster.\nCreate destinations\nChoose Alerting, Destinations, Add destination.\nSpecify a name for the destination so that you can identify it later.\nFor Type, choose Slack, Amazon Chime, custom webhook, or email.\nFor Email, refer to the Email as a destination section below. For all other types, specify the webhook URL. See the documentation for Slack and Amazon Chime to learn more about webhooks.\nIf you’re using custom webhooks, you must specify more information: parameters and headers. For example, if your endpoint requires basic authentication, you might need to add a header with a key of Authorization and a value of Basic &lt;Base64-encoded-credential-string&gt;. You might also need to change Content-Type to whatever your webhook requires. Popular values are application/json, application/xml, and text/plain.\nThis information is stored in plain text in the OpenSearch cluster. We will improve this design in the future, but for now, the encoded credentials (which are neither encrypted nor hashed) might be visible to other OpenSearch users.\nEmail as a destination\nTo send or receive an alert notification as an email, choose Email as the destination type. Next, add at least one sender and recipient. We recommend adding email groups if you want to notify more than a few people of an alert. You can configure senders and recipients using Manage senders and Manage email groups.\nManage senders\nYou need to specify an email account from which the Alerting plugin can send notifications.\nTo configure a sender email, do the following:\nAfter you choose Email as the destination type, choose Manage senders.\nChoose Add sender, New sender and enter a unique name.\nEnter the email address, SMTP host (e.g. smtp.gmail.com for a Gmail account), and the port.\nChoose an encryption method, or use the default value of None. However, most email providers require SSL or TLS, which require a username and password in OpenSearch keystore. Refer to Authenticate sender account to learn more.\nChoose Save to save the configuration and create the sender. You can create a sender even before you add your credentials to the OpenSearch keystore. However, you must authenticate each sender account before you use the destination to send your alert.\nYou can reuse senders across many different destinations, but each destination only supports one sender.\nManage email groups or recipients\nUse email groups to create and manage reusable lists of email addresses. For example, one alert might email the DevOps team, whereas another might email the executive team and the engineering team.\nYou can enter individual email addresses or an email group in the Recipients field.\nAfter you choose Email as the destination type, choose Manage email groups. Then choose Add email group, New email group.\nEnter a unique name.\nFor recipient emails, enter any number of email addresses.\nChoose Save.\nAuthenticate sender account\nIf your email provider requires SSL or TLS, you must authenticate each sender account before you can send an email. Enter these credentials in the OpenSearch keystore using the CLI. Run the following commands (in your OpenSearch directory) to enter your username and password. The &lt;sender_name&gt; is the name you entered for Sender earlier../bin/opensearch-keystore add plugins.alerting.destination.email.&lt;sender_name&gt;.username./bin/opensearch-keystore add plugins.alerting.destination.email.&lt;sender_name&gt;.password Note: Keystore settings are node-specific. You must run these commands on each node.\nTo change or update your credentials (after you’ve added them to the keystore on every node), call the reload API to automatically update those credentials without restarting OpenSearch: POST _nodes/reload_secure_settings { \"secure_settings_password\": \"1234\" } Create a monitor\nChoose Alerting, Monitors, Create monitor.\nSpecify a name for the monitor.\nChoose either Per query monitor, Per bucket monitor, Per cluster metrics monitor, or Per document monitor.\nOpenSearch supports the following types of monitors: Per query monitors run your specified query and then check whether the query’s results trigger any alerts. Per query monitors can only trigger one alert at a time. Per bucket monitors let you create buckets based on selected fields and then categorize your results into those buckets. The Alerting plugin runs each bucket’s unique results against a script you define later, so you have finer control over which results should trigger alerts. Furthermore, each bucket can trigger an alert.\nThe maximum number of monitors you can create is 1,000. You can change the default maximum number of alerts for your cluster by calling the cluster settings API plugins.alerting.monitor.max_monitors.\nDecide how you want to define your query and triggers. You can use any of the following methods: visual editor, query editor, or anomaly detector.\nVisual definition works well for monitors that you can define as “some value is above or below some threshold for some amount of time.”\nQuery definition gives you flexibility in terms of what you query for (using OpenSearch query DSL) and how you evaluate the results of that query (Painless scripting).\nThis example averages the cpu_usage field: { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"avg_cpu\": { \"avg\": { \"field\": \"cpu_usage\" } } } } You can even filter query results using {{period_start}} and {{period_end}}: { \"size\": 0, \"query\": { \"bool\": { \"filter\": [{ \"range\": { \"timestamp\": { \"from\": \"{{period_end}}||-1h\", \"to\": \"{{period_end}}\", \"include_lower\": true, \"include_upper\": true, \"format\": \"epoch_millis\", \"boost\": 1 } } }], \"adjust_pure_negative\": true, \"boost\": 1 } }, \"aggregations\": {} } “Start” and “end” refer to the interval at which the monitor runs. See Available variables.\nTo define a monitor visually, choose Visual editor. Then choose a source index, a timeframe, an aggregation (for example, count() or average()), a data filter if you want to monitor a subset of your source index, and a group-by field if you want to include an aggregation field in your query. At least one group-by field is required if you’re defining a bucket-level monitor. Visual definition works well for most monitors.\nIf you use the Security plugin, you can only choose indexes that you have permission to access. For details, see Alerting security.\nTo use a query, choose Extraction query editor, add your query (using OpenSearch query DSL), and test it using the Run button.\nThe monitor makes this query to OpenSearch as often as the schedule dictates; check the Query Performance section and make sure you’re comfortable with the performance implications.\nTo use an anomaly detector, choose Anomaly detector and select your Detector.\nThe anomaly detection option is for pairing with the anomaly detection plugin. See Anomaly Detection.\nFor anomaly detector, choose an appropriate schedule for the monitor based on the detector interval. Otherwise, the alerting monitor might miss reading the results.\nFor example, assume you set the monitor interval and the detector interval as 5 minutes, and you start the detector at 12:00. If an anomaly is detected at 12:05, it might be available at 12:06 because of the delay between writing the anomaly and it being available for queries. The monitor reads the anomaly results between 12:00 and 12:05, so it does not get the anomaly results available at 12:06.\nTo avoid this issue, make sure the alerting monitor is at least twice the detector interval.\nWhen you create a monitor using OpenSearch Dashboards, the anomaly detector plugin generates a default monitor schedule that’s twice the detector interval.\nWhenever you update a detector’s interval, make sure to update the associated monitor interval as well, as the anomaly detection plugin does not do this automatically. Note: Anomaly detection is available only if you are defining a per query monitor.\nChoose how frequently to run your monitor. You can run it either by time intervals (minutes, hours, or days) or on a schedule. If you run it on a daily, weekly or monthly schedule or according to a custom custom cron expression, then you need to also provide the time zone.\nAdd a trigger to your monitor.\nCreate triggers\nSteps to create a trigger differ depending on whether you chose Visual editor, Extraction query editor, or Anomaly detector when you created the monitor.\nYou begin by specifying a name and severity level for the trigger. Severity levels help you manage alerts. A trigger with a high severity level (e.g. 1) might page a specific individual, whereas a trigger with a low severity level might message a chat room.\nRemember that query-level monitors run your trigger’s script just once against the query’s results, but bucket-level monitors execute your trigger’s script on each bucket, so you should create a trigger that best fits the monitor you chose. If you want to execute multiple scripts, you must create multiple triggers.\nVisual editor\nFor a query-level monitor’s Trigger condition, specify a threshold for the aggregation and timeframe you chose earlier, such as “is below 1,000” or “is exactly 10.”\nThe line moves up and down as you increase and decrease the threshold. Once this line is crossed, the trigger evaluates to true.\nBucket-level monitors also require you to specify a threshold and value for your aggregation and timeframe, but you can use a maximum of five conditions to better refine your trigger. Optionally, you can also use a keyword filter to filter for a specific field in your index.\nDocument-level monitors provide the added option to use tags that represent multiple queries connected by the logical OR operator.\nTo create a multiple query combination trigger, do the following steps:\nCreate a per document monitor with more than one query.\nCreate the first query with a field, an operator, and a value. For example, set the query to search for the region field with either operator: “is” or “is not”, and set the value “us-west-2”.\nSelect Add Tag and give the tag a name.\nCreate the second query and add the same tag to it.\nNow you can create the trigger condition and specify the tag name. This creates a combination trigger that checks two queries that both contain the same tag. The monitor checks both queries with a logical OR operation and if either query’s conditions are met, then it will generate the alert notification.\nExtraction query\nIf you’re using a query-level monitor, specify a Painless script that returns true or false. Painless is the default OpenSearch scripting language and has a syntax similar to Groovy.\nTrigger condition scripts revolve around the ctx.results[0] variable, which corresponds to the extraction query response. For example, your script might reference ctx.results[0].hits.total.value or ctx.results[0].hits.hits[i]._source.error_code.\nA return value of true means the trigger condition has been met, and the trigger should execute its actions. Test your script using the Run button.\nThe Info link next to Trigger condition contains a useful summary of the variables and results available to your query.\nBucket-level monitors require you to specify more information in your trigger condition. At a minimum, you must have the following fields: buckets_path, which maps variable names to metrics to use in your script. parent_bucket_path, which is a path to a multi-bucket aggregation. The path can include single-bucket aggregations, but the last aggregation must be multi-bucket. For example, if you have a pipeline such as agg1&gt;agg2&gt;agg3, agg1 and agg2 are single-bucket aggregations, but agg3 must be a multi-bucket aggregation. script, which is the script that OpenSearch runs to evaluate whether to trigger any alerts.\nFor example, you might have a script that looks like the following: { \"buckets_path\": { \"count_var\": \"_count\" }, \"parent_bucket_path\": \"composite_agg\", \"script\": { \"source\": \"params.count_var &gt; 5\" } } After mapping the count_var variable to the _count metric, you can use count_var in your script and reference _count data. Finally, composite_agg is a path to a multi-bucket aggregation.\nAnomaly detector\nFor Trigger type, choose Anomaly detector grade and confidence.\nSpecify the Anomaly grade condition for the aggregation and timeframe you chose earlier, “IS ABOVE 0.7” or “IS EXACTLY 0.5.” The anomaly grade is a number between 0 and 1 that indicates the level of severity of how anomalous a data point is.\nSpecify the Anomaly confidence condition for the aggregation and timeframe you chose earlier, “IS ABOVE 0.7” or “IS EXACTLY 0.5.” The anomaly confidence is an estimate of the probability that the reported anomaly grade matches the expected anomaly grade.\nThe line moves up and down as you increase and decrease the threshold. Once this line is crossed, the trigger evaluates to true.\nSample scripts\n<!-- These scripts are Painless, not Groovy, but calling them Groovy in Jekyll gets us syntax highlighting in the generated HTML. --> // Evaluates to true if the query returned any documents ctx. results [ 0]. hits. total. value &gt; 0 // Returns true if the avg_cpu aggregation exceeds 90 if ( ctx. results [ 0]. aggregations. avg_cpu. value &gt; 90) { return true; } // Performs some crude custom scoring and returns true if that score exceeds a certain value int score = 0; for ( int i = 0; i &lt; ctx. results [ 0]. hits. hits. length; i ++) { // Weighs 500 errors 10 times as heavily as 503 errors if ( ctx. results [ 0]. hits. hits [ i]. _source. http_status_code == \"500\") { score += 10; } else if ( ctx. results [ 0]. hits. hits [ i]. _source. http_status_code == \"503\") { score += 1; } } if ( score &gt; 99) { return true; } else { return false; } Below are some variables you can include in your message using Mustache templates to see more information about your monitors.\nAvailable variables\nMonitor variables Variable Data type Description ctx.monitor Object\nIncludes ctx.monitor.name, ctx.monitor.type, ctx.monitor.enabled, ctx.monitor.enabled_time, ctx.monitor.schedule, ctx.monitor.inputs, triggers and ctx.monitor.last_update_time. ctx.monitor.user Object\nIncludes information about the user who created the monitor. Includes ctx.monitor.user.backend_roles and ctx.monitor.user.roles, which are arrays that contain the backend roles and roles assigned to the user. See alerting security for more information. ctx.monitor.enabled Boolean\nWhether the monitor is enabled. ctx.monitor.enabled_time Milliseconds\nUnix epoch time of when the monitor was last enabled. ctx.monitor.schedule Object\nContains a schedule of how often or when the monitor should run. ctx.monitor.schedule.period.interval Integer\nThe interval at which the monitor runs. ctx.monitor.schedule.period.unit String\nThe interval’s unit of time. ctx.monitor.inputs Array\nAn array that contains the indexes and definition used to create the monitor. ctx.monitor.inputs.search.indices Array\nAn array that contains the indexes the monitor observes. ctx.monitor.inputs.search.query N/A\nThe definition used to define the monitor. Trigger variables Variable Data type Description ctx.trigger.id String\nThe trigger’s ID. ctx.trigger.name String\nThe trigger’s name. ctx.trigger.severity String\nThe trigger’s severity. ctx.trigger.condition Object\nContains the Painless script used when creating the monitor. ctx.trigger.condition.script.source String\nThe language used to define the script. Must be painless. ctx.trigger.condition.script.lang String\nThe script used to define the trigger. ctx.trigger.actions Array\nAn array with one element that contains information about the action the monitor needs to trigger. Action variables Variable Data type Description ctx.trigger.actions.id String\nThe action’s ID. ctx.trigger.actions.name String\nThe action’s name. ctx.trigger.actions.message_template.source String\nThe message to send in the alert. ctx.trigger.actions.message_template.lang String\nThe scripting language used to define the message. Must be Mustache. ctx.trigger.actions.throttle_enabled Boolean\nWhether throttling is enabled for this trigger. See adding actions for more information about throttling. ctx.trigger.actions.subject_template.source String\nThe message’s subject in the alert. ctx.trigger.actions.subject_template.lang String\nThe scripting language used to define the subject. Must be mustache. Other variables Variable Data type Description ctx.results Array\nAn array with one element (i.e. ctx.results[0]). Contains the query results. This variable is empty if the trigger was unable to retrieve results. See ctx.error. ctx.last_update_time Milliseconds\nUnix epoch time of when the monitor was last updated. ctx.periodStart String\nUnix timestamp for the beginning of the period during which the alert triggered. For example, if a monitor runs every ten minutes, a period might begin at 10:40 and end at 10:50. ctx.periodEnd String\nThe end of the period during which the alert triggered. ctx.error String\nThe error message if the trigger was unable to retrieve results or unable to evaluate the trigger, typically due to a compile error or null pointer exception. Null otherwise. ctx.alert Object\nThe current, active alert (if it exists). Includes ctx.alert.id, ctx.alert.version, and ctx.alert.isAcknowledged. Null if no alert is active. Only available with query-level monitors. ctx.dedupedAlerts Object\nAlerts that have already been triggered. OpenSearch keeps the existing alert to prevent the plugin from creating endless amounts of the same alerts. Only available with bucket-level monitors. ctx.newAlerts Object\nNewly created alerts. Only available with bucket-level monitors. ctx.completedAlerts Object\nAlerts that are no longer ongoing. Only available with bucket-level monitors. bucket_keys String\nComma-separated list of the monitor’s bucket key values. Available only for ctx.dedupedAlerts, ctx.newAlerts, and ctx.completedAlerts. Accessed through ctx.dedupedAlerts[0].bucket_keys. parent_bucket_path String\nThe parent bucket path of the bucket that triggered the alert. Accessed through ctx.dedupedAlerts[0].parent_bucket_path. Add actions\nThe final step in creating a monitor is to add one or more actions. Actions send notifications when trigger conditions are met. See the Notifications plugin to see what communication channels are supported.\nIf you don’t want to receive notifications for alerts, you don’t have to add actions to your triggers. Instead, you can periodically check OpenSearch Dashboards.\nSpecify a name for the action.\nChoose a notification channel.\nAdd a subject and body for the message.\nYou can add variables to your messages using Mustache templates. You have access to ctx.action.name, the name of the current action, as well as all trigger variables.\nIf your destination is a custom webhook that expects a particular data format, you might need to include JSON (or even XML) directly in the message body: { \"text\": \"Monitor {{ctx.monitor.name}} just entered alert status. Please investigate the issue. - Trigger: {{ctx.trigger.name}} - Severity: {{ctx.trigger.severity}} - Period start: {{ctx.periodStart}} - Period end: {{ctx.periodEnd}}\" } In this case, the message content must conform to the Content-Type header in the custom webhook.\nIf you’re using a bucket-level monitor, you can choose whether the monitor should perform an action for each execution or for each alert.\n(Optional) Use action throttling to limit the number of notifications you receive within a given span of time.\nFor example, if a monitor checks a trigger condition every minute, you could receive one notification per minute. If you set action throttling to 60 minutes, you receive no more than one notification per hour, even if the trigger condition is met dozens of times in that hour.\nChoose Create.\nAfter an action sends a message, the content of that message has left the purview of the Security plugin. Securing access to the message (e.g. access to the Slack channel) is your responsibility.\nSample message Monitor {{ ctx. monitor. name }} just entered an alert state. Please investigate the issue.\n- Trigger: {{ ctx. trigger. name }} - Severity: {{ ctx. trigger. severity }} - Period start: {{ ctx. periodStart }} - Period end: {{ ctx. periodEnd }} If you want to use the ctx.results variable in a message, use {{ctx.results.0}} rather than {{ctx.results[0]}}. This difference is due to how Mustache handles bracket notation.\nQuestions about destinations\nQ: What plugins do I need installed besides Alerting?\nA: To continue using the notification action in the Alerting plugin, you need to install the backend plugins notifications-core and notifications. You can also install the Notifications Dashboards plugin to manage Notification channels via OpenSearch Dashboards.\nQ: Can I still create destinations?\nA: No, destinations have been deprecated and can no longer be created/edited.\nQ: Will I need to move my destinations to the Notifications plugin?\nA: No. To upgrade users, a background process will automatically move destinations to notification channels. These channels will have the same ID as the destinations, and monitor execution will choose the correct ID, so you don’t have to make any changes to the monitor’s definition. The migrated destinations will be deleted.\nQ: What happens if any destinations fail to migrate?\nA: If a destination failed to migrate, the monitor will continue using it until the monitor is migrated to a notification channel. You don’t need to do anything in this case.\nQ: Do I need to install the Notifications plugins if monitors can still use destinations?\nA: Yes. The fallback on destination is to prevent failures in sending messages if migration fails; however, the Notification plugin is what actually sends the message. Not having the Notification plugin installed will lead to the action failing.\nWork with alerts\nAlerts persist until you resolve the root cause and have the following states: State Description Active\nThe alert is ongoing and unacknowledged. Alerts remain in this state until you acknowledge them, delete the trigger associated with the alert, or delete the monitor entirely.\nAcknowledged\nSomeone has acknowledged the alert, but not fixed the root cause.\nCompleted\nThe alert is no longer ongoing. Alerts enter this state after the corresponding trigger evaluates to false.\nError\nAn error occurred while executing the trigger—usually the result of a a bad trigger or destination.\nDeleted\nSomeone deleted the monitor or trigger associated with this alert while the alert was ongoing. Create cluster metrics monitor\nIn addition to monitoring conditions for indexes, the Alerting plugin allows monitoring conditions for clusters. Alerts can be set by cluster metrics to watch for the following conditions:\nThe health of your cluster reaches a status of yellow or red\nCluster-level metrics, such as CPU usage and JVM memory usage, reach specified thresholds\nNode-level metrics, such as available disk space, JVM memory usage, and CPU usage, reach a specified threshold\nThe total number of documents stores reaches a specified amount\nTo create a cluster metrics monitor:\nSelect Alerting &gt; Monitors &gt; Create monitor.\nSelect the Per cluster metrics monitor option.\nIn the Query section, pick the Request type from the dropdown.\n(Optional) If you want to filter the API response to use only certain path parameters, enter those parameters under Query parameters. Most APIs that can be used to monitor cluster status support path parameters as described in their documentation (e.g., comma-separated lists of index names).\nIn the Triggers section, indicate what conditions trigger an alert. The trigger condition autopopulates a painless ctx variable. For example, a cluster monitor watching for Cluster Stats uses the trigger condition ctx.results[0].indices.count &lt;= 0, which triggers an alert based on the number of indexes returned by the query. For more specificity, add any additional painless conditions supported by the API. To see an example of the condition response, select Preview condition response.\nIn the Actions section, indicate how you want your users to be notified when a trigger condition is met.\nSelect Create. Your new monitor appears in the Monitors list.\nSupported APIs\nTrigger conditions use responses from the following cat API endpoints. Most APIs that can be used to monitor cluster status support path parameters as described in their documentation (e.g., comma-separated lists of index names). However, they do not support query parameters. _cluster/health _cluster/stats _cluster/settings _nodes/stats _cat/pending_tasks _cat/recovery _cat/snapshots _cat/tasks Restrict API fields\nIf you want to hide fields from the API response that you do not want exposed for alerting, reconfigure the supported_json_payloads.json file inside the Alerting plugin. The file functions as an allow list for the API fields you want to use in an alert. By default, all APIs and their parameters can be used for monitors and trigger conditions.\nHowever, you can modify the file so that cluster metric monitors can only be created for APIs referenced. Furthermore, only fields referenced in the supported files can create trigger conditions. This supported_json_payloads.json allows for a cluster metrics monitor to be created for the _cluster/stats API, and triggers conditions for the indices.shards.total and indices.shards.index.shards.min fields. \"/_cluster/stats\": { \"indices\": [ \"shards.total\", \"shards.index.shards.min\"] } Painless triggers\nPainless scripts define triggers for cluster metrics monitors, similar to query or bucket-level monitors that are defined using the extraction query definition option. Painless scripts are comprised of at least one statement and any additional functions you wish to execute.\nThe cluster metrics monitor supports up to ten triggers.\nIn this example, a JSON object creates a trigger that sends an alert when the Cluster Health is yellow. script points the source to the painless script ctx.results[0].status == \\\"yellow\\. { \"name\": \"Cluster Health Monitor\", \"type\": \"monitor\", \"monitor_type\": \"query_level_monitor\", \"enabled\": true, \"schedule\": { \"period\": { \"unit\": \"MINUTES\", \"interval\": 1 } }, \"inputs\": [ { \"uri\": { \"api_type\": \"CLUSTER_HEALTH\", \"path\": \"_cluster/health/\", \"path_params\": \"\", \"url\": \"http://localhost:9200/_cluster/health/\" } }], \"triggers\": [ { \"query_level_trigger\": { \"id\": \"Tf_L_nwBti6R6Bm-18qC\", \"name\": \"Yellow status trigger\", \"severity\": \"1\", \"condition\": { \"script\": { \"source\": \"ctx.results[0].status == \\\" yellow \\\" \", \"lang\": \"painless\" } }, \"actions\": [] } }] } See trigger variables for more painless ctx options.\nLimitations\nCurrently, the cluster metrics monitor has the following limitations:\nYou cannot create monitors for remote clusters.\nThe OpenSearch cluster must be in a state where an index’s conditions can be monitored and actions can be executed against the index.\nRemoving resource permissions from a user will not prevent that user’s preexisting monitors for that resource from executing.\nUsers with permissions to create monitors are not blocked from creating monitors for resources for which they do not have permissions; however, those monitors will not execute.",
    "ancestors": [
      "Observability",
      "Alerting"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/alerting/security/",
    "title": "Alerting security",
    "content": "If you use the Security plugin alongside alerting, you might want to limit certain users to certain actions. For example, you might want some users to only be able to view and acknowledge alerts, while others can modify monitors and destinations.\nBasic permissions\nThe Security plugin has three built-in roles that cover most alerting use cases: alerting_read_access, alerting_ack_alerts, and alerting_full_access. For descriptions of each, see Predefined roles.\nIf these roles don’t meet your needs, mix and match individual alerting permissions to suit your use case. Each action corresponds to an operation in the REST API. For example, the cluster:admin/opensearch/alerting/destination/delete permission lets you delete destinations.\nHow monitors access data\nMonitors run with the permissions of the user who created or last modified them. For example, consider the user jdoe, who works at a chain of retail stores. jdoe has two roles. Together, these two roles allow read access to three indices: store1-returns, store2-returns, and store3-returns. jdoe creates a monitor that sends an email to management whenever the number of returns across all three indices exceeds 40 per hour.\nLater, the user psantos wants to edit the monitor to run every two hours, but psantos only has access to store1-returns. To make the change, psantos has two options:\nUpdate the monitor so that it only checks store1-returns.\nAsk an administrator for read access to the other two indices.\nAfter making the change, the monitor now runs with the same permissions as psantos, including any document-level security queries, excluded fields, and masked fields. If you use an extraction query to define your monitor, use the Run button to ensure that the response includes the fields you need.\nOnce a monitor is created, the Alerting plugin will continue executing the monitor, even if the user who created the monitor has their permissions removed. Only a user with the correct cluster permissions can manually disable or delete a monitor to stop it from executing:\nDisable a monitor: cluster:admin/opendistro/alerting/monitor/write Delete a monitor: cluster:admin/opendistro/alerting/monitor/delete If your monitor’s trigger has notifications configured, the Alerting plugin continues to send out notifications regardless of destination type. To stop notifications, a user must manually delete them in the trigger’s actions.\nA note on alerts and fine-grained access control\nWhen a trigger generates an alert, the monitor configuration, the alert itself, and any notification that is sent to a channel may include metadata describing the index being queried. By design, the plugin must extract the data and store it as metadata outside of the index. Document-level security (DLS) and field-level security (FLS) access controls are designed to protect the data in the index. But once the data is stored outside the index as metadata, users with access to the monitor configurations, alerts, and their notifications will be able to view this metadata and possibly infer the contents and quality of data in the index, which would otherwise be concealed by DLS and FLS access control.\nTo reduce the chances of unintended users viewing metadata that could describe an index, we recommend that administrators enable role-based access control and keep these kinds of design elements in mind when assigning permissions to the intended group of users. See Limit access by backend role for details.\n(Advanced) Limit access by backend role\nOut of the box, the alerting plugin has no concept of ownership. For example, if you have the cluster:admin/opensearch/alerting/monitor/write permission, you can edit all monitors, regardless of whether you created them. If a small number of trusted users manage your monitors and destinations, this lack of ownership generally isn’t a problem. A larger organization might need to segment access by backend role.\nFirst, make sure that your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually with a create user operation. To add a backend role to a create user request, follow the Create user instructions in the Security plugin API documentation.\nNext, enable the following setting: PUT _cluster/settings { \"transient\": { \"plugins.alerting.filter_by_backend_roles\": \"true\" } } Now when users view alerting resources in OpenSearch Dashboards (or make REST API calls), they only see monitors and destinations that are created by users who share at least one backend role. For example, consider three users who all have full access to alerting: jdoe, jroe, and psantos. jdoe and jroe are on the same team at work and both have the analyst backend role. psantos has the human-resources backend role.\nIf jdoe creates a monitor, jroe can see and modify it, but psantos can’t. If that monitor generates an alert, the situation is the same: jroe can see and acknowledge it, but psantos can’t. If psantos creates a destination, jdoe and jroe can’t see or modify it.\n<!-- ## (Advanced) Limit access by individual\nIf you only want users to be able to see and modify their own monitors and destinations, duplicate the `alerting_full_access` role and add the following [DLS query](https://opensearch.org/docs/latest/security/access-control/document-level-security/) to it:\n```json\n{\n\"bool\": {\n\"should\": [{\n\"match\": {\n\"monitor.created_by\": \"${user.name}\"\n}\n}, {\n\"match\": {\n\"destination.created_by\": \"${user.name}\"\n}\n}]\n}\n}\n```\nThen, use this new role for all alerting users. -->\nSpecify RBAC backend roles\nYou can specify role-based access control (RBAC) backend roles when you create or update a monitor with the Alerting API.\nIn a create monitor scenario, follow these guidelines to specify roles: User type Role is specified by user or not (Y/N) How to use the RBAC roles Admin user\nYes\nUse all the specified backend roles to associate to the monitor.\nRegular user\nYes\nUse all the specified backend roles from the list of backend roles that the user has permission to use to associate with the monitor.\nRegular user\nNo\nCopy user’s backend roles and associate them to the monitor. In an update monitor scenario, follow these guidelines to specify roles: User type Role is specified by user or not (Y/N) How to use the RBAC roles Admin user\nYes\nRemove all the backend roles associate to the monitor and then use all the specified backend roles associated to the monitor.\nRegular user\nYes\nRemove backend roles associated to the monitor that the user has access to, but didn’t specify. Then add all the other specified backend roles from the list of backend roles that the user has permission to use to the monitor.\nRegular user\nNo\nDon’t update the backend roles on the monitor. For admin users, an empty list is considered the same as removing all permissions that the user possesses. If a non-admin user passes in an empty list, that will throw an exception, because that is not allowed by non-admin users.\nIf the user tries to associate roles that they don’t have permission to use, it will throw an exception.\nTo create an RBAC role, follow instructions in the Security plugin API documentation to Create role.\nCreate a monitor with an RBAC role\nWhen you create a monitor with the Alerting API, you can specify the RBAC roles at the bottom of the request body. Use the rbac_roles parameter.\nThe following sample shows the RBAC roles specified by the RBAC parameter:... \"rbac_roles\": [ \"role1\", \"role2\"] } To see a full request sample, see Create a monitor.",
    "ancestors": [
      "Observability",
      "Alerting"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/alerting/settings/",
    "title": "Management",
    "content": "Alerting indices\nThe alerting feature creates several indices and one alias. The Security plugin demo script configures them as system indices for an extra layer of protection. Don’t delete these indices or modify their contents without using the alerting APIs. Index Purpose.opendistro-alerting-alerts Stores ongoing alerts..opendistro-alerting-alert-history-&lt;date&gt; Stores a history of completed alerts..opendistro-alerting-config Stores monitors, triggers, and destinations. Take a snapshot of this index to back up your alerting configuration..opendistro-alerting-alert-history-write (alias)\nProvides a consistent URI for the.opendistro-alerting-alert-history-&lt;date&gt; index. All alerting indices are hidden by default. For a summary, make the following request: GET _cat/indices?expand_wildcards=open,hidden Alerting settings\nWe don’t recommend changing these settings; the defaults should work well for most use cases.\nAll settings are available using the OpenSearch _cluster/settings API. None require a restart, and all can be marked persistent or transient. Setting Default Description plugins.scheduled_jobs.enabled true\nWhether the alerting plugin is enabled or not. If disabled, all monitors immediately stop running. plugins.alerting.index_timeout 60s\nThe timeout for creating monitors and destinations using the REST APIs. plugins.alerting.request_timeout 10s\nThe timeout for miscellaneous requests from the plugin. plugins.alerting.action_throttle_max_value 24h\nThe maximum amount of time you can set for action throttling. By default, this value displays as 1440 minutes in OpenSearch Dashboards. plugins.alerting.input_timeout 30s\nHow long the monitor can take to issue the search request. plugins.alerting.bulk_timeout 120s\nHow long the monitor can write alerts to the alert index. plugins.alerting.alert_backoff_count 3\nThe number of retries for writing alerts before the operation fails. plugins.alerting.alert_backoff_millis 50ms\nThe amount of time to wait between retries—increases exponentially after each failed retry. plugins.alerting.alert_history_rollover_period 12h\nHow frequently to check whether the.opendistro-alerting-alert-history-write alias should roll over to a new history index and whether the Alerting plugin should delete any history indices. plugins.alerting.move_alerts_backoff_millis 250\nThe amount of time to wait between retries—increases exponentially after each failed retry. plugins.alerting.move_alerts_backoff_count 3\nThe number of retries for moving alerts to a deleted state after their monitor or trigger has been deleted. plugins.alerting.monitor.max_monitors 1000\nThe maximum number of monitors users can create. plugins.alerting.alert_history_max_age 30d\nThe oldest document to store in the.opendistro-alert-history-&lt;date&gt; index before creating a new index. If the number of alerts in this time period does not exceed alert_history_max_docs, alerting creates one history index per period (e.g. one index every 30 days). plugins.alerting.alert_history_max_docs 1000\nThe maximum number of alerts to store in the.opendistro-alert-history-&lt;date&gt; index before creating a new index. plugins.alerting.alert_history_enabled true\nWhether to create.opendistro-alerting-alert-history-&lt;date&gt; indices. plugins.alerting.alert_history_retention_period 60d\nThe amount of time to keep history indices before automatically deleting them. plugins.alerting.destination.allow_list [“chime”, “slack”, “custom_webhook”, “email”, “test_action”]\nThe list of allowed destinations. If you don’t want to allow users to a certain type of destination, you can remove it from this list, but we recommend leaving this setting as-is. plugins.alerting.filter_by_backend_roles “false”\nRestricts access to monitors by backend role. See Alerting security. plugins.scheduled_jobs.sweeper.period 5m\nThe alerting feature uses its “job sweeper” component to periodically check for new or updated jobs. This setting is the rate at which the sweeper checks to see if any jobs (monitors) have changed and need to be rescheduled. plugins.scheduled_jobs.sweeper.page_size 100\nThe page size for the sweeper. You shouldn’t need to change this value. plugins.scheduled_jobs.sweeper.backoff_millis 50ms\nThe amount of time the sweeper waits between retries—increases exponentially after each failed retry. plugins.scheduled_jobs.sweeper.retry_count 3\nThe total number of times the sweeper should retry before throwing an error. plugins.scheduled_jobs.request_timeout 10s\nThe timeout for the request that sweeps shards for jobs.",
    "ancestors": [
      "Observability",
      "Alerting"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/app-analytics/",
    "title": "Application analytics",
    "content": "You can use application analytics to create custom observability applications to view the availability status of your systems, where you can combine log events with trace and metric data into a single view of overall system health. This lets you quickly pivot between logs, traces, and metrics to dig into the source of any issues.\nGet started with application analytics\nTo get started, select the Menu button on the upper left corner of the OpenSearch Dashboards interface. Next, select Observability, and then choose Application analytics.\nCreate an application\nChoose Create application.\nEnter a name for your application and optionally add a description.\nDo at least one of the following:\nUse PPL to specify the base query.\nYou can’t change the base query after the application is created.\nSelect services &amp; entities from the dropdown or the service map.\nSelect trace groups from the dropdown or the table.\n4. Choose Create.\nCreate a visualization\nChoose the Log Events tab.\nUse PPL to build upon your base query.\nChoose the Visualizations tab to see your visualizations.\nExpand the Save dropdown menu, enter a name for your visualization, then choose Save.\nTo see your visualizations, choose the Panel tab.\nConfigure availability\nAvailability is the status of your application determined by availability levels set on a time series metric.\nTo create an availability level, you must configure the following:\ncolor: The color of the availability badge on the home page.\nname: The text in the availability badge on the home page.\nexpression: Comparison operator to determine the availability.\nvalue: Value to use when calculating availability. By default, application analytics shows results from the last 24 hours of your data. To see data from a different time frame, use the date and time selector.\nTime series metric\nA time series metric is any visualization that has a query that spans over a timestamp and is a line chart. You can then use PPL to define arbitrary conditions on your logs to create a visualization over time.\nExample source = &lt;index_name&gt; |... |... | stats... by span(&lt;timestamp_field&gt;, 1h) Choose Line in visualization configurations to create a time series metric.",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/event-analytics/",
    "title": "Event analytics",
    "content": "Event analytics in Observability is where you can use Piped Processing Language (PPL) queries to build and view different visualizations of your data.\nGetting started with event analytics\nTo get started, choose Observability in OpenSearch Dashboards and then choose Event analytics. If you want to start exploring without adding any of your own data, choose Add samples, and Dashboards adds sample visualizations you can interact with.\nBuilding a query\nTo generate custom visualizations, you must first specify a PPL query. OpenSearch Dashboards then automatically creates a visualization based on the results of your query.\nFor example, the following PPL query returns a count of how many host addresses are currently in your data. source = opensearch_dashboards_sample_data_logs | fields host | stats count() By default, Dashboards shows results from the last 15 minutes of your data. To see data from a different time frame, use the date and time selector.\nFor more information about building PPL queries, see Piped Processing Language.\nSaving a visualization\nAfter Dashboards generates a visualization, you must save it if you want to return to it at a later time or if you want to add it to an operational panel.\nTo save a visualization, expand the save dropdown menu next to Refresh, enter a name for your visualization, then choose Save. You can reopen any saved visualizations on the event analytics page.\nCreating event analytics visualizations and adding them to dashboards\nThis feature is available in OpenSearch Dashboards version 2.7 and later. It works with new visualizations created in version 2.7 or later that use PPL to query data from OpenSearch or federated data sources such as Prometheus.\nPresenting your visualizations on a dashboard, instead of the event analytics page, makes it easier for users to understand and interpret the data at a glance.\nTo create a PPL visualization, follow these steps:\nOn the main menu, choose Visualize &gt; PPL.\nIn the Observability &gt; Logs &gt; Explorer window, enter the index source in the PPL query field, for example, source = opensearch_dashboards_sample_data_flights | stats count() by DestCountry. You must enter the query using PPL syntax.\nSet the time filter, for example, This week, and then select Refresh.\nChoose the visualization type, for example, Pie, from the right sidebar dropdown menu.\nSelect Save and enter a name for the visualization.\nYou’ve created a new visualization that can be added to a new or existing dashboard. To add a PPL query to a dashboard, follow these steps:\nSelect Dashboard from the main menu.\nIn the Dashboards window, select Create &gt; Dashboard.\nIn the Editing New Dashboard window, choose Add an existing.\nIn the Add panels window, choose PPL and select the visualization. It is now displayed on your dashboard.\nSelect Save and enter a name for the dashboard.\nTo add more visualizations to the dashboard, choose Select existing visualization and follow the steps above. Alternatively, choose Create new and then select PPL in the New Visualization window. You’ll return to the event analytics page and follow steps 1–6 in the preceding instructions. Limitations of event analytics visualizations\nEvent analytics visualizations currently do not support Dashboards Query Language (DQL) or query domain-specific language (DSL), and they do not use index patterns. Note the following limitations:\nEvent analytics visualizations only use filters created using the dropdown interface. If you have DQL query or DSL filters in a dashboard, the visualizations do not use them.\nThe Dashboard filter dropdown interface only shows fields from the default index pattern or index patterns used by other visualizations in the same dashboard.\nViewing logs\nThe following are methods you can use to view logs.\nCorrelating logs and traces\nIf you regularly track events across applications, you can correlate logs and traces. To view the correlation, you have to index the traces according to Open Telemetry standards (similar to trace analytics). Once you add a TraceId field to your logs, you can view the correlated trace information in the event explorer log details. This method lets you correlate logs and traces that correspond to the same execution context. Viewing surrounding events\nIf you want to know more about a log event you’re looking at, you can select View surrounding events to get a bigger picture of what was happening around the time of interest. Livestreaming logs\nIf you prefer watching events happen live, you can configure an interval so event analytics automatically refreshes the content. Live tail lets you stream logs live to OpenSearch observability event analytics based on the provided PPL query, as well as provide rich functionality such as filters. Doing so improves your debugging experience and lets you monitor your logs in real-time without having to manually refresh.\nYou can also choose intervals and switch between them to dictate how often live tail should stream live logs. This feature is similar to the CLI’s tail -f command in that it only retrieves the most recent live logs by possibly eliminating a large portion of live logs. Live tail also provides you with the total count of live logs received by OpenSearch during the live stream, which you can use to better understand the incoming traffic.",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/index/",
    "title": "Observability",
    "content": "OpenSearch Dashboards\nObservability is collection of plugins and applications that let you visualize data-driven events by using Piped Processing Language to explore, discover, and query data stored in OpenSearch.\nYour experience of exploring data might differ, but if you’re new to exploring data to create visualizations, we recommend trying a workflow like the following:\nExplore data within a certain timeframe using Piped Processing Language.\nUse event analytics to turn data-driven events into visualizations. Create operational panels and add visualizations to compare data the way you like. Use log analytics to transform unstructured log data.\nUse trace analytics to create traces and dive deep into your data. Leverage notebooks to combine different visualizations and code blocks that you can share with team members.",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/log-ingestion/",
    "title": "Log ingestion",
    "content": "Log ingestion provides a way to transform unstructured log data into structured data and ingest into OpenSearch. Structured log data allows for improved queries and filtering based on the data format when searching logs for an event.\nGet started with log ingestion\nOpenSearch Log Ingestion consists of three components— Data Prepper, OpenSearch and OpenSearch Dashboards —that fit into the OpenSearch ecosystem. The Data Prepper repository has several sample applications to help you get started.\nBasic flow of data Log Ingestion relies on you adding log collection to your application’s environment to gather and send log data.\n(In the example below, FluentBit is used as a log collector that collects log data from a file and sends the log data to Data Prepper). Data Prepper receives the log data, transforms the data into a structure format, and indexes it on an OpenSearch cluster.\nThe data can then be explored through OpenSearch search queries or the Discover page in OpenSearch Dashboards.\nExample\nThis example mimics the writing of log entries to a log file that are then processed by Data Prepper and stored in OpenSearch.\nDownload or clone the Data Prepper repository. Then navigate to examples/log-ingestion/ and open docker-compose.yml in a text editor. This file contains a container for: Fluent Bit ( fluent-bit)\nData Prepper ( data-prepper)\nA single-node OpenSearch cluster ( opensearch)\nOpenSearch Dashboards ( opensearch-dashboards).\nClose the file and run docker-compose up --build to start the containers.\nAfter the containers start, your ingestion pipeline is set up and ready to ingest log data. The fluent-bit container is configured to read log data from test.log. Run the following command to generate log data to send to the log ingestion pipeline. echo '63.173.168.120 - - [04/Nov/2021:15:07:25 -0500] \"GET /search/tag/list HTTP/1.0\" 200 5003' &gt;&gt; test.log Fluent-Bit will collect the log data and send it to Data Prepper: [2021/12/02 15:35:41] [ info] [output:http:http.0] data-prepper:2021, HTTP status=200\n200 OK Data Prepper will process the log and index it: 2021-12-02T15:35:44,499 [log-pipeline-processor-worker-1-thread-1] INFO com.amazon.dataprepper.pipeline.ProcessWorker - log-pipeline Worker: Processing 1 records from buffer This should result in a single document being written to the OpenSearch cluster in the apache-logs index as defined in the log_pipeline.yaml file.\nRun the following command to see one of the raw documents in the OpenSearch cluster: curl -X GET -u 'admin:admin' -k 'https://localhost:9200/apache_logs/_search?pretty&amp;size=1' The response should show the parsed log data: \"hits\": [\n{\n\"_index\": \"apache_logs\",\n\"_type\": \"_doc\",\n\"_id\": \"yGrJe30BgI2EWNKtDZ1g\",\n\"_score\": 1.0,\n\"_source\": {\n\"date\": 1.638459307042312E9,\n\"log\": \"63.173.168.120 - - [04/Nov/2021:15:07:25 -0500] \\\"GET /search/tag/list HTTP/1.0\\\" 200 5003\",\n\"request\": \"/search/tag/list\",\n\"auth\": \"-\",\n\"ident\": \"-\",\n\"response\": \"200\",\n\"bytes\": \"5003\",\n\"clientip\": \"63.173.168.120\",\n\"verb\": \"GET\",\n\"httpversion\": \"1.0\",\n\"timestamp\": \"04/Nov/2021:15:07:25 -0500\"\n}\n}] The same data can be viewed in OpenSearch Dashboards by visiting the Discover page and searching the apache_logs index. Remember, you must create the index in OpenSearch Dashboards if this is your first time searching for the index.",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/notebooks/",
    "title": "Notebooks",
    "content": "An OpenSearch Dashboards notebook is an interface that lets you easily combine code snippets, live visualizations, and narrative text in a single notebook interface.\nNotebooks let you interactively explore data by running different visualizations that you can share with team members to collaborate on a project.\nA notebook is a document composed of two elements: code blocks (Markdown/SQL/PPL) and visualizations. Choose multiple timelines to compare and contrast visualizations.\nYou can also generate reports directly from your notebooks.\nCommon use cases include creating postmortem reports, designing runbooks, building live infrastructure reports, and writing documentation.\nTenants in OpenSearch Dashboards are spaces for saving notebooks and other OpenSearch Dashboards objects. For more information, see OpenSearch Dashboards multi-tenancy.\nGet started with notebooks\nTo get started, choose Notebooks within OpenSearch Dashboards.\nStep 1: Create a notebook\nA notebook is an interface for creating reports.\nChoose Create notebook and enter a descriptive name.\nChoose Create.\nChoose Actions to rename, duplicate, or delete a notebook. Step 2: Add a paragraph\nParagraphs combine code blocks and visualizations for describing data.\nAdd a code block\nCode blocks support markdown, SQL, and PPL languages.\nSpecify the input language on the first line using %[language type] syntax.\nFor example, type %md for markdown, %sql for SQL, and %ppl for PPL.\nSample markdown block %md\nAdd in text formatted in markdown. Sample SQL block % sql Select * from opensearch_dashboards_sample_data_flights limit 20; Sample PPL block %ppl\nsource=opensearch_dashboards_sample_data_logs | head 20 Add a visualization\nTo add a visualization, choose Add paragraph and select Visualization.\nIn Title, select your visualization and choose a date range. You can choose multiple timelines to compare and contrast visualizations.\nTo run and save a paragraph, choose Run. Paragraph actions\nYou can perform the following actions on paragraphs:\nAdd a new paragraph to the top of a report.\nAdd a new paragraph to the bottom of a report.\nRun all the paragraphs at the same time.\nClear the outputs of all paragraphs.\nDelete all the paragraphs. Sample notebooks\nWe prepared the following sample notebooks that showcase a variety of use cases:\nUsing SQL to query the OpenSearch Dashboards sample flight data.\nUsing PPL to query the OpenSearch Dashboards sample web logs data.\nUsing PPL and visualizations to perform sample root cause event analysis on the OpenSearch Dashboards sample web logs data.\nTo add a sample notebook, choose Actions and select Add sample notebooks. Create a report\nYou can use notebooks to create PNG and PDF reports:\nFrom the top menu bar, choose Reporting actions.\nYou can choose to Download PDF or Download PNG.\nReports generate asynchronously in the background and might take a few minutes, depending on the size of the report. A notification appears when your report is ready to download.\nTo create a schedule-based report, choose Create report definition. For steps to create a report definition, see Create reports using a definition.\nTo see all your reports, choose View all reports.",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/notifications/api/",
    "title": "API",
    "content": "If you want to programmatically define your notification channels and sources for versioning and reuse, you can use the Notifications REST API to define, configure, and delete notification channels and send test messages.\nTable of contents List supported channel configurations List all notification configurations Create channel configuration Get channel configuration Update channel configuration Delete channel configuration Send test notification List supported channel configurations\nTo retrieve a list of all supported notification configuration types, send a GET request to the features resource.\nSample Request GET /_plugins/_notifications/features Sample Response { \"allowed_config_type_list\": [ \"slack\", \"chime\", \"webhook\", \"email\", \"sns\", \"ses_account\", \"smtp_account\", \"email_group\"], \"plugin_features\": { \"tooltip_support\": \"true\" } } List all notification configurations\nTo retrieve a list of all notification configurations, send a GET request to the configs resource.\nSample Request GET _plugins/_notifications/configs Sample Response { \"start_index\": 0, \"total_hits\": 2, \"total_hit_relation\": \"eq\", \"config_list\": [ { \"config_id\": \"sample-id\", \"last_updated_time_ms\": 1652760532774, \"created_time_ms\": 1652760532774, \"config\": { \"name\": \"Sample Slack Channel\", \"description\": \"This is a Slack channel\", \"config_type\": \"slack\", \"is_enabled\": true, \"slack\": { \"url\": \"https://sample-slack-webhook\" } } }, { \"config_id\": \"sample-id2\", \"last_updated_time_ms\": 1652760735380, \"created_time_ms\": 1652760735380, \"config\": { \"name\": \"Test chime channel\", \"description\": \"A test chime channel\", \"config_type\": \"chime\", \"is_enabled\": true, \"chime\": { \"url\": \"https://sample-chime-webhook\" } } }] } To filter the notification configuration types this request returns, you can refine your query with the following optional path parameters. Parameter Description config_id\nSpecifies the channel identifier.\nconfig_id_list\nSpecifies a comma-separated list of channel IDs.\nfrom_index\nThe starting index to search from.\nmax_items\nThe maximum amount of items to return in your request.\nsort_order\nSpecifies the direction to sort results in. Valid options are asc and desc.\nsort_field\nField to sort results with.\nlast_updated_time_ms\nThe Unix time in milliseconds of when the channel was last updated.\ncreated_time_ms\nThe Unix time in milliseconds of when the channel was created.\nis_enabled\nIndicates whether the channel is enabled.\nconfig_type\nThe channel type. Valid options are sns, slack, chime, webhook, smtp_account, ses_account, email_group, and email.\nname\nThe channel’s name.\ndescription\nThe channel’s description.\nemail.email_account_id\nThe sender email addresses the channel uses.\nemail.email_group_id_list\nThe email groups the channel uses.\nemail.recipient_list\nThe channel’s recipient list.\nemail_group.recipient_list\nThe channel’s list of email recipient groups.\nsmtp_account.method\nThe email encryption method.\nslack.url\nThe Slack channel’s URL.\nchime.url\nThe Amazon Chime connection’s URL.\nwebhook.url\nThe webhook’s URL.\nsmtp_account.host\nThe domain of the SMTP account.\nsmtp_account.from_address\nThe email account’s sender address.\nsmtp_account.method\nThe SMTP account’s encryption method.\nsns.topic_arn\nThe Amazon Simple Notification Service (SNS) topic’s ARN.\nsns.role_arn\nThe Amazon SNS topic’s role ARN.\nses_account.region\nThe Amazon Simple Email Service (SES) account’s AWS Region.\nses_account.role_arn\nThe Amazon SES account’s role ARN.\nses_account.from_address\nThe Amazon SES account’s sender email address. Create channel configuration\nTo create a notification channel configuration, send a POST request to the configs resource.\nSample Request POST /_plugins/_notifications/configs/ { \"config_id\": \"sample-id\", \"name\": \"sample-name\", \"config\": { \"name\": \"Sample Slack Channel\", \"description\": \"This is a Slack channel\", \"config_type\": \"slack\", \"is_enabled\": true, \"slack\": { \"url\": \"https://sample-slack-webhook\" } } } The create channel API operation accepts the following fields in its request body: Field Data type Description Required config_id\nString\nThe configuration’s custom ID.\nNo\nconfig\nObject\nContains all relevant information, such as channel name, configuration type, and plugin source.\nYes\nname\nString\nName of the channel.\nYes\ndescription\nString\nThe channel’s description.\nNo\nconfig_type\nString\nThe destination of your notification. Valid options are sns, slack, chime, webhook, smtp_account, ses_account, email_group, and email.\nYes\nis_enabled\nBoolean\nIndicates whether the channel is enabled for sending and receiving notifications. Default is true.\nNo The create channel operation accepts multiple config_types as possible notification destinations, so follow the format for your preferred config_type. \"sns\": { \"topic_arn\": \"&lt;arn&gt;\", \"role_arn\": \"&lt;arn&gt;\" //optional } \"slack\": { \"url\": \"https://sample-chime-webhoook\" } \"chime\": { \"url\": \"https://sample-amazon-chime-webhoook\" } \"webhook\": { \"url\": \"https://custom-webhook-test-url.com:8888/test-path?params1=value1&amp;params2=value2\" } \"smtp_account\": { \"host\": \"test-host.com\", \"port\": 123, \"method\": \"start_tls\", \"from_address\": \"test@email.com\" } \"ses_account\": { \"region\": \"us-east-1\", \"role_arn\": \"arn:aws:iam::012345678912:role/NotificationsSESRole\", \"from_address\": \"test@email.com\" } \"email_group\": { //Email recipient group \"recipient_list\": [ { \"recipient\": \"test-email1@test.com\" }, { \"recipient\": \"test-email2@test.com\" }] } \"email\": { //The channel that sends emails \"email_account_id\": \"&lt;smtp or ses account config id&gt;\", \"recipient_list\": [ { \"recipient\": \"custom.email@test.com\" }], \"email_group_id_list\": [] } The following example demonstrates how to create a channel using email as a config_type: POST /_plugins/_notifications/configs/ { \"id\": \"sample-email-id\", \"name\": \"sample-name\", \"config\": { \"name\": \"Sample Email Channel\", \"description\": \"Sample email description\", \"config_type\": \"email\", \"is_enabled\": true, \"email\": { \"email_account_id\": \"&lt;email_account_id&gt;\", \"recipient_list\": [ \"sample@email.com\"] } } } Sample Response { \"config_id\": \"&lt;config_id&gt;\" } Get channel configuration\nTo get a channel configuration by config_id, send a GET request and specify the config_id as a path parameter.\nSample Request GET _plugins/_notifications/configs/&lt;config_id&gt; Sample Response { \"start_index\": 0, \"total_hits\": 1, \"total_hit_relation\": \"eq\", \"config_list\": [ { \"config_id\": \"sample-id\", \"last_updated_time_ms\": 1652760532774, \"created_time_ms\": 1652760532774, \"config\": { \"name\": \"Sample Slack Channel\", \"description\": \"This is a Slack channel\", \"config_type\": \"slack\", \"is_enabled\": true, \"slack\": { \"url\": \"https://sample-slack-webhook\" } } }] } Update channel configuration\nTo update a channel configuration, send a POST request to the configs resource and specify the channel’s config_id as a path parameter. Specify the new configuration details in the request body.\nSample Request PUT _plugins/_notifications/configs/&lt;config_id&gt; { \"config\": { \"name\": \"Slack Channel\", \"description\": \"This is an updated channel configuration\", \"config_type\": \"slack\", \"is_enabled\": true, \"slack\": { \"url\": \"https://hooks.slack.com/sample-url\" } } } Sample Response { \"config_id\": \"&lt;config_id&gt;\" } Delete channel configuration\nTo delete a channel configuration, send a DELETE request to the configs resource and specify the config_id as a path parameter.\nSample Request DELETE /_plugins/_notifications/configs/&lt;config_id&gt; Sample Response { \"delete_response_list\": { \"&lt;config_id&gt;\": \"OK\" } } You can also submit a comma-separated list of channel IDs you want to delete, and OpenSearch deletes all of the specified notification channels.\nSample Request DELETE /_plugins/_notifications/configs/?config_id_list=&lt;config_id 1 &gt;,&lt;config_id 2 &gt;,&lt;config_id 3 &gt;... Sample Response { \"delete_response_list\": { \"&lt;config_id1&gt;\": \"OK\", \"&lt;config_id2&gt;\": \"OK\", \"&lt;config_id3&gt;\": \"OK\" } } Send test notification\nTo send a test notification, send a GET request to /feature/test/ and specify the channel configuration’s config_id as a path parameter.\nSample Request GET _plugins/_notifications/feature/test/&lt;config_id&gt; Sample Response { \"event_source\": { \"title\": \"Test Message Title-0Jnlh4ABa4TCWn5C5H2G\", \"reference_id\": \"0Jnlh4ABa4TCWn5C5H2G\", \"severity\": \"info\", \"tags\": [] }, \"status_list\": [ { \"config_id\": \"0Jnlh4ABa4TCWn5C5H2G\", \"config_type\": \"slack\", \"config_name\": \"sample-id\", \"email_recipient_status\": [], \"delivery_status\": { \"status_code\": \"200\", \"status_text\": \"\"\"&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;div&gt;\n&lt;h1&gt;Example Domain&lt;/h1&gt;\n&lt;p&gt;Sample paragraph.&lt;/p&gt;\n&lt;p&gt;&lt;a href=\" sample.example.com \"&gt;TO BE OR NOT TO BE, THAT IS THE QUESTION&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\" } }] }",
    "ancestors": [
      "Observability",
      "Notifications"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/notifications/index/",
    "title": "Notifications",
    "content": "The Notifications plugin provides a central location for all of your notifications from OpenSearch plugins. Using the plugin, you can configure which communication service you want to use and see relevant statistics and troubleshooting information. Currently, the Alerting and ISM plugins have integrated with the Notifications plugin.\nYou can use either OpenSearch Dashboards or the REST API to configure notifications. Dashboards offers a more organized way of selecting a channel type and selecting which OpenSearch plugin sources you want to use, whereas the REST API lets you programmatically define your notification channels for better versioning and reuse later on.\nUse the Dashboards UI to first create a channel that receives notifications from other plugins. Supported communication channels include Amazon Chime, Amazon Simple Notification Service (Amazon SNS), Amazon Simple Email Service (Amazon SES), email through SMTP, Slack, and custom webhooks. After you’ve configured your channel and plugin sources, send messages and start tracking your notifications from the Notifications plugin’s dashboard.\nUse the Notifications REST API to configure all of your channel’s settings. To use the API, you must have your notification’s name, description, channel type, which OpenSearch plugins to use as sources, and other associated URLs or groups.\nCreate a channel\nIn OpenSearch Dashboards, choose Notifications, Channels, and Create channel.\nIn the Name and description section, specify a name and optional description for your channel.\nIn the Configurations section, select the channel type and enter the necessary information for each type. For more information about configuring a channel that uses Amazon SNS or email, refer to the sections below. If you want to use Amazon Chime or Slack, you need to specify the webhook URL. For more information about using webhooks, see the documentation for Slack and Amazon Chime.\nIf you want to use custom webhooks, you must specify more information: parameters and headers. For example, if your endpoint requires basic authentication, you might need to add a header with an authorization key and a value of Basic &lt;Base64-encoded-credential-string&gt;. You might also need to change Content-Type to whatever your webhook requires. Popular values are application/json, application/xml, and text/plain.\nThis information is stored in plain text in the OpenSearch cluster. We will improve this design in the future, but for now, the encoded credentials (which are neither encrypted nor hashed) might be visible to other OpenSearch users.\nIn the Availability section, select the OpenSearch plugins you want to use with the notification channel.\nChoose Create.\nAmazon SNS as a channel type\nOpenSearch supports Amazon SNS for notifications. This integration with Amazon SNS means that, in addition to the other channel types, the Notifications plugin can send email messages, text messages, and even run AWS Lambda functions using SNS topics. For more information about Amazon SNS, see the Amazon Simple Notification Service Developer Guide.\nThe Notifications plugin currently supports two ways to authenticate users:\nProvide the user with full access to Amazon SNS.\nLet the user assume an AWS Identity and Access Management (IAM) role that has permissions to access Amazon SNS. Once you configure the notification channel to use the right Amazon SNS permissions, select the OpenSearch plugins that can trigger notifications.\nProvide full Amazon SNS access permissions\nIf you want to provide full Amazon SNS access to the IAM user, ensure that the user has the following permissions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"sns:*\"], \"Effect\": \"Allow\", \"Resource\": \"*\" }] } Assuming an IAM role with Amazon SNS permissions\nIf you want to let the user send notifications without directly having full permissions to Amazon SNS, let the user assume a role that does have the necessary permissions.\nThe IAM user must have the following permissions to assume a role: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:Describe*\", \"iam:ListRoles\", \"sts:AssumeRole\"], \"Resource\": \"*\" }] } Then add this policy into the IAM user’s trust relationship to actually assume the role: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::&lt;arn_number&gt;:user/&lt;iam_username&gt;\", }, \"Action\": \"sts:AssumeRole\" }] } Email as a channel type\nTo send or receive notifications with email, choose Email as the channel type. Next, select at least one sender and default recipient. To send notifications to more than a few people at a time, specify multiple email addresses or select a recipient group. If the Notifications plugin doesn’t currently have the necessary senders or groups, you can add them by first selecting SMTP sender and then choosing Create SMTP sender or Create recipient group. Choose SES sender to use Amazon Simple Email Service (Amazon SES).\nCreate email sender\nSpecify a unique name to associate with the sender.\nEnter an email address and, if applicable, its host (for example, smtp.gmail.com) and the port. If you’re using Amazon SES, enter the IAM role Amazon Resource Name (ARN) of the AWS account to send notifications from, along with the AWS Region.\nChoose an encryption method. Most email providers require Secure Sockets Layer (SSL) or Transport Layer Security (TLS), which require a user name and password in the OpenSearch keystore. See Authenticate sender account to learn more. Selecting an encryption method is only applicable if you’re creating an SMTP sender.\nChoose Create to save the configuration and create the sender. You can create a sender before you add your credentials to the OpenSearch keystore; however, you must authenticate each sender account before you use the sender in your channel configuration.\nCreate email recipient group\nAfter choosing Create recipient group, enter a unique name to associate with the email group and an optional description.\nSelect or enter the email addresses you want to add to the recipient group.\nChoose Create.\nAuthenticate sender account\nIf your email provider requires SSL or TLS, you must authenticate each sender account before you can send an email. Enter the sender account credentials in the OpenSearch keystore using the command line interface (CLI). Run the following commands (in your OpenSearch directory) to enter your user name and password. The &lt;sender_name&gt; is the name you entered for Sender earlier. opensearch.notifications.core.email.&lt;sender_name&gt;.username opensearch.notifications.core.email.&lt;sender_name&gt;.password To change or update your credentials (after you’ve added them to the keystore on every node), call the reload API to automatically update those credentials without restarting OpenSearch. POST _nodes/reload_secure_settings { \"secure_settings_password\": \"1234\" }",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/observability-security/",
    "title": "Observability security",
    "content": "You can use the Security plugin with Observability in OpenSearch to limit non-admin users to specific actions. For example, you might want some users to only view visualizations, notebooks, and other Observability objects, while others can create and modify them.\nBasic permissions\nThe Security plugin has two built-in roles that cover most Observability use cases: observability_full_access and observability_read_access. For descriptions of each, see Predefined roles. If you don’t see these predefined roles in OpenSearch Dashboards, you can create them with the following commands: PUT _plugins/_security/api/roles/observability_read_access { \"cluster_permissions\": [ \"cluster:admin/opensearch/observability/get\"] } PUT _plugins/_security/api/roles/observability_full_access { \"cluster_permissions\": [ \"cluster:admin/opensearch/observability/*\"] } If these roles don’t meet your needs, mix and match individual Observability permissions to suit your use case. For example, the cluster:admin/opensearch/observability/create permission lets you create Observability objects (visualizations, operational panels, notebooks, etc.).\nThe following is an example role that provides access to Observability: PUT _plugins/_security/api/roles/observability_permissions { \"cluster_permissions\": [ \"cluster:admin/opensearch/observability/create\", \"cluster:admin/opensearch/observability/update\", \"cluster:admin/opensearch/observability/delete\", \"cluster:admin/opensearch/observability/get\"], \"index_permissions\": [{ \"index_patterns\": [ \".opensearch-observability\"], \"allowed_actions\": [ \"write\", \"read\", \"search\"] }], \"tenant_permissions\": [{ \"tenant_patterns\": [ \"global_tenant\"], \"allowed_actions\": [ \"opensearch_dashboards_all_write\"] }] }",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/operational-panels/",
    "title": "Operational panels",
    "content": "Operational panels in OpenSearch Dashboards are collections of visualizations generated using Piped Processing Language (PPL) queries.\nGet started with operational panels\nIf you want to start using operational panels without adding any data, expand the Action menu, choose Add samples, and Dashboards adds a set of operational panels with saved visualizations for you to explore.\nCreate an operational panel\nTo create an operational panel and add visualizations:\nFrom the Add Visualization dropdown menu, choose Select Existing Visualization or Create New Visualization, which takes you to the event analytics explorer, where you can use PPL to create visualizations.\nIf you’re adding already existing visualizations, choose a visualization from the dropdown menu.\nChoose Add. To search for a particular visualization in your operation panels, use PPL queries to search for data you’ve already added to your panel.",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/prometheusmetrics/",
    "title": "Metrics analytics",
    "content": "Introduced 2.4\nStarting with OpenSearch 2.4, you can ingest and visualize metric data from log data aggregated within OpenSearch, allowing you to analyze and correlate data across logs, traces, and metrics. Previously, you could ingest and visualize only logs and traces from your monitored environments. With this feature, you can observe your digital assets with more granularity, gain deeper insight into the health of your infrastructure, and better inform your root cause analysis.\nThe following image shows the process of ingesting metrics from Prometheus and visualizing them in a dashboard. Configuring a data source connection from Prometheus to OpenSearch\nYou can view metrics collected from Prometheus in OpenSearch Dashboards by first creating a connection from Prometheus to OpenSearch using the SQL plugin.\nTo configure a connection to Prometheus, create a file on your OpenSearch nodes named datasources.json containing the Prometheus data source settings. The following examples demonstrate the various Prometheus data source configurations using different authentication methods.\nNo authentication: [{ \"name\": \"my_prometheus\", \"connector\": \"prometheus\", \"properties\": { \"prometheus.uri\": \"http://localhost:9090\" } }] Basic authentication: [{ \"name\": \"my_prometheus\", \"connector\": \"prometheus\", \"properties\": { \"prometheus.uri\": \"http://localhost:9090\", \"prometheus.auth.type\": \"basicauth\", \"prometheus.auth.username\": \"admin\", \"prometheus.auth.password\": \"admin\" } }] AWS SigV4 authentication: [{ \"name\": \"my_prometheus\", \"connector\": \"prometheus\", \"properties\": { \"prometheus.uri\": \"http://localhost:8080\", \"prometheus.auth.type\": \"awssigv4\", \"prometheus.auth.region\": \"us-east-1\", \"prometheus.auth.access_key\": \"\" \"prometheus.auth.secret_key\": \"\" } }] After configuring Prometheus in the datasources.json file, run the following command to load the configuration into the OpenSearch keystore. The configuration is securely stored in the keystore because it contains sensitive credential information. bin/opensearch-keystore add-file plugins.query.federation.datasources.config datasources.json If you are updating the keystore during runtime, refresh the keystore using following API command: POST /_nodes/reload_secure_settings { \"secure_settings_password\": \"\" } copy After configuring the connection from Prometheus to OpenSearch, Prometheus metrics are displayed in Dashboards in the Observability &gt; Metrics analytics window, as shown in the following image. For more information, see the Prometheus Connector GitHub page.\nCreating visualizations based on metrics\nYou can create visualizations based on Prometheus metrics and other metrics collected by your OpenSearch cluster.\nTo create a visualization, do the following:\nIn Observability &gt; Metrics analytics &gt; Available Metrics, select the metrics you would like to include in your visualization.\nThese visualizations can now be saved.\nFrom the Metrics analytics window, select Save.\nWhen prompted for a Custom operational dashboards/application, choose one of the available options.\nOptionally, you can edit the predefined name values under the Metric Name fields to suit your needs.\nSelect Save.\nThe following image shows an example of the visualizations that are displayed in the Observability &gt; Metrics analytics window. Defining PPL queries for use with Prometheus\nYou can define Piped Processing Language (PPL) queries against metrics collected by Prometheus. The following example shows a metric-selecting query with specific dimensions: source = my_prometheus.prometheus_http_requests_total | stats avg(@value) by span(@timestamp,15s), handler, code Additionally, you can create a custom visualization by performing the following steps:\nFrom the Events Analytics window, enter your PPL query and select Refresh. The Explorer page is now displayed.\nFrom the Explorer page, select Save.\nWhen prompted for a Custom operational dashboards/application, choose one of the available options.\nOptionally, you can edit the predefined name values under the Metric Name fields to suit your needs.\nOptionally, you can choose to save the visualization as a metric.\nSelect Save.\nNote: Only queries that include a time-series visualization and stats/span can be saved as a metric, as shown in the following image.",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/ssfo/",
    "title": "Simple Schema for Observability",
    "content": "Introduced 2.6\nOpenSearch 2.6 introduced a standardization for conforming to a common and unified observability schema: Simple Schema for Observability (SSFO). Observability is a collection of plugins and applications that let you visualize data-driven events by using Piped Processing Language (PPL) to explore and query data stored in OpenSearch.\nWith the schema in place, Observability tools can ingest, automatically extract, and aggregate data and create custom dashboards, making it easier to understand the system at a higher level.\nSSFO is inspired by both OpenTelemetry and the Elastic Common Schema (ECS) and uses Amazon Elastic Container Service ( Amazon ECS) event logs and OpenTelemetry metadata.\nAlerts will be supported in a future release.\nUse cases\nUse cases for SSFO include:\nIngesting observability data from different data types.\nMoving from proprietary configurations that are non-transferable to a consolidated, sharable observability solution that allows users to ingest and display an analysis of any type of telemetry data from any type of provider.\nData Prepper conforms to the SSFO schema for metrics and will gradually support traces and logs.\nData Prepper’s trace mapping currently provides service-map data in a different way than SSFO traces. To make the trace mapping compatible with Observability, it will be integrated with the SSFO traces schema and will introduce service-map as an enriched field.\nTraces and metrics\nSchema definitions for traces and metrics are defined and supported by the Observability plugin.\nThese schema definitions include:\nThe index structure (mapping).\nThe index naming conventions.\nA JSON schema for enforcement and validation of the structure.\nThe integration feature for adding preconfigured dashboards and assets.",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/trace/getting-started/",
    "title": "Getting Started",
    "content": "OpenSearch Trace Analytics consists of two components—Data Prepper and the Trace Analytics OpenSearch Dashboards plugin—that fit into the OpenTelemetry and OpenSearch ecosystems. The Data Prepper repository has several sample applications to help you get started.\nBasic flow of data Trace Analytics relies on you adding instrumentation to your application and generating trace data. The OpenTelemetry documentation contains example applications for many programming languages that can help you get started, including Java, Python, Go, and JavaScript.\n(In the Jaeger HotROD example below, an extra component, the Jaeger agent, runs alongside the application and sends the data to the OpenTelemetry Collector, but the concept is similar.)\nThe OpenTelemetry Collector receives data from the application and formats it into OpenTelemetry data. Data Prepper processes the OpenTelemetry data, transforms it for use in OpenSearch, and indexes it on an OpenSearch cluster.\nThe Trace Analytics OpenSearch Dashboards plugin displays the data in near real-time as a series of charts and tables, with an emphasis on service architecture, latency, error rate, and throughput.\nJaeger HotROD\nOne Trace Analytics sample application is the Jaeger HotROD demo, which mimics the flow of data through a distributed application.\nDownload or clone the Data Prepper repository. Then navigate to examples/jaeger-hotrod/ and open docker-compose.yml in a text editor. This file contains a container for each element from Basic flow of data:\nA distributed application ( jaeger-hot-rod) with the Jaeger agent ( jaeger-agent)\nThe OpenTelemetry Collector ( otel-collector)\nData Prepper ( data-prepper)\nA single-node OpenSearch cluster ( opensearch)\nOpenSearch Dashboards ( opensearch-dashboards).\nClose the file and run docker-compose up --build. After the containers start, navigate to http://localhost:8080 in a web browser. Click one of the buttons in the web interface to send a request to the application. Each request starts a series of operations across the services that make up the application. From the console logs, you can see that these operations share the same trace-id, which lets you track all of the operations in the request as a single trace: jaeger-hot-rod | http://0.0.0.0:8081/customer?customer=392\njaeger-hot-rod | 2020-11-19T16:29:53.425Z\tINFO\tfrontend/server.go:92\tHTTP request received\t{\"service\": \"frontend\", \"trace_id\": \"12091bd60f45ea2c\", \"span_id\": \"12091bd60f45ea2c\", \"method\": \"GET\", \"url\": \"/dispatch?customer=392&amp;nonse=0.6509021735471818\"}\njaeger-hot-rod | 2020-11-19T16:29:53.426Z\tINFO\tcustomer/client.go:54\tGetting customer{\"service\": \"frontend\", \"component\": \"customer_client\", \"trace_id\": \"12091bd60f45ea2c\", \"span_id\": \"12091bd60f45ea2c\", \"customer_id\": \"392\"}\njaeger-hot-rod | 2020-11-19T16:29:53.430Z\tINFO\tcustomer/server.go:67\tHTTP request received\t{\"service\": \"customer\", \"trace_id\": \"12091bd60f45ea2c\", \"span_id\": \"252ff7d0e1ac533b\", \"method\": \"GET\", \"url\": \"/customer?customer=392\"}\njaeger-hot-rod | 2020-11-19T16:29:53.430Z\tINFO\tcustomer/database.go:73\tLoading customer{\"service\": \"customer\", \"component\": \"mysql\", \"trace_id\": \"12091bd60f45ea2c\", \"span_id\": \"252ff7d0e1ac533b\", \"customer_id\": \"392\"} These operations also have a span_id. Spans are units of work from a single service. Each trace contains some number of spans. Shortly after the application starts processing the request, you can see the OpenTelemetry Collector starts exporting the spans: otel-collector | 2020-11-19T16:29:53.781Z\tINFO\tloggingexporter/logging_exporter.go:296\tTraceExporter\t{\"#spans\": 1}\notel-collector | 2020-11-19T16:29:53.787Z\tINFO\tloggingexporter/logging_exporter.go:296\tTraceExporter\t{\"#spans\": 3} Then Data Prepper processes the data from the OpenTelemetry Collector and indexes it: data-prepper | 1031918 [service-map-pipeline-process-worker-2-thread-1] INFO com.amazon.dataprepper.pipeline.ProcessWorker – service-map-pipeline Worker: Processing 3 records from buffer\ndata-prepper | 1031923 [entry-pipeline-process-worker-1-thread-1] INFO com.amazon.dataprepper.pipeline.ProcessWorker – entry-pipeline Worker: Processing 1 records from buffer Finally, you can see the OpenSearch node responding to the indexing request. node-0.example.com | [2020-11-19T16:29:55,064][INFO][o.e.c.m.MetadataMappingService] [9fb4fb37a516] [otel-v1-apm-span-000001/NGYbmVD9RmmqnxjfTzBQsQ] update_mapping [_doc]\nnode-0.example.com | [2020-11-19T16:29:55,267][INFO][o.e.c.m.MetadataMappingService] [9fb4fb37a516] [otel-v1-apm-span-000001/NGYbmVD9RmmqnxjfTzBQsQ] update_mapping [_doc] In a new terminal window, run the following command to see one of the raw documents in the OpenSearch cluster: curl -X GET -u 'admin:admin' -k 'https://localhost:9200/otel-v1-apm-span-000001/_search?pretty&amp;size=1' Navigate to http://localhost:5601 in a web browser and choose Trace Analytics. You can see the results of your single click in the Jaeger HotROD web interface: the number of traces per API and HTTP method, latency trends, a color-coded map of the service architecture, and a list of trace IDs that you can use to drill down on individual operations.\nIf you don’t see your trace, adjust the timeframe in OpenSearch Dashboards. For more information on using the plugin, see OpenSearch Dashboards plugin.",
    "ancestors": [
      "Observability",
      "Trace analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/trace/index/",
    "title": "Trace Analytics",
    "content": "Trace Analytics provides a way to ingest and visualize OpenTelemetry data in OpenSearch. This data can help you find and fix performance problems in distributed applications.\nA single operation, such as a user choosing a button, can trigger an extended series of events. The frontend might call a backend service, which calls another service, which queries a database, processes the data, and sends it to the original service, which sends a confirmation to the frontend.\nTrace Analytics can help you visualize this flow of events and identify performance problems, as shown in the following image. Trace Analytics with Jaeger data\nTrace Analytics supports Jaeger trace data in the OpenSearch Observability plugin. If you use OpenSearch as the backend for Jaeger trace data, you can use the built-in Trace Analytics capabilities.\nTo set up your environment to use Trace Analytics, see Analyze Jaeger trace data.",
    "ancestors": [
      "Observability"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/trace/ta-dashboards/",
    "title": "OpenSearch Dashboards plugin",
    "content": "The Trace Analytics plugin for OpenSearch Dashboards provides at-a-glance visibility into your application performance, along with the ability to drill down on individual traces. For installation instructions, see Standalone OpenSearch Dashboards plugin install.\nThe Dashboard view groups traces together by HTTP method and path so that you can see the average latency, error rate, and trends associated with a particular operation. For a more focused view, try filtering by trace group name. To drill down on the traces that make up a trace group, choose the number of traces in righthand column. Then choose an individual trace for a detailed summary. The Services view lists all services in the application, plus an interactive map that shows how the various services connect to each other. In contrast to the dashboard, which helps identify problems by operation, the service map helps identify problems by service. Try sorting by error rate or latency to get a sense of potential problem areas of your application.",
    "ancestors": [
      "Observability",
      "Trace analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/observing-your-data/trace/trace-analytics-jaeger/",
    "title": "Analyzing Jaeger trace data",
    "content": "Introduced 2.5\nThe trace analytics functionality in the OpenSearch Observability plugin now supports Jaeger trace data. If you use OpenSearch as the backend for Jaeger trace data, you can use the built-in trace analytics capabilities. This provides support for OpenTelemetry (OTel) trace data.\nWhen you perform trace analytics, you can select from two data sources: Data Prepper – Data ingested into OpenSearch through Data Prepper Jaeger – Trace data stored within OpenSearch as its backend\nIf you store your Jaeger trace data in OpenSearch, you can now use the built-in trace analytics capabilities to analyze the error rates and latency. You can also filter the traces and analyze the span details of a trace to pinpoint any service issues.\nWhen you ingest Jaeger data into OpenSearch, it gets stored in a different index than the OTel-generated index that gets created when you run data through Data Prepper. Use the data source selector in OpenSearch Dashboards to indicate the data source on which you want to perform trace analytics.\nJaeger trace data that you can analyze includes span data as well as service and operation endpoint data. <!-- Need more info for next release. add how to configure for span analysis. Jaeger span data analysis requires some configuration.-->\nBy default, each time you ingest data for Jaeger, it creates a separate index for that day.\nTo learn more about Jaeger data tracing, see the Jaeger documentation.\nData ingestion requirements\nTo perform trace analytics on Jaeger data, you need to configure error capability.\nJaeger data that is ingested into OpenSearch must have the environment variable ES_TAGS_AS_FIELDS_ALL set to true for errors. If data is not ingested in this format, it will not work for errors, and error data will not be available for traces in trace analytics with OpenSearch.\nAbout data ingestion with Jaeger indexes\nTrace analytics for non-Jaeger data uses OTel indexes with the naming conventions otel-v1-apm-span-* or otel-v1-apm-service-map*.\nJaeger indexes follow the naming conventions jaeger-span-* or jaeger-service-*.\nSetting up OpenSearch to use Jaeger data\nThe following section provides a sample Docker Compose file that contains the configuration required to enable errors for trace analytics.\nStep 1: Run the Docker Compose file\nUse the following Docker Compose file to enable Jaeger data for trace analytics. Set the ES_TAGS_AS_FIELDS_ALL environment variable set to true to enable errors to be added to trace data.\nCopy the following Docker Compose file and save it as docker-compose.yml: version: '3'\nservices:\nopensearch-node1: # This is also the hostname of the container within the Docker network (i.e. https://opensearch-node1/)\nimage: opensearchproject/opensearch:latest # Specifying the latest available image - modify if you want a specific version\ncontainer_name: opensearch-node1\nenvironment:\n- cluster.name=opensearch-cluster # Name the cluster\n- node.name=opensearch-node1 # Name the node that will run in this container\n- discovery.seed_hosts=opensearch-node1,opensearch-node2 # Nodes to look for when discovering the cluster\n- cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2 # Nodes eligible to serve as cluster manager\n- bootstrap.memory_lock=true # Disable JVM heap memory swapping\n- \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # Set min and max JVM heap sizes to at least 50% of system RAM\nulimits:\nmemlock:\nsoft: -1 # Set memlock to unlimited (no soft or hard limit)\nhard: -1\nnofile:\nsoft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536\nhard: 65536\nvolumes:\n- opensearch-data1:/usr/share/opensearch/data # Creates volume called opensearch-data1 and mounts it to the container\nports:\n- \"9200:9200\"\n- \"9600:9600\"\nnetworks:\n- opensearch-net # All of the containers will join the same Docker bridge network\nopensearch-node2:\nimage: opensearchproject/opensearch:latest # This should be the same image used for opensearch-node1 to avoid issues\ncontainer_name: opensearch-node2\nenvironment:\n- cluster.name=opensearch-cluster\n- node.name=opensearch-node2\n- discovery.seed_hosts=opensearch-node1,opensearch-node2\n- cluster.initial_cluster_manager_nodes=opensearch-node1,opensearch-node2\n- bootstrap.memory_lock=true\n- \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\"\nulimits:\nmemlock:\nsoft: -1\nhard: -1\nnofile:\nsoft: 65536\nhard: 65536\nvolumes:\n- opensearch-data2:/usr/share/opensearch/data\nnetworks:\n- opensearch-net\nopensearch-dashboards:\nimage: opensearchproject/opensearch-dashboards:latest # Make sure the version of opensearch-dashboards matches the version of opensearch installed on other nodes\ncontainer_name: opensearch-dashboards\nports:\n- 5601:5601 # Map host port 5601 to container port 5601\nexpose:\n- \"5601\" # Expose port 5601 for web access to OpenSearch Dashboards\nenvironment:\nOPENSEARCH_HOSTS: '[\"https://opensearch-node1:9200\",\"https://opensearch-node2:9200\"]' # Define the OpenSearch nodes that OpenSearch Dashboards will query\nnetworks:\n- opensearch-net\njaeger-collector:\nimage: jaegertracing/jaeger-collector:latest\nports:\n- \"14269:14269\"\n- \"14268:14268\"\n- \"14267:14267\"\n- \"14250:14250\"\n- \"9411:9411\"\nnetworks:\n- opensearch-net\nrestart: on-failure\nenvironment:\n- SPAN_STORAGE_TYPE=opensearch\n- ES_TAGS_AS_FIELDS_ALL=true\n- ES_USERNAME=admin\n- ES_PASSWORD=admin\n- ES_TLS_SKIP_HOST_VERIFY=true\ncommand: [\n\"--es.server-urls=https://opensearch-node1:9200\",\n\"--es.tls.enabled=true\",]\ndepends_on:\n- opensearch-node1\njaeger-agent:\nimage: jaegertracing/jaeger-agent:latest\nhostname: jaeger-agent\ncommand: [\"--reporter.grpc.host-port=jaeger-collector:14250\"]\nports:\n- \"5775:5775/udp\"\n- \"6831:6831/udp\"\n- \"6832:6832/udp\"\n- \"5778:5778\"\nnetworks:\n- opensearch-net\nrestart: on-failure\nenvironment:\n- SPAN_STORAGE_TYPE=opensearch\ndepends_on:\n- jaeger-collector\nhotrod:\nimage: jaegertracing/example-hotrod:latest\nports:\n- \"8080:8080\"\ncommand: [\"all\"]\nenvironment:\n- JAEGER_AGENT_HOST=jaeger-agent\n- JAEGER_AGENT_PORT=6831\nnetworks:\n- opensearch-net\ndepends_on:\n- jaeger-agent\nvolumes:\nopensearch-data1:\nopensearch-data2:\nnetworks:\nopensearch-net: Step 2: Start the cluster\nRun the following command to deploy the Docker compose YAML file: docker compose up -d To stop the cluster, run the following command: docker compose down Step 3: Generate sample data\nUse the sample app provided with the Docker file to generate data. After you run the Docker Compose file, it runs the sample app on local host port 8080. To open the app, go to http://localhost:8080. In the sample app, Hot R.O.D., select any button to generate data. Now you can view trace data in Dashboards.\nStep 4: View trace data in OpenSearch Dashboards\nAfter you generate Jaeger trace data, you can view it in Dashboards.\nGo to Trace analytics at http://localhost:5601/app/observability-dashboards#/trace_analytics/home.\nUsing trace analytics in OpenSearch Dashboards\nTo analyze the Jaeger trace data in Dashboards, first set up the trace analytics functionality. To get started, see Get started with trace analytics.\nData sources\nYou can specify either Data Prepper or Jaeger as the data source when you perform trace analytics.\nFrom Dashboards, go to Observability &gt; Trace analytics and select Jaeger. Dashboard view\nAfter you select Jaeger as the data source, you can view all of the indexed data in Dashboard view, including Error rate and Throughput.\nError rate\nYou can view the trace error count over time in Dashboard view and also see the top five combinations of services and operations that have a non-zero error rate. Throughput\nWith Throughput selected, you can see the throughput of Jaeger index traces over time.\nYou can select an individual trace from the Top 5 Service and Operation Latency list and view the detailed trace data. You can also see the combinations of services and operations that have the highest latency.\nIf you select one of the entries for Service and Operation Name and go to the Traces column to select a trace, it will automatically add the service and operation as filters.\nTraces\nIn Traces, you can see the latency and errors for the filtered service and operation for each individual trace ID in the list. If you select an individual trace ID, you can see more detailed information about the trace, such as Time spent by the service and Spans. You can also view the index payload in JSON format. Services\nYou can also view the individual error rates and latency for each individual service. Go to Observability &gt; Trace analytics &gt; Services. In Services, you can see the average latency, error rate, throughput and trace for each service in the list.",
    "ancestors": [
      "Observability",
      "Trace analytics"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/aggregations/",
    "title": "Aggregations",
    "content": "OpenSearch isn’t just for search. Aggregations let you tap into OpenSearch’s powerful analytics engine to analyze your data and extract statistics from it.\nThe use cases of aggregations vary from analyzing data in real time to take some action to using OpenSearch Dashboards to create a visualization dashboard.\nOpenSearch can perform aggregations on massive datasets in milliseconds. Compared to queries, aggregations consume more CPU cycles and memory.\nAggregations on text fields\nBy default, OpenSearch doesn’t support aggregations on a text field. Because text fields are tokenized, an aggregation on a text field has to reverse the tokenization process back to its original string and then formulate an aggregation based on that. This kind of an operation consumes significant memory and degrades cluster performance.\nWhile you can enable aggregations on text fields by setting the fielddata parameter to true in the mapping, the aggregations are still based on the tokenized words and not on the raw text.\nWe recommend keeping a raw version of the text field as a keyword field that you can aggregate on.\nIn this case, you can perform aggregations on the title.raw field, instead of on the title field: PUT movies { \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\", \"fielddata\": true, \"fields\": { \"raw\": { \"type\": \"keyword\" } } } } } } General aggregation structure\nThe structure of an aggregation query is as follows: GET _search { \"size\": 0, \"aggs\": { \"NAME\": { \"AGG_TYPE\": {} } } } If you’re only interested in the aggregation result and not in the results of the query, set size to 0.\nIn the aggs property (you can use aggregations if you want), you can define any number of aggregations. Each aggregation is defined by its name and one of the types of aggregations that OpenSearch supports.\nThe name of the aggregation helps you to distinguish between different aggregations in the response. The AGG_TYPE property is where you specify the type of aggregation.\nSample aggregation\nThis section uses the OpenSearch Dashboards sample ecommerce data and web log data. To add the sample data, log in to OpenSearch Dashboards, choose Home, and then choose Try our sample data. For Sample eCommerce orders and Sample web logs, choose Add data.\navg\nTo find the average value of the taxful_total_price field: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"avg_taxful_total_price\": { \"avg\": { \"field\": \"taxful_total_price\" } } } } Example response { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4675, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"avg_taxful_total_price\": { \"value\": 75.05542864304813 } } } The aggregation block in the response shows the average value for the taxful_total_price field.\nTypes of aggregations\nThere are three main types of aggregations:\nMetric aggregations - Calculate metrics such as sum, min, max, and avg on numeric fields.\nBucket aggregations - Sort query results into groups based on some criteria.\nPipeline aggregations - Pipe the output of one aggregation as an input to another.\nNested aggregations\nAggregations within aggregations are called nested or subaggregations.\nMetric aggregations produce simple results and can’t contain nested aggregations.\nBucket aggregations produce buckets of documents that you can nest in other aggregations. You can perform complex analysis on your data by nesting metric and bucket aggregations within bucket aggregations.\nGeneral nested aggregation syntax { \"aggs\": { \"name\": { \"type\": { \"data\" }, \"aggs\": { \"nested\": { \"type\": { \"data\" } } } } } } The inner aggs keyword begins a new nested aggregation. The syntax of the parent aggregation and the nested aggregation is the same. Nested aggregations run in the context of the preceding parent aggregations.\nYou can also pair your aggregations with search queries to narrow down things you’re trying to analyze before aggregating. If you don’t add a query, OpenSearch implicitly uses the match_all query.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/aggregations/bucket-agg/",
    "title": "Bucket aggregations",
    "content": "Bucket aggregations categorize sets of documents as buckets. The type of bucket aggregation determines whether a given document falls into a bucket or not.\nYou can use bucket aggregations to implement faceted navigation (usually placed as a sidebar on a search result landing page) to help your users narrow down the results.\nTerms\nThe terms aggregation dynamically creates a bucket for each unique term of a field.\nThe following example uses the terms aggregation to find the number of documents per response code in web log data: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"response_codes\": { \"terms\": { \"field\": \"response.keyword\", \"size\": 10 } } } } Sample Response... \"aggregations\": { \"response_codes\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"200\", \"doc_count\": 12832 }, { \"key\": \"404\", \"doc_count\": 801 }, { \"key\": \"503\", \"doc_count\": 441 }] } } } The values are returned with the key key. doc_count specifies the number of documents in each bucket. By default, the buckets are sorted in descending order of doc-count.\nThe response also includes two keys named doc_count_error_upper_bound and sum_other_doc_count.\nThe terms aggregation returns the top unique terms. So, if the data has many unique terms, then some of them might not appear in the results. The sum_other_doc_count field is the sum of the documents that are left out of the response. In this case, the number is 0 because all the unique values appear in the response.\nThe doc_count_error_upper_bound field represents the maximum possible count for a unique value that’s left out of the final results. Use this field to estimate the error margin for the count.\nThe count might not be accurate. A coordinating node that’s responsible for the aggregation prompts each shard for its top unique terms. Imagine a scenario where the size parameter is 3.\nThe terms aggregation requests each shard for its top 3 unique terms. The coordinating node takes each of the results and aggregates them to compute the final result. If a shard has an object that’s not part of the top 3, then it won’t show up in the response.\nThis is especially true if size is set to a low number. Because the default size is 10, an error is unlikely to happen. If you don’t need high accuracy and want to increase the performance, you can reduce the size.\nAccount for pre-aggregated data\nWhile the doc_count field provides a representation of the number of individual documents aggregated in a bucket, doc_count by itself does not have a way to correctly increment documents that store pre-aggregated data. To account for pre-aggregated data and accurately calculate the number of documents in a bucket, you can use the _doc_count field to add the number of documents in a single summary field. When a document includes the _doc_count field, all bucket aggregations recognize its value and increase the bucket doc_count cumulatively. Keep these considerations in mind when using the _doc_count field:\nThe field does not support nested arrays; only positive integers can be used.\nIf a document does not contain the _doc_count field, aggregation uses the document to increase the count by 1.\nOpenSearch features that rely on an accurate document count illustrate the importance of using the _doc_count field. To see how this field can be used to support other search tools, refer to Index rollups, an OpenSearch feature for the Index Management (IM) plugin that stores documents with pre-aggregated data in rollup indexes.\nExample usage PUT /my_index/_doc/ 1 { \"response_code\": 404, \"date\": \"2022-08-05\", \"_doc_count\": 20 } PUT /my_index/_doc/ 2 { \"response_code\": 404, \"date\": \"2022-08-06\", \"_doc_count\": 10 } PUT /my_index/_doc/ 3 { \"response_code\": 200, \"date\": \"2022-08-06\", \"_doc_count\": 300 } GET /my_index/_search { \"size\": 0, \"aggs\": { \"response_codes\": { \"terms\": { \"field\": \"response_code\" } } } } Example response { \"took\": 20, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"response_codes\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": 200, \"doc_count\": 300 }, { \"key\": 404, \"doc_count\": 30 }] } } } Multi-terms\nSimilar to the terms bucket aggregation, you can also search for multiple terms using the multi_terms aggregation. Multi-terms aggregations are useful when you need to sort by document count, or when you need to sort by a metric aggregation on a composite key and get the top n results. For example, you could search for a specific number of documents (e.g., 1000) and the number of servers per location that show CPU usage greater than 90%. The top number of results would be returned for this multi-term query.\nThe multi_terms aggregation does consume more memory than a terms aggregation, so its performance might be slower.\nMulti-terms aggregation parameters Parameter Description multi_terms\nIndicates a multi-terms aggregation that gathers buckets of documents together based on criteria specified by multiple terms.\nsize\nSpecifies the number of buckets to return. Default is 10.\norder\nIndicates the order to sort the buckets. By default, buckets are ordered according to document count per bucket. If the buckets contain the same document count, then order can be explicitly set to the term value instead of document count. (e.g., set order to “max-cpu”).\ndoc_count\nSpecifies the number of documents to be returned in each bucket. By default, the top 10 terms are returned. Sample Request GET sample-index 100 /_search { \"size\": 0, \"aggs\": { \"hot\": { \"multi_terms\": { \"terms\": [{ \"field\": \"region\" },{ \"field\": \"host\" }], \"order\": { \"max-cpu\": \"desc\" } }, \"aggs\": { \"max-cpu\": { \"max\": { \"field\": \"cpu\" } } } } } } Sample Response { \"took\": 118, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 8, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"multi-terms\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": [ \"dub\", \"h1\"], \"key_as_string\": \"dub|h1\", \"doc_count\": 2, \"max-cpu\": { \"value\": 90.0 } }, { \"key\": [ \"dub\", \"h2\"], \"key_as_string\": \"dub|h2\", \"doc_count\": 2, \"max-cpu\": { \"value\": 70.0 } }, { \"key\": [ \"iad\", \"h2\"], \"key_as_string\": \"iad|h2\", \"doc_count\": 2, \"max-cpu\": { \"value\": 50.0 } }, { \"key\": [ \"iad\", \"h1\"], \"key_as_string\": \"iad|h1\", \"doc_count\": 2, \"max-cpu\": { \"value\": 15.0 } }] } } } sampler, diversified_sampler\nIf you’re aggregating over millions of documents, you can use a sampler aggregation to reduce its scope to a small sample of documents for a faster response. The sampler aggregation selects the samples by top-scoring documents.\nThe results are approximate but closely represent the distribution of the real data. The sampler aggregation significantly improves query performance, but the estimated responses are not entirely reliable.\nThe basic syntax is: “aggs”: { \"SAMPLE\": { \"sampler\": { \"shard_size\": 100 }, \"aggs\": {... } } } The shard_size property tells OpenSearch how many documents (at most) to collect from each shard.\nThe following example limits the number of documents collected on each shard to 1,000 and then buckets the documents by a terms aggregation: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sample\": { \"sampler\": { \"shard_size\": 1000 }, \"aggs\": { \"terms\": { \"terms\": { \"field\": \"agent.keyword\" } } } } } } Example response... \"aggregations\": { \"sample\": { \"doc_count\": 1000, \"terms\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\", \"doc_count\": 368 }, { \"key\": \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.50 Safari/534.24\", \"doc_count\": 329 }, { \"key\": \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1;.NET CLR 1.1.4322)\", \"doc_count\": 303 }] } } } } The diversified_sampler aggregation lets you reduce the bias in the distribution of the sample pool. You can use the field setting to control the maximum number of documents collected on any one shard which shares a common value: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sample\": { \"diversified_sampler\": { \"shard_size\": 1000, \"field\": \"response.keyword\" }, \"aggs\": { \"terms\": { \"terms\": { \"field\": \"agent.keyword\" } } } } } } Example response... \"aggregations\": { \"sample\": { \"doc_count\": 3, \"terms\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\", \"doc_count\": 2 }, { \"key\": \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1;.NET CLR 1.1.4322)\", \"doc_count\": 1 }] } } } } significant_terms, significant_text\nThe significant_terms aggregation lets you spot unusual or interesting term occurrences in a filtered subset relative to the rest of the data in an index.\nA foreground set is the set of documents that you filter. A background set is a set of all documents in an index.\nThe significant_terms aggregation examines all documents in the foreground set and finds a score for significant occurrences in contrast to the documents in the background set.\nIn the sample web log data, each document has a field containing the user-agent of the visitor. This example searches for all requests from an iOS operating system. A regular terms aggregation on this foreground set returns Firefox because it has the most number of documents within this bucket. On the other hand, a significant_terms aggregation returns Internet Explorer (IE) because IE has a significantly higher appearance in the foreground set as compared to the background set. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"query\": { \"terms\": { \"machine.os.keyword\": [ \"ios\"] } }, \"aggs\": { \"significant_response_codes\": { \"significant_terms\": { \"field\": \"agent.keyword\" } } } } Example response... \"aggregations\": { \"significant_response_codes\": { \"doc_count\": 2737, \"bg_count\": 14074, \"buckets\": [ { \"key\": \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1;.NET CLR 1.1.4322)\", \"doc_count\": 818, \"score\": 0.01462731514608217, \"bg_count\": 4010 }, { \"key\": \"Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1\", \"doc_count\": 1067, \"score\": 0.009062566630410223, \"bg_count\": 5362 }] } } } If the significant_terms aggregation doesn’t return any result, you might have not filtered the results with a query. Alternatively, the distribution of terms in the foreground set might be the same as the background set, implying that there isn’t anything unusual in the foreground set.\nThe significant_text aggregation is similar to the significant_terms aggregation but it’s for raw text fields.\nSignificant text measures the change in popularity measured between the foreground and background sets using statistical analysis. For example, it might suggest Tesla when you look for its stock acronym TSLA.\nThe significant_text aggregation re-analyzes the source text on the fly, filtering noisy data like duplicate paragraphs, boilerplate headers and footers, and so on, which might otherwise skew the results.\nRe-analyzing high-cardinality datasets can be a very CPU-intensive operation. We recommend using the significant_text aggregation inside a sampler aggregation to limit the analysis to a small selection of top-matching documents, for example 200.\nYou can set the following parameters: min_doc_count - Return results that match more than a configured number of top hits. We recommend not setting min_doc_count to 1 because it tends to return terms that are typos or misspellings. Finding more than one instance of a term helps reinforce that the significance is not the result of a one-off accident. The default value of 3 is used to provide a minimum weight-of-evidence. shard_size - Setting a high value increases stability (and accuracy) at the expense of computational performance. shard_min_doc_count - If your text contains many low frequency words and you’re not interested in these (for example typos), then you can set the shard_min_doc_count parameter to filter out candidate terms at a shard level with a reasonable certainty to not reach the required min_doc_count even after merging the local significant text frequencies. The default value is 1, which has no impact until you explicitly set it. We recommend setting this value much lower than the min_doc_count value.\nAssume that you have the complete works of Shakespeare indexed in an OpenSearch cluster. You can find significant texts in relation to the word “breathe” in the text_entry field: GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"breathe\" } }, \"aggregations\": { \"my_sample\": { \"sampler\": { \"shard_size\": 100 }, \"aggregations\": { \"keywords\": { \"significant_text\": { \"field\": \"text_entry\", \"min_doc_count\": 4 } } } } } } Example response \"aggregations\": { \"my_sample\": { \"doc_count\": 59, \"keywords\": { \"doc_count\": 59, \"bg_count\": 111396, \"buckets\": [ { \"key\": \"breathe\", \"doc_count\": 59, \"score\": 1887.0677966101694, \"bg_count\": 59 }, { \"key\": \"air\", \"doc_count\": 4, \"score\": 2.641295376716233, \"bg_count\": 189 }, { \"key\": \"dead\", \"doc_count\": 4, \"score\": 0.9665839666414213, \"bg_count\": 495 }, { \"key\": \"life\", \"doc_count\": 5, \"score\": 0.9090787433467572, \"bg_count\": 805 }] } } } } The most significant texts in relation to breathe are air, dead, and life.\nThe significant_text aggregation has the following limitations:\nDoesn’t support child aggregations because child aggregations come at a high memory cost. As a workaround, you can add a follow-up query using a terms aggregation with an include clause and a child aggregation.\nDoesn’t support nested objects because it works with the document JSON source.\nThe counts of documents might have some (typically small) inaccuracies as it’s based on summing the samples returned from each shard. You can use the shard_size parameter to fine-tune the trade-off between accuracy and performance. By default, the shard_size is set to -1 to automatically estimate the number of shards and the size parameter.\nFor both significant_terms and significant_text aggregations, the default source of statistical information for background term frequencies is the entire index. You can narrow this scope with a background filter for more focus: GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"breathe\" } }, \"aggregations\": { \"my_sample\": { \"sampler\": { \"shard_size\": 100 }, \"aggregations\": { \"keywords\": { \"significant_text\": { \"field\": \"text_entry\", \"background_filter\": { \"term\": { \"speaker\": \"JOHN OF GAUNT\" } } } } } } } } missing\nIf you have documents in your index that don’t contain the aggregating field at all or the aggregating field has a value of NULL, use the missing parameter to specify the name of the bucket such documents should be placed in.\nThe following example adds any missing values to a bucket named “N/A”: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"response_codes\": { \"terms\": { \"field\": \"response.keyword\", \"size\": 10, \"missing\": \"N/A\" } } } } Because the default value for the min_doc_count parameter is 1, the missing parameter doesn’t return any buckets in its response. Set min_doc_count parameter to 0 to see the “N/A” bucket in the response: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"response_codes\": { \"terms\": { \"field\": \"response.keyword\", \"size\": 10, \"missing\": \"N/A\", \"min_doc_count\": 0 } } } } Example response... \"aggregations\": { \"response_codes\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"200\", \"doc_count\": 12832 }, { \"key\": \"404\", \"doc_count\": 801 }, { \"key\": \"503\", \"doc_count\": 441 }, { \"key\": \"N/A\", \"doc_count\": 0 }] } } } histogram, date_histogram\nThe histogram aggregation buckets documents based on a specified interval.\nWith histogram aggregations, you can visualize the distributions of values in a given range of documents very easily. Now OpenSearch doesn’t give you back an actual graph of course, that’s what OpenSearch Dashboards is for. But it’ll give you the JSON response that you can use to construct your own graph.\nThe following example buckets the number_of_bytes field by 10,000 intervals: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"number_of_bytes\": { \"histogram\": { \"field\": \"bytes\", \"interval\": 10000 } } } } Sample Response... \"aggregations\": { \"number_of_bytes\": { \"buckets\": [ { \"key\": 0.0, \"doc_count\": 13372 }, { \"key\": 10000.0, \"doc_count\": 702 }] } } } The date_histogram aggregation uses date math to generate histograms for time-series data.\nFor example, you can find how many hits your website gets per month: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"logs_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"interval\": \"month\" } } } } Example response... \"aggregations\": { \"logs_per_month\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635 }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844 }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595 }] } } } The response has three months worth of logs. If you graph these values, you can see the peak and valleys of the request traffic to your website month over month.\nrange, date_range, ip_range\nThe range aggregation lets you define the range for each bucket.\nFor example, you can find the number of bytes between 1000 and 2000, 2000 and 3000, and 3000 and 4000.\nWithin the range parameter, you can define ranges as objects of an array. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"number_of_bytes_distribution\": { \"range\": { \"field\": \"bytes\", \"ranges\": [ { \"from\": 1000, \"to\": 2000 }, { \"from\": 2000, \"to\": 3000 }, { \"from\": 3000, \"to\": 4000 }] } } } } The response includes the from key values and excludes the to key values:\nExample response... \"aggregations\": { \"number_of_bytes_distribution\": { \"buckets\": [ { \"key\": \"1000.0-2000.0\", \"from\": 1000.0, \"to\": 2000.0, \"doc_count\": 805 }, { \"key\": \"2000.0-3000.0\", \"from\": 2000.0, \"to\": 3000.0, \"doc_count\": 1369 }, { \"key\": \"3000.0-4000.0\", \"from\": 3000.0, \"to\": 4000.0, \"doc_count\": 1422 }] } } } The date_range aggregation is conceptually the same as the range aggregation, except that it lets you perform date math.\nFor example, you can get all documents from the last 10 days. To make the date more readable, include the format with a format parameter: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"number_of_bytes\": { \"date_range\": { \"field\": \"@timestamp\", \"format\": \"MM-yyyy\", \"ranges\": [ { \"from\": \"now-10d/d\", \"to\": \"now\" }] } } } } Example response... \"aggregations\": { \"number_of_bytes\": { \"buckets\": [ { \"key\": \"03-2021-03-2021\", \"from\": 1.6145568E12, \"from_as_string\": \"03-2021\", \"to\": 1.615451329043E12, \"to_as_string\": \"03-2021\", \"doc_count\": 0 }] } } } The ip_range aggregation is for IP addresses.\nIt works on ip type fields. You can define the IP ranges and masks in the CIDR notation. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"access\": { \"ip_range\": { \"field\": \"ip\", \"ranges\": [ { \"from\": \"1.0.0.0\", \"to\": \"126.158.155.183\" }, { \"mask\": \"1.0.0.0/8\" }] } } } } Example response... \"aggregations\": { \"access\": { \"buckets\": [ { \"key\": \"1.0.0.0/8\", \"from\": \"1.0.0.0\", \"to\": \"2.0.0.0\", \"doc_count\": 98 }, { \"key\": \"1.0.0.0-126.158.155.183\", \"from\": \"1.0.0.0\", \"to\": \"126.158.155.183\", \"doc_count\": 7184 }] } } } If you add a document with malformed fields to an index that has ip_range set to false in its mappings, OpenSearch rejects the entire document. You can set ignore_malformed to true to specify that OpenSearch should ignore malformed fields. The default is false.... \"mappings\": { \"properties\": { \"ips\": { \"type\": \"ip_range\", \"ignore_malformed\": true } } } filter, filters\nA filter aggregation is a query clause, exactly like a search query — match or term or range. You can use the filter aggregation to narrow down the entire set of documents to a specific set before creating buckets.\nThe following example shows the avg aggregation running within the context of a filter. The avg aggregation only aggregates the documents that match the range query: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"low_value\": { \"filter\": { \"range\": { \"taxful_total_price\": { \"lte\": 50 } } }, \"aggs\": { \"avg_amount\": { \"avg\": { \"field\": \"taxful_total_price\" } } } } } } Example response... \"aggregations\": { \"low_value\": { \"doc_count\": 1633, \"avg_amount\": { \"value\": 38.363175998928355 } } } } A filters aggregation is the same as the filter aggregation, except that it lets you use multiple filter aggregations.\nWhile the filter aggregation results in a single bucket, the filters aggregation returns multiple buckets, one for each of the defined filters.\nTo create a bucket for all the documents that didn’t match the any of the filter queries, set the other_bucket property to true: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"200_os\": { \"filters\": { \"other_bucket\": true, \"filters\": [ { \"term\": { \"response.keyword\": \"200\" } }, { \"term\": { \"machine.os.keyword\": \"osx\" } }] }, \"aggs\": { \"avg_amount\": { \"avg\": { \"field\": \"bytes\" } } } } } } Example response... \"aggregations\": { \"200_os\": { \"buckets\": [ { \"doc_count\": 12832, \"avg_amount\": { \"value\": 5897.852711970075 } }, { \"doc_count\": 2825, \"avg_amount\": { \"value\": 5620.347256637168 } }, { \"doc_count\": 1017, \"avg_amount\": { \"value\": 3247.0963618485744 } }] } } } global\nThe global aggregations lets you break out of the aggregation context of a filter aggregation. Even if you have included a filter query that narrows down a set of documents, the global aggregation aggregates on all documents as if the filter query wasn’t there. It ignores the filter aggregation and implicitly assumes the match_all query.\nThe following example returns the avg value of the taxful_total_price field from all documents in the index: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"query\": { \"range\": { \"taxful_total_price\": { \"lte\": 50 } } }, \"aggs\": { \"total_avg_amount\": { \"global\": {}, \"aggs\": { \"avg_price\": { \"avg\": { \"field\": \"taxful_total_price\" } } } } } } Example response... \"aggregations\": { \"total_avg_amount\": { \"doc_count\": 4675, \"avg_price\": { \"value\": 75.05542864304813 } } } } You can see that the average value for the taxful_total_price field is 75.05 and not the 38.36 as seen in the filter example when the query matched.\ngeo_distance, geohash_grid\nThe geo_distance aggregation groups documents into concentric circles based on distances from an origin geo_point field.\nIt’s the same as the range aggregation, except that it works on geo locations.\nFor example, you can use the geo_distance aggregation to find all pizza places within 1 km of you. The search results are limited to the 1 km radius specified by you, but you can add another result found within 2 km.\nYou can only use the geo_distance aggregation on fields mapped as geo_point.\nA point is a single geographical coordinate, such as your current location shown by your smart-phone. A point in OpenSearch is represented as follows: { \"location\": { \"type\": \"point\", \"coordinates\": { \"lat\": 83.76, \"lon\": -81.2 } } } You can also specify the latitude and longitude as an array [-81.20, 83.76] or as a string \"83.76, -81.20\" This table lists the relevant fields of a geo_distance aggregation: Field Description Required field Specify the geo point field that you want to work on.\nYes origin Specify the geo point that’s used to compute the distances from.\nYes ranges Specify a list of ranges to collect documents based on their distance from the target point.\nYes unit Define the units used in the ranges array. The unit defaults to m (meters), but you can switch to other units like km (kilometers), mi (miles), in (inches), yd (yards), cm (centimeters), and mm (millimeters).\nNo distance_type Specify how OpenSearch calculates the distance. The default is sloppy_arc (faster but less accurate), but can also be set to arc (slower but most accurate) or plane (fastest but least accurate). Because of high error margins, use plane only for small geographic areas.\nNo The syntax is as follows: { \"aggs\": { \"aggregation_name\": { \"geo_distance\": { \"field\": \"field_1\", \"origin\": \"x, y\", \"ranges\": [ { \"to\": \"value_1\" }, { \"from\": \"value_2\", \"to\": \"value_3\" }, { \"from\": \"value_4\" }] } } } } This example forms buckets from the following distances from a geo-point field:\nFewer than 10 km\nFrom 10 to 20 km\nFrom 20 to 50 km\nFrom 50 to 100 km\nAbove 100 km GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"position\": { \"geo_distance\": { \"field\": \"geo.coordinates\", \"origin\": { \"lat\": 83.76, \"lon\": -81.2 }, \"ranges\": [ { \"to\": 10 }, { \"from\": 10, \"to\": 20 }, { \"from\": 20, \"to\": 50 }, { \"from\": 50, \"to\": 100 }, { \"from\": 100 }] } } } } Example response... \"aggregations\": { \"position\": { \"buckets\": [ { \"key\": \"*-10.0\", \"from\": 0.0, \"to\": 10.0, \"doc_count\": 0 }, { \"key\": \"10.0-20.0\", \"from\": 10.0, \"to\": 20.0, \"doc_count\": 0 }, { \"key\": \"20.0-50.0\", \"from\": 20.0, \"to\": 50.0, \"doc_count\": 0 }, { \"key\": \"50.0-100.0\", \"from\": 50.0, \"to\": 100.0, \"doc_count\": 0 }, { \"key\": \"100.0-*\", \"from\": 100.0, \"doc_count\": 14074 }] } } } The geohash_grid aggregation buckets documents for geographical analysis. It organizes a geographical region into a grid of smaller regions of different sizes or precisions. Lower values of precision represent larger geographical areas and higher values represent smaller, more precise geographical areas.\nThe number of results returned by a query might be far too many to display each geo point individually on a map. The geohash_grid aggregation buckets nearby geo points together by calculating the Geohash for each point, at the level of precision that you define (between 1 to 12; the default is 5). To learn more about Geohash, see Wikipedia.\nThe web logs example data is spread over a large geographical area, so you can use a lower precision value. You can zoom in on this map by increasing the precision value: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"geo_hash\": { \"geohash_grid\": { \"field\": \"geo.coordinates\", \"precision\": 4 } } } } Example response... \"aggregations\": { \"geo_hash\": { \"buckets\": [ { \"key\": \"c1cg\", \"doc_count\": 104 }, { \"key\": \"dr5r\", \"doc_count\": 26 }, { \"key\": \"9q5b\", \"doc_count\": 20 }, { \"key\": \"c20g\", \"doc_count\": 19 }, { \"key\": \"dr70\", \"doc_count\": 18 }...] } } } You can visualize the aggregated response on a map using OpenSearch Dashboards.\nThe more accurate you want the aggregation to be, the more resources OpenSearch consumes, because of the number of buckets that the aggregation has to calculate. By default, OpenSearch does not generate more than 10,000 buckets. You can change this behavior by using the size attribute, but keep in mind that the performance might suffer for very wide queries consisting of thousands of buckets.\nadjacency_matrix\nThe adjacency_matrix aggregation lets you define filter expressions and returns a matrix of the intersecting filters where each non-empty cell in the matrix represents a bucket. You can find how many documents fall within any combination of filters.\nUse the adjacency_matrix aggregation to discover how concepts are related by visualizing the data as graphs.\nFor example, in the sample eCommerce dataset, to analyze how the different manufacturing companies are related: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"interactions\": { \"adjacency_matrix\": { \"filters\": { \"grpA\": { \"match\": { \"manufacturer.keyword\": \"Low Tide Media\" } }, \"grpB\": { \"match\": { \"manufacturer.keyword\": \"Elitelligence\" } }, \"grpC\": { \"match\": { \"manufacturer.keyword\": \"Oceanavigations\" } } } } } } } Example response {... \"aggregations\": { \"interactions\": { \"buckets\": [ { \"key\": \"grpA\", \"doc_count\": 1553 }, { \"key\": \"grpA&amp;grpB\", \"doc_count\": 590 }, { \"key\": \"grpA&amp;grpC\", \"doc_count\": 329 }, { \"key\": \"grpB\", \"doc_count\": 1370 }, { \"key\": \"grpB&amp;grpC\", \"doc_count\": 299 }, { \"key\": \"grpC\", \"doc_count\": 1218 }] } } } Let’s take a closer look at the result: { \"key\": \"grpA&amp;grpB\", \"doc_count\": 590 } grpA: Products manufactured by Low Tide Media. grpB: Products manufactured by Elitelligence. 590: Number of products that are manufactured by both.\nYou can use OpenSearch Dashboards to represent this data with a network graph.\nnested, reverse_nested\nThe nested aggregation lets you aggregate on fields inside a nested object. The nested type is a specialized version of the object data type that allows arrays of objects to be indexed in a way that they can be queried independently of each other\nWith the object type, all the data is stored in the same document, so matches for a search can go across sub documents. For example, imagine a logs index with pages mapped as an object datatype: PUT logs/_doc/ 0 { \"response\": \"200\", \"pages\": [ { \"page\": \"landing\", \"load_time\": 200 }, { \"page\": \"blog\", \"load_time\": 500 }] } OpenSearch merges all sub-properties of the entity relations that looks something like this: { \"logs\": { \"pages\": [ \"landing\", \"blog\"], \"load_time\": [ \"200\", \"500\"] } } So, if you wanted to search this index with pages=landing and load_time=500, this document matches the criteria even though the load_time value for landing is 200.\nIf you want to make sure such cross-object matches don’t happen, map the field as a nested type: PUT logs { \"mappings\": { \"properties\": { \"pages\": { \"type\": \"nested\", \"properties\": { \"page\": { \"type\": \"text\" }, \"load_time\": { \"type\": \"double\" } } } } } } Nested documents allow you to index the same JSON document but will keep your pages in separate Lucene documents, making only searches like pages=landing and load_time=200 return the expected result. Internally, nested objects index each object in the array as a separate hidden document, meaning that each nested object can be queried independently of the others.\nYou have to specify a nested path relative to parent that contains the nested documents: GET logs/_search { \"query\": { \"match\": { \"response\": \"200\" } }, \"aggs\": { \"pages\": { \"nested\": { \"path\": \"pages\" }, \"aggs\": { \"min_load_time\": { \"min\": { \"field\": \"pages.load_time\" } } } } } } Example response... \"aggregations\": { \"pages\": { \"doc_count\": 2, \"min_price\": { \"value\": 200.0 } } } } You can also aggregate values from nested documents to their parent; this aggregation is called reverse_nested.\nYou can use reverse_nested to aggregate a field from the parent document after grouping by the field from the nested object. The reverse_nested aggregation “joins back” the root page and gets the load_time for each for your variations.\nThe reverse_nested aggregation is a sub-aggregation inside a nested aggregation. It accepts a single option named path. This option defines how many steps backwards in the document hierarchy OpenSearch takes to calculate the aggregations. GET logs/_search { \"query\": { \"match\": { \"response\": \"200\" } }, \"aggs\": { \"pages\": { \"nested\": { \"path\": \"pages\" }, \"aggs\": { \"top_pages_per_load_time\": { \"terms\": { \"field\": \"pages.load_time\" }, \"aggs\": { \"comment_to_logs\": { \"reverse_nested\": {}, \"aggs\": { \"min_load_time\": { \"min\": { \"field\": \"pages.load_time\" } } } } } } } } } } Example response... \"aggregations\": { \"pages\": { \"doc_count\": 2, \"top_pages_per_load_time\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": 200.0, \"doc_count\": 1, \"comment_to_logs\": { \"doc_count\": 1, \"min_load_time\": { \"value\": null } } }, { \"key\": 500.0, \"doc_count\": 1, \"comment_to_logs\": { \"doc_count\": 1, \"min_load_time\": { \"value\": null } } }] } } } } The response shows the logs index has one page with a load_time of 200 and one with a load_time of 500.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Aggregations"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/aggregations/geohexgrid/",
    "title": "GeoHex grid aggregations",
    "content": "The Hexagonal Hierarchical Geospatial Indexing System (H3) partitions the Earth’s areas into identifiable hexagon-shaped cells.\nThe H3 grid system works well for proximity applications because it overcomes the limitations of Geohash’s non-uniform partitions. Geohash encodes latitude and longitude pairs, leading to significantly smaller partitions near the poles and a degree of longitude near the equator. However, the H3 grid system’s distortions are low and limited to 5 partitions of 122. These five partitions are placed in low-use areas (for example, in the middle of the ocean), leaving the essential areas error free. Thus, grouping documents based on the H3 grid system provides a better aggregation than the Geohash grid.\nThe GeoHex grid aggregation groups geopoints into grid cells for geographical analysis. Each grid cell corresponds to an H3 cell and is identified using the H3Index representation.\nPrecision\nThe precision parameter controls the level of granularity that determines the grid cell size. The lower the precision, the larger the grid cells.\nThe following example illustrates low-precision and high-precision aggregation requests.\nTo start, create an index and map the location field as a geo_point: PUT national_parks { \"mappings\": { \"properties\": { \"location\": { \"type\": \"geo_point\" } } } } Index the following documents into the sample index: PUT national_parks/_doc/ 1 { \"name\": \"Yellowstone National Park\", \"location\": \"44.42, -110.59\" } PUT national_parks/_doc/ 2 { \"name\": \"Yosemite National Park\", \"location\": \"37.87, -119.53\" } PUT national_parks/_doc/ 3 { \"name\": \"Death Valley National Park\", \"location\": \"36.53, -116.93\" } You can index geopoints in several formats. For a list of all supported formats, see the geopoint documentation.\nLow-precision requests\nRun a low-precision request that buckets all three documents together: GET national_parks/_search { \"aggregations\": { \"grouped\": { \"geohex_grid\": { \"field\": \"location\", \"precision\": 1 } } } } You can use either the GET or POST HTTP method for GeoHex grid aggregation queries.\nThe response groups documents 2 and 3 together because they are close enough to be bucketed in one grid cell: { \"took\": 4, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"national_parks\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"name\": \"Yellowstone National Park\", \"location\": \"44.42, -110.59\" } }, { \"_index\": \"national_parks\", \"_id\": \"2\", \"_score\": 1.0, \"_source\": { \"name\": \"Yosemite National Park\", \"location\": \"37.87, -119.53\" } }, { \"_index\": \"national_parks\", \"_id\": \"3\", \"_score\": 1.0, \"_source\": { \"name\": \"Death Valley National Park\", \"location\": \"36.53, -116.93\" } }] }, \"aggregations\": { \"grouped\": { \"buckets\": [ { \"key\": \"8129bffffffffff\", \"doc_count\": 2 }, { \"key\": \"8128bffffffffff\", \"doc_count\": 1 }] } } } High-precision requests\nNow run a high-precision request: GET national_parks/_search { \"aggregations\": { \"grouped\": { \"geohex_grid\": { \"field\": \"location\", \"precision\": 6 } } } } All three documents are bucketed separately because of higher granularity: { \"took\": 5, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"national_parks\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"name\": \"Yellowstone National Park\", \"location\": \"44.42, -110.59\" } }, { \"_index\": \"national_parks\", \"_id\": \"2\", \"_score\": 1.0, \"_source\": { \"name\": \"Yosemite National Park\", \"location\": \"37.87, -119.53\" } }, { \"_index\": \"national_parks\", \"_id\": \"3\", \"_score\": 1.0, \"_source\": { \"name\": \"Death Valley National Park\", \"location\": \"36.53, -116.93\" } }] }, \"aggregations\": { \"grouped\": { \"buckets\": [ { \"key\": \"8629ab6dfffffff\", \"doc_count\": 1 }, { \"key\": \"8629857a7ffffff\", \"doc_count\": 1 }, { \"key\": \"862896017ffffff\", \"doc_count\": 1 }] } } } Filtering requests\nHigh-precision requests are resource intensive, so we recommend using a filter like geo_bounding_box to limit the geographical area. For example, the following query applies a filter to limit the search area: GET national_parks/_search { \"size\": 0, \"aggregations\": { \"filtered\": { \"filter\": { \"geo_bounding_box\": { \"location\": { \"top_left\": \"38, -120\", \"bottom_right\": \"36, -116\" } } }, \"aggregations\": { \"grouped\": { \"geohex_grid\": { \"field\": \"location\", \"precision\": 6 } } } } } } The response contains the two documents that are within the geo_bounding_box bounds: { \"took\": 4, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"filtered\": { \"doc_count\": 2, \"grouped\": { \"buckets\": [ { \"key\": \"8629ab6dfffffff\", \"doc_count\": 1 }, { \"key\": \"8629857a7ffffff\", \"doc_count\": 1 }] } } } } You can also restrict the geographical area by providing the coordinates of the bounding envelope in the bounds parameter. Both bounds and geo_bounding_box coordinates can be specified in any of the geopoint formats. The following query uses the well-known text (WKT) “POINT( longitude latitude)” format for the bounds parameter: GET national_parks/_search { \"size\": 0, \"aggregations\": { \"grouped\": { \"geohex_grid\": { \"field\": \"location\", \"precision\": 6, \"bounds\": { \"top_left\": \"POINT (-120 38)\", \"bottom_right\": \"POINT (-116 36)\" } } } } } The response contains only the two results that are within the specified bounds: { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"grouped\": { \"buckets\": [ { \"key\": \"8629ab6dfffffff\", \"doc_count\": 1 }, { \"key\": \"8629857a7ffffff\", \"doc_count\": 1 }] } } } The bounds parameter can be used with or without the geo_bounding_box filter; these two parameters are independent and can have any spatial relationship to each other.\nSupported parameters\nGeoHex grid aggregation requests support the following parameters. Parameter Data type Description field\nString\nThe field that contains the geopoints. This field must be mapped as a geo_point field. If the field contains an array, all array values are aggregated. Required.\nprecision\nInteger\nThe zoom level used to determine grid cells for bucketing results. Valid values are in the [0, 15] range. Optional. Default is 5.\nbounds\nObject\nThe bounding box for filtering geopoints. The bounding box is defined by the top left and bottom right vertices. The vertices are specified as geopoints in one of the following formats: - An object with a latitude and longitude - An array in the [ longitude, latitude] format - A string in the “ latitude, longitude ” format - A Geohash - WKT See the geopoint formats for formatting examples. Optional.\nsize\nInteger\nThe maximum number of buckets to return. When there are more buckets than size, OpenSearch returns buckets with more documents. Optional. Default is 10,000.\nshard_size\nInteger\nThe maximum number of buckets to return from each shard. Optional. Default is max (10, size · number of shards), which provides a more accurate count of more highly prioritized buckets.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Aggregations"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/aggregations/metric-agg/",
    "title": "Metric aggregations",
    "content": "Metric aggregations let you perform simple calculations such as finding the minimum, maximum, and average values of a field.\nTypes of metric aggregations\nMetric aggregations are of two types: single-value metric aggregations and multi-value metric aggregations.\nSingle-value metric aggregations\nSingle-value metric aggregations return a single metric. For example, sum, min, max, avg, cardinality, and value_count.\nMulti-value metric aggregations\nMulti-value metric aggregations return more than one metric. For example, stats, extended_stats, matrix_stats, percentile, percentile_ranks, geo_bound, top_hits, and scripted_metric.\nsum, min, max, avg\nThe sum, min, max, and avg metrics are single-value metric aggregations that return the sum, minimum, maximum, and average values of a field, respectively.\nThe following example calculates the total sum of the taxful_total_price field: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"sum_taxful_total_price\": { \"sum\": { \"field\": \"taxful_total_price\" } } } } Sample Response... \"aggregations\": { \"sum_taxful_total_price\": { \"value\": 350884.12890625 } } } In a similar fashion, you can find the minimum, maximum, and average values of a field.\ncardinality\nThe cardinality metric is a single-value metric aggregation that counts the number of unique or distinct values of a field.\nThe following example finds the number of unique products in an eCommerce store: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"unique_products\": { \"cardinality\": { \"field\": \"products.product_id\" } } } } Example response... \"aggregations\": { \"unique_products\": { \"value\": 7033 } } } Cardinality count is approximate.\nIf you have tens of thousands of products in your hypothetical store, an accurate cardinality calculation requires loading all the values into a hash set and returning its size. This approach doesn’t scale well; it requires huge amounts of memory and can cause high latencies.\nYou can control the trade-off between memory and accuracy with the precision_threshold setting. This setting defines the threshold below which counts are expected to be close to accurate. Above this value, counts might become a bit less accurate. The default value of precision_threshold is 3,000. The maximum supported value is 40,000. GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"unique_products\": { \"cardinality\": { \"field\": \"products.product_id\", \"precision_threshold\": 10000 } } } } value_count\nThe value_count metric is a single-value metric aggregation that calculates the number of values that an aggregation is based on.\nFor example, you can use the value_count metric with the avg metric to find how many numbers the aggregation uses to calculate an average value. GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"number_of_values\": { \"value_count\": { \"field\": \"taxful_total_price\" } } } } Example response... \"aggregations\": { \"number_of_values\": { \"value\": 4675 } } } stats, extended_stats, matrix_stats\nThe stats metric is a multi-value metric aggregation that returns all basic metrics such as min, max, sum, avg, and value_count in one aggregation query.\nThe following example returns the basic stats for the taxful_total_price field: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"stats_taxful_total_price\": { \"stats\": { \"field\": \"taxful_total_price\" } } } } Example response... \"aggregations\": { \"stats_taxful_total_price\": { \"count\": 4675, \"min\": 6.98828125, \"max\": 2250.0, \"avg\": 75.05542864304813, \"sum\": 350884.12890625 } } } The extended_stats aggregation is an extended version of the stats aggregation. Apart from including basic stats, extended_stats also returns stats such as sum_of_squares, variance, and std_deviation. GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"extended_stats_taxful_total_price\": { \"extended_stats\": { \"field\": \"taxful_total_price\" } } } } Sample Response... \"aggregations\": { \"extended_stats_taxful_total_price\": { \"count\": 4675, \"min\": 6.98828125, \"max\": 2250.0, \"avg\": 75.05542864304813, \"sum\": 350884.12890625, \"sum_of_squares\": 3.9367749294174194E7, \"variance\": 2787.59157113862, \"variance_population\": 2787.59157113862, \"variance_sampling\": 2788.187974983536, \"std_deviation\": 52.79764740155209, \"std_deviation_population\": 52.79764740155209, \"std_deviation_sampling\": 52.80329511482722, \"std_deviation_bounds\": { \"upper\": 180.6507234461523, \"lower\": -30.53986616005605, \"upper_population\": 180.6507234461523, \"lower_population\": -30.53986616005605, \"upper_sampling\": 180.66201887270256, \"lower_sampling\": -30.551161586606312 } } } } The std_deviation_bounds object provides a visual variance of the data with an interval of plus/minus two standard deviations from the mean.\nTo set the standard deviation to a different value, say 3, set sigma to 3: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"extended_stats_taxful_total_price\": { \"extended_stats\": { \"field\": \"taxful_total_price\", \"sigma\": 3 } } } } The matrix_stats aggregation generates advanced stats for multiple fields in a matrix form.\nThe following example returns advanced stats in a matrix form for the taxful_total_price and products.base_price fields: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"matrix_stats_taxful_total_price\": { \"matrix_stats\": { \"fields\": [ \"taxful_total_price\", \"products.base_price\"] } } } } Example response... \"aggregations\": { \"matrix_stats_taxful_total_price\": { \"doc_count\": 4675, \"fields\": [ { \"name\": \"products.base_price\", \"count\": 4675, \"mean\": 34.994239430147196, \"variance\": 360.5035285833703, \"skewness\": 5.530161335032702, \"kurtosis\": 131.16306324042148, \"covariance\": { \"products.base_price\": 360.5035285833703, \"taxful_total_price\": 846.6489362233166 }, \"correlation\": { \"products.base_price\": 1.0, \"taxful_total_price\": 0.8444765264325268 } }, { \"name\": \"taxful_total_price\", \"count\": 4675, \"mean\": 75.05542864304839, \"variance\": 2788.1879749835402, \"skewness\": 15.812149139924037, \"kurtosis\": 619.1235507385902, \"covariance\": { \"products.base_price\": 846.6489362233166, \"taxful_total_price\": 2788.1879749835402 }, \"correlation\": { \"products.base_price\": 0.8444765264325268, \"taxful_total_price\": 1.0 } }] } } } Statistic Description count The number of samples measured. mean The average value of the field measured from the sample. variance How far the values of the field measured are spread out from its mean value. The larger the variance, the more it’s spread from its mean value. skewness An asymmetric measure of the distribution of the field’s values around the mean. kurtosis A measure of the tail heaviness of a distribution. As the tail becomes lighter, kurtosis decreases. As the tail becomes heavier, kurtosis increases. To learn about kurtosis, see Wikipedia. covariance A measure of the joint variability between two fields. A positive value means their values move in the same direction and vice versa. correlation A measure of the strength of the relationship between two fields. The valid values are between [-1, 1]. A value of -1 means that the value is negatively correlated and a value of 1 means that it’s positively correlated. A value of 0 means that there’s no identifiable relationship between them. percentile, percentile_ranks\nPercentile is the percentage of the data that’s at or below a certain threshold value.\nThe percentile metric is a multi-value metric aggregation that lets you find outliers in your data or figure out the distribution of your data.\nLike the cardinality metric, the percentile metric is also approximate.\nThe following example calculates the percentile in relation to the taxful_total_price field: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"percentile_taxful_total_price\": { \"percentiles\": { \"field\": \"taxful_total_price\" } } } } Example response... \"aggregations\": { \"percentile_taxful_total_price\": { \"values\": { \"1.0\": 21.984375, \"5.0\": 27.984375, \"25.0\": 44.96875, \"50.0\": 64.22061688311689, \"75.0\": 93.0, \"95.0\": 156.0, \"99.0\": 222.0 } } } } Percentile rank is the percentile of values at or below a threshold grouped by a specified value. For example, if a value is greater than or equal to 80% of the values, it has a percentile rank of 80. GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"percentile_rank_taxful_total_price\": { \"percentile_ranks\": { \"field\": \"taxful_total_price\", \"values\": [ 10, 15] } } } } Example response... \"aggregations\": { \"percentile_rank_taxful_total_price\": { \"values\": { \"10.0\": 0.055096056411283456, \"15.0\": 0.0830092961834656 } } } } geo_bound\nThe geo_bound metric is a multi-value metric aggregation that calculates the bounding box in terms of latitude and longitude around a geo_point field.\nThe following example returns the geo_bound metrics for the geoip.location field: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"geo\": { \"geo_bounds\": { \"field\": \"geoip.location\" } } } } Example response \"aggregations\": { \"geo\": { \"bounds\": { \"top_left\": { \"lat\": 52.49999997206032, \"lon\": -118.20000001229346 }, \"bottom_right\": { \"lat\": 4.599999985657632, \"lon\": 55.299999956041574 } } } } } top_hits\nThe top_hits metric is a multi-value metric aggregation that ranks the matching documents based on a relevance score for the field that’s being aggregated.\nYou can specify the following options: from: The starting position of the hit. size: The maximum size of hits to return. The default value is 3. sort: How the matching hits are sorted. By default, the hits are sorted by the relevance score of the aggregation query.\nThe following example returns the top 5 products in your eCommerce data: GET opensearch_dashboards_sample_data_ecommerce/_search { \"size\": 0, \"aggs\": { \"top_hits_products\": { \"top_hits\": { \"size\": 5 } } } } Example response... \"aggregations\": { \"top_hits_products\": { \"hits\": { \"total\": { \"value\": 4675, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"_type\": \"_doc\", \"_id\": \"glMlwXcBQVLeQPrkHPtI\", \"_score\": 1.0, \"_source\": { \"category\": [ \"Women's Accessories\", \"Women's Clothing\"], \"currency\": \"EUR\", \"customer_first_name\": \"rania\", \"customer_full_name\": \"rania Evans\", \"customer_gender\": \"FEMALE\", \"customer_id\": 24, \"customer_last_name\": \"Evans\", \"customer_phone\": \"\", \"day_of_week\": \"Sunday\", \"day_of_week_i\": 6, \"email\": \"rania@evans-family.zzz\", \"manufacturer\": [ \"Tigress Enterprises\"], \"order_date\": \"2021-02-28T14:16:48+00:00\", \"order_id\": 583581, \"products\": [ { \"base_price\": 10.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 19024, \"category\": \"Women's Accessories\", \"sku\": \"ZO0082400824\", \"taxless_price\": 10.99, \"unit_discount_amount\": 0, \"min_price\": 5.17, \"_id\": \"sold_product_583581_19024\", \"discount_amount\": 0, \"created_on\": \"2016-12-25T14:16:48+00:00\", \"product_name\": \"Snood - white/grey/peach\", \"price\": 10.99, \"taxful_price\": 10.99, \"base_unit_price\": 10.99 }, { \"base_price\": 32.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 19260, \"category\": \"Women's Clothing\", \"sku\": \"ZO0071900719\", \"taxless_price\": 32.99, \"unit_discount_amount\": 0, \"min_price\": 17.15, \"_id\": \"sold_product_583581_19260\", \"discount_amount\": 0, \"created_on\": \"2016-12-25T14:16:48+00:00\", \"product_name\": \"Cardigan - grey\", \"price\": 32.99, \"taxful_price\": 32.99, \"base_unit_price\": 32.99 }], \"sku\": [ \"ZO0082400824\", \"ZO0071900719\"], \"taxful_total_price\": 43.98, \"taxless_total_price\": 43.98, \"total_quantity\": 2, \"total_unique_products\": 2, \"type\": \"order\", \"user\": \"rani\", \"geoip\": { \"country_iso_code\": \"EG\", \"location\": { \"lon\": 31.3, \"lat\": 30.1 }, \"region_name\": \"Cairo Governorate\", \"continent_name\": \"Africa\", \"city_name\": \"Cairo\" }, \"event\": { \"dataset\": \"sample_ecommerce\" } }... }] } } } } scripted_metric\nThe scripted_metric metric is a multi-value metric aggregation that returns metrics calculated from a specified script.\nA script has four stages: the initial stage, the map stage, the combine stage, and the reduce stage. init_script: (OPTIONAL) Sets the initial state and executes before any collection of documents. map_script: Checks the value of the type field and executes the aggregation on the collected documents. combine_script: Aggregates the state returned from every shard. The aggregated value is returned to the coordinating node. reduce_script: Provides access to the variable states; this variable combines the results from the combine_script on each shard into an array.\nThe following example aggregates the different HTTP response types in web log data: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggregations\": { \"responses.counts\": { \"scripted_metric\": { \"init_script\": \"state.responses = ['error':0L,'success':0L,'other':0L]\", \"map_script\": \"\"\"\ndef code = doc['response.keyword'].value;\nif (code.startsWith('5') || code.startsWith('4')) {\nstate.responses.error += 1;\n} else if(code.startsWith('2')) {\nstate.responses.success += 1;\n} else {\nstate.responses.other += 1;\n}\n\"\"\", \"combine_script\": \"state.responses\", \"reduce_script\": \"\"\"\ndef counts = ['error': 0L, 'success': 0L, 'other': 0L];\nfor (responses in states) {\ncounts.error += responses['error'];\ncounts.success += responses['success'];\ncounts.other += responses['other'];\n}\nreturn counts;\n\"\"\" } } } } Sample Response... \"aggregations\": { \"responses.counts\": { \"value\": { \"other\": 0, \"success\": 12832, \"error\": 1242 } } } }",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Aggregations"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/aggregations/pipeline-agg/",
    "title": "Pipeline aggregations",
    "content": "With pipeline aggregations, you can chain aggregations by piping the results of one aggregation as an input to another for a more nuanced output.\nYou can use pipeline aggregations to compute complex statistical and mathematical measures like derivatives, moving averages, cumulative sums, and so on.\nPipeline aggregation syntax\nA pipeline aggregation uses the buckets_path property to access the results of other aggregations.\nThe buckets_path property has a specific syntax: buckets_path = &lt;AGG_NAME&gt;[&lt;AGG_SEPARATOR&gt;,&lt;AGG_NAME&gt;]*[&lt;METRIC_SEPARATOR&gt;, &lt;METRIC&gt;]; where: AGG_NAME is the name of the aggregation. AGG_SEPARATOR separates aggregations. It’s represented as &gt;. METRIC_SEPARATOR separates aggregations from its metrics. It’s represented as.. METRIC is the name of the metric, in case of multi-value metric aggregations.\nFor example, my_sum.sum selects the sum metric of an aggregation called my_sum. popular_tags&gt;my_sum.sum nests my_sum.sum into the popular_tags aggregation.\nYou can also specify the following additional parameters: gap_policy: Real-world data can contain gaps or null values. You can specify the policy to deal with such missing data with the gap_policy property. You can either set the gap_policy property to skip to skip the missing data and continue from the next available value, or insert_zeros to replace the missing values with zero and continue running. format: The type of format for the output value. For example, yyyy-MM-dd for a date value.\nQuick example\nTo sum all the buckets returned by the sum_total_memory aggregation: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"number_of_bytes\": { \"histogram\": { \"field\": \"bytes\", \"interval\": 10000 }, \"aggs\": { \"sum_total_memory\": { \"sum\": { \"field\": \"phpmemory\" } } } }, \"sum_copies\": { \"sum_bucket\": { \"buckets_path\": \"number_of_bytes&gt;sum_total_memory\" } } } } Example response... \"aggregations\": { \"number_of_bytes\": { \"buckets\": [ { \"key\": 0.0, \"doc_count\": 13372, \"sum_total_memory\": { \"value\": 9.12664E7 } }, { \"key\": 10000.0, \"doc_count\": 702, \"sum_total_memory\": { \"value\": 0.0 } }] }, \"sum_copies\": { \"value\": 9.12664E7 } } } Types of pipeline aggregations\nPipeline aggregations are of two types:\nSibling aggregations\nSibling aggregations take the output of a nested aggregation and produce new buckets or new aggregations at the same level as the nested buckets.\nSibling aggregations must be a multi-bucket aggregation (have multiple grouped values for a certain field) and the metric must be a numeric value. min_bucket, max_bucket, sum_bucket, and avg_bucket are common sibling aggregations.\nParent aggregations\nParent aggregations take the output of an outer aggregation and produce new buckets or new aggregations at the same level as the existing buckets.\nParent aggregations must have min_doc_count set to 0 (default for histogram aggregations) and the specified metric must be a numeric value. If min_doc_count is greater than 0, some buckets are omitted, which might lead to incorrect results. derivatives and cumulative_sum are common parent aggregations.\navg_bucket, sum_bucket, min_bucket, max_bucket\nThe avg_bucket, sum_bucket, min_bucket, and max_bucket aggregations are sibling aggregations that calculate the average, sum, minimum, and maximum values of a metric in each bucket of a previous aggregation.\nThe following example creates a date histogram with a one-month interval. The sum sub-aggregation calculates the sum of all bytes for each month. Finally, the avg_bucket aggregation uses this sum to calculate the average number of bytes per month: POST opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"visits_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } } } }, \"avg_monthly_bytes\": { \"avg_bucket\": { \"buckets_path\": \"visits_per_month&gt;sum_of_bytes\" } } } } Example response... \"aggregations\": { \"visits_per_month\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"sum_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"sum_of_bytes\": { \"value\": 3.8880434E7 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"sum_of_bytes\": { \"value\": 3.1445055E7 } }] }, \"avg_monthly_bytes\": { \"value\": 2.6575229666666668E7 } } } In a similar fashion, you can calculate the sum_bucket, min_bucket, and max_bucket values for the bytes per month.\nstats_bucket, extended_stats_bucket\nThe stats_bucket aggregation is a sibling aggregation that returns a variety of stats ( count, min, max, avg, and sum) for the buckets of a previous aggregation.\nThe following example returns the basic stats for the buckets returned by the sum_of_bytes aggregation nested into the visits_per_month aggregation: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"visits_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } } } }, \"stats_monthly_bytes\": { \"stats_bucket\": { \"buckets_path\": \"visits_per_month&gt;sum_of_bytes\" } } } } Example response... \"stats_monthly_bytes\": { \"count\": 3, \"min\": 9400200.0, \"max\": 3.8880434E7, \"avg\": 2.6575229666666668E7, \"sum\": 7.9725689E7 } } } The extended_stats aggregation is an extended version of the stats aggregation. Apart from including basic stats, extended_stats also provides stats such as sum_of_squares, variance, and std_deviation.\nExample response \"stats_monthly_visits\": { \"count\": 3, \"min\": 9400200.0, \"max\": 3.8880434E7, \"avg\": 2.6575229666666668E7, \"sum\": 7.9725689E7, \"sum_of_squares\": 2.588843392021381E15, \"variance\": 1.5670496550438025E14, \"variance_population\": 1.5670496550438025E14, \"variance_sampling\": 2.3505744825657038E14, \"std_deviation\": 1.251818539183616E7, \"std_deviation_population\": 1.251818539183616E7, \"std_deviation_sampling\": 1.5331583357780447E7, \"std_deviation_bounds\": { \"upper\": 5.161160045033899E7, \"lower\": 1538858.8829943463, \"upper_population\": 5.161160045033899E7, \"lower_population\": 1538858.8829943463, \"upper_sampling\": 5.723839638222756E7, \"lower_sampling\": -4087937.0488942266 } } } } bucket_script, bucket_selector\nThe bucket_script aggregation is a parent aggregation that executes a script to perform per-bucket calculations of a previous aggregation. Make sure the metrics are of numeric type and the returned values are also numeric.\nUse the script parameter to add your script. The script can be inline, in a file, or in an index. To enable inline scripting, add the following line to your opensearch.yml file in the config folder: script.inline: on The buckets_path property consists of multiple entries. Each entry is a key and a value. The key is the name of the value that you can use in the script.\nThe basic syntax is: { \"bucket_script\": { \"buckets_path\": { \"my_var1\": \"the_sum\", \"my_var2\": \"the_value_count\" }, \"script\": \"params.my_var1 / params.my_var2\" } } The following example uses the sum aggregation on the buckets generated by a date histogram. From the resultant buckets values, the percentage of RAM is calculated in an interval of 10,000 bytes in the context of a zip extension: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sales_per_month\": { \"histogram\": { \"field\": \"bytes\", \"interval\": \"10000\" }, \"aggs\": { \"total_ram\": { \"sum\": { \"field\": \"machine.ram\" } }, \"ext-type\": { \"filter\": { \"term\": { \"extension.keyword\": \"zip\" } }, \"aggs\": { \"total_ram\": { \"sum\": { \"field\": \"machine.ram\" } } } }, \"ram-percentage\": { \"bucket_script\": { \"buckets_path\": { \"machineRam\": \"ext-type&gt;total_ram\", \"totalRam\": \"total_ram\" }, \"script\": \"params.machineRam / params.totalRam\" } } } } } } Example response \"aggregations\": { \"sales_per_month\": { \"buckets\": [ { \"key\": 0.0, \"doc_count\": 13372, \"os-type\": { \"doc_count\": 1558, \"total_ram\": { \"value\": 2.0090783268864E13 } }, \"total_ram\": { \"value\": 1.7214228922368E14 }, \"ram-percentage\": { \"value\": 0.11671032934131736 } }, { \"key\": 10000.0, \"doc_count\": 702, \"os-type\": { \"doc_count\": 116, \"total_ram\": { \"value\": 1.622423896064E12 } }, \"total_ram\": { \"value\": 9.015136354304E12 }, \"ram-percentage\": { \"value\": 0.17996665078608862 } }] } } } The RAM percentage is calculated and appended at the end of each bucket.\nThe bucket_selector aggregation is a script-based aggregation that selects buckets returned by a histogram (or date_histogram) aggregation. Use it in scenarios where you don’t want certain buckets in the output based on conditions supplied by you.\nThe bucket_selector aggregation executes a script to decide if a bucket stays in the parent multi-bucket aggregation.\nThe basic syntax is: { \"bucket_selector\": { \"buckets_path\": { \"my_var1\": \"the_sum\", \"my_var2\": \"the_value_count\" }, \"script\": \"params.my_var1 / params.my_var2\" } } The following example calculates the sum of bytes and then evaluates if this sum is greater than 20,000. If true, then the bucket is retained in the bucket list. Otherwise, it’s deleted from the final output. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"bytes_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"total_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"bytes_bucket_filter\": { \"bucket_selector\": { \"buckets_path\": { \"totalBytes\": \"total_bytes\" }, \"script\": \"params.totalBytes &gt; 20000\" } } } } } } Example response \"aggregations\": { \"bytes_per_month\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"total_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"total_bytes\": { \"value\": 3.8880434E7 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"total_bytes\": { \"value\": 3.1445055E7 } }] } } } bucket_sort\nThe bucket_sort aggregation is a parent aggregation that sorts buckets of a previous aggregation.\nYou can specify several sort fields together with the corresponding sort order. Additionally, you can sort each bucket based on its key, count, or its sub-aggregations. You can also truncate the buckets by setting from and size parameters.\nSyntax { \"bucket_sort\": { \"sort\": [ { \"sort_field_1\": { \"order\": \"asc\" }}, { \"sort_field_2\": { \"order\": \"desc\" }}, \"sort_field_3\"], \"from\": 1, \"size\": 3 } } The following example sorts the buckets of a date_histogram aggregation based on the computed total_sum values. We sort the buckets in descending order so that the buckets with the highest number of bytes are returned first. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sales_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"total_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"bytes_bucket_sort\": { \"bucket_sort\": { \"sort\": [ { \"total_bytes\": { \"order\": \"desc\" } }], \"size\": 3 } } } } } } Example response \"aggregations\": { \"sales_per_month\": { \"buckets\": [ { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"total_bytes\": { \"value\": 3.8880434E7 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"total_bytes\": { \"value\": 3.1445055E7 } }, { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"total_bytes\": { \"value\": 9400200.0 } }] } } } You can also use this aggregation to truncate the resulting buckets without sorting. For this, just use the from and/or size parameters without sort.\ncumulative_sum\nThe cumulative_sum aggregation is a parent aggregation that calculates the cumulative sum of each bucket of a previous aggregation.\nA cumulative sum is a sequence of partial sums of a given sequence. For example, the cumulative sums of the sequence {a,b,c,…} are a, a+b, a+b+c, and so on. You can use the cumulative sum to visualize the rate of change of a field over time.\nThe following example calculates the cumulative number of bytes over a monthly basis: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sales_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"no-of-bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"cumulative_bytes\": { \"cumulative_sum\": { \"buckets_path\": \"no-of-bytes\" } } } } } } Example response... \"aggregations\": { \"sales_per_month\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"no-of-bytes\": { \"value\": 9400200.0 }, \"cumulative_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"no-of-bytes\": { \"value\": 3.8880434E7 }, \"cumulative_bytes\": { \"value\": 4.8280634E7 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"no-of-bytes\": { \"value\": 3.1445055E7 }, \"cumulative_bytes\": { \"value\": 7.9725689E7 } }] } } } derivative\nThe derivative aggregation is a parent aggregation that calculates 1st order and 2nd order derivates of each bucket of a previous aggregation.\nIn mathematics, the derivative of a function measures its sensitivity to change. In other words, a derivative evaluates the rate of change in some function with respect to some variable. To learn more about derivates, see Wikipedia.\nYou can use derivates to calculate the rate of change of numeric values compared to its previous time periods.\nThe 1st order derivative indicates whether a metric is increasing or decreasing, and by how much it’s increasing or decreasing.\nThe following example calculates the 1st order derivative for the sum of bytes per month. The 1st order derivative is the difference between the number of bytes in the current month and the previous month: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sales_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"number_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"bytes_deriv\": { \"derivative\": { \"buckets_path\": \"number_of_bytes\" } } } } } } Example response... \"aggregations\": { \"sales_per_month\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"number_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"number_of_bytes\": { \"value\": 3.8880434E7 }, \"bytes_deriv\": { \"value\": 2.9480234E7 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"number_of_bytes\": { \"value\": 3.1445055E7 }, \"bytes_deriv\": { \"value\": -7435379.0 } }] } } } The 2nd order derivative is a double derivative or a derivative of the derivative.\nIt indicates how the rate of change of a quantity is itself changing. It’s the difference between the 1st order derivatives of adjacent buckets.\nTo calculate a 2nd order derivative, chain one derivative aggregation to another: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"sales_per_month\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"number_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"bytes_deriv\": { \"derivative\": { \"buckets_path\": \"number_of_bytes\" } }, \"bytes_2nd_deriv\": { \"derivative\": { \"buckets_path\": \"bytes_deriv\" } } } } } } Example response... \"aggregations\": { \"sales_per_month\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"number_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"number_of_bytes\": { \"value\": 3.8880434E7 }, \"bytes_deriv\": { \"value\": 2.9480234E7 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"number_of_bytes\": { \"value\": 3.1445055E7 }, \"bytes_deriv\": { \"value\": -7435379.0 }, \"bytes_2nd_deriv\": { \"value\": -3.6915613E7 } }] } } } The first bucket doesn’t have a 1st order derivate as a derivate needs at least two points for comparison. The first and second buckets don’t have a 2nd order derivate because a 2nd order derivate needs at least two data points from the 1st order derivative.\nThe 1st order derivative for the “2020-11-01” bucket is 2.9480234E7 and the “2020-12-01” bucket is -7435379. So, the 2nd order derivative of the “2020-12-01” bucket is -3.6915613E7 (-7435379-2.9480234E7).\nTheoretically, you could continue chaining derivate aggregations to calculate the third, the fourth, and even higher-order derivatives. That would, however, provide little to no value for most datasets.\nmoving_avg\nA moving_avg aggregation is a parent aggregation that calculates the moving average metric.\nThe moving_avg aggregation finds the series of averages of different windows (subsets) of a dataset. A window’s size represents the number of data points covered by the window on each iteration (specified by the window property and set to 5 by default). On each iteration, the algorithm calculates the average for all data points that fit into the window and then slides forward by excluding the first member of the previous window and including the first member from the next window.\nFor example, given the data [1, 5, 8, 23, 34, 28, 7, 23, 20, 19], you can calculate a simple moving average with a window’s size of 5 as follows: (1 + 5 + 8 + 23 + 34) / 5 = 14.2\n(5 + 8 + 23 + 34+ 28) / 5 = 19.6\n(8 + 23 + 34 + 28 + 7) / 5 = 20\nso on... For more information, see Wikipedia.\nYou can use the moving_avg aggregation to either smoothen out short-term fluctuations or to highlight longer-term trends or cycles in your time-series data.\nSpecify a small window size (for example, window: 10) that closely follows the data to smoothen out small-scale fluctuations.\nAlternatively, specify a larger window size (for example, window: 100) that lags behind the actual data by a substantial amount to smoothen out all higher-frequency fluctuations or random noise, making lower frequency trends more visible.\nThe following example nests a moving_avg aggregation into a date_histogram aggregation: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"my_date_histogram\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"moving_avg_of_sum_of_bytes\": { \"moving_avg\": { \"buckets_path\": \"sum_of_bytes\" } } } } } } Example response... \"aggregations\": { \"my_date_histogram\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"sum_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"sum_of_bytes\": { \"value\": 3.8880434E7 }, \"moving_avg_of_sum_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"sum_of_bytes\": { \"value\": 3.1445055E7 }, \"moving_avg_of_sum_of_bytes\": { \"value\": 2.4140317E7 } }] } } } You can also use the moving_avg aggregation to predict future buckets.\nTo predict buckets, add the predict property and set it to the number of predictions that you want to see.\nThe following example adds five predictions to the preceding query: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"my_date_histogram\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"moving_avg_of_sum_of_bytes\": { \"moving_avg\": { \"buckets_path\": \"sum_of_bytes\", \"predict\": 5 } } } } } } Example response \"aggregations\": { \"my_date_histogram\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"sum_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"sum_of_bytes\": { \"value\": 3.8880434E7 }, \"moving_avg_of_sum_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"sum_of_bytes\": { \"value\": 3.1445055E7 }, \"moving_avg_of_sum_of_bytes\": { \"value\": 2.4140317E7 } }, { \"key_as_string\": \"2021-01-01T00:00:00.000Z\", \"key\": 1609459200000, \"doc_count\": 0, \"moving_avg_of_sum_of_bytes\": { \"value\": 2.6575229666666668E7 } }, { \"key_as_string\": \"2021-02-01T00:00:00.000Z\", \"key\": 1612137600000, \"doc_count\": 0, \"moving_avg_of_sum_of_bytes\": { \"value\": 2.6575229666666668E7 } }, { \"key_as_string\": \"2021-03-01T00:00:00.000Z\", \"key\": 1614556800000, \"doc_count\": 0, \"moving_avg_of_sum_of_bytes\": { \"value\": 2.6575229666666668E7 } }, { \"key_as_string\": \"2021-04-01T00:00:00.000Z\", \"key\": 1617235200000, \"doc_count\": 0, \"moving_avg_of_sum_of_bytes\": { \"value\": 2.6575229666666668E7 } }, { \"key_as_string\": \"2021-05-01T00:00:00.000Z\", \"key\": 1619827200000, \"doc_count\": 0, \"moving_avg_of_sum_of_bytes\": { \"value\": 2.6575229666666668E7 } }] } } } The moving_avg aggregation supports five models — simple, linear, exponentially weighted, holt-linear, and holt-winters. These models differ in how the values of the window are weighted. As data points become “older” (i.e., the window slides away from them), they might be weighted differently. You can specify a model of your choice by setting the model property. The model property holds the name of the model and the settings object, which you can use to provide model properties. For more information on these models, see Wikipedia.\nA simple model first calculates the sum of all data points in the window, and then divides that sum by the size of the window. In other words, a simple model calculates a simple arithmetic mean for each window in your dataset.\nThe following example uses a simple model with a window size of 30: GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"my_date_histogram\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"moving_avg_of_sum_of_bytes\": { \"moving_avg\": { \"buckets_path\": \"sum_of_bytes\", \"window\": 30, \"model\": \"simple\" } } } } } } Example response... \"aggregations\": { \"my_date_histogram\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"sum_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"sum_of_bytes\": { \"value\": 3.8880434E7 }, \"moving_avg_of_sum_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"sum_of_bytes\": { \"value\": 3.1445055E7 }, \"moving_avg_of_sum_of_bytes\": { \"value\": 2.4140317E7 } }] } } } The following example uses a holt model. You can set the speed at which the importance decays occurs with the alpha and beta setting. The default value of alpha is 0.3 and beta is 0.1. You can specify any float value between 0-1 inclusive. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"my_date_histogram\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"sum_of_bytes\": { \"sum\": { \"field\": \"bytes\" } }, \"moving_avg_of_sum_of_bytes\": { \"moving_avg\": { \"buckets_path\": \"sum_of_bytes\", \"model\": \"holt\", \"settings\": { \"alpha\": 0.6, \"beta\": 0.4 } } } } } } } Example response... \"aggregations\": { \"my_date_histogram\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"sum_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"sum_of_bytes\": { \"value\": 3.8880434E7 }, \"moving_avg_of_sum_of_bytes\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"sum_of_bytes\": { \"value\": 3.1445055E7 }, \"moving_avg_of_sum_of_bytes\": { \"value\": 2.70883404E7 } }] } } } serial_diff\nThe serial_diff aggregation is a parent pipeline aggregation that computes a series of value differences between a time lag of the buckets from previous aggregations.\nYou can use the serial_diff aggregation to find the data changes between time periods instead of finding the whole value.\nWith the lag parameter (a positive, non-zero integer value), you can tell which previous bucket to subtract from the current one. If you don’t specify the lag parameter, OpenSearch sets it to 1.\nLets say that the population of a city grows with time. If you use the serial differencing aggregation with the period of one day, you can see the daily growth. For example, you can compute a series of differences of the weekly average changes of a total price. GET opensearch_dashboards_sample_data_logs/_search { \"size\": 0, \"aggs\": { \"my_date_histogram\": { \"date_histogram\": { \"field\": \"@timestamp\", \"calendar_interval\": \"month\" }, \"aggs\": { \"the_sum\": { \"sum\": { \"field\": \"bytes\" } }, \"thirtieth_difference\": { \"serial_diff\": { \"buckets_path\": \"the_sum\", \"lag\": 30 } } } } } } Example response... \"aggregations\": { \"my_date_histogram\": { \"buckets\": [ { \"key_as_string\": \"2020-10-01T00:00:00.000Z\", \"key\": 1601510400000, \"doc_count\": 1635, \"the_sum\": { \"value\": 9400200.0 } }, { \"key_as_string\": \"2020-11-01T00:00:00.000Z\", \"key\": 1604188800000, \"doc_count\": 6844, \"the_sum\": { \"value\": 3.8880434E7 } }, { \"key_as_string\": \"2020-12-01T00:00:00.000Z\", \"key\": 1606780800000, \"doc_count\": 5595, \"the_sum\": { \"value\": 3.1445055E7 } }] } } }",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Aggregations"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/analyzers/language-analyzers/",
    "title": "Language analyzers",
    "content": "OpenSearch supports the following language values with the analyzer option:\narabic, armenian, basque, bengali, brazilian, bulgarian, catalan, czech, danish, dutch, english, estonian, finnish, french, galician, german, greek, hindi, hungarian, indonesian, irish, italian, latvian, lithuanian, norwegian, persian, portuguese, romanian, russian, sorani, spanish, swedish, turkish, and thai.\nTo use the analyzer when you map an index, specify the value within your query. For example, to map your index with the French language analyzer, specify the french value for the analyzer field: \"analyzer\": \"french\" Sample Request\nThe following query maps an index with the language analyzer set to french: PUT my-index -000001 { \"mappings\": { \"properties\": { \"text\": { \"type\": \"text\", \"fields\": { \"french\": { \"type\": \"text\", \"analyzer\": \"french\" } } } } } } <!-- TO do: each of the options needs its own section with an example. Convert table to individual sections, and then give a streamlined list with valid values. -->",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Text analyzers"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/analyzers/refresh-analyzer/",
    "title": "Refresh search analyzer",
    "content": "With ISM installed, you can refresh search analyzers in real time with the following API: POST /_plugins/_refresh_search_analyzers/&lt;index or alias or wildcard&gt; For example, if you change the synonym list in your analyzer, the change takes effect without you needing to close and reopen the index.\nTo work, the token filter must have an updateable flag of true: { \"analyzer\": { \"my_synonyms\": { \"tokenizer\": \"whitespace\", \"filter\": [ \"synonym\"] } }, \"filter\": { \"synonym\": { \"type\": \"synonym_graph\", \"synonyms_path\": \"synonyms.txt\", \"updateable\": true } } }",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Text analyzers"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/analyzers/text-analyzers/",
    "title": "Text analyzers",
    "content": "OpenSearch applies text analysis during indexing or searching for text fields. There is a standard analyzer that OpenSearch uses by default for text analysis. To optimize unstructured text for search, you can convert it into structured text with our text analyzers.\nText analyzers\nOpenSearch provides several text analyzers to convert your structured text into the format that works best for your searches.\nOpenSearch supports the following text analyzers: Standard analyzer – Parses strings into terms at word boundaries according to the Unicode text segmentation algorithm. It removes most, but not all, punctuation and converts strings to lowercase. You can remove stop words if you enable that option, but it does not remove stop words by default. Simple analyzer – Converts strings to lowercase and removes non-letter characters when it splits a string into tokens on any non-letter character. Whitespace analyzer – Parses strings into terms between each whitespace. Stop analyzer – Converts strings to lowercase and removes non-letter characters by splitting strings into tokens at each non-letter character. It also removes stop words (for example, “but” or “this”) from strings. Keyword analyzer – Receives a string as input and outputs the entire string as one term. Pattern analyzer – Splits strings into terms using regular expressions and supports converting strings to lowercase. It also supports removing stop words. Language analyzer – Provides analyzers specific to multiple languages. Fingerprint analyzer – Creates a fingerprint to use as a duplicate detector.\nThe full specialized text analyzers reference is in progress and will be published soon.\nHow to use text analyzers\nIf you want to use a text analyzer, specify the name of the analyzer for the analyzer field: standard, simple, whitespace, stop, keyword, pattern, fingerprint, or language.\nEach analyzer consists of one tokenizer and zero or more token filters. Different analyzers have different character filters, tokenizers, and token filters. To pre-process the string before the tokenizer is applied, you can use one or more character filters.\nExample: Specify the standard analyzer in a simple query GET _search { \"query\": { \"match\": { \"title\": \"A brief history of Time\", \"analyzer\": \"standard\" } } } Analyzer options Option Valid values Description analyzer standard, simple, whitespace, stop, keyword, pattern, language, fingerprint The analyzer you want to use for the query. Different analyzers have different character filters, tokenizers, and token filters. The stop analyzer, for example, removes stop words (for example, “an,” “but,” “this”) from the query string. For a full list of acceptable language values, see Language analyzer on this page. quote_analyzer String\nThis option lets you choose to use the standard analyzer without any options, such as language or other analyzers. Usage is \"quote_analyzer\": \"standard\". <!-- This is a list of the 7 individual new pages we need to write\nIf you want to select one of the text analyzers, see [Text analyzers reference](https://opensearch.org/docs/latest/opensearch/query-dsl/specialized-analyzers).\n## Specialized text analyzers\n1. Standard analyzer\n1. Simple\n1. Whitespace\n1. Stop\n1. Keyword\n1. Pattern\n1. Language\n1. Fingerprint\n-->",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/compound/bool/",
    "title": "Boolean queries",
    "content": "You can perform a Boolean query with the bool query type. A Boolean query compounds query clauses so you can combine multiple search queries with Boolean logic. To narrow or broaden your search results, use the bool query clause rules.\nAs a compound query type, bool allows you to construct an advanced query by combining several simple queries.\nUse the following rules to define how to combine multiple sub-query clauses within a bool query: Clause rule Behavior must Logical and operator. The results must match the queries in this clause. If you have multiple queries, all of them must match. must_not Logical not operator. All matches are excluded from the results. should Logical or operator. The results must match at least one of the queries, but, optionally, they can match more than one query. Each matching should clause increases the relevancy score. You can set the minimum number of queries that must match using the minimum_number_should_match parameter. minimum_number_should_match Optional parameter for use with a should query clause. Specifies the minimum number of queries that the document must match for it to be returned in the results. The default value is 1. filter Logical and operator that is applied first to reduce your dataset before applying the queries. A query within a filter clause is a yes or no option. If a document matches the query, it is returned in the results; otherwise, it is not. The results of a filter query are generally cached to allow for a faster return. Use the filter query to filter the results based on exact matches, ranges, dates, numbers, and so on. Boolean query structure\nThe structure of a Boolean query contains the bool query type followed by clause rules, as follows: GET _search { \"query\": { \"bool\": { \"must\": [ {}], \"must_not\": [ {}], \"should\": [ {}], \"filter\": {} } } } For example, assume you have the complete works of Shakespeare indexed in an OpenSearch cluster. You want to construct a single query that meets the following requirements:\nThe text_entry field must contain the word love and should contain either life or grace.\nThe speaker field must not contain ROMEO.\nFilter these results to the play Romeo and Juliet without affecting the relevancy score.\nUse the following query: GET shakespeare/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"text_entry\": \"love\" } }], \"should\": [ { \"match\": { \"text_entry\": \"life\" } }, { \"match\": { \"text_entry\": \"grace\" } }], \"minimum_should_match\": 1, \"must_not\": [ { \"match\": { \"speaker\": \"ROMEO\" } }], \"filter\": { \"term\": { \"play_name\": \"Romeo and Juliet\" } } } } } Sample output { \"took\": 12, \"timed_out\": false, \"_shards\": { \"total\": 4, \"successful\": 4, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 11.356054, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"88020\", \"_score\": 11.356054, \"_source\": { \"type\": \"line\", \"line_id\": 88021, \"play_name\": \"Romeo and Juliet\", \"speech_number\": 19, \"line_number\": \"4.5.61\", \"speaker\": \"PARIS\", \"text_entry\": \"O love! O life! not life, but love in death!\" } }] } } If you want to identify which of these clauses actually caused the matching results, name each query with the _name parameter.\nTo add the _name parameter, change the field name in the match query to an object: GET shakespeare/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"text_entry\": { \"query\": \"love\", \"_name\": \"love-must\" } } }], \"should\": [ { \"match\": { \"text_entry\": { \"query\": \"life\", \"_name\": \"life-should\" } } }, { \"match\": { \"text_entry\": { \"query\": \"grace\", \"_name\": \"grace-should\" } } }], \"minimum_should_match\": 1, \"must_not\": [ { \"match\": { \"speaker\": { \"query\": \"ROMEO\", \"_name\": \"ROMEO-must-not\" } } }], \"filter\": { \"term\": { \"play_name\": \"Romeo and Juliet\" } } } } } OpenSearch returns a matched_queries array that lists the queries that matched these results: \"matched_queries\": [ \"love-must\", \"life-should\"] If you remove the queries not in this list, you will still see the exact same result.\nBy examining which should clause matched, you can better understand the relevancy score of the results.\nYou can also construct complex Boolean expressions by nesting bool queries.\nFor example, to find a text_entry field that matches ( love OR hate) AND ( life OR grace) in the play Romeo and Juliet: GET shakespeare/_search { \"query\": { \"bool\": { \"must\": [ { \"bool\": { \"should\": [ { \"match\": { \"text_entry\": \"love\" } }, { \"match\": { \"text\": \"hate\" } }] } }, { \"bool\": { \"should\": [ { \"match\": { \"text_entry\": \"life\" } }, { \"match\": { \"text\": \"grace\" } }] } }], \"filter\": { \"term\": { \"play_name\": \"Romeo and Juliet\" } } } } } Sample output { \"took\": 10, \"timed_out\": false, \"_shards\": { \"total\": 2, \"successful\": 2, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 1, \"max_score\": 11.37006, \"hits\": [ { \"_index\": \"shakespeare\", \"_type\": \"doc\", \"_id\": \"88020\", \"_score\": 11.37006, \"_source\": { \"type\": \"line\", \"line_id\": 88021, \"play_name\": \"Romeo and Juliet\", \"speech_number\": 19, \"line_number\": \"4.5.61\", \"speaker\": \"PARIS\", \"text_entry\": \"O love! O life! not life, but love in death!\" } }] } }",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL",
      "Compound queries"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/compound/",
    "title": "Compound queries",
    "content": "Compound queries serve as wrappers for multiple leaf or compound clauses either to combine their results or to modify their behavior.\nOpenSearch supports the following compound query types: Boolean: Combines multiple query clauses with Boolean logic. To learn more, see Boolean queries. Constant score: Wraps a query or a filter and assigns a constant score to all matching documents. This score is equal to the boost value. Disjunction max: Returns documents that match one or more query clauses. If a document matches multiple query clauses, it is assigned a higher relevance score. The relevance score is calculated using the highest score from any matching clause and, optionally, the scores from the other matching clauses multiplied by the tiebreaker value. Function score: Recalculates the relevance score of documents that are returned by a query using a function that you define. Boosting: Changes the relevance score of documents without removing them from the search results. Returns documents that match a positive query, but downgrades the relevance of documents in the results that match a negative query.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/full-text/",
    "title": "Full-text queries",
    "content": "This page lists all full-text query types and common options. There are many optional fields that you can use to create subtle search behaviors, so we recommend that you test out some basic query types against representative indexes and verify the output before you perform more advanced or complex searches with multiple options.\nOpenSearch uses the Apache Lucene search library, which provides highly efficient data structures and algorithms for ingesting, indexing, searching, and aggregating data.\nTo learn more about search query classes, see Lucene query JavaDocs.\nThe full-text query types shown in this section use the standard analyzer, which analyzes text automatically when the query is submitted.\nYou can also analyze fields when you index them. To learn more about how to convert unstructured text into structured text that is optimized for search, see Optimizing text for searches with text analyzers.\n<!-- to do: rewrite query type definitions per issue: https://github.com/opensearch-project/documentation-website/issues/1116\n-->\nTable of contents Query types Match Multi-match Match Boolean prefix Match phrase Match phrase prefix Query string Simple query string Match all Advanced filter options Wildcard options Fuzzy query options Synonyms in a multiple terms search Other advanced options Common terms queries and the optional query field cutoff_frequency are now deprecated.\nQuery types\nOpenSearch Query DSL provides multiple query types that you can use in your searches.\nMatch\nUse the match query for full-text search of a specific document field. The match query analyzes the provided search string and returns documents that match any of the string’s terms.\nYou can use Boolean query operators to combine searches.\n<!-- we don't need to include Lucene query definitions >\nCreates a [boolean query](https://lucene.apache.org/core/8_9_0/core/org/apache/lucene/search/BooleanQuery.html) that returns results if the search term is present in the field.\n-->\nThe following example shows a basic match search for the title field set to the value wind: GET _search { \"query\": { \"match\": { \"title\": \"wind\" } } } For an example that uses curl, try: curl --insecure -XGET -u 'admin:admin' https://&lt;host&gt;:&lt;port&gt;/&lt;index&gt;/_search \\ -H \"content-type: application/json\" \\ -d '{\n\"query\": {\n\"match\": {\n\"title\": \"wind\"\n}\n}\n}' The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"match\": { \"title\": { \"query\": \"wind\", \"fuzziness\": \"AUTO\", \"fuzzy_transpositions\": true, \"operator\": \"or\", \"minimum_should_match\": 1, \"analyzer\": \"standard\", \"zero_terms_query\": \"none\", \"lenient\": false, \"prefix_length\": 0, \"max_expansions\": 50, \"boost\": 1 } } } } Multi-match\nYou can use the multi_match query type to search multiple fields. Multi-match operation functions similarly to the match operation.\nThe ^ lets you “boost” certain fields. Boosts are multipliers that weigh matches in one field more heavily than matches in other fields. In the following example, a match for “wind” in the title field influences _score four times as much as a match in the plot field. The result is that films like The Wind Rises and Gone with the Wind are near the top of the search results, and films like Twister and Sharknado, which presumably have “wind” in their plot summaries, are near the bottom. GET _search { \"query\": { \"multi_match\": { \"query\": \"wind\", \"fields\": [ \"title^4\", \"plot\"] } } } The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"multi_match\": { \"query\": \"wind\", \"fields\": [ \"title^4\", \"description\"], \"type\": \"most_fields\", \"operator\": \"and\", \"minimum_should_match\": 3, \"tie_breaker\": 0.0, \"analyzer\": \"standard\", \"boost\": 1, \"fuzziness\": \"AUTO\", \"fuzzy_transpositions\": true, \"lenient\": false, \"prefix_length\": 0, \"max_expansions\": 50, \"auto_generate_synonyms_phrase_query\": true, \"zero_terms_query\": \"none\" } } } Match Boolean prefix\nThe match_bool_prefix query analyzes the provided search string and creates a bool query from the string’s terms. It uses every term except the last term as a whole word for matching. The last term is used as a prefix. The match_bool_prefix query returns documents that contain either the whole-word terms or terms that start with the prefix term, in any order. GET _search { \"query\": { \"match_bool_prefix\": { \"title\": \"rises wi\" } } } The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"match_bool_prefix\": { \"title\": { \"query\": \"rises wi\", \"fuzziness\": \"AUTO\", \"fuzzy_transpositions\": true, \"max_expansions\": 50, \"prefix_length\": 0, \"operator\": \"or\", \"minimum_should_match\": 2, \"analyzer\": \"standard\" } } } } For more reference information about prefix queries, see the Lucene documentation.\nMatch phrase\nUse the match_phrase query to match documents that contain an exact phrase in a specified order. You can add flexibility to phrase matching by providing the slop parameter.\nCreates a phrase query that matches a sequence of terms. GET _search { \"query\": { \"match_phrase\": { \"title\": \"the wind rises\" } } } The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"match_phrase\": { \"title\": { \"query\": \"wind rises the\", \"slop\": 3, \"analyzer\": \"standard\", \"zero_terms_query\": \"none\" } } } } Match phrase prefix\nUse the match_phrase_prefix query to specify a phrase to match in order. The documents that contain the phrase you specify will be returned. The last partial term in the phrase is interpreted as a prefix, so any documents that contain phrases that begin with the phrase and prefix of the last term will be returned.\nSimilar to match phrase, but creates a prefix query out of the last term in the query string. GET _search { \"query\": { \"match_phrase_prefix\": { \"title\": \"the wind ri\" } } } The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"match_phrase_prefix\": { \"title\": { \"query\": \"the wind ri\", \"analyzer\": \"standard\", \"max_expansions\": 50, \"slop\": 3 } } } } <!-- Common terms query has been deprecated. Saving docs in-case we get a request to add it back later. See code deprecation https://github.com/opensearch-project/OpenSearch/blob/main/server/src/main/java/org/opensearch/index/query/CommonTermsQueryBuilder.java#L72-L73>\n## Common terms\nThe common terms query separates the query string into high- and low-frequency terms based on number of occurrences on the shard. Low-frequency terms are weighed more heavily in the results, and high-frequency terms are considered only for documents that already matched one or more low-frequency terms. In that sense, you can think of this query as having a built-in, ever-changing list of stop words.\n```json\nGET _search\n{\n\"query\": {\n\"common\": {\n\"title\": {\n\"query\": \"the wind rises\"\n}\n}\n}\n}\n```\nThe query accepts the following options. For descriptions of each, see [Advanced filter options](#advanced-filter-options).\n```json\nGET _search\n{\n\"query\": {\n\"common\": {\n\"title\": {\n\"query\": \"the wind rises\",\n\"cutoff_frequency\": 0.002,\n\"low_freq_operator\": \"or\",\n\"boost\": 1,\n\"analyzer\": \"standard\",\n\"minimum_should_match\": {\n\"low_freq\": 2,\n\"high_freq\": 3\n}\n}\n}\n}\n}\n```\n-->\nQuery string\nThe query string query splits text based on operators and analyzes each individually.\nIf you search using the HTTP request parameters (i.e. _search?q=wind), OpenSearch creates a query string query. GET _search { \"query\": { \"query_string\": { \"query\": \"the wind AND (rises OR rising)\" } } } The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"query_string\": { \"query\": \"the wind AND (rises OR rising)\", \"default_field\": \"title\", \"type\": \"best_fields\", \"fuzziness\": \"AUTO\", \"fuzzy_transpositions\": true, \"fuzzy_max_expansions\": 50, \"fuzzy_prefix_length\": 0, \"minimum_should_match\": 1, \"default_operator\": \"or\", \"analyzer\": \"standard\", \"lenient\": false, \"boost\": 1, \"allow_leading_wildcard\": true, \"enable_position_increments\": true, \"phrase_slop\": 3, \"max_determinized_states\": 10000, \"time_zone\": \"-08:00\", \"quote_field_suffix\": \"\", \"quote_analyzer\": \"standard\", \"analyze_wildcard\": false, \"auto_generate_synonyms_phrase_query\": true } } } Simple query string\nUse the simple_query_string type to specify directly in the query string multiple arguments delineated by regular expressions. Searches with this type will discard any invalid portions of the string. GET _search { \"query\": { \"simple_query_string\": { \"query\": \" \\\" rises wind the \\\" ~4 | *ising~2\", \"fields\": [ \"title\"] } } } Special character Behavior + Acts as the and operator. | Acts as the or operator. * Acts as a wildcard. \"\" Wraps several terms into a phrase. () Wraps a clause for precedence. ~n When used after a term (for example, wnid~3), sets fuzziness. When used after a phrase, sets slop. Advanced filter options. - Negates the term. The query accepts the following options. For descriptions of each, see Advanced filter options. GET _search { \"query\": { \"simple_query_string\": { \"query\": \" \\\" rises wind the \\\" ~4 | *ising~2\", \"fields\": [ \"title\"], \"flags\": \"ALL\", \"fuzzy_transpositions\": true, \"fuzzy_max_expansions\": 50, \"fuzzy_prefix_length\": 0, \"minimum_should_match\": 1, \"default_operator\": \"or\", \"analyzer\": \"standard\", \"lenient\": false, \"quote_field_suffix\": \"\", \"analyze_wildcard\": false, \"auto_generate_synonyms_phrase_query\": true } } } Match all\nThe match_all query type will return all documents. This type can be useful in testing large document sets if you need to return the entire set. GET _search { \"query\": { \"match_all\": {} } } <!-- need to research why a customer would need to match zero documents in a search >\n## Match none\nMatches no documents. Rarely useful.\n```json\nGET _search\n{\n\"query\": {\n\"match_none\": {}\n}\n}\n```\n-->\nAdvanced filter options\nYou can filter your query results by using some of the optional query fields, such as wildcards, fuzzy query fields, and synonyms. You can also use analyzers as optional query fields. To learn more, see How to use text analyzers.\nWildcard options Option Valid values Description allow_leading_wildcard Boolean\nWhether * and? are allowed as the first character of a search term. The default is true. analyze_wildcard Boolean\nWhether OpenSearch should attempt to analyze wildcard terms. Some analyzers do a poor job at this task, so the default is false. Fuzzy query options Option Valid values Description fuzziness AUTO, 0, or a positive integer\nThe number of character edits (insert, delete, substitute) that it takes to change one word to another when determining whether a term matched a value. For example, the distance between wined and wind is 1. The default, AUTO, chooses a value based on the length of each term and is a good choice for most use cases. fuzzy_transpositions Boolean\nSetting fuzzy_transpositions to true (default) adds swaps of adjacent characters to the insert, delete, and substitute operations of the fuzziness option. For example, the distance between wind and wnid is 1 if fuzzy_transpositions is true (swap “n” and “i”) and 2 if it is false (delete “n”, insert “n”). If fuzzy_transpositions is false, rewind and wnid have the same distance (2) from wind, despite the more human-centric opinion that wnid is an obvious typo. The default is a good choice for most use cases. fuzzy_max_expansions Positive integer\nFuzzy queries “expand to” a number of matching terms that are within the distance specified in fuzziness. Then OpenSearch tries to match those terms against its indexes. Synonyms in a multiple terms search\nYou can also use synonyms with the terms query type to search for multiple terms. Use the auto_generate_synonyms_phrase_query Boolean field. By default it is set to true. It automatically generates phrase queries for multiple term synonyms. For example, if you have the synonym \"ba, batting average\" and search for “ba,” OpenSearch searches for ba OR \"batting average\" when the option is true or ba OR (batting AND average) when the option is false.\nTo learn more about the multiple terms query type, see Terms. For more reference information about phrase queries, see the Lucene documentation.\nOther advanced options\nYou can also use the following optional query fields to filter your query results. Option Valid values Description boost Floating-point\nBoosts the clause by the given multiplier. Useful for weighing clauses in compound queries. The default is 1.0. enable_position_increments Boolean\nWhen true, result queries are aware of position increments. This setting is useful when the removal of stop words leaves an unwanted “gap” between terms. The default is true. fields String array\nThe list of fields to search (e.g. \"fields\": [\"title^4\", \"description\"]). If unspecified, defaults to the index.query.default_field setting, which defaults to [\"*\"]. flags String\nA | -delimited string of flags to enable (e.g., AND|OR|NOT). The default is ALL. You can explicitly set the value for default_field. For example, to return all titles, set it to \"default_field\": \"title\". lenient Boolean\nSetting lenient to true lets you ignore data type mismatches between the query and the document field. For example, a query string of “8.2” could match a field of type float. The default is false. low_freq_operator and, or The operator for low-frequency terms. The default is or. See also operator in this table. max_determinized_states Positive integer\nThe maximum number of “ states ” (a measure of complexity) that Lucene can create for query strings that contain regular expressions (e.g. \"query\": \"/wind.+?/\"). Larger numbers allow for queries that use more memory. The default is 10,000. max_expansions Positive integer max_expansions specifies the maximum number of terms to which the query can expand. The default is 50. minimum_should_match Positive or negative integer, positive or negative percentage, combination\nIf the query string contains multiple search terms and you used the or operator, the number of terms that need to match for the document to be considered a match. For example, if minimum_should_match is 2, “wind often rising” does not match “The Wind Rises.” If minimum_should_match is 1, it matches. operator or, and If the query string contains multiple search terms, whether all terms need to match ( and) or only one term needs to match ( or) for a document to be considered a match. phrase_slop 0 (default) or a positive integer\nSee slop. prefix_length 0 (default) or a positive integer\nThe number of leading characters that are not considered in fuzziness. quote_field_suffix String\nThis option lets you search different fields depending on whether terms are wrapped in quotes. For example, if quote_field_suffix is \".exact\" and you search for \"lightly\" (in quotes) in the title field, OpenSearch searches the title.exact field. This second field might use a different type (e.g. keyword rather than text) or a different analyzer. The default is null. rewrite constant_score, scoring_boolean, constant_score_boolean, top_terms_N, top_terms_boost_N, top_terms_blended_freqs_N Determines how OpenSearch rewrites and scores multi-term queries. The default is constant_score. slop 0 (default) or a positive integer\nControls the degree to which words in a query can be misordered and still be considered a match. From the Lucene documentation: “The number of other words permitted between words in query phrase. For example, to switch the order of two words requires two moves (the first move places the words atop one another), so to permit re-orderings of phrases, the slop must be at least two. A value of zero requires an exact match.” tie_breaker 0.0 (default) to 1.0 Changes the way OpenSearch scores searches. For example, a type of best_fields typically uses the highest score from any one field. If you specify a tie_breaker value between 0.0 and 1.0, the score changes to highest score + tie_breaker * score for all other matching fields. If you specify a value of 1.0, OpenSearch adds together the scores for all matching fields (effectively defeating the purpose of best_fields). time_zone UTC offset hours\nSpecifies the number of hours to offset the desired time zone from UTC. You need to indicate the time zone offset number if the query string contains a date range. For example, set time_zone\": \"-08:00\" for a query with a date range such as \"query\": \"wind rises release_date[2012-01-01 TO 2014-01-01]\"). The default time zone format used to specify number of offset hours is UTC. type best_fields, most_fields, cross_fields, phrase, phrase_prefix Determines how OpenSearch executes the query and scores the results. The default is best_fields. zero_terms_query none, all If the analyzer removes all terms from a query string, whether to match no documents (default) or all documents. For example, the stop analyzer removes all terms from the string “an but this.” <!-- cutoff_frequency is now deprecated. See https://github.com/opensearch-project/OpenSearch/blob/main/server/src/main/java/org/opensearch/index/query/MatchQueryBuilder.java#L61-L72 >\n`cutoff_frequency` | Between `0.0` and `1.0` or a positive integer | This value lets you define high and low frequency terms based on number of occurrences in the index. Numbers between 0 and 1 are treated as a percentage. For example, 0.10 is 10%. This value means that if a word occurs within the search field in more than 10% of the documents on the shard, OpenSearch considers the word \"high frequency\" and deemphasizes it when calculating search score. Because this setting is *per shard*, testing its impact on search results can be challenging unless a cluster has many documents. -->",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/full-text/query-string/",
    "title": "Query string queries",
    "content": "A query_string query parses the query string based on the query_string syntax. It lets you create powerful yet concise queries that can incorporate wildcards and search multiple fields.\nExample\nThe following query searches for the speaker KING in the play name that ends with well: GET shakespeare/_search { \"query\": { \"query_string\": { \"query\": \"speaker:KING AND play_name: *well\" } } } Parameters\nThe following table lists the parameters that query_string query supports. All parameters except query are optional. Parameter Data type Description query String\nThe query string to use for search. Required. fields String array\nThe list of fields to search (for example, \"fields\": [\"title^4\", \"description\"]). Supports wildcards. default_field String\nThe field in which to search if the field is not specified in the query string. Supports wildcards. Defaults to the value specified in the index.query.default_field index setting. By default, the index.query.default_field is *, which means extract all fields eligible for term query and filter the metadata fields. The extracted fields are combined into a query if the prefix is not specified. Eligible fields do not include nested documents. Searching all eligible fields could be a resource-intensive operation. The indices.query.bool.max_clause_count search setting defines the maximum value for the product of the number of fields and the number of terms that can be queried at one time. The default value for indices.query.bool.max_clause_count is 4,096. allow_leading_wildcard Boolean\nSpecifies whether * and? are allowed as first characters of a search term. Default is true. analyze_wildcard Boolean\nSpecifies whether OpenSearch should attempt to analyze wildcard terms. Default is false. analyzer String\nThe analyzer used to tokenize the query string text. Default is the index-time analyzer specified for the default_field. If no analyzer is specified for the default_field, the analyzer is the default analyzer for the index. quote_analyzer String\nThe analyzer used to tokenize quoted text in the query string. Overrides the analyzer parameter for quoted text. Default is the search_quote_analyzer specified for the default_field. quote_field_suffix String\nThis option lets you search for exact matches (surrounded with quotation marks) using a different analysis method than non-exact matches use. For example, if quote_field_suffix is.exact and you search for \\\"lightly\\\" in the title field, OpenSearch searches for the word lightly in the title.exact field. This second field might use a different type (for example, keyword rather than text) or a different analyzer. phrase_slop Integer\nThe maximum number of words that are allowed between the matched words. If phrase_slop is 2, a maximum of two words is allowed between matched words in a phrase. Transposed words have a slop of 2. Default is 0 (an exact phrase match where matched words must be next to each other). minimum_should_match Positive or negative integer, positive or negative percentage, combination\nIf the query string contains multiple search terms and you used the or operator, the number of terms that need to match for the document to be considered a match. For example, if minimum_should_match is 2, “wind often rising” does not match “The Wind Rises.” If minimum_should_match is 1, it matches. rewrite String\nDetermines how OpenSearch rewrites and scores multi-term queries. Valid values are constant_score, scoring_boolean, constant_score_boolean, top_terms_N, top_terms_boost_N, and top_terms_blended_freqs_N. Default is constant_score. auto_generate_synonyms_phrase_query Boolean\nSpecifies whether to create match queries automatically for multi-term synonyms. Default is true. boost Floating-point\nBoosts the clause by the given multiplier. Values less than 1.0 decrease relevance, and values greater than 1.0 increase relevance. Default is 1.0. default_operator String\nThe default Boolean operator used if no operators are specified. Valid values are: - OR: The string to be is interpreted as to OR be - AND: The string to be is interpreted as to AND be Default is OR. enable_position_increments Boolean\nWhen true, resulting queries are aware of position increments. This setting is useful when the removal of stop words leaves an unwanted “gap” between terms. Default is true. fuzziness String\nThe number of character edits (insert, delete, substitute) that it takes to change one word to another when determining whether a term matched a value. For example, the distance between wined and wind is 1. Valid values are non-negative integers or AUTO. The default, AUTO, chooses a value based on the length of each term and is a good choice for most use cases. fuzzy_transpositions Boolean\nSetting fuzzy_transpositions to true (default) adds swaps of adjacent characters to the insert, delete, and substitute operations of the fuzziness option. For example, the distance between wind and wnid is 1 if fuzzy_transpositions is true (swap “n” and “i”) and 2 if it is false (delete “n”, insert “n”). If fuzzy_transpositions is false, rewind and wnid have the same distance (2) from wind, despite the more human-centric opinion that wnid is an obvious typo. The default is a good choice for most use cases. fuzzy_max_expansions Positive integer\nThe maximum number of terms the fuzzy query will match. Default is 50. lenient Boolean\nSetting lenient to true lets you ignore data type mismatches between the query and the document field. For example, a query string of “8.2” could match a field of type float. Default is false. max_determinized_states Positive integer\nThe maximum number of “ states ” (a measure of complexity) that Lucene can create for query strings that contain regular expressions (for example, \"query\": \"/wind.+?/\"). Larger numbers allow for queries that use more memory. Default is 10,000. time_zone String\nSpecifies the number of hours to offset the desired time zone from UTC. You need to indicate the time zone offset number if the query string contains a date range. For example, set time_zone\": \"-08:00\" for a query with a date range such as \"query\": \"wind rises release_date[2012-01-01 TO 2014-01-01]\"). The default time zone format used to specify number of offset hours is UTC.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL",
      "Full-text queries"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/geo-and-xy/geo-bounding-box/",
    "title": "Geo-bounding box queries",
    "content": "To search for documents that contain geopoint fields, use a geo-bounding box query. The geo-bounding box query returns documents whose geopoints are within the bounding box specified in the query. A document with multiple geopoints matches the query if at least one geopoint is within the bounding box.\nExample\nYou can use a geo-bounding box query to search for documents that contain geopoints.\nCreate a mapping with the point field mapped as geo_point: PUT testindex 1 { \"mappings\": { \"properties\": { \"point\": { \"type\": \"geo_point\" } } } } Index three geopoints as objects with latitudes and longitudes: PUT testindex 1 /_doc/ 1 { \"point\": { \"lat\": 74.00, \"lon\": 40.71 } } PUT testindex 1 /_doc/ 2 { \"point\": { \"lat\": 72.64, \"lon\": 22.62 } } PUT testindex 1 /_doc/ 3 { \"point\": { \"lat\": 75.00, \"lon\": 28.00 } } Search for all documents and filter the documents whose points lie within the rectangle defined in the query: GET testindex 1 /_search { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"geo_bounding_box\": { \"point\": { \"top_left\": { \"lat\": 75, \"lon\": 28 }, \"bottom_right\": { \"lat\": 73, \"lon\": 41 } } } } } } } The response contains the matching document: { \"took\": 20, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"testindex1\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"point\": { \"lat\": 74.0, \"lon\": 40.71 } } }] } } The preceding response does not include the document with a geopoint of \"lat\": 75.00, \"lon\": 28.00 because of the geopoint’s limited precision.\nPrecision\nGeopoint coordinates are always rounded down at index time. At query time, the upper boundaries of the bounding box are rounded down, and the lower boundaries are rounded up. Therefore, the documents with geopoints that lie on the lower and left edges of the bounding box might not be included in the results due to rounding error. On the other hand, geopoints that lie on the upper and right edges of the bounding box might be included in the results even though they are outside the boundaries. The rounding error is less than 4.20 × 10 −8 degrees for latitude and less than 8.39 × 10 −8 degrees for longitude (around 1 cm).\nSpecifying the bounding box\nYou can specify the bounding box by providing any of the following combinations of its vertex coordinates: top_left and bottom_right top_right and bottom_left top, left, bottom, and right The following example shows how to specify the bounding box using the top, left, bottom, and right coordinates: GET testindex 1 /_search { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"geo_bounding_box\": { \"point\": { \"top\": 75, \"left\": 28, \"bottom\": 73, \"right\": 41 } } } } } } Request fields\nGeo-bounding box queries accept the following fields. Field Data type Description _name\nString\nThe name of the filter. Optional.\nvalidation_method\nString\nThe validation method. Valid values are IGNORE_MALFORMED (accept geopoints with invalid coordinates), COERCE (try to coerce coordinates to valid values), and STRICT (return an error when coordinates are invalid). Default is STRICT.\ntype\nString\nSpecifies how to execute the filter. Valid values are indexed (index the filter) and memory (execute the filter in memory). Default is memory.\nignore_unmapped\nBoolean\nSpecifies whether to ignore an unmapped field. If set to true, the query does not return any documents that have an unmapped field. If set to false, an exception is thrown when the field is unmapped. Default is false. Accepted formats\nYou can specify coordinates of the bounding box vertices in any format that the geopoint field type accepts.\nUsing a geohash to specify the bounding box\nIf you use a geohash to specify the bounding box, the geohash is treated as a rectangle. The upper-left vertex of the bounding box corresponds to the upper-left vertex of the top_left geohash, and the lower-right vertex of the bounding box corresponds to the lower-right vertex of the bottom_right geohash.\nThe following example shows how to use a geohash to specify the same bounding box as the previous examples: GET testindex 1 /_search { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"geo_bounding_box\": { \"point\": { \"top_left\": \"ut7ftjkfxm34\", \"bottom_right\": \"uuvpkcprc4rc\" } } } } } } To specify a bounding box that covers the whole area of a geohash, provide that geohash as both top_left and bottom_right parameters of the bounding box: GET testindex 1 /_search { \"query\": { \"bool\": { \"must\": { \"match_all\": {} }, \"filter\": { \"geo_bounding_box\": { \"point\": { \"top_left\": \"ut\", \"bottom_right\": \"ut\" } } } } } }",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL",
      "Geographic and xy queries"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/geo-and-xy/",
    "title": "Geographic and xy queries",
    "content": "Geographic and xy queries let you search fields that contain points and shapes on a map or coordinate plane. Geographic queries work on geospatial data, while xy queries work on two-dimensional coordinate data. Out of all geographic queries, the geoshape query is very similar to the xy query, but the former searches geographic fields, while the latter searches Cartesian fields.\nxy queries xy queries search for documents that contain geometries in a Cartesian coordinate system. These geometries can be specified in xy_point fields, which support points, and xy_shape fields, which support points, lines, circles, and polygons.\nxy queries return documents that contain:\nxy shapes and xy points that have one of four spatial relations to the provided shape: INTERSECTS, DISJOINT, WITHIN, or CONTAINS.\nxy points that intersect the provided shape.\nGeographic queries\nGeographic queries search for documents that contain geospatial geometries. These geometries can be specified in geo_point fields, which support points on a map, and geo_shape fields, which support points, lines, circles, and polygons.\nOpenSearch provides the following geographic query types: Geo-bounding box queries: Return documents with geopoint field values that are within a bounding box. Geodistance queries return documents with geopoints that are within a specified distance from the provided geopoint. Geopolygon queries return documents with geopoints that are within a polygon. Geoshape queries return documents that contain:\ngeoshapes and geopoints that have one of four spatial relations to the provided shape: INTERSECTS, DISJOINT, WITHIN, or CONTAINS.\ngeopoints that intersect the provided shape.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/geo-and-xy/xy/",
    "title": "xy queries",
    "content": "To search for documents that contain xy point and xy shape fields, use an xy query.\nSpatial relations\nWhen you provide an xy shape to the xy query, the xy fields are matched using the following spatial relations to the provided shape. Relation Description Supporting xy Field Type INTERSECTS (Default) Matches documents whose xy point or xy shape intersects the shape provided in the query. xy_point, xy_shape DISJOINT Matches documents whose xy shape does not intersect with the shape provided in the query. xy_shape WITHIN Matches documents whose xy shape is completely within the shape provided in the query. xy_shape CONTAINS Matches documents whose xy shape completely contains the shape provided in the query. xy_shape The following examples illustrate searching for documents that contain xy shapes. To learn how to search for documents that contain xy points, see the Querying xy points section.\nDefining the shape in an xy query\nYou can define the shape in an xy query either by providing a new shape definition at query time or by referencing the name of a shape pre-indexed in another index.\nUsing a new shape definition\nTo provide a new shape to an xy query, define it in the xy_shape field.\nThe following example illustrates searching for documents with xy shapes that match an xy shape defined at query time.\nFirst, create an index and map the geometry field as an xy_shape: PUT testindex { \"mappings\": { \"properties\": { \"geometry\": { \"type\": \"xy_shape\" } } } } Index a document with a point and a document with a polygon: PUT testindex/_doc/ 1 { \"geometry\": { \"type\": \"point\", \"coordinates\": [ 0.5, 3.0] } } PUT testindex/_doc/ 2 { \"geometry\": { \"type\": \"polygon\", \"coordinates\": [ [[ 2.5, 6.0], [ 0.5, 4.5], [ 1.5, 2.0], [ 3.5, 3.5], [ 2.5, 6.0]]] } } Define an envelope —a bounding rectangle in the [[minX, maxY], [maxX, minY]] format. Search for documents with xy points or shapes that intersect that envelope: GET testindex/_search { \"query\": { \"xy_shape\": { \"geometry\": { \"shape\": { \"type\": \"envelope\", \"coordinates\": [ [ 0.0, 6.0], [ 4.0, 2.0]] }, \"relation\": \"WITHIN\" } } } } The following image depicts the example. Both the point and the polygon are within the bounding envelope. The response contains both documents: { \"took\": 363, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 0.0, \"hits\": [ { \"_index\": \"testindex\", \"_id\": \"1\", \"_score\": 0.0, \"_source\": { \"geometry\": { \"type\": \"point\", \"coordinates\": [ 0.5, 3.0] } } }, { \"_index\": \"testindex\", \"_id\": \"2\", \"_score\": 0.0, \"_source\": { \"geometry\": { \"type\": \"polygon\", \"coordinates\": [ [ [ 2.5, 6.0], [ 0.5, 4.5], [ 1.5, 2.0], [ 3.5, 3.5], [ 2.5, 6.0]]] } } }] } } Using a pre-indexed shape definition\nWhen constructing an xy query, you can also reference the name of a shape pre-indexed in another index. Using this method, you can define an xy shape at index time and refer to it by name, providing the following parameters in the indexed_shape object. Parameter Description index\nThe name of the index that contains the pre-indexed shape.\nid\nThe document ID of the document that contains the pre-indexed shape.\npath\nThe field name of the field that contains the pre-indexed shape as a path. The following example illustrates referencing the name of a shape pre-indexed in another index. In this example, the index pre-indexed-shapes contains the shape that defines the boundaries, and the index testindex contains the shapes whose locations are checked against those boundaries.\nFirst, create an index pre-indexed-shapes and map the geometry field for this index as an xy_shape: PUT pre-indexed-shapes { \"mappings\": { \"properties\": { \"geometry\": { \"type\": \"xy_shape\" } } } } Index an envelope that specifies the boundaries and name it rectangle: PUT pre-indexed-shapes/_doc/rectangle { \"geometry\": { \"type\": \"envelope\", \"coordinates\": [ [ 0.0, 6.0], [ 4.0, 2.0]] } } Index a document with a point and a document with a polygon into the index testindex: PUT testindex/_doc/ 1 { \"geometry\": { \"type\": \"point\", \"coordinates\": [ 0.5, 3.0] } } PUT testindex/_doc/ 2 { \"geometry\": { \"type\": \"polygon\", \"coordinates\": [ [[ 2.5, 6.0], [ 0.5, 4.5], [ 1.5, 2.0], [ 3.5, 3.5], [ 2.5, 6.0]]] } } Search for documents with shapes that intersect rectangle in the index testindex using a filter: GET testindex/_search { \"query\": { \"bool\": { \"filter\": { \"xy_shape\": { \"geometry\": { \"indexed_shape\": { \"index\": \"pre-indexed-shapes\", \"id\": \"rectangle\", \"path\": \"geometry\" } } } } } } } The preceding query uses the default spatial relation INTERSECTS and returns both the point and the polygon: { \"took\": 26, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 0.0, \"hits\": [ { \"_index\": \"testindex\", \"_id\": \"1\", \"_score\": 0.0, \"_source\": { \"geometry\": { \"type\": \"point\", \"coordinates\": [ 0.5, 3.0] } } }, { \"_index\": \"testindex\", \"_id\": \"2\", \"_score\": 0.0, \"_source\": { \"geometry\": { \"type\": \"polygon\", \"coordinates\": [ [ [ 2.5, 6.0], [ 0.5, 4.5], [ 1.5, 2.0], [ 3.5, 3.5], [ 2.5, 6.0]]] } } }] } } Querying xy points\nYou can also use an xy query to search for documents that contain xy points.\nCreate a mapping with point as xy_point: PUT testindex 1 { \"mappings\": { \"properties\": { \"point\": { \"type\": \"xy_point\" } } } } Index three points: PUT testindex 1 /_doc/ 1 { \"point\": \"1.0, 1.0\" } PUT testindex 1 /_doc/ 2 { \"point\": \"2.0, 0.0\" } PUT testindex 1 /_doc/ 3 { \"point\": \"-2.0, 2.0\" } Search for points that lie within the circle with the center at (0, 0) and a radius of 2: GET testindex 1 /_search { \"query\": { \"xy_shape\": { \"point\": { \"shape\": { \"type\": \"circle\", \"coordinates\": [ 0.0, 0.0], \"radius\": 2 } } } } } xy point only supports the default INTERSECTS spatial relation, so you don’t need to provide the relation parameter.\nThe following image depicts the example. Points 1 and 2 are within the circle, and point 3 is outside the circle. The response returns documents 1 and 2: { \"took\": 575, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 0.0, \"hits\": [ { \"_index\": \"testindex1\", \"_id\": \"1\", \"_score\": 0.0, \"_source\": { \"point\": \"1.0, 1.0\" } }, { \"_index\": \"testindex1\", \"_id\": \"2\", \"_score\": 0.0, \"_source\": { \"point\": \"2.0, 0.0\" } }] } }",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL",
      "Geographic and xy queries"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/",
    "title": "Query DSL",
    "content": "OpenSearch provides a search language called query domain-specific language (DSL) that you can use to search your data. Query DSL is a flexible language with a JSON interface.\nWith query DSL, you need to specify a query in the query parameter of the search. One of the simplest searches in OpenSearch uses the match_all query, which matches all documents in an index: GET testindex/_search { \"query\": { \"match_all\": { } } } A query can consist of many query clauses. You can combine query clauses to produce complex queries.\nBroadly, you can classify queries into two categories— leaf queries and compound queries: Leaf queries: Leaf queries search for a specified value in a certain field or fields. You can use leaf queries on their own. They include the following query types: Full-text queries: Use full-text queries to search text documents. For an analyzed text field search, full-text queries split the query string into terms with the same analyzer that was used when the field was indexed. For an exact value search, full-text queries look for the specified value without applying text analysis. To learn more, see Full-text queries. Term-level queries: Use term-level queries to search documents for an exact specified term, such as an ID or value range. Term-level queries do not analyze search terms or sort results by relevance score. To learn more, see Term-level queries. Geographic and xy queries: Use geographic queries to search documents that include geographic data. Use xy queries to search documents that include points and shapes in a two-dimensional coordinate system. To learn more, see Geographic and xy queries. Joining queries: Use joining queries to search nested fields or return parent and child documents that match a specific query. Types of joining queries include nested, has_child, has_parent, and parent_id queries. Span queries: Use span queries to perform precise positional searches. Span queries are low-level, specific queries that provide control over the order and proximity of specified query terms. They are primarily used to search legal documents. To learn more, see Span queries. Specialized queries: Specialized queries include all other query types ( distance_feature, more_like_this, percolate, rank_feature, script, script_score, wrapper, and pinned_query). Compound queries: Compound queries serve as wrappers for multiple leaf or compound clauses either to combine their results or to modify their behavior. They include the Boolean, disjunction max, constant score, function score, and boosting query types. To learn more, see Compound queries.\nA note on Unicode special characters in text fields\nDue to word boundaries associated with Unicode special characters, the Unicode standard analyzer cannot index a text field type value as a whole value when it includes one of these special characters. As a result, a text field value that includes a special character is parsed by the standard analyzer as multiple values separated by the special character, effectively tokenizing the different elements on either side of it. This can lead to unintentional filtering of documents and potentially compromise control over their access.\nThe examples below illustrate values containing special characters that will be parsed improperly by the standard analyzer. In this example, the existence of the hyphen/minus sign in the value prevents the analyzer from distinguishing between the two different users for user.id and interprets them as one and the same: { \"bool\": { \"must\": { \"match\": { \"user.id\": \"User-1\" } } } } { \"bool\": { \"must\": { \"match\": { \"user.id\": \"User-2\" } } } } To avoid this circumstance when using either query DSL or the REST API, you can use a custom analyzer or map the field as keyword, which performs an exact-match search. See Keyword field type for the latter option.\nFor a list of characters that should be avoided for text field types, see Word Boundaries.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/query-filter-context/",
    "title": "Query and filter context",
    "content": "Queries consist of query clauses, which can be run in a filter context or query context. A query clause in a filter context asks the question “ Does the document match the query clause?” and returns matching documents. A query clause in a query context asks the question “ How well does the document match the query clause?”, returns matching documents, and provides the relevance of each document in the form of a relevance score.\nRelevance score\nA relevance score measures how well a document matches a query. It is a positive floating-point number that OpenSearch records in the _score metadata field for each document: \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"32437\", \"_score\": 18.781435, \"_source\": { \"type\": \"line\", \"line_id\": 32438, \"play_name\": \"Hamlet\", \"speech_number\": 3, \"line_number\": \"1.1.3\", \"speaker\": \"BERNARDO\", \"text_entry\": \"Long live the king!\" } },... A higher score indicates a more relevant document. While different query types calculate relevance scores differently, all query types take into account whether a query clause is run in a filter or query context.\nUse query clauses that you want to affect the relevance score in a query context, and use all other query clauses in a filter context.\nFilter context\nA query clause in a filter context asks the question “ Does the document match the query clause?”, which has a binary answer. For example, if you have an index with student data, you might use a filter context to answer the following questions about a student:\nIs the student’s honors status set to true?\nIs the student’s graduation_year in the 2020–2022 range?\nWith a filter context, OpenSearch returns matching documents without calculating a relevance score. Thus, you should use a filter context for fields with exact values.\nTo run a query clause in a filter context, pass it to a filter parameter. For example, the following Boolean query searches for students who graduated with honors in 2020–2022: GET students/_search { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"honors\": true }}, { \"range\": { \"graduation_year\": { \"gte\": 2020, \"lte\": 2022 }}}] } } } To improve performance, OpenSearch caches frequently used filters.\nQuery context\nA query clause in a query context asks the question “ How well does the document match the query clause?”, which does not have a binary answer. A query context is suitable for a full-text search, where you not only want to receive matching documents but also to determine the relevance of each document. For example, if you have an index with the complete works of Shakespeare, you might use a query context for the following searches:\nFind documents that contain the word dream, including its various forms ( dreaming or dreams) and synonyms ( contemplate).\nFind documents that match the words long live king.\nWith a query context, every matching document contains a relevance score in the _score field, which you can use to sort documents by relevance.\nTo run a query clause in a query context, pass it to a query parameter. For example, the following query searches for documents that match the words long live king in the shakespeare index: GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"long live king\" } } } Relevance scores are single-precision floating-point numbers with 24-bit significand precision. A loss of precision may occur if a score calculation exceeds the significand precision.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/span-query/",
    "title": "Span queries",
    "content": "You can use span queries to perform precise positional searches. Span queries are low-level, specific queries that provide control over the order and proximity of specified query terms. They are primarily used to search legal documents and patents.\nSpan queries include the following query types: Span containing: Wraps a list of span queries and only returns spans that match a second span query. Span field masking: Combines span_near or span_or across different fields. Span first: Matches spans close to the beginning of the field. Span multi-term: Provides a wrapper around the following query types: term, range, prefix, wildcard, regexp or fuzzy. Span near: Matches spans that are near each other. Wraps multiple span queries that must match within the specified slop distance of each other, and optionally in the same order. Slop represents the maximum number of intervening unmatched positions and indicates whether matches are required to be returned in order. Span not: Provides a wrapper for another span query and excludes any documents that match the internal query. Span or: Provides a wrapper for multiple span queries and includes any documents that match any of the specified queries. Span term: Functions in the same way as a term query, but is designed to be used with other span queries. Span within: Used with other span queries to return a single span query if its span is within the spans that are returned by a list of other span queries.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/term-vs-full-text/",
    "title": "Term-level and full-text queries compared",
    "content": "You can use both term-level and full-text queries to search text, but while term-level queries are usually used to search structured data, full-text queries are used for full-text search. The main difference between term-level and full-text queries is that term-level queries search documents for an exact specified term, while full-text queries analyze the query string. The following table summarizes the differences between term-level and full-text queries.   Term-level queries Full-text queries Description Term-level queries answer which documents match a query.\nFull-text queries answer how well the documents match a query. Analyzer The search term isn’t analyzed. This means that the term query searches for your search term as it is.\nThe search term is analyzed by the same analyzer that was used for the specific document field at the time it was indexed. This means that your search term goes through the same analysis process as the document’s field. Relevance Term-level queries simply return documents that match without sorting them based on the relevance score. They still calculate the relevance score, but this score is the same for all the documents that are returned.\nFull-text queries calculate a relevance score for each match and sort the results by decreasing order of relevance. Use Case Use term-level queries when you want to match exact values such as numbers, dates, or tags and don’t need the matches to be sorted by relevance.\nUse full-text queries to match text fields and sort by relevance after taking into account factors like casing and stemming variants. OpenSearch uses the BM25 ranking algorithm to calculate relevance scores. To learn more, see Okapi BM25.\nShould I use a full-text or a term-level query?\nTo clarify the difference between full-text and term-level queries, consider the following two examples that search for a specific text phrase. The complete works of Shakespeare are indexed in an OpenSearch cluster.\nExample: Phrase search\nIn this example, you’ll search the complete works of Shakespeare for the phrase “To be, or not to be” in the text_entry field.\nFirst, use a term-level query for this search: GET shakespeare/_search { \"query\": { \"term\": { \"text_entry\": \"To be, or not to be\" } } } The response contains no matches, indicated by zero hits: { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] } } This is because the term “To be, or not to be” is searched literally in the inverted index, where only the analyzed values of the text fields are stored. Term-level queries aren’t suited for searching analyzed text fields because they often yield unexpected results. When working with text data, use term-level queries only for fields mapped as keyword.\nNow search for the same phrase using a full-text query: GET shakespeare/_search { \"query\": { \"match\": { \"text_entry\": \"To be, or not to be\" } } } The search query “To be, or not to be” is analyzed and tokenized into an array of tokens just like the text_entry field of the documents. The full-text query takes an intersection of tokens between the search query and the text_entry fields for all the documents, and then sorts the results by relevance score: { \"took\": 19, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 10000, \"relation\": \"gte\" }, \"max_score\": 17.419369, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"34229\", \"_score\": 17.419369, \"_source\": { \"type\": \"line\", \"line_id\": 34230, \"play_name\": \"Hamlet\", \"speech_number\": 19, \"line_number\": \"3.1.64\", \"speaker\": \"HAMLET\", \"text_entry\": \"To be, or not to be: that is the question:\" } }, { \"_index\": \"shakespeare\", \"_id\": \"109930\", \"_score\": 14.883024, \"_source\": { \"type\": \"line\", \"line_id\": 109931, \"play_name\": \"A Winters Tale\", \"speech_number\": 23, \"line_number\": \"4.4.153\", \"speaker\": \"PERDITA\", \"text_entry\": \"Not like a corse; or if, not to be buried,\" } }, { \"_index\": \"shakespeare\", \"_id\": \"103117\", \"_score\": 14.782743, \"_source\": { \"type\": \"line\", \"line_id\": 103118, \"play_name\": \"Twelfth Night\", \"speech_number\": 53, \"line_number\": \"1.3.95\", \"speaker\": \"SIR ANDREW\", \"text_entry\": \"will not be seen; or if she be, its four to one\" } }] } }... For a list of all full-text queries, see Full-text queries.\nExample: Exact term search\nIf you want to search for an exact term like “HAMLET” in the speaker field and don’t need the results to be sorted by relevance score, a term-level query is more efficient: GET shakespeare/_search { \"query\": { \"term\": { \"speaker\": \"HAMLET\" } } } The response contains document matches: { \"took\": 5, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1582, \"relation\": \"eq\" }, \"max_score\": 4.2540946, \"hits\": [ { \"_index\": \"shakespeare\", \"_id\": \"32700\", \"_score\": 4.2540946, \"_source\": { \"type\": \"line\", \"line_id\": 32701, \"play_name\": \"Hamlet\", \"speech_number\": 9, \"line_number\": \"1.2.66\", \"speaker\": \"HAMLET\", \"text_entry\": \"[Aside] A little more than kin, and less than kind.\" } }, { \"_index\": \"shakespeare\", \"_id\": \"32702\", \"_score\": 4.2540946, \"_source\": { \"type\": \"line\", \"line_id\": 32703, \"play_name\": \"Hamlet\", \"speech_number\": 11, \"line_number\": \"1.2.68\", \"speaker\": \"HAMLET\", \"text_entry\": \"Not so, my lord; I am too much i' the sun.\" } }, { \"_index\": \"shakespeare\", \"_id\": \"32709\", \"_score\": 4.2540946, \"_source\": { \"type\": \"line\", \"line_id\": 32710, \"play_name\": \"Hamlet\", \"speech_number\": 13, \"line_number\": \"1.2.75\", \"speaker\": \"HAMLET\", \"text_entry\": \"Ay, madam, it is common.\" } }] } }... The term-level queries provide exact matches. So if you search for “Hamlet”, you don’t receive any matches, because “HAMLET” is a keyword field and is stored in OpenSearch literally and not in an analyzed form.\nThe search query “HAMLET” is also searched literally. So to get a match for this field, we need to enter the exact same characters.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/query-dsl/term/",
    "title": "Term-level queries",
    "content": "Term-level queries search an index for documents that contain an exact search term. Documents returned by a term-level query are not sorted by their relevance scores.\nWhen working with text data, use term-level queries for fields mapped as keyword only.\nTerm-level queries are not suited for searching analyzed text fields. To return analyzed fields, use a full-text query.\nTerm-level query types\nThe following table lists all term-level query types. Query type Description term Searches for documents with an exact term in a specific field. terms Searches for documents with one or more terms in a specific field. terms_set Searches for documents that match a minimum number of terms in a specific field. ids Searches for documents by document ID. range Searches for documents with field values in a specific range. prefix Searches for documents with terms that begin with a specific prefix. exists Searches for documents with any indexed value in a specific field. fuzzy Searches for documents with terms that are similar to the search term within the maximum allowed Levenshtein distance. The Levenshtein distance measures the number of one-character changes needed to change one term to another term. wildcard Searches for documents with terms that match a wildcard pattern. regexp Searches for documents with terms that match a regular expression. Term\nUse the term query to search for an exact term in a field. GET shakespeare/_search { \"query\": { \"term\": { \"line_id\": { \"value\": \"61809\" } } } } copy Terms\nUse the terms query to search for multiple terms in the same field. GET shakespeare/_search { \"query\": { \"terms\": { \"line_id\": [ \"61809\", \"61810\"] } } } copy You get back documents that match any of the terms.\nTerms set\nWith a terms set query, you can search for documents that match a minimum number of exact terms in a specified field. The terms_set query is similar to the terms query, but you can specify the minimum number of matching terms that are required to return a document. You can specify this number either in a field in the index or with a script.\nAs an example, consider an index that contains students with classes they have taken. When setting up the mapping for this index, you need to provide a numeric field that specifies the minimum number of matching terms that are required to return a document: PUT students { \"mappings\": { \"properties\": { \"name\": { \"type\": \"keyword\" }, \"classes\": { \"type\": \"keyword\" }, \"min_required\": { \"type\": \"integer\" } } } } copy Next, index two documents that correspond to students: PUT students/_doc/ 1 { \"name\": \"Mary Major\", \"classes\": [ \"CS101\", \"CS102\", \"MATH101\"], \"min_required\": 2 } copy PUT students/_doc/ 2 { \"name\": \"John Doe\", \"classes\": [ \"CS101\", \"MATH101\", \"ENG101\"], \"min_required\": 2 } copy Now search for students who have taken at least two of the following classes: CS101, CS102, MATH101: GET students/_search { \"query\": { \"terms_set\": { \"classes\": { \"terms\": [ \"CS101\", \"CS102\", \"MATH101\"], \"minimum_should_match_field\": \"min_required\" } } } } copy The response contains both students: { \"took\": 44, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 1.4544616, \"hits\": [ { \"_index\": \"students\", \"_id\": \"1\", \"_score\": 1.4544616, \"_source\": { \"name\": \"Mary Major\", \"classes\": [ \"CS101\", \"CS102\", \"MATH101\"], \"min_required\": 2 } }, { \"_index\": \"students\", \"_id\": \"2\", \"_score\": 0.5013843, \"_source\": { \"name\": \"John Doe\", \"classes\": [ \"CS101\", \"MATH101\", \"ENG101\"], \"min_required\": 2 } }] } } To specify the minimum number of terms a document should match with a script, provide the script in the minimum_should_match_script field: GET students/_search { \"query\": { \"terms_set\": { \"classes\": { \"terms\": [ \"CS101\", \"CS102\", \"MATH101\"], \"minimum_should_match_script\": { \"source\": \"Math.min(params.num_terms, doc['min_required'].value)\" } } } } } copy IDs\nUse the ids query to search for one or more document ID values. GET shakespeare/_search { \"query\": { \"ids\": { \"values\": [ 34229, 91296] } } } copy Range\nYou can search for a range of values in a field with the range query.\nTo search for documents where the line_id value is &gt;= 10 and &lt;= 20: GET shakespeare/_search { \"query\": { \"range\": { \"line_id\": { \"gte\": 10, \"lte\": 20 } } } } copy Parameter Behavior gte Greater than or equal to. gt Greater than. lte Less than or equal to. lt Less than. In addition to the range query parameters, you can provide date formats or relation operators such as “contains” or “within.” To see the supported field types for range queries, see Range query optional parameters. To see all date formats, see Formats.\nAssume that you have a products index and you want to find all the products that were added in the year 2019: GET products/_search { \"query\": { \"range\": { \"created\": { \"gte\": \"2019/01/01\", \"lte\": \"2019/12/31\" } } } } copy Specify relative dates by using date math.\nTo subtract 1 year and 1 day from the specified date, use the following query: GET products/_search { \"query\": { \"range\": { \"created\": { \"gte\": \"2019/01/01||-1y-1d\" } } } } copy The first date that we specify is the anchor date or the starting point for the date math. Add two trailing pipe symbols. You could then add one day ( +1d) or subtract two weeks ( -2w). This math expression is relative to the anchor date that you specify.\nYou could also round off dates by adding a forward slash to the date or time unit.\nTo find products added in the last year and rounded off by month: GET products/_search { \"query\": { \"range\": { \"created\": { \"gte\": \"now-1y/M\" } } } } copy The keyword now refers to the current date and time.\nPrefix\nUse the prefix query to search for terms that begin with a specific prefix. GET shakespeare/_search { \"query\": { \"prefix\": { \"speaker\": \"KING\" } } } copy Exists\nUse the exists query to search for documents that contain a specific field. GET shakespeare/_search { \"query\": { \"exists\": { \"field\": \"speaker\" } } } copy Fuzzy\nA fuzzy query searches for documents with terms that are similar to the search term within the maximum allowed Levenshtein distance. The Levenshtein distance measures the number of one-character changes needed to change one term to another term. These changes include:\nReplacements: c at to b at\nInsertions: cat to cat s Deletions: c at to at\nTranspositions: ca t to ac t\nA fuzzy query creates a list of all possible expansions of the search term that fall within the Levenshtein distance. You can specify the maximum number of such expansions in the max_expansions field. Then is searches for documents that match any of the expansions.\nThe following example query searches for the speaker HALET (misspelled HAMLET). The maximum edit distance is not specified, so the default AUTO edit distance is used: GET shakespeare/_search { \"query\": { \"fuzzy\": { \"speaker\": { \"value\": \"HALET\" } } } } copy The response contains all documents where HAMLET is the speaker.\nThe following example query searches for the word cat with advanced parameters: GET shakespeare/_search { \"query\": { \"fuzzy\": { \"speaker\": { \"value\": \"HALET\", \"fuzziness\": \"2\", \"max_expansions\": 40, \"prefix_length\": 0, \"transpositions\": true, \"rewrite\": \"constant_score\" } } } } copy Wildcard\nUse wildcard queries to search for terms that match a wildcard pattern. Feature Behavior * Specifies all valid values.? Specifies a single valid value. To search for terms that start with H and end with Y: GET shakespeare/_search { \"query\": { \"wildcard\": { \"speaker\": { \"value\": \"H*Y\" } } } } copy If we change * to?, we get no matches, because? refers to a single character.\nWildcard queries tend to be slow because they need to iterate over a lot of terms. Avoid placing wildcard characters at the beginning of a query because it could be a very expensive operation in terms of both resources and time.\nRegexp\nUse the regexp query to search for terms that match a regular expression.\nThis regular expression matches any single uppercase or lowercase letter: GET shakespeare/_search { \"query\": { \"regexp\": { \"play_name\": \"[a-zA-Z]amlet\" } } } copy A few important notes:\nRegular expressions are applied to the terms in the field (i.e. tokens), not the entire field.\nRegular expressions use the Lucene syntax, which differs from more standardized implementations. Test thoroughly to ensure that you receive the results you expect. To learn more, see the Lucene documentation. regexp queries can be expensive operations and require the search.allow_expensive_queries setting to be set to true. Before making frequent regexp queries, test their impact on cluster performance and examine alternative queries for achieving similar results.",
    "ancestors": [
      "Query DSL, Aggregations, and Analyzers",
      "Query DSL"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/alias/",
    "title": "Alias",
    "content": "An alias field type creates another name for an existing field. You can use aliases in the search and field capabilities API operations, with some exceptions. To set up an alias, you need to specify the original field name in the path parameter.\nExample PUT movies { \"mappings\": { \"properties\": { \"year\": { \"type\": \"date\" }, \"release_date\": { \"type\": \"alias\", \"path\": \"year\" } } } } copy Parameters Parameter Description path The full path to the original field, including all parent objects. For example, parent.child.field_name. Required. Alias field\nAlias fields must obey the following rules:\nAn alias field can only have one original field.\nIn nested objects, the alias must have the same nesting level as the original field.\nTo change the field that the alias references, update the mappings. Note that aliases in any previously stored percolator queries will still reference the original field.\nOriginal field\nThe original field for an alias must obey the following rules:\nThe original field must be created before the alias is created.\nThe original field cannot be an object or another alias.\nUsing aliases in search API operations\nYou can use aliases in the following read operations of the search API:\nQueries\nSorts\nAggregations stored_fields docvalue_fields Suggestions\nHighlights\nScripts that access field values\nUsing aliases in field capabilities API operations\nTo use an alias in the field capabilities API, specify it in the fields parameter. GET movies/_field_caps?fields=release_date copy Exceptions\nYou cannot use aliases in the following situations:\nIn write requests, such as update requests.\nIn multi-fields or as a target of copy_to.\nAs a _source parameter for filtering results.\nIn APIs that take field names, such as term vectors.\nIn terms, more_like_this, and geo_shape queries (aliases are not supported when retrieving documents).\nWildcards\nIn search and field capabilities wildcard queries, both the original field and the alias are matched against the wildcard pattern. GET movies/_field_caps?fields=release* copy",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/autocomplete/",
    "title": "Autocomplete field types",
    "content": "The following table lists all autocomplete field types that OpenSearch supports. Field data type Description completion A completion suggester that provides autocomplete functionality using prefix completion. You need to upload a list of all possible completions into the index before using this feature. search_as_you_type Provides search-as-you-type functionality using both prefix and infix completion.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/binary/",
    "title": "Binary",
    "content": "A binary field type contains a binary value in Base64 encoding that is not searchable.\nExample\nCreate a mapping with a binary field: PUT testindex { \"mappings\": { \"properties\": { \"binary_value\": { \"type\": \"binary\" } } } } copy Index a document with a binary value: PUT testindex/_doc/ 1 { \"binary_value\": \"bGlkaHQtd29rfx4=\" } copy Use = as a padding character. Embedded newline characters are not allowed.\nParameters\nThe following table lists the parameters accepted by binary field types. All parameters are optional. Parameter Description doc_values A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Optional. Default is false. store A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Optional. Default is false.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/boolean/",
    "title": "Boolean",
    "content": "A Boolean field type takes true or false values, or \"true\" or \"false\" strings. You can also pass an empty string ( \"\") in place of a false value.\nExample\nCreate a mapping where a, b, and c are Boolean fields: PUT testindex { \"mappings\": { \"properties\": { \"a\": { \"type\": \"boolean\" }, \"b\": { \"type\": \"boolean\" }, \"c\": { \"type\": \"boolean\" } } } } copy Index a document with Boolean values: PUT testindex/_doc/ 1 { \"a\": true, \"b\": \"true\", \"c\": \"\" } copy As a result, a and b will be set to true, and c will be set to false.\nSearch for all documents where c is false: GET testindex/_search { \"query\": { \"term\": { \"c\": false } } } copy Parameters\nThe following table lists the parameters accepted by Boolean field types. All parameters are optional. Parameter Description boost A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. doc_values A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting or scripting. Default is false. index A Boolean value that specifies whether the field should be searchable. Default is true. meta Accepts metadata for this field. null_value A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. store A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. Boolean values in aggregations and scripts\nIn aggregations on Boolean fields, key returns numeric values (1 for true or 0 for false), and key_as_string returns strings ( \"true\" or \"false\"). Scripts return true and false for Boolean values.\nExample\nRun a terms aggregation query on the field a: GET testindex/_search { \"aggs\": { \"agg1\": { \"terms\": { \"field\": \"a\" } } }, \"script_fields\": { \"a\": { \"script\": { \"lang\": \"painless\", \"source\": \"doc['a'].value\" } } } } copy The script returns the value of a as true, key returns the value of a as 1, and key_as_string returns the value of a as \"true\": { \"took\": 1133, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"testindex\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 1.0, \"fields\": { \"a\": [ true] } }] }, \"aggregations\": { \"agg1\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": 1, \"key_as_string\": \"true\", \"doc_count\": 1 }] } } }",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/completion/",
    "title": "Completion",
    "content": "A completion field type provides autocomplete functionality through a completion suggester. The completion suggester is a prefix suggester, so it matches the beginning of text only. A completion suggester creates an in-memory data structure, which provides faster lookups but leads to increased memory usage. You need to upload a list of all possible completions into the index before using this feature.\nExample\nCreate a mapping with a completion field: PUT chess_store { \"mappings\": { \"properties\": { \"suggestions\": { \"type\": \"completion\" }, \"product\": { \"type\": \"keyword\" } } } } copy Index suggestions into OpenSearch: PUT chess_store/_doc/ 1 { \"suggestions\": { \"input\": [ \"Books on openings\", \"Books on endgames\"], \"weight\": 10 } } copy Parameters\nThe following table lists the parameters accepted by completion fields. Parameter Description input A list of possible completions as a string or array of strings. Cannot contain \\u0000 (null), \\u001f (information separator one), or \\u001e (information separator two). Required. weight A positive integer or a positive integer string for ranking suggestions. Optional. Multiple suggestions can be indexed as follows: PUT chess_store/_doc/ 2 { \"suggestions\": [ { \"input\": \"Chess set\", \"weight\": 20 }, { \"input\": \"Chess pieces\", \"weight\": 10 }, { \"input\": \"Chess board\", \"weight\": 5 }] } copy As an alternative, you can use the following shorthand notation (note that you cannot provide the weight parameter in this notation): PUT chess_store/_doc/ 3 { \"suggestions\": [ \"Chess clock\", \"Chess timer\"] } copy Querying completion field types\nTo query completion field types, specify the prefix that you want to search for and the name of the field in which to look for suggestions.\nQuery the index for suggestions that start with the word “chess”: GET chess_store/_search { \"suggest\": { \"product-suggestions\": { \"prefix\": \"chess\", \"completion\": { \"field\": \"suggestions\" } } } } copy The response contains autocomplete suggestions: { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"product-suggestions\": [ { \"text\": \"chess\", \"offset\": 0, \"length\": 5, \"options\": [ { \"text\": \"Chess set\", \"_index\": \"chess_store\", \"_type\": \"_doc\", \"_id\": \"2\", \"_score\": 20.0, \"_source\": { \"suggestions\": [ { \"input\": \"Chess set\", \"weight\": 20 }, { \"input\": \"Chess pieces\", \"weight\": 10 }, { \"input\": \"Chess board\", \"weight\": 5 }] } }, { \"text\": \"Chess clock\", \"_index\": \"chess_store\", \"_type\": \"_doc\", \"_id\": \"3\", \"_score\": 1.0, \"_source\": { \"suggestions\": [ \"Chess clock\", \"Chess timer\"] } }] }] } } In the response, the _score field contains the value of the weight parameter that was set up at index time. The text field is populated with the suggestion’s input parameter.\nBy default, the response contains the whole document, including the _source field, which may impact performance. To return only the suggestions field, you can specify that in the _source parameter. You can also restrict the number of returned suggestions by specifying the size parameter. GET chess_store/_search { \"_source\": \"suggestions\", \"suggest\": { \"product-suggestions\": { \"prefix\": \"chess\", \"completion\": { \"field\": \"suggestions\", \"size\": 3 } } } } copy The response contains the suggestions: { \"took\": 5, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"product-suggestions\": [ { \"text\": \"chess\", \"offset\": 0, \"length\": 5, \"options\": [ { \"text\": \"Chess set\", \"_index\": \"chess_store\", \"_type\": \"_doc\", \"_id\": \"2\", \"_score\": 20.0, \"_source\": { \"suggestions\": [ { \"input\": \"Chess set\", \"weight\": 20 }, { \"input\": \"Chess pieces\", \"weight\": 10 }, { \"input\": \"Chess board\", \"weight\": 5 }] } }, { \"text\": \"Chess clock\", \"_index\": \"chess_store\", \"_type\": \"_doc\", \"_id\": \"3\", \"_score\": 1.0, \"_source\": { \"suggestions\": [ \"Chess clock\", \"Chess timer\"] } }] }] } } To take advantage of source filtering, use the suggest functionality on the _search endpoint. The _suggest endpoint does not support source filtering.\nCompletion query parameters\nThe following table lists the parameters accepted by the completion suggester query. Parameter Description field A string that specifies the field on which to run the query. Required. size An integer that specifies the maximum number of returned suggestions. Optional. Default is 5. skip_duplicates A Boolean value that specifies whether to skip duplicate suggestions. Optional. Default is false. Fuzzy completion query\nTo allow for fuzzy matching, you can specify the fuzziness parameter for the completion query. In this case, even if the user mistypes a search term, the completion query still returns results. Additionally, the longer the prefix that matches the query, the higher the document’s score. GET chess_store/_search { \"suggest\": { \"product-suggestions\": { \"prefix\": \"chesc\", \"completion\": { \"field\": \"suggestions\", \"size\": 3, \"fuzzy\": { \"fuzziness\": \"AUTO\" } } } } } copy To use all default fuzziness options, specify \"fuzzy\": {} or \"fuzzy\": true.\nThe following table lists the parameters accepted by the fuzzy completion suggester query. All of the parameters are optional. Parameter Description fuzziness Fuzziness can be set as one of the following: 1. An integer that specifies the maximum allowed Levenshtein distance for this edit. 2. AUTO: Strings of 0–2 characters must match exactly, strings of 3–5 characters allow 1 edit, and strings longer than 5 characters allow 2 edits. Default is AUTO. min_length An integer that specifies the minimum length the input must be to start returning suggestions. If the search term is shorter than min_length, no suggestions are returned. Default is 3. prefix_length An integer that specifies the minimum length the matched prefix must be to start returning suggestions. If the prefix of prefix_length is not matched, but the search term is still within the Levenshtein distance, no suggestions are returned. Default is 1. transpositions A Boolean value that specifies to count transpositions (interchanges of adjacent characters) as one edit instead of two. Example: The suggestion’s input parameter is abcde and the fuzziness is 1. If transpositions is set to true, abdce will match, but if transpositions is set to false, abdce will not match. Default is true. unicode_aware A Boolean value that specifies whether to use Unicode code points when measuring the edit distance, transposition, and length. If unicode_aware is set to true, the measurement is slower. Default is false, in which case distances are measured in bytes. Regex queries\nYou can use a regular expression to define the prefix for the completion suggester query.\nFor example, to search for strings that start with “a” and have a “d” later on, use the following query: GET chess_store/_search { \"suggest\": { \"product-suggestions\": { \"regex\": \"a.*d\", \"completion\": { \"field\": \"suggestions\" } } } } copy The response matches the string “abcde”: { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] }, \"suggest\": { \"product-suggestions\": [ { \"text\": \"a.*d\", \"offset\": 0, \"length\": 4, \"options\": [ { \"text\": \"abcde\", \"_index\": \"chess_store\", \"_type\": \"_doc\", \"_id\": \"2\", \"_score\": 20.0, \"_source\": { \"suggestions\": [ { \"input\": \"abcde\", \"weight\": 20 }] } }] }] } }",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "Autocomplete field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/date/",
    "title": "Date",
    "content": "A date in OpenSearch can be represented as one of the following:\nA long value that corresponds to milliseconds since the epoch (the value must be non-negative). Dates are stored in this form internally.\nA formatted string.\nAn integer value that corresponds to seconds since the epoch (the value must be non-negative).\nTo represent date ranges, there is a date range field type.\nExample\nCreate a mapping with a date field and two date formats: PUT testindex { \"mappings\": { \"properties\": { \"release_date\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" } } } } copy Parameters\nThe following table lists the parameters accepted by date field types. All parameters are optional. Parameter Description boost A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. doc_values A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Default is false. format The format for parsing dates. Default is strict_date_optional_time||epoch_millis. ignore_malformed A Boolean value that specifies to ignore malformed values and not to throw an exception. Default is false. index A Boolean value that specifies whether the field should be searchable. Default is true. locale A region- and language-specific way of representing the date. Default is ROOT (a region- and language-neutral locale). meta Accepts metadata for this field. null_value A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. store A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. Formats\nOpenSearch has built-in date formats, but you can also create your own custom formats. The default format is strict_date_optional_time||epoch_millis. You can specify multiple date formats, separated by ||.\nBuilt-in formats\nMost of the date formats have a strict_ counterpart. When the format starts with strict_, the date must have the correct number of digits specified in the format. For example, if the format is set to strict_year_month_day (“yyyy-MM-dd”), both month and day have to be two-digit numbers. So, “2020-06-09” is valid, while “2020-6-9” is invalid.\nEpoch is defined as 00:00:00 UTC on January 1, 1970.\ny: year Y: week-based year M: month w: ordinal week of the year from 01 to 53 d: day D: ordinal day of the year from 001 to 365 (366 for leap years) e: ordinal day of the week from 1 (Monday) to 7 (Sunday) H: hour from 0 to 23 m: minute s: second S: fraction of a second Z: time zone offset (for example, +0400; -0400; -04:00) Numeric date formats Format name and description Examples epoch_millis The number of milliseconds since the epoch. Minimum is -2 63. Maximum is 2 63 − 1.\n1553391286000 epoch_second The number of seconds since the epoch. Minimum is -2 63 ÷ 1000. Maximum is (2 63 − 1) ÷ 1000.\n1553391286 Basic date formats\nComponents of basic date formats are not separated by a delimiter. For example, “20190323”. Format name and description Pattern and examples Dates   basic_date_time A basic date and time separated by T.\n“yyyyMMdd T HHmmss.SSSZ” “20190323T213446.123-04:00” basic_date_time_no_millis A basic date and time without milliseconds, separated by T.\n“yyyyMMdd T HHmmssZ” “20190323T213446-04:00” basic_date A date with a four-digit year, two-digit month, and two-digit day.\n“yyyyMMdd” “20190323” Times   basic_time A time with a two-digit hour, two-digit minute, two-digit second, three-digit millisecond, and time zone offset.\n“HHmmss.SSSZ” “213446.123-04:00” basic_time_no_millis A basic time without milliseconds.\n“HHmmssZ” “213446-04:00” T times   basic_t_time A basic time preceded by T.\n“ T HHmmss.SSSZ” “T213446.123-04:00” basic_t_time_no_millis A basic time without milliseconds, preceded by T.\n“ T HHmmssZ” “T213446-04:00” Ordinal dates   basic_ordinal_date_time A full ordinal date and time.\n“yyyyDDD T HHmmss.SSSZ” “2019082T213446.123-04:00” basic_ordinal_date_time_no_millis A full ordinal date and time without milliseconds.\n“yyyyDDD T HHmmssZ” “2019082T213446-04:00” basic_ordinal_date A date with a four-digit year and three-digit ordinal day of the year.\n“yyyyDDD” “2019082” Week-based dates   basic_week_date_time strict_basic_week_date_time A full week-based date and time separated by T.\n“YYYY W wwe T HHmmss.SSSZ” “2019W126213446.123-04:00” basic_week_date_time_no_millis strict_basic_week_date_time_no_millis A basic week-based year date and time without milliseconds, separated by T.\n“YYYY W wwe T HHmmssZ” “2019W126213446-04:00” basic_week_date strict_basic_week_date A full week-based date with a four-digit week-based year, two-digit ordinal week of the year, and one-digit ordinal day of the week separated by W.\n“YYYY W wwe” “2019W126” Full date formats\nComponents of full date formats are separated by a - delimiter for date and: delimiter for time. For example, “2019-03-23T21:34”. Format name and description Pattern and examples Dates   date_optional_time strict_date_optional_time A generic full date and time. Year is required. Month, day, and time are optional. Time is separated from date by T.\nMultiple patterns. “2019-03-23T21:34:46.123456789-04:00” “2019-03-23T21:34:46” “2019-03-23T21:34” “2019” strict_date_optional_time_nanos A generic full date and time. Year is required. Month, day, and time are optional. If time is specified, it must contain hours, minutes, and seconds, but fraction of a second is optional. Fraction of a second is one to nine digits long and has nanosecond resolution. Time is separated from date by T.\nMultiple patterns. “2019-03-23T21:34:46.123456789-04:00” “2019-03-23T21:34:46” “2019” date_time strict_date_time A full date and time separated by T.\n“yyyy-MM-dd T HH:mm:ss.SSSZ” “2019-03-23T21:34:46.123-04:00” date_time_no_millis strict_date_time_no_millis A full date and time without milliseconds, separated by T.\n“yyyy-MM-dd’T’HH:mm:ssZ” “2019-03-23T21:34:46-04:00” date_hour_minute_second_fraction strict_date_hour_minute_second_fraction A full date, two-digit hour, two-digit minute, two-digit second, and one- to nine-digit fraction of a second separated by T.\n“yyyy-MM-dd T HH:mm:ss.SSSSSSSSS” “2019-03-23T21:34:46.123456789” “2019-03-23T21:34:46.1” date_hour_minute_second_millis strict_date_hour_minute_second_millis A full date, two-digit hour, two-digit minute, two-digit second, and three-digit millisecond separated by T.\n“yyyy-MM-dd T HH:mm:ss.SSS” “2019-03-23T21:34:46.123” date_hour_minute_second strict_date_hour_minute_second A full date, two-digit hour, two-digit minute, and two-digit second separated by T.\n“yyyy-MM-dd T HH:mm:ss” “2019-03-23T21:34:46” date_hour_minute strict_date_hour_minute A full date, two-digit hour, and two-digit minute.\n“yyyy-MM-dd T HH:mm” “2019-03-23T21:34” date_hour strict_date_hour A full date and two-digit hour, separated by T.\n“yyyy-MM-dd T HH” “2019-03-23T21” date strict_date A four-digit year, two-digit month, and two-digit day.\n“yyyy-MM-dd” “2019-03-23” year_month_day strict_year_month_day A four-digit year, two-digit month, and two-digit day.\n“yyyy-MM-dd” “2019-03-23” year_month strict_year_month A four-digit year and two-digit month.\n“yyyy-MM” “2019-03” year strict_year A four-digit year.\n“yyyy” “2019” Times   time strict_time A two-digit hour, two-digit minute, two-digit second, one- to nine-digit fraction of a second, and time zone offset.\n“HH:mm:ss.SSSSSSSSSZ” “21:34:46.123456789-04:00” “21:34:46.1-04:00” time_no_millis strict_time_no_millis A two-digit hour, two-digit minute, two-digit second, and time zone offset.\n“HH:mm:ssZ” “21:34:46-04:00” hour_minute_second_fraction strict_hour_minute_second_fraction A two-digit hour, two-digit minute, two-digit second, and one- to nine-digit fraction of a second.\n“HH:mm:ss.SSSSSSSSS” “21:34:46.1” “21:34:46.123456789” hour_minute_second_millis strict_hour_minute_second_millis A two-digit hour, two-digit minute, two-digit second, and three-digit millisecond.\n“HH:mm:ss.SSS” “21:34:46.123” hour_minute_second strict_hour_minute_second A two-digit hour, two-digit minute, and two-digit second.\n“HH:mm:ss” “21:34:46” hour_minute strict_hour_minute A two-digit hour and two-digit minute.\n“HH:mm” “21:34” hour strict_hour A two-digit hour.\n“HH” “21” T times   t_time strict_t_time A two-digit hour, two-digit minute, two-digit second, one- to nine-digit fraction of a second, and time zone offset, preceded by T.\n“ T HH:mm:ss.SSSSSSSSSZ” “T21:34:46.123456789-04:00” “T21:34:46.1-04:00” t_time_no_millis strict_t_time_no_millis A two-digit hour, two-digit minute, two-digit second, and time zone offset, preceded by T.\n“ T HH:mm:ssZ” “T21:34:46-04:00” Ordinal dates   ordinal_date_time strict_ordinal_date_time A full ordinal date and time separated by T.\n“yyyy-DDD T HH:mm:ss.SSSZ” “2019-082T21:34:46.123-04:00” ordinal_date_time_no_millis strict_ordinal_date_time_no_millis A full ordinal date and time without milliseconds, separated by T.\n“yyyy-DDD T HH:mm:ssZ” “2019-082T21:34:46-04:00” ordinal_date strict_ordinal_date A full ordinal date with a four-digit year and three-digit ordinal day of the year.\n“yyyy-DDD” “2019-082” Week-based dates   week_date_time strict_week_date_time A full week-based date and time separated by T. Week date is a four-digit week-based year, two-digit ordinal week of the year, and one-digit ordinal day of the week. Time is a two-digit hour, two-digit minute, two-digit second, one- to nine-digit fraction of a second, and a time zone offset.\n“YYYY- W ww-e T HH:mm:ss.SSSSSSSSSZ” “2019-W12-6T21:34:46.1-04:00” “2019-W12-6T21:34:46.123456789-04:00” week_date_time_no_millis strict_week_date_time_no_millis A full week-based date and time without milliseconds, separated by T. Week date is a four-digit week-based year, two-digit ordinal week of the year, and one-digit ordinal day of the week. Time is a two-digit hour, two-digit minute, two-digit second, and time zone offset.\n“YYYY- W ww-e T HH:mm:ssZ” “2019-W12-6T21:34:46-04:00” week_date strict_week_date A full week-based date with a four-digit week-based year, two-digit ordinal week of the year, and one-digit ordinal day of the week.\n“YYYY- W ww-e” “2019-W12-6” weekyear_week_day strict_weekyear_week_day A four-digit week-based year, two-digit ordinal week of the year, and one digit day of the week.\n“YYYY-‘W’ww-e” “2019-W12-6” weekyear_week strict_weekyear_week A four-digit week-based year and two-digit ordinal week of the year.\n“YYYY- W ww” “2019-W12” weekyear strict_weekyear A four-digit week-based year.\n“YYYY” “2019” Custom formats\nYou can create custom formats for date fields. For example, the following request specifies a date in the common “MM/dd/yyyy” format: PUT testindex { \"mappings\": { \"properties\": { \"release_date\": { \"type\": \"date\", \"format\": \"MM/dd/yyyy\" } } } } copy Index a document with a date: PUT testindex/_doc/ 21 { \"release_date\": \"03/21/2019\" } copy When searching for an exact date, provide that date in the same format: GET testindex/_search { \"query\": { \"match\": { \"release_date\": { \"query\": \"03/21/2019\" } } } } copy Range queries by default use the field’s mapped format. You can also specify the range of dates in a different format by providing the format parameter: GET testindex/_search { \"query\": { \"range\": { \"release_date\": { \"gte\": \"2019-01-01\", \"lte\": \"2019-12-31\", \"format\": \"yyyy-MM-dd\" } } } } copy Date math\nThe date field type supports using date math to specify durations in queries. For example, the gt, gte, lt, and lte parameters in range queries and the from and to parameters in date range aggregations accept date math expressions.\nA date math expression contains a fixed date, optionally followed by one or more mathematical expressions. The fixed date may be either now (current date and time in milliseconds since the epoch) or a string ending with || that specifies a date (for example, 2022-05-18||). The date must be in the strict_date_optional_time||epoch_millis format.\nDate math supports the following mathematical operators. Operator Description Example + Addition +1M: Add 1 month. - Subtraction -1y: Subtract 1 year. / Rounding down /h: Round to the beginning of the hour. Date math supports the following time units: y: Years M: Months w: Weeks d: Days h or H: Hours m: Minutes s: Seconds\nExample expressions\nThe following example expressions illustrate using date math: now+1M: The current date and time in milliseconds since the epoch, plus 1 month. 2022-05-18||/M: 05/18/2022, rounded to the beginning of the month. Resolves to 2022-05-01. 2022-05-18T15:23||/h: 15:23 on 05/18/2022, rounded to the beginning of the hour. Resolves to 2022-05-18T15. 2022-05-18T15:23:17.789||+2M-1d/d: 15:23:17.789 on 05/18/2022 plus 2 months minus 1 day, rounded to the beginning of the day. Resolves to 2022-07-17.\nUsing date math in a range query\nThe following example illustrates using date math in a range query.\nSet up an index with release_date mapped as date: PUT testindex { \"mappings\": { \"properties\": { \"release_date\": { \"type\": \"date\" } } } } copy Index two documents into the index: PUT testindex/_doc/ 1 { \"release_date\": \"2022-09-14\" } copy PUT testindex/_doc/ 2 { \"release_date\": \"2022-11-15\" } copy The following query searches for documents with release_date within 2 months and 1 day of 09/14/2022. The lower boundary of the range is rounded to the beginning of the day on 09/14/2022: GET testindex/_search { \"query\": { \"range\": { \"release_date\": { \"gte\": \"2022-09-14T15:23||/d\", \"lte\": \"2022-09-14||+2M+1d\" } } } } copy The response contains both documents: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"testindex\", \"_id\": \"2\", \"_score\": 1.0, \"_source\": { \"release_date\": \"2022-11-14\" } }, { \"_index\": \"testindex\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"release_date\": \"2022-09-14\" } }] } }",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/flat-object/",
    "title": "Flat object",
    "content": "In OpenSearch, you don’t have to specify a mapping before indexing documents. If you don’t specify a mapping, OpenSearch uses dynamic mapping to map every field and its subfields in the document automatically. When you ingest documents such as logs, you may not know every field’s subfield name and type in advance. In this case, dynamically mapping all new subfields can quickly lead to a “mapping explosion,” where the growing number of fields may degrade the performance of your cluster.\nThe flat object field type solves this problem by treating the entire JSON object as a string. Subfields within the JSON object are accessible using standard dot path notation, but they are not indexed for fast lookup.\nThe maximum field value length in the dot notation is 2 24 − 1.\nThe flat object field type provides the following benefits:\nEfficient reads: Fetching performance is similar to that of a keyword field.\nMemory efficiency: Storing the entire complex JSON object in one field without indexing all of its subfields reduces the number of fields in an index.\nSpace efficiency: OpenSearch does not create an inverted index for subfields in flat objects, thereby saving space.\nCompatibility for migration: You can migrate your data from systems that support similar flat types to OpenSearch.\nMapping a field as a flat object applies when a field and its subfields are mostly read and not used as search criteria because the subfields are not indexed. Flat objects are useful for objects with a large number of fields or when you don’t know the keys in advance.\nFlat objects support exact match queries with and without dot path notation. For a complete list of supported query types, see Supported queries.\nSearching for a specific value of a nested field in a document may be inefficient because it may require a full scan of the index, which can be an expensive operation.\nFlat objects do not support:\nType-specific parsing.\nNumerical operations, such as numerical comparison or numerical sorting.\nText analysis.\nHighlighting.\nAggregations of subfields using dot notation.\nFiltering by subfields.\nSupported queries\nThe flat object field type supports the following queries: Term Terms Terms set Prefix Range Match Multi-match Query string Simple query string Exists Limitations\nThe following limitations apply to flat objects in OpenSearch 2.7:\nFlat objects do not support open parameters.\nPainless scripting and wildcard queries are not supported for retrieving values of subfields.\nThis functionality is planned for a future release.\nUsing flat object\nThe following example illustrates mapping a field as a flat object, indexing documents with flat object fields, and searching for leaf values of the flat object in those documents.\nOnly the root field of a document can be defined as a flat object. You cannot define an object that is part of another JSON object as a flat object because when a flat object is flattened to a string, the nested architecture of the leaves is lost.\nFirst, create a mapping for your index, where issue is of type flat_object: PUT /test-index/ { \"mappings\": { \"properties\": { \"issue\": { \"type\": \"flat_object\" } } } } copy Next, index two documents with flat object fields: PUT /test-index/_doc/ 1 { \"issue\": { \"number\": \"123456\", \"labels\": { \"version\": \"2.1\", \"backport\": [ \"2.0\", \"1.3\"], \"category\": { \"type\": \"API\", \"level\": \"enhancement\" } } } } copy PUT /test-index/_doc/ 2 { \"issue\": { \"number\": \"123457\", \"labels\": { \"version\": \"2.2\", \"category\": { \"type\": \"API\", \"level\": \"bug\" } } } } copy To search for a leaf value of the flat object, use either a GET or a POST request. Even if you don’t know the field names, you can search for a leaf value in the entire flat object. For example, the following request searches for all issues labeled as bugs: GET /test-index/_search { \"query\": { \"match\": { \"issue\": \"bug\" } } } Alternatively, if you know the subfield name in which to search, provide the field’s path in dot notation: GET /test-index/_search { \"query\": { \"match\": { \"issue.labels.category.level\": \"bug\" } } } copy In both cases, the response is the same and contains document 2: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.0303539, \"hits\": [ { \"_index\": \"test-index\", \"_id\": \"2\", \"_score\": 1.0303539, \"_source\": { \"issue\": { \"number\": \"123457\", \"labels\": { \"version\": \"2.2\", \"category\": { \"type\": \"API\", \"level\": \"bug\" } } } } }] } } Using a prefix query, you can search for all issues for the versions that start with 2.: GET /test-index/_search { \"query\": { \"prefix\": { \"issue.labels.version\": \"2.\" } } } With a range query, you can search for all issues for versions 2.0–2.1: GET /test-index/_search { \"query\": { \"range\": { \"issue\": { \"gte\": \"2.0\", \"lte\": \"2.1\" } } } }",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "Object field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/geo-point/",
    "title": "Geopoint",
    "content": "A geopoint field type contains a geographic point specified by latitude and longitude.\nExample\nCreate a mapping with a geopoint field type: PUT testindex 1 { \"mappings\": { \"properties\": { \"point\": { \"type\": \"geo_point\" } } } } copy Formats\nGeopoints can be indexed in the following formats:\nAn object with a latitude and longitude PUT testindex 1 /_doc/ 1 { \"point\": { \"lat\": 40.71, \"lon\": 74.00 } } copy A string in the “ latitude, longitude ” format PUT testindex 1 /_doc/ 2 { \"point\": \"40.71,74.00\" } copy A geohash PUT testindex 1 /_doc/ 3 { \"point\": \"txhxegj0uyp3\" } copy An array in the [ longitude, latitude] format PUT testindex 1 /_doc/ 4 { \"point\": [ 74.00, 40.71] } copy A Well-Known Text POINT in the “POINT( longitude latitude)” format PUT testindex 1 /_doc/ 5 { \"point\": \"POINT (74.00 40.71)\" } copy GeoJSON format, where the coordinates are in the [ longitude, latitude] format PUT testindex 1 /_doc/ 6 { \"point\": { \"type\": \"Point\", \"coordinates\": [ 74.00, 40.71] } } copy Parameters\nThe following table lists the parameters accepted by geopoint field types. All parameters are optional. Parameter Description ignore_malformed A Boolean value that specifies to ignore malformed values and not to throw an exception. Valid values for latitude are [-90, 90]. Valid values for longitude are [-180, 180]. Default is false. ignore_z_value Specific to points with three coordinates. If ignore_z_value is true, the third coordinate is not indexed but is still stored in the _source field. If ignore_z_value is false, an exception is thrown. null_value A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "Geographic field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/geo-shape/",
    "title": "Geoshape",
    "content": "A geoshape field type contains a geographic shape, such as a polygon or a collection of geographic points. To index a geoshape, OpenSearch tesselates the shape into a triangular mesh and stores each triangle in a BKD tree. This provides a 10 -7 decimal degree of precision, which represents near-perfect spatial resolution. Performance of this process is mostly impacted by the number of vertices in a polygon you are indexing.\nExample\nCreate a mapping with a geoshape field type: PUT testindex { \"mappings\": { \"properties\": { \"location\": { \"type\": \"geo_shape\" } } } } copy Formats\nGeoshapes can be indexed in the following formats: GeoJSON Well-Known Text (WKT) In both GeoJSON and WKT, the coordinates must be specified in the longitude, latitude order within coordinate arrays. Note that the longitude comes first in this format.\nGeoshape types\nThe following table describes the possible geoshape types and their relationship to the GeoJSON and WKT types. OpenSearch type GeoJSON type WKT type Description point Point\nPOINT\nA geographic point specified by latitude and longitude. OpenSearch uses World Geodetic System (WGS84) coordinates. linestring LineString\nLINESTRING\nA line specified by two or more points. May be a straight line or a path of connected line segments. polygon Polygon\nPOLYGON\nA polygon specified by a list of vertices in coordinate form. The polygon must be closed, meaning the last point must be the same as the first point. Therefore, to create an n-gon, n+1 vertices are required. The minimum number of vertices is four, which creates a triangle. multipoint MultiPoint\nMULTIPOINT\nAn array of discrete related points that are not connected. multilinestring MultiLineString\nMULTILINESTRING\nAn array of linestrings. multipolygon MultiPolygon\nMULTIPOLYGON\nAn array of polygons. geometrycollection GeometryCollection\nGEOMETRYCOLLECTION\nA collection of geoshapes that may be of different types. envelope N/A\nBBOX\nA bounding rectangle specified by upper-left and lower-right vertices. Point\nA point is a single pair of coordinates specified by latitude and longitude.\nIndex a point in GeoJSON format: PUT testindex/_doc/ 1 { \"location\": { \"type\": \"point\", \"coordinates\": [ 74.00, 40.71] } } copy Index a point in WKT format: PUT testindex/_doc/ 1 { \"location\": \"POINT (74.0060 40.7128)\" } copy Linestring\nA linestring is a line specified by two or more points. If the points are collinear, the linestring is a straight line. Otherwise, the linestring represents a path made of line segments.\nIndex a linestring in GeoJSON format: PUT testindex/_doc/ 2 { \"location\": { \"type\": \"linestring\", \"coordinates\": [[ 74.0060, 40.7128], [ 71.0589, 42.3601]] } } copy Index a linestring in WKT format: PUT testindex/_doc/ 2 { \"location\": \"LINESTRING (74.0060 40.7128, 71.0589 42.3601)\" } copy Polygon\nA polygon is specified by a list of vertices in coordinate form. The polygon must be closed, meaning the last point must be the same as the first point. In the following example, a triangle is created using four points.\nGeoJSON requires that you list the vertices of the polygon counterclockwise. WKT does not impose a specific order on vertices.\nIndex a polygon (triangle) in GeoJSON format: PUT testindex/_doc/ 3 { \"location\": { \"type\": \"polygon\", \"coordinates\": [ [[ 74.0060, 40.7128], [ 71.0589, 42.3601], [ 73.7562, 42.6526], [ 74.0060, 40.7128]]] } } copy Index a polygon (triangle) in WKT format: PUT testindex/_doc/ 3 { \"location\": \"POLYGON ((74.0060 40.7128, 71.0589 42.3601, 73.7562 42.6526, 74.0060 40.7128))\" } copy The polygon may have holes inside. In this case, the coordinates field will contain multiple arrays. The first array represents the outer polygon, and each subsequent array represents a hole. Holes are represented as polygons and specified as arrays of coordinates.\nGeoJSON requires that you list the vertices of the polygon counterclockwise and the vertices of the hole clockwise. WKT does not impose a specific order on vertices.\nIndex a polygon (triangle) with a triangular hole in GeoJSON format: PUT testindex/_doc/ 4 { \"location\": { \"type\": \"polygon\", \"coordinates\": [ [[ 74.0060, 40.7128], [ 71.0589, 42.3601], [ 73.7562, 42.6526], [ 74.0060, 40.7128]], [[ 72.6734, 41.7658], [ 72.6506, 41.5623], [ 73.0515, 41.5582], [ 72.6734, 41.7658]]] } } copy Index a polygon (triangle) with a triangular hole in WKT format: PUT testindex/_doc/ 4 { \"location\": \"POLYGON ((40.7128 74.0060, 42.3601 71.0589, 42.6526 73.7562, 40.7128 74.0060), (41.7658 72.6734, 41.5623 72.6506, 41.5582 73.0515, 41.7658 72.6734))\" } copy In OpenSearch, you can specify a polygon by listing its vertices clockwise or counterclockwise. This works well for polygons that do not cross the date line (are narrower than 180°). However, a polygon that crosses the date line (is wider than 180°) might be ambiguous because WKT does not impose a specific order on vertices. Thus, you must specify polygons that cross the date line by listing their vertices counterclockwise.\nYou can define an orientation parameter to specify the vertex traversal order at mapping time: PUT testindex { \"mappings\": { \"properties\": { \"location\": { \"type\": \"geo_shape\", \"orientation\": \"left\" } } } } copy Subsequently indexed documents can override the orientation setting: PUT testindex/_doc/ 3 { \"location\": { \"type\": \"polygon\", \"orientation\": \"cw\", \"coordinates\": [ [[ 74.0060, 40.7128], [ 71.0589, 42.3601], [ 73.7562, 42.6526], [ 74.0060, 40.7128]]] } } copy Multipoint\nA multipoint is an array of discrete related points that are not connected.\nIndex a multipoint in GeoJSON format: PUT testindex/_doc/ 6 { \"location\": { \"type\": \"multipoint\", \"coordinates\": [ [ 74.0060, 40.7128], [ 71.0589, 42.3601]] } } copy Index a multipoint in WKT format: PUT testindex/_doc/ 6 { \"location\": \"MULTIPOINT (74.0060 40.7128, 71.0589 42.3601)\" } copy Multilinestring\nA multilinestring is an array of linestrings.\nIndex a linestring in GeoJSON format: PUT testindex/_doc/ 2 { \"location\": { \"type\": \"multilinestring\", \"coordinates\": [ [[ 74.0060, 40.7128], [ 71.0589, 42.3601]], [[ 73.7562, 42.6526], [ 72.6734, 41.7658]]] } } copy Index a linestring in WKT format: PUT testindex/_doc/ 2 { \"location\": \"MULTILINESTRING ((74.0060 40.7128, 71.0589 42.3601), (73.7562 42.6526, 72.6734 41.7658))\" } copy Multipolygon\nA multipolygon is an array of polygons. In this example, the first polygon contains a hole, and the second does not.\nIndex a multipolygon in GeoJSON format: PUT testindex/_doc/ 4 { \"location\": { \"type\": \"multipolygon\", \"coordinates\": [ [ [[ 74.0060, 40.7128], [ 71.0589, 42.3601], [ 73.7562, 42.6526], [ 74.0060, 40.7128]], [[ 72.6734, 41.7658], [ 72.6506, 41.5623], [ 73.0515, 41.5582], [ 72.6734, 41.7658]]], [ [[ 73.9776, 40.7614], [ 73.9554, 40.7827], [ 73.9631, 40.7812], [ 73.9776, 40.7614]]]] } } copy Index a multipolygon in WKT format: PUT testindex/_doc/ 4 { \"location\": \"MULTIPOLYGON (((40.7128 74.0060, 42.3601 71.0589, 42.6526 73.7562, 40.7128 74.0060), (41.7658 72.6734, 41.5623 72.6506, 41.5582 73.0515, 41.7658 72.6734)), ((73.9776 40.7614, 73.9554 40.7827, 73.9631 40.7812, 73.9776 40.7614)))\" } copy Geometry collection\nA geometry collection is a collection of geoshapes that may be of different types.\nIndex a geometry collection in GeoJSON format: PUT testindex/_doc/ 7 { \"location\": { \"type\": \"geometrycollection\", \"geometries\": [ { \"type\": \"point\", \"coordinates\": [ 74.0060, 40.7128] }, { \"type\": \"linestring\", \"coordinates\": [[ 73.7562, 42.6526], [ 72.6734, 41.7658]] }] } } copy Index a geometry collection in WKT format: PUT testindex/_doc/ 7 { \"location\": \"GEOMETRYCOLLECTION (POINT (74.0060 40.7128), LINESTRING(73.7562 42.6526, 72.6734 41.7658))\" } copy Envelope\nAn envelope is a bounding rectangle specified by upper-left and lower-right vertices. The GeoJSON format is [[minLon, maxLat], [maxLon, minLat]].\nIndex an envelope in GeoJSON format: PUT testindex/_doc/ 2 { \"location\": { \"type\": \"envelope\", \"coordinates\": [[ 71.0589, 42.3601], [ 74.0060, 40.7128]] } } copy In WKT format, use BBOX (minLon, maxLon, maxLat, minLat).\nIndex an envelope in WKT BBOX format: PUT testindex/_doc/ 8 { \"location\": \"BBOX (71.0589, 74.0060, 42.3601, 40.7128)\" } copy Parameters\nThe following table lists the parameters accepted by geoshape field types. All parameters are optional. Parameter Description coerce A Boolean value that specifies whether to automatically close unclosed linear rings. Default is false. ignore_malformed A Boolean value that specifies to ignore malformed GeoJSON or WKT geoshapes and not to throw an exception. Default is false (throw an exception when geoshapes are malformed). ignore_z_value Specific to points with three coordinates. If ignore_z_value is true, the third coordinate is not indexed but is still stored in the _source field. If ignore_z_value is false, an exception is thrown. Default is true. orientation Specifies the traversal order of the vertices in the geoshape’s list of coordinates. orientation takes the following values: 1. RIGHT: counterclockwise. Specify RIGHT orientation by using one of the following strings (uppercase or lowercase): right, counterclockwise, ccw. 2. LEFT: clockwise. Specify LEFT orientation by using one of the following strings (uppercase or lowercase): left, clockwise, cw. This value can be overridden by individual documents. Default is RIGHT.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "Geographic field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/geographic/",
    "title": "Geographic field types",
    "content": "The following table lists all geographic field types that OpenSearch supports. Field data type Description geo_point A geographic point specified by latitude and longitude. geo_shape A geographic shape, such as a polygon or a collection of geographic points.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/index/",
    "title": "Supported field types",
    "content": "You can specify data types for your fields when creating a mapping. The following table lists all data field types that OpenSearch supports. Field data type Description alias An additional name for an existing field. binary A binary value in Base64 encoding. Numeric byte, double, float, half_float, integer, long, scaled_float, short. boolean A Boolean value. date A date value as a formatted string, a long value, or an integer. ip An IP address in IPv4 or IPv6 format. Range integer_range, long_range, double_range, float_range, date_range, ip_range. Object object, nested, flat_object, join.\nString keyword, text, token_count. Autocomplete completion, search_as_you_type. Geographic geo_point, geo_shape. Rank rank_feature, rank_features. percolator Specifies to treat this field as a query. Arrays\nThere is no dedicated array field type in OpenSearch. Instead, you can pass an array of values into any field. All values in the array must have the same field type. PUT testindex 1 /_doc/ 1 { \"number\": 1 } PUT testindex 1 /_doc/ 2 { \"number\": [ 1, 2, 3] } Multifields\nMultifields are used to index the same field differently. Strings are often mapped as text for full-text queries and keyword for exact-value queries.\nMultifields can be created using the fields parameter. For example, you can map a book title to be of type text and keep a title.raw subfield of type keyword. PUT books { \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\", \"fields\": { \"raw\": { \"type\": \"keyword\" } } } } } } Null value\nSetting a field’s value to null, an empty array or an array of null values makes this field equivalent to an empty field. Therefore, you cannot search for documents that have null in this field.\nTo make a field searchable for null values, you can specify its null_value parameter in the index’s mappings. Then, all null values passed to this field will be replaced with the specified null_value.\nThe null_value parameter must be of the same type as the field. For example, if your field is a string, the null_value for this field must also be a string.\nExample\nCreate a mapping to replace null values in the emergency_phone field with the string “NONE”: PUT testindex { \"mappings\": { \"properties\": { \"name\": { \"type\": \"keyword\" }, \"emergency_phone\": { \"type\": \"keyword\", \"null_value\": \"NONE\" } } } } Index three documents into testindex. The emergency_phone fields of documents 1 and 3 contain null, while the emergency_phone field of document 2 has an empty array: PUT testindex/_doc/ 1 { \"name\": \"Akua Mansa\", \"emergency_phone\": null } PUT testindex/_doc/ 2 { \"name\": \"Diego Ramirez\", \"emergency_phone\": [] } PUT testindex/_doc/ 3 { \"name\": \"Jane Doe\", \"emergency_phone\": [ null, null] } Search for people who do not have an emergency phone: GET testindex/_search { \"query\": { \"term\": { \"emergency_phone\": \"NONE\" } } } The response contains documents 1 and 3 but not document 2 because only explicit null values are replaced with the string “NONE”: { \"took\": 1, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 0.18232156, \"hits\": [ { \"_index\": \"testindex\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 0.18232156, \"_source\": { \"name\": \"Akua Mansa\", \"emergency_phone\": null } }, { \"_index\": \"testindex\", \"_type\": \"_doc\", \"_id\": \"3\", \"_score\": 0.18232156, \"_source\": { \"name\": \"Jane Doe\", \"emergency_phone\": [ null, null] } }] } } The _source field still contains explicit null values because it is not affected by the null_value.",
    "ancestors": [
      "Mappings and field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/ip/",
    "title": "IP address",
    "content": "An ip field type contains an IP address in IPv4 or IPv6 format.\nTo represent IP address ranges, there is an IP range field type.\nExample\nCreate a mapping with an IP address: PUT testindex { \"mappings\": { \"properties\": { \"ip_address\": { \"type\": \"ip\" } } } } copy Index a document with an IP address: PUT testindex/_doc/ 1 { \"ip_address\": \"10.24.34.0\" } copy Query an index for a specific IP address: GET testindex/_doc/ 1 { \"query\": { \"term\": { \"ip_address\": \"10.24.34.0\" } } } copy Searching for an IP address and its associated network mask\nYou can query an index for an IP address in Classless Inter-Domain Routing (CIDR) notation. Using CIDR notation, specify the IP address and the prefix length (0–32), separated by /. For example, the prefix length of 24 will match all IP addresses with the same initial 24 bits.\nExample query in IPv4 format GET testindex/_search { \"query\": { \"term\": { \"ip_address\": \"10.24.34.0/24\" } } } copy Example query in IPv6 format GET testindex/_search { \"query\": { \"term\": { \"ip_address\": \"2001:DB8::/24\" } } } copy If you use an IP address in IPv6 format in a query_string query, you need to escape: characters because they are parsed as special characters. You can accomplish this by wrapping the IP address in quotation marks and escaping those quotation marks with \\. GET testindex/_search { \"query\": { \"query_string\": { \"query\": \"ip_address: \\\" 2001:DB8::/24 \\\" \" } } } copy Parameters\nThe following table lists the parameters accepted by ip field types. All parameters are optional. Parameter Description boost A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. doc_values A Boolean value that specifies if the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Default is false. ignore_malformed A Boolean value that specifies to ignore malformed values and not to throw an exception. Default is false. index A Boolean value that specifies whether the field should be searchable. Default is true. null_value A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. store A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/join/",
    "title": "Join",
    "content": "A join field type establishes a parent/child relationship between documents in the same index.\nExample\nCreate a mapping to establish a parent-child relationship between products and their brands: PUT testindex 1 { \"mappings\": { \"properties\": { \"product_to_brand\": { \"type\": \"join\", \"relations\": { \"brand\": \"product\" } } } } } copy Then, index a parent document with a join field type: PUT testindex 1 /_doc/ 1 { \"name\": \"Brand 1\", \"product_to_brand\": { \"name\": \"brand\" } } copy You can also use a shortcut without object notation to index a parent document: PUT testindex 1 /_doc/ 1 { \"name\": \"Brand 1\", \"product_to_brand\": \"brand\" } copy When indexing child documents, you have to specify the routing query parameter because parent and child documents in the same relation have to be indexed on the same shard. Each child document refers to its parent’s ID in the parent field.\nIndex two child documents, one for each parent: PUT testindex 1 /_doc/ 3?routing= 1 { \"name\": \"Product 1\", \"product_to_brand\": { \"name\": \"product\", \"parent\": \"1\" } } copy PUT testindex 1 /_doc/ 4?routing= 1 { \"name\": \"Product 2\", \"product_to_brand\": { \"name\": \"product\", \"parent\": \"1\" } } copy Querying a join field\nWhen you query a join field, the response contains subfields that specify whether the returned document is a parent or a child. For child objects, the parent ID is also returned.\nSearch for all documents GET testindex 1 /_search { \"query\": { \"match_all\": {} } } copy The response indicates whether a document is a parent or a child: { \"took\": 4, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"testindex1\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"name\": \"Brand 1\", \"product_to_brand\": { \"name\": \"brand\" } } }, { \"_index\": \"testindex1\", \"_type\": \"_doc\", \"_id\": \"3\", \"_score\": 1.0, \"_routing\": \"1\", \"_source\": { \"name\": \"Product 1\", \"product_to_brand\": { \"name\": \"product\", \"parent\": \"1\" } } }, { \"_index\": \"testindex1\", \"_type\": \"_doc\", \"_id\": \"4\", \"_score\": 1.0, \"_routing\": \"1\", \"_source\": { \"name\": \"Product 2\", \"product_to_brand\": { \"name\": \"product\", \"parent\": \"1\" } } }] } } Search for all children of a parent\nFind all products associated with Brand 1: GET testindex 1 /_search { \"query\": { \"has_parent\": { \"parent_type\": \"brand\", \"query\": { \"match\": { \"name\": \"Brand 1\" } } } } } copy The response contains Product 1 and Product 2, which are associated with Brand 1: { \"took\": 7, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"testindex1\", \"_type\": \"_doc\", \"_id\": \"3\", \"_score\": 1.0, \"_routing\": \"1\", \"_source\": { \"name\": \"Product 1\", \"product_to_brand\": { \"name\": \"product\", \"parent\": \"1\" } } }, { \"_index\": \"testindex1\", \"_type\": \"_doc\", \"_id\": \"4\", \"_score\": 1.0, \"_routing\": \"1\", \"_source\": { \"name\": \"Product 2\", \"product_to_brand\": { \"name\": \"product\", \"parent\": \"1\" } } }] } } Search for the parent of a child\nFind the parent of Product 1: GET testindex 1 /_search { \"query\": { \"has_child\": { \"type\": \"product\", \"query\": { \"match\": { \"name\": \"Product 1\" } } } } } copy The response returns Brand 1 as Product 1’s parent: { \"took\": 4, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"testindex1\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"name\": \"Brand 1\", \"product_to_brand\": { \"name\": \"brand\" } } }] } } Parent with many children\nOne parent can have many children. Create a mapping with multiple children: PUT testindex 1 { \"mappings\": { \"properties\": { \"parent_to_child\": { \"type\": \"join\", \"relations\": { \"parent\": [ \"child 1\", \"child 2\"] } } } } } copy Join field type notes\nThere can only be one join field mapping in an index.\nYou need to provide the routing parameter when retrieving, updating, or deleting a child document. This is because parent and child documents in the same relation have to be indexed on the same shard.\nMultiple parents are not supported.\nYou can add a child document to an existing document only if the existing document is already marked as a parent.\nYou can add a new relation to an existing join field.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "Object field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/keyword/",
    "title": "Keyword",
    "content": "A keyword field type contains a string that is not analyzed. It allows only exact, case-sensitive matches.\nIf you need to use a field for full-text search, map it as text instead.\nExample\nCreate a mapping with a keyword field: PUT movies { \"mappings\": { \"properties\": { \"genre\": { \"type\": \"keyword\" } } } } copy Parameters\nThe following table lists the parameters accepted by keyword field types. All parameters are optional. Parameter Description boost A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. doc_values A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Default is false. eager_global_ordinals Specifies whether global ordinals should be loaded eagerly on refresh. If the field is often used for aggregations, this parameter should be set to true. Default is false. fields To index the same string in several ways (for example, as a keyword and text), provide the fields parameter. You can specify one version of the field to be used for search and another to be used for sorting and aggregations. ignore_above Any string longer than this integer value should not be indexed. Default is 2147483647. Default dynamic mapping creates a keyword subfield for which ignore_above is set to 256. index A Boolean value that specifies whether the field should be searchable. Default is true. index_options Information to be stored in the index that will be considered when calculating relevance scores. Can be set to freqs for term frequency. Default is docs. meta Accepts metadata for this field. normalizer Specifies how to preprocess this field before indexing (for example, make it lowercase). Default is null (no preprocessing). norms A Boolean value that specifies whether the field length should be used when calculating relevance scores. Default is false. null_value A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. similarity The ranking algorithm for calculating relevance scores. Default is BM25. split_queries_on_whitespace A Boolean value that specifies whether full-text queries should be split on whitespace. Default is false. store A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "String field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/mappings/",
    "title": "Mapping",
    "content": "You can define how documents and their fields are stored and indexed by creating a mapping.\nIf you’re just starting to build out your cluster and data, you may not know exactly how your data should be stored. In those cases, you can use dynamic mappings, which tell OpenSearch to dynamically add data and its fields. However, if you know exactly what types your data falls under and want to enforce that standard, then you can use explicit mappings.\nFor example, if you want to indicate that year should be of type text instead of an integer, and age should be an integer, you can do so with explicit mappings. Using dynamic mapping OpenSearch might interpret both year and age as integers.\nThis section provides an example for how to create an index mapping, and how to add a document to it that will get ip_range validated.\nTable of contents Dynamic mapping Explicit mapping Response Mapping example usage Create an index with an ip mapping Get a mapping Dynamic mapping\nWhen you index a document, OpenSearch adds fields automatically with dynamic mapping. You can also explicitly add fields to an index mapping.\nDynamic mapping types Type Description null\nA null field can’t be indexed or searched. When a field is set to null, OpenSearch behaves as if that field has no values.\nboolean\nOpenSearch accepts true and false as boolean values. An empty string is equal to false. float\nA single-precision 32-bit floating point number.\ndouble\nA double-precision 64-bit floating point number.\ninteger\nA signed 32-bit number.\nobject\nObjects are standard JSON objects, which can have fields and mappings of their own. For example, a movies object can have additional properties such as title, year, and director.\narray\nArrays in OpenSearch can only store values of one type, such as an array of just integers or strings. Empty arrays are treated as though they are fields with no values.\ntext\nA string sequence of characters that represent full-text values.\nkeyword\nA string sequence of structured characters, such as an email address or ZIP code.\ndate detection string\nEnabled by default, if new string fields match a date’s format, then the string is processed as a date field. For example, date: \"2012/03/11\" is processed as a date.\nnumeric detection string\nIf disabled, OpenSearch may automatically process numeric values as strings when they should be processed as numbers. When enabled, OpenSearch can process strings into long, integer, short, byte, double, float, half_float, scaled_float, and unsigned_long. Default is disabled. Explicit mapping\nIf you know exactly what your field data types need to be, you can specify them in your request body when creating your index. PUT sample-index 1 { \"mappings\": { \"properties\": { \"year\": { \"type\": \"text\" }, \"age\": { \"type\": \"integer\" }, \"director\":{ \"type\": \"text\" } } } } Response { \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"sample-index1\" } To add mappings to an existing index or data stream, you can send a request to the _mapping endpoint using the PUT or POST HTTP method: POST sample-index 1 /_mapping { \"properties\": { \"year\": { \"type\": \"text\" }, \"age\": { \"type\": \"integer\" }, \"director\":{ \"type\": \"text\" } } } You cannot change the mapping of an existing field, you can only modify the field’s mapping parameters.\nMapping example usage\nThe following example shows how to create a mapping to specify that OpenSearch should ignore any documents with malformed IP addresses that do not conform to the ip data type. You accomplish this by setting the ignore_malformed parameter to true.\nCreate an index with an ip mapping\nTo create an index, use a PUT request: PUT /test-index { \"mappings\": { \"properties\": { \"ip_address\": { \"type\": \"ip\", \"ignore_malformed\": true } } } } You can add a document that has a malformed IP address to your index: PUT /test-index/_doc/ 1 { \"ip_address\": \"malformed ip address\" } This indexed IP address does not throw an error because ignore_malformed is set to true.\nYou can query the index using the following request: GET /test-index/_search The response shows that the ip_address field is ignored in the indexed document: { \"took\": 14, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \"test-index\", \"_id\": \"1\", \"_score\": 1, \"_ignored\": [ \"ip_address\"], \"_source\": { \"ip_address\": \"malformed ip address\" } }] } } Get a mapping\nTo get all mappings for one or more indexes, use the following request: GET &lt;index&gt;/_mapping In the above request, &lt;index&gt; may be an index name or a comma-separated list of index names.\nTo get all mappings for all indexes, use the following request: GET _mapping To get a mapping for a specific field, provide the index name and the field name: GET _mapping/field/&lt;fields&gt; GET /&lt;index&gt;/_mapping/field/&lt;fields&gt; Both &lt;index&gt; and &lt;fields&gt; can be specified as one value or a comma-separated list.\nFor example, the following request retrieves the mapping for the year and age fields in sample-index1: GET sample-index 1 /_mapping/field/year,age The response contains the specified fields: { \"sample-index1\": { \"mappings\": { \"year\": { \"full_name\": \"year\", \"mapping\": { \"year\": { \"type\": \"text\" } } }, \"age\": { \"full_name\": \"age\", \"mapping\": { \"age\": { \"type\": \"integer\" } } } } } }",
    "ancestors": [
      "Mappings and field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/nested/",
    "title": "Nested",
    "content": "A nested field type is a special type of object field type.\nAny object field can take an array of objects. Each of the objects in the array is dynamically mapped as an object field type and stored in flattened form. This means that the objects in the array are broken down into individual fields, and values for each field across all objects are stored together. It is sometimes necessary to use the nested type to preserve a nested object as a whole so that you can perform a search on it.\nFlattened form\nBy default, each of the nested objects is dynamically mapped as object field type. Any object field can take an array of objects. PUT testindex 1 /_doc/ 100 { \"patients\": [ { \"name\": \"John Doe\", \"age\": 56, \"smoker\": true }, { \"name\": \"Mary Major\", \"age\": 85, \"smoker\": false }] } copy When these objects are stored, they are flattened, so their internal representation has an array of all values for each field: { \"patients.name\": [ \"John Doe\", \"Mary Major\"], \"patients.age\": [ 56, 85], \"smoker\": [ true, false] } Some queries will work correctly in this representation. If you search for patients older than 75 OR smokers, document 100 should match. GET testindex 1 /_search { \"query\": { \"bool\": { \"should\": [ { \"term\": { \"patients.smoker\": true } }, { \"range\": { \"patients.age\": { \"gte\": 75 } } }] } } } copy The query correctly returns document 100: { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.3616575, \"hits\": [ { \"_index\": \"testindex1\", \"_type\": \"_doc\", \"_id\": \"100\", \"_score\": 1.3616575, \"_source\": { \"patients\": [ { \"name\": \"John Doe\", \"age\": \"56\", \"smoker\": true }, { \"name\": \"Mary Major\", \"age\": \"85\", \"smoker\": false }] } }] } } Alternatively, if you search for patients older than 75 AND smokers, document 100 should not match. GET testindex 1 /_search { \"query\": { \"bool\": { \"must\": [ { \"term\": { \"patients.smoker\": true } }, { \"range\": { \"patients.age\": { \"gte\": 75 } } }] } } } copy However, this query still incorrectly returns document 100. This is because the relation between age and smoking was lost when arrays of values for individual fields were created.\nNested field type\nNested objects are stored as separate documents, and the parent object has references to its children. To mark objects as nested, create a mapping with a nested field type. PUT testindex 1 { \"mappings\": { \"properties\": { \"patients\": { \"type\": \"nested\" } } } } copy Then, index a document with a nested field type: PUT testindex 1 /_doc/ 100 { \"patients\": [ { \"name\": \"John Doe\", \"age\": 56, \"smoker\": true }, { \"name\": \"Mary Major\", \"age\": 85, \"smoker\": false }] } copy Now if you run the same query to search for patients older than 75 AND smokers, nothing is returned, which is correct. { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 0, \"relation\": \"eq\" }, \"max_score\": null, \"hits\": [] } } Parameters\nThe following table lists the parameters accepted by object field types. All parameters are optional. Parameter Description dynamic Specifies whether new fields can be dynamically added to this object. Valid values are true, false, and strict. Default is true. include_in_parent A Boolean value that specifies whether all fields in the child nested object should also be added to the parent document in flattened form. Default is false. incude_in_root A Boolean value that specifies whether all fields in the child nested object should also be added to the root document in flattened form. Default is false. properties Fields of this object, which can be of any supported type. New properties can be dynamically added to this object if dynamic is set to true.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "Object field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/numeric/",
    "title": "Numeric field types",
    "content": "The following table lists all numeric field types that OpenSearch supports. Field data type Description byte A signed 8-bit integer. Minimum is -128. Maximum is 127. double A double-precision 64-bit IEEE 754 floating-point value. Minimum magnitude is 2 -1074. Maximum magnitude is (2 − 2 -52) · 2 1023. The number of significant bits is 53. The number of significant digits is 15.95. float A single-precision 32-bit IEEE 754 floating-point value. Minimum magnitude is 2 -149. Maximum magnitude is (2 − 2 -23) · 2 127. The number of significant bits is 24. The number of significant digits is 7.22. half_float A half-precision 16-bit IEEE 754 floating-point value. Minimum magnitude is 2 -24. Maximum magnitude is 65504. The number of significant bits is 11. The number of significant digits is 3.31. integer A signed 32-bit integer. Minimum is -2 31. Maximum is 2 31 − 1. long A signed 64-bit integer. Minimum is -2 63. Maximum is 2 63 − 1. short A signed 16-bit integer. Minimum is -2 15. Maximum is 2 15 − 1. scaled_float A floating-point value that is multiplied by the double scale factor and stored as a long value. Integer, long, float, and double field types have corresponding range field types.\nIf your numeric field contains an identifier such as an ID, you can map this field as a keyword to optimize for faster term-level queries. If you need to use range queries on this field, you can map this field as a numeric field type in addition to a keyword field type.\nExample\nCreate a mapping where integer_value is an integer field: PUT testindex { \"mappings\": { \"properties\": { \"integer_value\": { \"type\": \"integer\" } } } } copy Index a document with an integer value: PUT testindex/_doc/ 1 { \"integer_value\": 123 } copy Scaled float field type\nA scaled float field type is a floating-point value that is multiplied by the scale factor and stored as a long value. It takes all optional parameters taken by number field types, plus an additional scaling_factor parameter. The scale factor is required when creating a scaled float.\nScaled floats are useful for saving disk space. Larger scaling_factor values lead to better accuracy but higher space overhead.\nScaled float example\nCreate a mapping where scaled is a scaled_float field: PUT testindex { \"mappings\": { \"properties\": { \"scaled\": { \"type\": \"scaled_float\", \"scaling_factor\": 10 } } } } copy Index a document with a scaled_float value: PUT testindex/_doc/ 1 { \"scaled\": 2.3 } copy The scaled value will be stored as 23.\nParameters\nThe following table lists the parameters accepted by numeric field types. All parameters are optional. Parameter Description boost A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. coerce A Boolean value that signals to truncate decimals for integer values and to convert strings to numeric values. Default is true. doc_values A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Default is false. ignore_malformed A Boolean value that specifies to ignore malformed values and not to throw an exception. Default is false. index A Boolean value that specifies whether the field should be searchable. Default is true. meta Accepts metadata for this field. null_value A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. store A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. Scaled float has an additional required parameter: scaling_factor. Parameter Description scaling_factor A double value that is multiplied by the field value and rounded to the nearest long. Required.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/object-fields/",
    "title": "Object field types",
    "content": "The following table lists all object field types that OpenSearch supports. Field data type Description object A JSON object. nested Used when objects in an array need to be indexed independently as separate documents. flat_object A JSON object treated as a string. join Establishes a parent-child relationship between documents in the same index.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/object/",
    "title": "Object",
    "content": "An object field type contains a JSON object (a set of name/value pairs). A value in a JSON object may be another JSON object. It is not necessary to specify object as the type when mapping object fields because object is the default type.\nExample\nCreate a mapping with an object field: PUT testindex 1 /_mappings { \"properties\": { \"patient\": { \"properties\": { \"name\": { \"type\": \"text\" }, \"id\": { \"type\": \"keyword\" } } } } } copy Index a document with an object field: PUT testindex 1 /_doc/ 1 { \"patient\": { \"name\": \"John Doe\", \"id\": \"123456\" } } copy Nested objects are stored as flat key/value pairs internally. To refer to a field in a nested object, use parent field. child field (for example, patient.id).\nSearch for a patient with ID 123456: GET testindex 1 /_search { \"query\": { \"term\": { \"patient.id\": \"123456\" } } } copy Parameters\nThe following table lists the parameters accepted by object field types. All parameters are optional. Parameter Description dynamic Specifies whether new fields can be dynamically added to this object. Valid values are true, false, and strict. Default is true. enabled A Boolean value that specifies whether the JSON contents of the object should be parsed. If enabled is set to false, the object’s contents are not indexed or searchable, but they are still retrievable from the _source field. Default is true. properties Fields of this object, which can be of any supported type. New properties can be dynamically added to this object if dynamic is set to true. The dynamic parameter\nThe dynamic parameter specifies whether new fields can be dynamically added to an object that is already indexed.\nFor example, you can initially create a mapping with a patient object that has only one field: PUT testindex 1 /_mappings { \"properties\": { \"patient\": { \"properties\": { \"name\": { \"type\": \"text\" } } } } } copy Then you index a document with a new id field in patient: PUT testindex 1 /_doc/ 1 { \"patient\": { \"name\": \"John Doe\", \"id\": \"123456\" } } copy As a result, the field id is added to the mappings: { \"testindex1\": { \"mappings\": { \"properties\": { \"patient\": { \"properties\": { \"id\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"name\": { \"type\": \"text\" } } } } } } } The dynamic parameter has the following valid values. Value Description true New fields can be added to the mapping dynamically. This is the default. false New fields cannot be added to the mapping dynamically. If a new field is detected, it is not indexed or searchable. However, it is still retrievable from the _source field. strict When new fields are added to the mapping dynamically, an exception is thrown. To add a new field to an object, you have to add it to the mapping first. Inner objects inherit the dynamic parameter value from their parent unless they declare their own dynamic parameter value.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "Object field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/percolator/",
    "title": "Percolator",
    "content": "A percolator field type specifies to treat this field as a query. Any JSON object field can be marked as a percolator field. Normally, documents are indexed and searches are run against them. When you use a percolator field, you store a search, and later the percolate query matches documents to that search.\nExample\nA customer is searching for a table priced at $400 or less and wants to create an alert for this search.\nCreate a mapping assigning a percolator field type to the query field: PUT testindex 1 { \"mappings\": { \"properties\": { \"search\": { \"properties\": { \"query\": { \"type\": \"percolator\" } } }, \"price\": { \"type\": \"float\" }, \"item\": { \"type\": \"text\" } } } } copy Index a query: PUT testindex 1 /_doc/ 1 { \"search\": { \"query\": { \"bool\": { \"filter\": [ { \"match\": { \"item\": { \"query\": \"table\" } } }, { \"range\": { \"price\": { \"lte\": 400.00 } } }] } } } } copy Fields referenced in the query must already exist in the mapping.\nRun a percolate query to search for matching documents: GET testindex 1 /_search { \"query\": { \"bool\": { \"filter\": { \"percolate\": { \"field\": \"search.query\", \"document\": { \"item\": \"Mahogany table\", \"price\": 399.99 } } } } } } copy The response contains the originally indexed query: { \"took\": 30, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 0.0, \"hits\": [ { \"_index\": \"testindex1\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 0.0, \"_source\": { \"search\": { \"query\": { \"bool\": { \"filter\": [ { \"match\": { \"item\": { \"query\": \"table\" } } }, { \"range\": { \"price\": { \"lte\": 400.0 } } }] } } } }, \"fields\": { \"_percolator_document_slot\": [ 0] } }] } }",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/range/",
    "title": "Range field types",
    "content": "The following table lists all range field types that OpenSearch supports. Field data type Description integer_range A range of integer values. long_range A range of long values. double_range A range of double values. float_range A range of float values. ip_range A range of IP addresses in IPv4 or IPv6 format. Start and end IP addresses may be in different formats. date_range A range of date values. Start and end dates may be in different formats. Internally, all dates are stored as unsigned 64-bit integers representing milliseconds since the epoch. Example\nCreate a mapping with a double range and a date range: PUT testindex { \"mappings\": { \"properties\": { \"gpa\": { \"type\": \"double_range\" }, \"graduation_date\": { \"type\": \"date_range\", \"format\": \"strict_year_month||strict_year_month_day\" } } } } copy Index a document with a double range and a date range: PUT testindex/_doc/ 1 { \"gpa\": { \"gte\": 1.0, \"lte\": 4.0 }, \"graduation_date\": { \"gte\": \"2019-05-01\", \"lte\": \"2019-05-15\" } } copy You can use a Term query or a Range query to search for values within range fields.\nTerm query\nA term query takes a value and matches all range fields for which the value is within the range.\nThe following query will return document 1 because 3.5 is within the range [1.0, 4.0]: GET testindex/_search { \"query\": { \"term\": { \"gpa\": { \"value\": 3.5 } } } } copy Range query\nA range query on a range field returns documents within that range. Along with the field to be matched, you can further specify a date format or relational operators with the following optional parameters: Parameter Description format\nA format for dates in this query. Default is the field’s mapped format.\nrelation\nProvides a relation between the query’s date range and the document’s date range. There are three types of relations that you can specify: 1. intersects matches documents for which there are dates that belong to both the query’s date range and document’s date range. This is the default. 2. contains matches documents for which the query’s date range is a subset of the document’s date range. 3. within matches documents for which the document’s date range is a subset of the query’s date range. To use a date format other than the field’s mapped format in a query, specify it in the format field.\nFor a full description of range query usage, including all range query parameters, see Range query.\nQuery for all graduation dates in 2019, providing the date range in a “MM/dd/yyyy” format: GET testindex 1 /_search { \"query\": { \"range\": { \"graduation_date\": { \"gte\": \"01/01/2019\", \"lte\": \"12/31/2019\", \"format\": \"MM/dd/yyyy\", \"relation\": \"within\" } } } } copy The above query will return document 1 for the within and intersects relations but will not return it for the contains relation.\nIP address ranges\nYou can specify IP address ranges in two formats: as a range and in CIDR notation.\nCreate a mapping with an IP address range: PUT testindex { \"mappings\": { \"properties\": { \"ip_address_range\": { \"type\": \"ip_range\" }, \"ip_address_cidr\": { \"type\": \"ip_range\" } } } } copy Index a document with IP address ranges in both formats: PUT testindex/_doc/ 2 { \"ip_address_range\": { \"gte\": \"10.24.34.0\", \"lte\": \"10.24.35.255\" }, \"ip_address_cidr\": \"10.24.34.0/24\" } copy Parameters\nThe following table lists the parameters accepted by range field types. All parameters are optional. Parameter Description boost A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. coerce A Boolean value that signals to truncate decimals for integer values and to convert strings to numeric values. Default is true. index A Boolean value that specifies whether the field should be searchable. Default is true. store A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/rank/",
    "title": "Rank field types",
    "content": "The following table lists all rank field types that OpenSearch supports. Field data type Description rank_feature Boosts or decreases the relevance score of documents. rank_features Boosts or decreases the relevance score of documents. Used when the list of features is sparse. Rank feature and rank features fields can be queried with rank feature queries only. They do not support aggregating or sorting.\nRank feature\nA rank feature field type uses a positive float value to boost or decrease the relevance score of a document in a rank_feature query. By default, this value boosts the relevance score. To decrease the relevance score, set the optional positive_score_impact parameter to false.\nExample\nCreate a mapping with a rank feature field: PUT chessplayers { \"mappings\": { \"properties\": { \"name\": { \"type\": \"text\" }, \"rating\": { \"type\": \"rank_feature\" }, \"age\": { \"type\": \"rank_feature\", \"positive_score_impact\": false } } } } copy Index three documents with a rank_feature field that boosts the score ( rating) and a rank_feature field that decreases the score ( age): PUT testindex 1 /_doc/ 1 { \"name\": \"John Doe\", \"rating\": 2554, \"age\": 75 } copy PUT testindex 1 /_doc/ 2 { \"name\": \"Kwaku Mensah\", \"rating\": 2067, \"age\": 10 } copy PUT testindex 1 /_doc/ 3 { \"name\": \"Nikki Wolf\", \"rating\": 1864, \"age\": 22 } copy Rank feature query\nUsing a rank feature query, you can rank players by rating, by age, or by both rating and age. If you rank players by rating, higher-rated players will have higher relevance scores. If you rank players by age, younger players will have higher relevance scores.\nUse a rank feature query to search for players based on age and rating: GET chessplayers/_search { \"query\": { \"bool\": { \"should\": [ { \"rank_feature\": { \"field\": \"rating\" } }, { \"rank_feature\": { \"field\": \"age\" } }] } } } copy When ranked by both age and rating, younger players and players who are more highly ranked score better: { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3, \"relation\": \"eq\" }, \"max_score\": 1.2093145, \"hits\": [ { \"_index\": \"chessplayers\", \"_type\": \"_doc\", \"_id\": \"2\", \"_score\": 1.2093145, \"_source\": { \"name\": \"Kwaku Mensah\", \"rating\": 1967, \"age\": 10 } }, { \"_index\": \"chessplayers\", \"_type\": \"_doc\", \"_id\": \"3\", \"_score\": 1.0150313, \"_source\": { \"name\": \"Nikki Wolf\", \"rating\": 1864, \"age\": 22 } }, { \"_index\": \"chessplayers\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 0.8098284, \"_source\": { \"name\": \"John Doe\", \"rating\": 2554, \"age\": 75 } }] } } Rank features\nA rank features field type is similar to the rank feature field type, but it is more suitable for a sparse list of features. A rank features field can index numeric feature vectors that are later used to boost or decrease documents’ relevance scores in rank_feature queries.\nExample\nCreate a mapping with a rank features field: PUT testindex 1 { \"mappings\": { \"properties\": { \"correlations\": { \"type\": \"rank_features\" } } } } copy To index a document with a rank features field, use a hashmap with string keys and positive float values: PUT testindex 1 /_doc/ 1 { \"correlations\": { \"young kids\": 1, \"older kids\": 15, \"teens\": 25.9 } } copy PUT testindex 1 /_doc/ 2 { \"correlations\": { \"teens\": 10, \"adults\": 95.7 } } copy Query the documents using a rank feature query: GET testindex 1 /_search { \"query\": { \"rank_feature\": { \"field\": \"correlations.teens\" } } } copy The response is ranked by relevance score: { \"took\": 123, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 0.6258503, \"hits\": [ { \"_index\": \"testindex1\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 0.6258503, \"_source\": { \"correlations\": { \"young kids\": 1, \"older kids\": 15, \"teens\": 25.9 } } }, { \"_index\": \"testindex1\", \"_type\": \"_doc\", \"_id\": \"2\", \"_score\": 0.39263803, \"_source\": { \"correlations\": { \"teens\": 10, \"adults\": 95.7 } } }] } } Rank feature and rank features fields use top nine significant bits for precision, leading to about 0.4% relative error. Values are stored with a relative precision of 2 −8 = 0.00390625.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/search-as-you-type/",
    "title": "Search as you type",
    "content": "A search-as-you-type field type provides search-as-you-type functionality using both prefix and infix completion.\nExample\nMapping a search-as-you-type field creates n-gram subfields of this field, where n is in the range [2, max_shingle_size]. Additionally, it creates an index prefix subfield.\nCreate a mapping with a search-as-you-type field: PUT books { \"mappings\": { \"properties\": { \"suggestions\": { \"type\": \"search_as_you_type\" } } } } copy In addition to the suggestions field, this creates suggestions._2gram, suggestions._3gram, and suggestions._index_prefix fields.\nIndex a document with a search-as-you-type field: PUT books/_doc/ 1 { \"suggestions\": \"one two three four\" } copy To match terms in any order, use a bool_prefix or multi-match query. These queries rank the documents in which search terms are in the specified order higher than the documents in which terms are out of order. GET books/_search { \"query\": { \"multi_match\": { \"query\": \"tw one\", \"type\": \"bool_prefix\", \"fields\": [ \"suggestions\", \"suggestions._2gram\", \"suggestions._3gram\"] } } } copy The response contains the matching document: { \"took\": 13, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"books\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"suggestions\": \"one two three four\" } }] } } To match terms in order, use a match_phrase_prefix query: GET books/_search { \"query\": { \"match_phrase_prefix\": { \"suggestions\": \"two th\" } } } copy The response contains the matching document: { \"took\": 23, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 0.4793051, \"hits\": [ { \"_index\": \"books\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 0.4793051, \"_source\": { \"suggestions\": \"one two three four\" } }] } } To match the last terms exactly, use a match_phrase query: GET books/_search { \"query\": { \"match_phrase\": { \"suggestions\": \"four\" } } } copy Response: { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 0.2876821, \"hits\": [ { \"_index\": \"books\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 0.2876821, \"_source\": { \"suggestions\": \"one two three four\" } }] } } Parameters\nThe following table lists the parameters accepted by search-as-you-type field types. All parameters are optional. Parameter Description analyzer The analyzer to be used for this field. By default, it will be used at index time and at search time. To override it at search time, set the search_analyzer parameter. Default is the standard analyzer, which uses grammar-based tokenization and is based on the Unicode Text Segmentation algorithm. Configures the root field and subfields. index A Boolean value that specifies whether the field should be searchable. Default is true. Configures the root field and subfields. index_options Specifies the information to be stored in the index for search and highlighting. Valid values: docs (doc number only), freqs (doc number and term frequencies), positions (doc number, term frequencies, and term positions), offsets (doc number, term frequencies, term positions, and start and end character offsets). Default is positions. Configures the root field and subfields. max_shingle_size An integer that specifies the maximum n-gram size. Valid values are in the range [2, 4]. N-grams to be created are in the range [2, max_shingle_size]. Default is 3, which creates a 2-gram and a 3-gram. Larger max_shingle_size values work better for more specific queries but lead to a larger index size. norms A Boolean value that specifies whether the field length should be used when calculating relevance scores. Configures the root field and n-gram subfields (default is false). Does not configure the prefix subfield (in the prefix subfield, norms is false). search_analyzer The analyzer to be used at search time. Default is the analyzer specified in the analyzer parameter. Configures the root field and subfields. search_quote_analyzer The analyzer to be used at search time with phrases. Default is the analyzer specified in the analyzer parameter. Configures the root field and subfields. similarity The ranking algorithm for calculating relevance scores. Default is BM25. Configures the root field and subfields. store A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false. Configures the root field only. term_vector A Boolean value that specifies whether a term vector for this field should be stored. Default is no. Configures the root field and n-gram subfields. Does not configure the prefix subfield.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "Autocomplete field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/string/",
    "title": "String field types",
    "content": "The following table lists all string field types that OpenSearch supports. Field data type Description keyword A string that is not analyzed. Useful for exact-value search. text A string that is analyzed. Useful for full-text search. token_count Counts the number of tokens in a string.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/text/",
    "title": "Text",
    "content": "A text field type contains a string that is analyzed. It is used for full-text search because it allows partial matches. Searches with multiple terms can match some but not all of them. Depending on the analyzer, results can be case insensitive, stemmed, stopwords removed, synonyms applied, etc.\nIf you need to use a field for exact-value search, map it as a keyword instead.\nExample\nCreate a mapping with a text field: PUT movies { \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\" } } } } copy Parameters\nThe following table lists the parameters accepted by text field types. All parameters are optional. Parameter Description analyzer The analyzer to be used for this field. By default, it will be used at index time and at search time. To override it at search time, set the search_analyzer parameter. Default is the standard analyzer, which uses grammar-based tokenization and is based on the Unicode Text Segmentation algorithm. boost A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. eager_global_ordinals Specifies whether global ordinals should be loaded eagerly on refresh. If the field is often used for aggregations, this parameter should be set to true. Default is false. fielddata A Boolean value that specifies whether to access analyzed tokens for this field for sorting, aggregation, and scripting. Default is false. fielddata_frequency_filter A JSON object that specifies to load into memory only those analyzed tokens whose document frequency is between the min and max values (provided as either an absolute number or a percentage). Frequency is computed per segment. Parameters: min, max, min_segment_size. Default is to load all analyzed tokens. fields To index the same string in several ways (for example, as a keyword and text), provide the fields parameter. You can specify one version of the field to be used for search and another to be used for sorting and aggregations. index A Boolean value that specifies whether the field should be searchable. Default is true. index_options Specifies the information to be stored in the index for search and highlighting. Valid values: docs (doc number only), freqs (doc number and term frequencies), positions (doc number, term frequencies, and term positions), offsets (doc number, term frequencies, term positions, and start and end character offsets). Default is positions. index_phrases A Boolean value that specifies to index 2-grams separately. 2-grams are combinations of two consecutive words in this field’s string. Leads to faster exact phrase queries with no slop but a larger index. Works best when stopwords are not removed. Default is false. index_prefixes A JSON object that specifies to index term prefixes separately. The number of characters in the prefix is between min_chars and max_chars, inclusive. Leads to faster prefix searches but a larger index. Optional parameters: min_chars, max_chars. Default min_chars is 2, max_chars is 5. meta Accepts metadata for this field. norms A Boolean value that specifies whether the field length should be used when calculating relevance scores. Default is false. position_increment_gap When text fields are analyzed, they are assigned positions. If a field contained an array of strings, and these positions were consecutive, this would lead to potentially matching across different array elements. To prevent this, an artificial gap is inserted between consecutive array elements. You can change this gap by specifying an integer position_increment_gap. Note: If slop is greater than position_element_gap, matching across different array elements may occur. Default is 100. similarity The ranking algorithm for calculating relevance scores. Default is BM25. term_vector A Boolean value that specifies whether a term vector for this field should be stored. Default is no. Term vector parameter\nA term vector is produced during analysis. It contains:\nA list of terms.\nThe ordinal position of each term.\nThe start and end character offsets of the search string within the field.\nPayloads (if available). Each term can have custom binary data associated with the term’s position.\nThe term_vector field contains a JSON object that accepts the following parameters: Parameter Stored values no None. This is the default. yes Terms in the field. with_offsets Terms and character offsets. with_positions_offsets Terms, positions, and character offsets. with_positions_offsets_payloads Terms, positions, character offsets, and payloads. with_positions Terms and positions. with_positions_payloads Terms, positions, and payloads. Storing positions is useful for proximity queries. Storing character offsets is useful for highlighting.\nTerm vector parameter example\nCreate a mapping with a text field that stores character offsets in a term vector: PUT testindex { \"mappings\": { \"properties\": { \"dob\": { \"type\": \"text\", \"term_vector\": \"with_positions_offsets\" } } } } copy Index a document with a text field: PUT testindex/_doc/ 1 { \"dob\": \"The patient's date of birth.\" } copy Query for “date of birth” and highlight it in the original field: GET testindex/_search { \"query\": { \"match\": { \"text\": \"date of birth\" } }, \"highlight\": { \"fields\": { \"text\": {} } } } copy The words “date of birth” are highlighted in the response: { \"took\": 854, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 0.8630463, \"hits\": [ { \"_index\": \"testindex\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 0.8630463, \"_source\": { \"text\": \"The patient's date of birth.\" }, \"highlight\": { \"text\": [ \"The patient's &lt;em&gt;date&lt;/em&gt; &lt;em&gt;of&lt;/em&gt; &lt;em&gt;birth&lt;/em&gt;.\"] } }] } }",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "String field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/token-count/",
    "title": "Token count",
    "content": "A token count field type stores the number of analyzed tokens in a string.\nExample\nCreate a mapping with a token count field: PUT testindex { \"mappings\": { \"properties\": { \"sentence\": { \"type\": \"text\", \"fields\": { \"num_words\": { \"type\": \"token_count\", \"analyzer\": \"english\" } } } } } } copy Index three documents with text fields: PUT testindex/_doc/ 1 { \"sentence\": \"To be, or not to be: that is the question.\" } copy PUT testindex/_doc/ 2 { \"sentence\": \"All the world’s a stage, and all the men and women are merely players.\" } copy PUT testindex/_doc/ 3 { \"sentence\": \"Now is the winter of our discontent.\" } copy Search for sentences with fewer than 10 words: GET testindex/_search { \"query\": { \"range\": { \"sentence.num_words\": { \"lt\": 10 } } } } copy The response contains one matching sentence: { \"took\": 8, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"testindex\", \"_type\": \"_doc\", \"_id\": \"3\", \"_score\": 1.0, \"_source\": { \"sentence\": \"Now is the winter of our discontent.\" } }] } } Parameters\nThe following table lists the parameters accepted by token count field types. The analyzer parameter is required; all other parameters are optional. Parameter Description analyzer The analyzer to be used for this field. Specify an analyzer without token filters for optimal performance. Required. boost A floating-point value that specifies the weight of this field toward the relevance score. Values above 1.0 increase the field’s relevance. Values between 0.0 and 1.0 decrease the field’s relevance. Default is 1.0. doc_values A Boolean value that specifies whether the field should be stored on disk so that it can be used for aggregations, sorting, or scripting. Default is false. enable_position_increments A Boolean value that specifies whether position increments should be counted. To avoid removing stopwords, set this field to false. Default is true. index A Boolean value that specifies whether the field should be searchable. Default is true. null_value A value to be used in place of null. Must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null. store A Boolean value that specifies whether the field value should be stored and can be retrieved separately from the _source field. Default is false.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "String field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/xy-point/",
    "title": "xy point",
    "content": "An xy point field type contains a point in a two-dimensional Cartesian coordinate system, specified by x and y coordinates. It is based on the Lucene XYPoint field type. The xy point field type is similar to the geopoint field type, but does not have the range limitations of geopoint. The coordinates of an xy point are single-precision floating-point values. For information about the range and precision of floating-point values, see Numeric field types.\nExample\nCreate a mapping with an xy point field type: PUT testindex 1 { \"mappings\": { \"properties\": { \"point\": { \"type\": \"xy_point\" } } } } copy Formats\nxy points can be indexed in the following formats:\nAn object with x and y coordinates PUT testindex 1 /_doc/ 1 { \"point\": { \"x\": 0.5, \"y\": 4.5 } } copy A string in the “ x, y ” format PUT testindex 1 /_doc/ 2 { \"point\": \"0.5, 4.5\" } copy An array in the [ x, y] format PUT testindex 1 /_doc/ 3 { \"point\": [ 0.5, 4.5] } copy A well-known text (WKT) POINT in the “POINT( x y)” format PUT testindex 1 /_doc/ 4 { \"point\": \"POINT (0.5 4.5)\" } copy GeoJSON format PUT testindex 1 /_doc/ 5 { \"point\": { \"type\": \"Point\", \"coordinates\": [ 0.5, 4.5] } } copy In all xy point formats, the coordinates must be specified in the x, y order.\nParameters\nThe following table lists the parameters accepted by xy point field types. All parameters are optional. Parameter Description ignore_malformed A Boolean value that specifies to ignore malformed values and not to throw an exception. Default is false. ignore_z_value Specific to points with three coordinates. If ignore_z_value is true, the third coordinate is not indexed but is still stored in the _source field. If ignore_z_value is false, an exception is thrown. null_value A value to be used in place of null. The value must be of the same type as the field. If this parameter is not specified, the field is treated as missing when its value is null. Default is null.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "Cartesian field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/xy-shape/",
    "title": "xy shape",
    "content": "An xy shape field type contains a shape, such as a polygon or a collection of xy points. It is based on the Lucene XYShape field type. To index an xy shape, OpenSearch tessellates the shape into a triangular mesh and stores each triangle in a BKD tree (a set of balanced k-dimensional trees). This provides a 10 -7 decimal degree of precision, which represents near-perfect spatial resolution.\nThe xy shape field type is similar to the geoshape field type, but it represents shapes on the Cartesian plane, which is not based on the Earth-fixed terrestrial reference system. The coordinates of an xy shape are single-precision floating-point values. For information about the range and precision of floating-point values, see Numeric field types.\nExample\nCreate a mapping with an xy shape field type: PUT testindex { \"mappings\": { \"properties\": { \"location\": { \"type\": \"xy_shape\" } } } } copy Formats\nxy shapes can be indexed in the following formats: GeoJSON Well-known text (WKT) In both GeoJSON and WKT, the coordinates must be specified in the x, y order within coordinate arrays.\nxy shape types\nThe following table describes the possible xy shape types and their relationship to the GeoJSON and WKT types. OpenSearch type GeoJSON type WKT type Description point Point\nPOINT\nA geographic point specified by the x and y coordinates. linestring LineString\nLINESTRING\nA line specified by two or more points. May be a straight line or a path of connected line segments. polygon Polygon\nPOLYGON\nA polygon specified by a list of vertices in coordinate form. The polygon must be closed, meaning the last point must be the same as the first point. Therefore, to create an n-gon, n+1 vertices are required. The minimum number of vertices is four, which creates a triangle. multipoint MultiPoint\nMULTIPOINT\nAn array of discrete related points that are not connected. multilinestring MultiLineString\nMULTILINESTRING\nAn array of linestrings. multipolygon MultiPolygon\nMULTIPOLYGON\nAn array of polygons. geometrycollection GeometryCollection\nGEOMETRYCOLLECTION\nA collection of xy shapes that may be of different types. envelope N/A\nBBOX\nA bounding rectangle specified by upper-left and lower-right vertices. Point\nA point is specified by a single pair of coordinates.\nIndex a point in GeoJSON format: PUT testindex/_doc/ 1 { \"location\": { \"type\": \"point\", \"coordinates\": [ 0.5, 4.5] } } copy Index a point in WKT format: PUT testindex/_doc/ 1 { \"location\": \"POINT (0.5 4.5)\" } copy Linestring\nA linestring is a line specified by two or more points. If the points are collinear, the linestring is a straight line. Otherwise, the linestring represents a path made of line segments.\nIndex a linestring in GeoJSON format: PUT testindex/_doc/ 2 { \"location\": { \"type\": \"linestring\", \"coordinates\": [[ 0.5, 4.5], [ -1.5, 2.3]] } } copy Index a linestring in WKT format: PUT testindex/_doc/ 2 { \"location\": \"LINESTRING (0.5 4.5, -1.5 2.3)\" } copy Polygon\nA polygon is specified by a list of vertices in coordinate form. The polygon must be closed, meaning the last point must be the same as the first point. In the following example, a triangle is created using four points.\nGeoJSON requires that you list the vertices of the polygon counterclockwise. WKT does not impose a specific order on vertices.\nIndex a polygon (triangle) in GeoJSON format: PUT testindex/_doc/ 3 { \"location\": { \"type\": \"polygon\", \"coordinates\": [ [[ 0.5, 4.5], [ 2.5, 6.0], [ 1.5, 2.0], [ 0.5, 4.5]]] } } copy Index a polygon (triangle) in WKT format: PUT testindex/_doc/ 3 { \"location\": \"POLYGON ((0.5 4.5, 2.5 6.0, 1.5 2.0, 0.5 4.5))\" } copy The polygon may have holes inside. In this case, the coordinates field will contain multiple arrays. The first array represents the outer polygon, and each subsequent array represents a hole. Holes are represented as polygons and specified as arrays of coordinates.\nGeoJSON requires that you list the vertices of the polygon counterclockwise and the vertices of the hole clockwise. WKT does not impose a specific order on vertices.\nIndex a polygon (triangle) with a triangular hole in GeoJSON format: PUT testindex/_doc/ 4 { \"location\": { \"type\": \"polygon\", \"coordinates\": [ [[ 0.5, 4.5], [ 2.5, 6.0], [ 1.5, 2.0], [ 0.5, 4.5]], [[ 1.0, 4.5], [ 1.5, 4.5], [ 1.5, 4.0], [ 1.0, 4.5]]] } } copy Index a polygon (triangle) with a triangular hole in WKT format: PUT testindex/_doc/ 4 { \"location\": \"POLYGON ((0.5 4.5, 2.5 6.0, 1.5 2.0, 0.5 4.5), (1.0 4.5, 1.5 4.5, 1.5 4.0, 1.0 4.5))\" } copy By default, the vertices of the polygon are traversed in a counterclockwise order. You can define an orientation parameter to specify the vertex traversal order at mapping time: PUT testindex { \"mappings\": { \"properties\": { \"location\": { \"type\": \"xy_shape\", \"orientation\": \"left\" } } } } copy Subsequently indexed documents can override the orientation setting: PUT testindex/_doc/ 3 { \"location\": { \"type\": \"polygon\", \"orientation\": \"cw\", \"coordinates\": [ [[ 0.5, 4.5], [ 2.5, 6.0], [ 1.5, 2.0], [ 0.5, 4.5]]] } } copy Multipoint\nA multipoint is an array of discrete related points that are not connected.\nIndex a multipoint in GeoJSON format: PUT testindex/_doc/ 6 { \"location\": { \"type\": \"multipoint\", \"coordinates\": [ [ 0.5, 4.5], [ 2.5, 6.0]] } } copy Index a multipoint in WKT format: PUT testindex/_doc/ 6 { \"location\": \"MULTIPOINT (0.5 4.5, 2.5 6.0)\" } copy Multilinestring\nA multilinestring is an array of linestrings.\nIndex a multilinestring in GeoJSON format: PUT testindex/_doc/ 2 { \"location\": { \"type\": \"multilinestring\", \"coordinates\": [ [[ 0.5, 4.5], [ 2.5, 6.0]], [[ 1.5, 2.0], [ 3.5, 3.5]]] } } copy Index a linestring in WKT format: PUT testindex/_doc/ 2 { \"location\": \"MULTILINESTRING ((0.5 4.5, 2.5 6.0), (1.5 2.0, 3.5 3.5))\" } copy Multipolygon\nA multipolygon is an array of polygons. In this example, the first polygon contains a hole, and the second does not.\nIndex a multipolygon in GeoJSON format: PUT testindex/_doc/ 4 { \"location\": { \"type\": \"multipolygon\", \"coordinates\": [ [ [[ 0.5, 4.5], [ 2.5, 6.0], [ 1.5, 2.0], [ 0.5, 4.5]], [[ 1.0, 4.5], [ 1.5, 4.5], [ 1.5, 4.0], [ 1.0, 4.5]]], [ [[ 2.0, 0.0], [ 1.0, 2.0], [ 3.0, 1.0], [ 2.0, 0.0]]]] } } copy Index a multipolygon in WKT format: PUT testindex/_doc/ 4 { \"location\": \"MULTIPOLYGON (((0.5 4.5, 2.5 6.0, 1.5 2.0, 0.5 4.5), (1.0 4.5, 1.5 4.5, 1.5 4.0, 1.0 4.5)), ((2.0 0.0, 1.0 2.0, 3.0 1.0, 2.0 0.0)))\" } copy Geometry collection\nA geometry collection is a collection of xy shapes that may be of different types.\nIndex a geometry collection in GeoJSON format: PUT testindex/_doc/ 7 { \"location\": { \"type\": \"geometrycollection\", \"geometries\": [ { \"type\": \"point\", \"coordinates\": [ 0.5, 4.5] }, { \"type\": \"linestring\", \"coordinates\": [[ 2.5, 6.0], [ 1.5, 2.0]] }] } } copy Index a geometry collection in WKT format: PUT testindex/_doc/ 7 { \"location\": \"GEOMETRYCOLLECTION (POINT (0.5 4.5), LINESTRING(2.5 6.0, 1.5 2.0))\" } copy Envelope\nAn envelope is a bounding rectangle specified by upper-left and lower-right vertices. The GeoJSON format is [[minX, maxY], [maxX, minY]].\nIndex an envelope in GeoJSON format: PUT testindex/_doc/ 2 { \"location\": { \"type\": \"envelope\", \"coordinates\": [[ 3.0, 2.0], [ 6.0, 0.0]] } } copy In WKT format, use BBOX (minX, maxY, maxX, minY).\nIndex an envelope in WKT BBOX format: PUT testindex/_doc/ 8 { \"location\": \"BBOX (3.0, 2.0, 6.0, 0.0)\" } copy Parameters\nThe following table lists the parameters accepted by xy shape field types. All parameters are optional. Parameter Description coerce A Boolean value that specifies whether to automatically close unclosed linear rings. Default is false. ignore_malformed A Boolean value that specifies to ignore malformed GeoJSON or WKT xy shapes and not to throw an exception. Default is false (throw an exception when xy shapes are malformed). ignore_z_value Specific to points with three coordinates. If ignore_z_value is true, the third coordinate is not indexed but is still stored in the _source field. If ignore_z_value is false, an exception is thrown. Default is true. orientation Specifies the traversal order of the vertices in the xy shape’s list of coordinates. orientation takes the following values: 1. RIGHT: counterclockwise. Specify RIGHT orientation by using one of the following strings (uppercase or lowercase): right, counterclockwise, ccw. 2. LEFT: clockwise. Specify LEFT orientation by using one of the following strings (uppercase or lowercase): left, clockwise, cw. This value can be overridden by individual documents. Default is RIGHT.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types",
      "Cartesian field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/field-types/xy/",
    "title": "Cartesian field types",
    "content": "Cartesian field types facilitate indexing and searching of points and shapes in a two-dimensional Cartesian coordinate system. Cartesian field types are similar to geographic field types, except they represent points and shapes on the Cartesian plane, which is not based on the Earth-fixed terrestrial reference system. Calculating distances on a plane is more efficient than calculating distances on a sphere, so distance sorting is faster for Cartesian field types.\nCartesian field types work well for spatial applications like virtual reality, computer-aided design (CAD), and amusement park and sporting venue mapping.\nThe coordinates for the Cartesian field types are single-precision floating-point values. For information about the range and precision of floating-point values, see Numeric field types.\nThe following table lists all Cartesian field types that OpenSearch supports. Field Data type Description xy_point A point in a two-dimensional Cartesian coordinate system, specified by x and y coordinates. xy_shape A shape, such as a polygon or a collection of xy points, in a two-dimensional Cartesian coordinate system. Currently, OpenSearch supports indexing and searching of Cartesian field types but not aggregations on Cartesian field types. If you’d like to see aggregations implemented, open a GitHub issue.",
    "ancestors": [
      "Mappings and field types",
      "Supported field types"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/OSC-dot-net/",
    "title": "Getting started with the high-level .NET client",
    "content": "OpenSearch.Client is a high-level.NET client. It provides strongly typed requests and responses as well as Query DSL. It frees you from constructing raw JSON requests and parsing raw JSON responses by providing models that parse and serialize/deserialize requests and responses automatically. OpenSearch.Client also exposes the OpenSearch.Net low-level client if you need it. For the client’s complete API documentation, see the OpenSearch.Client API documentation.\nThis getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-net repo.\nInstalling OpenSearch.Client\nTo install OpenSearch.Client, download the OpenSearch.Client NuGet package and add it to your project in an IDE of your choice. In Microsoft Visual Studio, follow the steps below:\nIn the Solution Explorer panel, right-click on your solution or project and select Manage NuGet Packages for Solution.\nSearch for the OpenSearch.Client NuGet package, and select Install.\nAlternatively, you can add OpenSearch.Client to your.csproj file: &lt;Project&gt;... &lt;ItemGroup&gt; &lt;PackageReference Include= \"OpenSearch.Client\" Version= \"1.0.0\" /&gt; &lt;/ItemGroup&gt; &lt;/Project&gt; copy Example\nThe following example illustrates connecting to OpenSearch, indexing documents, and sending queries on the data. It uses the Student class to represent one student, which is equivalent to one document in the index. public class Student { public int Id { get; init; } public string FirstName { get; init; } public string LastName { get; init; } public int GradYear { get; init; } public double Gpa { get; init; } } copy By default, OpenSearch.Client uses camel case to convert property names to field names.\nConnecting to OpenSearch\nUse the default constructor when creating an OpenSearchClient object to connect to the default OpenSearch host ( http://localhost:9200). var client = new OpenSearchClient (); copy To connect to your OpenSearch cluster through a single node with a known address, specify this address when creating an instance of OpenSearch.Client: var nodeAddress = new Uri ( \"http://myserver:9200\"); var client = new OpenSearchClient ( nodeAddress); copy You can also connect to OpenSearch through multiple nodes. Connecting to your OpenSearch cluster with a node pool provides advantages like load balancing and cluster failover support. To connect to your OpenSearch cluster using multiple nodes, specify their addresses and create a ConnectionSettings object for the OpenSearch.Client instance: var nodes = new Uri [] { new Uri ( \"http://myserver1:9200\"), new Uri ( \"http://myserver2:9200\"), new Uri ( \"http://myserver3:9200\") }; var pool = new StaticConnectionPool ( nodes); var settings = new ConnectionSettings ( pool); var client = new OpenSearchClient ( settings); copy Using ConnectionSettings ConnectionConfiguration is used to pass configuration options to the low-level OpenSearch.Net client. ConnectionSettings inherits from ConnectionConfiguration and provides additional configuration options.\nTo set the address of the node and the default index name for requests that don’t specify the index name, create a ConnectionSettings object: var node = new Uri ( \"http://myserver:9200\"); var config = new ConnectionSettings ( node). DefaultIndex ( \"students\"); var client = new OpenSearchClient ( config); copy Indexing one document\nCreate one instance of Student: var student = new Student { Id = 100, FirstName = \"Paulo\", LastName = \"Santos\", Gpa = 3.93, GradYear = 2021 }; copy To index one document, you can use either fluent lambda syntax or object initializer syntax.\nIndex this Student into the students index using fluent lambda syntax: var response = client. Index ( student, i =&gt; i. Index ( \"students\")); copy Index this Student into the students index using object initializer syntax: var response = client. Index ( new IndexRequest &lt; Student &gt;( student, \"students\")); copy Indexing many documents\nYou can index many documents from a collection at the same time by using the OpenSearch.Client’s IndexMany method: var studentArray = new Student [] { new () { Id = 200, FirstName = \"Shirley\", LastName = \"Rodriguez\", Gpa = 3.91, GradYear = 2019 }, new () { Id = 300, FirstName = \"Nikki\", LastName = \"Wolf\", Gpa = 3.87, GradYear = 2020 } }; var manyResponse = client. IndexMany ( studentArray, \"students\"); copy Searching for a document\nTo search for a student indexed above, you want to construct a query that is analogous to the following Query DSL query: GET students/_search { \"query\": { \"match\": { \"lastName\": \"Santos\" } } } The query above is a shorthand version of the following explicit query: GET students/_search { \"query\": { \"match\": { \"lastName\": { \"query\": \"Santos\" } } } } In OpenSearch.Client, this query looks like this: var searchResponse = client. Search &lt; Student &gt;( s =&gt; s. Index ( \"students\"). Query ( q =&gt; q. Match ( m =&gt; m. Field ( fld =&gt; fld. LastName). Query ( \"Santos\")))); copy You can print out the results by accessing the documents in the response: if ( searchResponse. IsValid) { foreach ( var s in searchResponse. Documents) { Console. WriteLine ( $\" { s. Id } { s. LastName } { s. FirstName } { s. Gpa } { s. GradYear } \"); } } copy The response contains one document, which corresponds to the correct student: 100 Santos Paulo 3.93 2021 Using OpenSearch.Client methods asynchronously\nFor applications that require asynchronous code, all method calls in OpenSearch.Client have asynchronous counterparts: // synchronous method var response = client. Index ( student, i =&gt; i. Index ( \"students\")); // asynchronous method var response = await client. IndexAsync ( student, i =&gt; i. Index ( \"students\")); Falling back on the low-level OpenSearch.Net client\nOpenSearch.Client exposes the low-level the OpenSearch.Net client you can use if anything is missing: var lowLevelClient = client. LowLevel; var searchResponseLow = lowLevelClient. Search &lt; SearchResponse &lt; Student &gt;&gt;( \"students\", PostData. Serializable ( new { query = new { match = new { lastName = new { query = \"Santos\" } } } })); if ( searchResponseLow. IsValid) { foreach ( var s in searchResponseLow. Documents) { Console. WriteLine ( $\" { s. Id } { s. LastName } { s. FirstName } { s. Gpa } { s. GradYear } \"); } } copy Sample program\nThe following is a complete sample program that illustrates all of the concepts described above. It uses the Student class defined above. using OpenSearch.Client; using OpenSearch.Net; namespace NetClientProgram; internal class Program { private static IOpenSearchClient osClient = new OpenSearchClient (); public static void Main ( string [] args) { Console. WriteLine ( \"Indexing one student......\"); var student = new Student { Id = 100, FirstName = \"Paulo\", LastName = \"Santos\", Gpa = 3.93, GradYear = 2021 }; var response = osClient. Index ( student, i =&gt; i. Index ( \"students\")); Console. WriteLine ( response. IsValid? \"Response received\": \"Error\"); Console. WriteLine ( \"Searching for one student......\"); SearchForOneStudent (); Console. WriteLine ( \"Searching using low-level client......\"); SearchLowLevel (); Console. WriteLine ( \"Indexing an array of Student objects......\"); var studentArray = new Student [] { new () { Id = 200, FirstName = \"Shirley\", LastName = \"Rodriguez\", Gpa = 3.91, GradYear = 2019 }, new () { Id = 300, FirstName = \"Nikki\", LastName = \"Wolf\", Gpa = 3.87, GradYear = 2020 } }; var manyResponse = osClient. IndexMany ( studentArray, \"students\"); Console. WriteLine ( manyResponse. IsValid? \"Response received\": \"Error\"); } private static void SearchForOneStudent () { var searchResponse = osClient. Search &lt; Student &gt;( s =&gt; s. Index ( \"students\"). Query ( q =&gt; q. Match ( m =&gt; m. Field ( fld =&gt; fld. LastName). Query ( \"Santos\")))); PrintResponse ( searchResponse); } private static void SearchLowLevel () { // Search for the student using the low-level client var lowLevelClient = osClient. LowLevel; var searchResponseLow = lowLevelClient. Search &lt; SearchResponse &lt; Student &gt;&gt; ( \"students\", PostData. Serializable ( new { query = new { match = new { lastName = new { query = \"Santos\" } } } })); PrintResponse ( searchResponseLow); } private static void PrintResponse ( ISearchResponse &lt; Student &gt; response) { if ( response. IsValid) { foreach ( var s in response. Documents) { Console. WriteLine ( $\" { s. Id } { s. LastName } \" + $\" { s. FirstName } { s. Gpa } { s. GradYear } \"); } } else { Console. WriteLine ( \"Student not found.\"); } } } copy",
    "ancestors": [
      "Clients",
      ".NET clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/OSC-example/",
    "title": "More advanced features of the high-level .NET client",
    "content": "The following example illustrates more advanced features of OpenSearch.Client. For a simple example, see the Getting started guide. This example uses the following Student class. public class Student { public int Id { get; init; } public string FirstName { get; init; } public string LastName { get; init; } public int GradYear { get; init; } public double Gpa { get; init; } } copy Mappings\nOpenSearch uses dynamic mapping to infer field types of the documents that are indexed. However, to have more control over the schema of your document, you can pass an explicit mapping to OpenSearch. You can define data types for some or all fields of your document in this mapping.\nSimilarly, OpenSearch.Client uses auto mapping to infer field data types based on the types of the class’s properties. To use auto mapping, create a students index using the AutoMap’s default constructor: var createResponse = await osClient. Indices. CreateAsync ( \"students\", c =&gt; c. Map ( m =&gt; m. AutoMap &lt; Student &gt;())); copy If you use auto mapping, Id and GradYear are mapped as integers, Gpa is mapped as a double, and FirstName and LastName are mapped as text with a keyword subfield. If you want to search for FirstName and LastName and allow only case-sensitive full matches, you can suppress analyzing by mapping these fields as keyword only. In Query DSL, you can accomplish this using the following query: PUT students { \"mappings\": { \"properties\": { \"firstName\": { \"type\": \"keyword\" }, \"lastName\": { \"type\": \"keyword\" } } } } In OpenSearch.Client, you can use fluid lambda syntax to mark these fields as keywords: var createResponse = await osClient. Indices. CreateAsync ( index, c =&gt; c. Map ( m =&gt; m. AutoMap &lt; Student &gt;(). Properties &lt; Student &gt;( p =&gt; p. Keyword ( k =&gt; k. Name ( f =&gt; f. FirstName)). Keyword ( k =&gt; k. Name ( f =&gt; f. LastName))))); copy Settings\nIn addition to mappings, you can specify settings like the number of primary and replica shards when creating an index. The following query sets the number of primary shards to 1 and the number of replica shards to 2: PUT students { \"mappings\": { \"properties\": { \"firstName\": { \"type\": \"keyword\" }, \"lastName\": { \"type\": \"keyword\" } } }, \"settings\": { \"number_of_shards\": 1, \"number_of_replicas\": 2 } } In OpenSearch.Client, the equivalent of the above query is the following: var createResponse = await osClient. Indices. CreateAsync ( index, c =&gt; c. Map ( m =&gt; m. AutoMap &lt; Student &gt;(). Properties &lt; Student &gt;( p =&gt; p. Keyword ( k =&gt; k. Name ( f =&gt; f. FirstName)). Keyword ( k =&gt; k. Name ( f =&gt; f. LastName)))). Settings ( s =&gt; s. NumberOfShards ( 1). NumberOfReplicas ( 2))); copy Indexing multiple documents using the Bulk API\nIn addition to indexing one document using Index and IndexDocument and indexing multiple documents using IndexMany, you can gain more control over document indexing by using Bulk or BulkAll. Indexing documents individually is inefficient because it creates an HTTP request for every document sent. The BulkAll helper frees you from handling retry, chunking or back off request functionality. It automatically retries if the request fails, backs off if the server is down, and controls how many documents are sent in one HTTP request.\nIn the following example, BulkAll is configured with the index name, number of back off retries, and back off time. Additionally, the maximum degrees of parallelism setting controls the number of parallel HTTP requests containing the data. Finally, the size parameter signals how many documents are sent in one HTTP request.\nWe recommend setting the size to 100–1000 documents in production. BulkAll takes a stream of data and returns an Observable that you can use to observe the background operation. var bulkAll = osClient. BulkAll ( ReadData (), r =&gt; r. Index ( index). BackOffRetries ( 2). BackOffTime ( \"30s\"). MaxDegreeOfParallelism ( 4). Size ( 100)); copy Searching with Boolean query\nOpenSearch.Client exposes full OpenSearch query capability. In addition to simple searches that use the match query, you can create a more complex Boolean query to search for students who graduated in 2022 and sort them by last name. In the example below, search is limited to 10 documents, and the scroll API is used to control the pagination of results. var gradResponse = await osClient. SearchAsync &lt; Student &gt;( s =&gt; s. Index ( index). From ( 0). Size ( 10). Scroll ( \"1m\"). Query ( q =&gt; q. Bool ( b =&gt; b. Filter ( f =&gt; f. Term ( t =&gt; t. Field ( fld =&gt; fld. GradYear). Value ( 2022))))). Sort ( srt =&gt; srt. Ascending ( f =&gt; f. LastName))); copy The response contains the Documents property with matching documents from OpenSearch. The data is in the form of deserialized JSON objects of Student type, so you can access their properties in a strongly typed fashion. All serialization and deserialization is handled by OpenSearch.Client.\nAggregations\nOpenSearch.Client includes the full OpenSearch query functionality, including aggregations. In addition to grouping search results into buckets (for example, grouping students by GPA ranges), you can calculate metrics like sum or average. The following query calculates the average GPA of all students in the index.\nSetting Size to 0 means OpenSearch will only return the aggregation, not the actual documents. var aggResponse = await osClient. SearchAsync &lt; Student &gt;( s =&gt; s. Index ( index). Size ( 0). Aggregations ( a =&gt; a. Average ( \"average gpa\", avg =&gt; avg. Field ( fld =&gt; fld. Gpa)))); copy Sample program for creating an index and indexing data\nThe following program creates an index, reads a stream of student records from a comma-separated file and indexes this data into OpenSearch. using OpenSearch.Client; namespace NetClientProgram; internal class Program { private const string index = \"students\"; public static IOpenSearchClient osClient = new OpenSearchClient (); public static async Task Main ( string [] args) { // Check if the index with the name \"students\" exists var existResponse = await osClient. Indices. ExistsAsync ( index); if (! existResponse. Exists) // There is no index with this name { // Create an index \"students\" // Map FirstName and LastName as keyword var createResponse = await osClient. Indices. CreateAsync ( index, c =&gt; c. Map ( m =&gt; m. AutoMap &lt; Student &gt;(). Properties &lt; Student &gt;( p =&gt; p. Keyword ( k =&gt; k. Name ( f =&gt; f. FirstName)). Keyword ( k =&gt; k. Name ( f =&gt; f. LastName)))). Settings ( s =&gt; s. NumberOfShards ( 1). NumberOfReplicas ( 1))); if (! createResponse. IsValid &amp;&amp;! createResponse. Acknowledged) { throw new Exception ( \"Create response is invalid.\"); } // Take a stream of data and send it to OpenSearch var bulkAll = osClient. BulkAll ( ReadData (), r =&gt; r. Index ( index). BackOffRetries ( 2). BackOffTime ( \"20s\"). MaxDegreeOfParallelism ( 4). Size ( 10)); // Wait until the data upload is complete. // FromMinutes specifies a timeout. // r is a response object that is returned as the data is indexed. bulkAll. Wait ( TimeSpan. FromMinutes ( 10), r =&gt; Console. WriteLine ( \"Data chunk indexed\")); } } // Reads student data in the form \"Id,FirsName,LastName,GradYear,Gpa\" public static IEnumerable &lt; Student &gt; ReadData () { var file = new StreamReader ( \"C:\\\\search\\\\students.csv\"); string s; while (( s = file. ReadLine ()) is not null) { yield return new Student ( s); } } } copy Sample program for search\nThe following program searches students by name and graduation date and calculates the average GPA. using OpenSearch.Client; namespace NetClientProgram; internal class Program { private const string index = \"students\"; public static IOpenSearchClient osClient = new OpenSearchClient (); public static async Task Main ( string [] args) { await SearchByName (); await SearchByGradDate (); await CalculateAverageGpa (); } private static async Task SearchByName () { Console. WriteLine ( \"Searching for name......\"); var nameResponse = await osClient. SearchAsync &lt; Student &gt;( s =&gt; s. Index ( index). Query ( q =&gt; q. Match ( m =&gt; m. Field ( fld =&gt; fld. FirstName). Query ( \"Zhang\")))); if (! nameResponse. IsValid) { throw new Exception ( \"Aggregation query response is not valid.\"); } foreach ( var s in nameResponse. Documents) { Console. WriteLine ( $\" { s. Id } { s. LastName } \" + $\" { s. FirstName } { s. Gpa } { s. GradYear } \"); } } private static async Task SearchByGradDate () { Console. WriteLine ( \"Searching for grad date......\"); // Search for all students who graduated in 2022 var gradResponse = await osClient. SearchAsync &lt; Student &gt;( s =&gt; s. Index ( index). From ( 0). Size ( 2). Scroll ( \"1m\"). Query ( q =&gt; q. Bool ( b =&gt; b. Filter ( f =&gt; f. Term ( t =&gt; t. Field ( fld =&gt; fld. GradYear). Value ( 2022))))). Sort ( srt =&gt; srt. Ascending ( f =&gt; f. LastName)). Size ( 10)); if (! gradResponse. IsValid) { throw new Exception ( \"Grad date query response is not valid.\"); } while ( gradResponse. Documents. Any ()) { foreach ( var data in gradResponse. Documents) { Console. WriteLine ( $\" { data. Id } { data. LastName } { data. FirstName } \" + $\" { data. Gpa } { data. GradYear } \"); } gradResponse = osClient. Scroll &lt; Student &gt;( \"1m\", gradResponse. ScrollId); } } public static async Task CalculateAverageGpa () { Console. WriteLine ( \"Calculating average GPA......\"); // Search and aggregate // Size 0 means documents are not returned, only aggregation is returned var aggResponse = await osClient. SearchAsync &lt; Student &gt;( s =&gt; s. Index ( index). Size ( 0). Aggregations ( a =&gt; a. Average ( \"average gpa\", avg =&gt; avg. Field ( fld =&gt; fld. Gpa)))); if (! aggResponse. IsValid) throw new Exception ( \"Aggregation response not valid\"); var avg = aggResponse. Aggregations. Average ( \"average gpa\"). Value; Console. WriteLine ( $\"Average GPA is { avg } \"); } } copy",
    "ancestors": [
      "Clients",
      ".NET clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/OpenSearch-dot-net/",
    "title": "Low-level .NET client",
    "content": "OpenSearch.Net is a low-level.NET client that provides the foundational layer of communication with OpenSearch. It is dependency free, and it can handle round-robin load balancing, transport, and the basic request/response cycle. OpenSearch.Net contains all OpenSearch API endpoints as methods. When using OpenSearch.Net, you need to construct the queries yourself.\nThis getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-net repo.\nExample\nThe following example illustrates connecting to OpenSearch, indexing documents, and sending queries on the data. It uses the Student class to represent one student, which is equivalent to one document in the index. public class Student { public int Id { get; init; } public string FirstName { get; init; } public string LastName { get; init; } public int GradYear { get; init; } public double Gpa { get; init; } } copy Installing the Opensearch.Net client\nTo install Opensearch.Net, download the Opensearch.Net NuGet package and add it to your project in an IDE of your choice. In Microsoft Visual Studio, follow the steps below:\nIn the Solution Explorer panel, right-click on your solution or project and select Manage NuGet Packages for Solution.\nSearch for the OpenSearch.Net NuGet package, and select Install.\nAlternatively, you can add OpenSearch.Net to your.csproj file: &lt;Project&gt;... &lt;ItemGroup&gt; &lt;PackageReference Include= \"Opensearch.Net\" Version= \"1.0.0\" /&gt; &lt;/ItemGroup&gt; &lt;/Project&gt; copy Connecting to OpenSearch\nUse the default constructor when creating an OpenSearchLowLevelClient object to connect to the default OpenSearch host ( http://localhost:9200). var client = new OpenSearchLowLevelClient (); copy To connect to your OpenSearch cluster through a single node with a known address, create a ConnectionConfiguration object with that address and pass it to the OpenSearch.Net constructor: var nodeAddress = new Uri ( \"http://myserver:9200\"); var config = new ConnectionConfiguration ( nodeAddress); var client = new OpenSearchLowLevelClient ( config); copy You can also use a connection pool to manage the nodes in the cluster. Additionally, you can set up a connection configuration to have OpenSearch return the response as formatted JSON. var uri = new Uri ( \"http://localhost:9200\"); var connectionPool = new SingleNodeConnectionPool ( uri); var settings = new ConnectionConfiguration ( connectionPool). PrettyJson (); var client = new OpenSearchLowLevelClient ( settings); copy To connect to your OpenSearch cluster using multiple nodes, create a connection pool with their addresses. In this example, a SniffingConnectionPool is used because it keeps track of nodes being removed or added to the cluster, so it works best for clusters that scale automatically. var uris = new [] { new Uri ( \"http://localhost:9200\"), new Uri ( \"http://localhost:9201\"), new Uri ( \"http://localhost:9202\") }; var connectionPool = new SniffingConnectionPool ( uris); var settings = new ConnectionConfiguration ( connectionPool). PrettyJson (); var client = new OpenSearchLowLevelClient ( settings); copy Connecting to Amazon OpenSearch Service\nThe following example illustrates connecting to Amazon OpenSearch Service: using OpenSearch.Client; using OpenSearch.Net.Auth.AwsSigV4; namespace Application { class Program { static void Main ( string [] args) { var endpoint = new Uri ( \"https://search-xxx.region.es.amazonaws.com\"); var connection = new AwsSigV4HttpConnection ( RegionEndpoint. APSoutheast2, service: AwsSigV4HttpConnection. OpenSearchService); var config = new ConnectionSettings ( endpoint, connection); var client = new OpenSearchClient ( config); Console. WriteLine ( $\" { client. RootNodeInfo (). Version. Distribution }: { client. RootNodeInfo (). Version. Number } \"); } } } copy Connecting to Amazon OpenSearch Serverless\nThe following example illustrates connecting to Amazon OpenSearch Serverless Service: using OpenSearch.Client; using OpenSearch.Net.Auth.AwsSigV4; namespace Application { class Program { static void Main ( string [] args) { var endpoint = new Uri ( \"https://search-xxx.region.aoss.amazonaws.com\"); var connection = new AwsSigV4HttpConnection ( RegionEndpoint. APSoutheast2, service: AwsSigV4HttpConnection. OpenSearchServerlessService); var config = new ConnectionSettings ( endpoint, connection); var client = new OpenSearchClient ( config); Console. WriteLine ( $\" { client. RootNodeInfo (). Version. Distribution }: { client. RootNodeInfo (). Version. Number } \"); } } } copy Using ConnectionSettings ConnectionConfiguration is used to pass configuration options to the OpenSearch.Net client. ConnectionSettings inherits from ConnectionConfiguration and provides additional configuration options.\nThe following example uses ConnectionSettings to:\nSet the default index name for requests that don’t specify the index name.\nEnable gzip-compressed requests and responses.\nSignal to OpenSearch to return formatted JSON.\nMake field names lowercase. var uri = new Uri ( \"http://localhost:9200\"); var connectionPool = new SingleNodeConnectionPool ( uri); var settings = new ConnectionSettings ( connectionPool). DefaultIndex ( \"students\"). EnableHttpCompression (). PrettyJson (). DefaultFieldNameInferrer ( f =&gt; f. ToLower ()); var client = new OpenSearchLowLevelClient ( settings); copy Indexing one document\nTo index a document, you first need to create an instance of the Student class: var student = new Student { Id = 100, FirstName = \"Paulo\", LastName = \"Santos\", Gpa = 3.93, GradYear = 2021 }; copy Alternatively, you can create an instance of Student using an anonymous type: var student = new { Id = 100, FirstName = \"Paulo\", LastName = \"Santos\", Gpa = 3.93, GradYear = 2021 }; copy Next, upload this Student into the students index using the Index method: var response = client. Index &lt; StringResponse &gt;( \"students\", \"100\", PostData. Serializable ( student)); Console. WriteLine ( response. Body); copy The generic type parameter of the Index method specifies the response body type. In the example above, the response is a string.\nIndexing many documents using the Bulk API\nTo index many documents, use the Bulk API to bundle many operations into one request: var studentArray = new object [] { new { index = new { _index = \"students\", _type = \"_doc\", _id = \"200\" }}, new { Id = 200, FirstName = \"Shirley\", LastName = \"Rodriguez\", Gpa = 3.91, GradYear = 2019 }, new { index = new { _index = \"students\", _type = \"_doc\", _id = \"300\" }}, new { Id = 300, FirstName = \"Nikki\", LastName = \"Wolf\", Gpa = 3.87, GradYear = 2020 } }; var manyResponse = client. Bulk &lt; StringResponse &gt;( PostData. MultiJson ( studentArray)); copy You can send the request body as an anonymous object, string, byte array, or stream in APIs that take a body. For APIs that take multiline JSON, you can send the body as a list of bytes or a list of objects, like in the example above. The PostData class has static methods to send the body in all of these forms.\nSearching for a document\nTo construct a Query DSL query, use anonymous types within the request body. The following query searches for all students who graduated in 2021: var searchResponseLow = client. Search &lt; StringResponse &gt;( \"students\", PostData. Serializable ( new { from = 0, size = 20, query = new { term = new { gradYear = new { value = 2019 } } } })); Console. WriteLine ( searchResponseLow. Body); copy Alternatively, you can use strings to construct the request. When using strings, you have to escape the \" character: var searchResponse = client. Search &lt; StringResponse &gt;( \"students\", @\" {\n\"\"query\"\":\n{\n\"\"match\"\":\n{\n\"\"lastName\"\":\n{\n\"\"query\"\": \"\"Santos\"\"\n}\n}\n}\n}\"); Console. WriteLine ( searchResponse. Body); copy Using OpenSearch.Net methods asynchronously\nFor applications that require asynchronous code, all method calls in OpenSearch.Client have asynchronous counterparts: // synchronous method var response = client. Index &lt; StringResponse &gt;( \"students\", \"100\", PostData. Serializable ( student)); // asynchronous method var response = client. IndexAsync &lt; StringResponse &gt;( \"students\", \"100\", PostData. Serializable ( student)); copy Handling exceptions\nBy default, OpenSearch.Net does not throw exceptions when an operation is unsuccessful. In particular, OpenSearch.Net does not throw exceptions if the response status code has one of the expected values for this request. For example, the following query searches for a document in an index that does not exist: var searchResponse = client. Search &lt; StringResponse &gt;( \"students1\", @\" {\n\"\"query\"\":\n{\n\"\"match\"\":\n{\n\"\"lastName\"\":\n{\n\"\"query\"\": \"\"Santos\"\"\n}\n}\n}\n}\"); Console. WriteLine ( searchResponse. Body); copy The response contains an error status code 404, which is one of the expected error codes for search requests, so no exception is thrown. You can see the status code in the status field: { \"error\": { \"root_cause\": [ { \"type\": \"index_not_found_exception\", \"reason\": \"no such index [students1]\", \"index\": \"students1\", \"resource.id\": \"students1\", \"resource.type\": \"index_or_alias\", \"index_uuid\": \"_na_\" }], \"type\": \"index_not_found_exception\", \"reason\": \"no such index [students1]\", \"index\": \"students1\", \"resource.id\": \"students1\", \"resource.type\": \"index_or_alias\", \"index_uuid\": \"_na_\" }, \"status\": 404 } To configure OpenSearch.Net to throw exceptions, turn on the ThrowExceptions() setting on ConnectionConfiguration: var uri = new Uri ( \"http://localhost:9200\"); var connectionPool = new SingleNodeConnectionPool ( uri); var settings = new ConnectionConfiguration ( connectionPool). PrettyJson (). ThrowExceptions (); var client = new OpenSearchLowLevelClient ( settings); copy You can use the following properties of the response object to determine response success: Console. WriteLine ( \"Success: \" + searchResponse. Success); Console. WriteLine ( \"SuccessOrKnownError: \" + searchResponse. SuccessOrKnownError); Console. WriteLine ( \"Original Exception: \" + searchResponse. OriginalException); Success returns true if the response code is in the 2xx range or the response code has one of the expected values for this request. SuccessOrKnownError returns true if the response is successful or the response code is in the 400–501 or 505–599 ranges. If SuccessOrKnownError is true, the request is not retried. OriginalException holds the original exception for the unsuccessful responses.\nSample program\nThe following program creates an index, indexes data, and searches for documents. using OpenSearch.Net; using OpenSearch.Client; namespace NetClientProgram; internal class Program { public static void Main ( string [] args) { // Create a client with custom settings var uri = new Uri ( \"http://localhost:9200\"); var connectionPool = new SingleNodeConnectionPool ( uri); var settings = new ConnectionSettings ( connectionPool). PrettyJson (); var client = new OpenSearchLowLevelClient ( settings); Console. WriteLine ( \"Indexing one student......\"); var student = new Student { Id = 100, FirstName = \"Paulo\", LastName = \"Santos\", Gpa = 3.93, GradYear = 2021 }; v var response = client. Index &lt; StringResponse &gt;( \"students\", \"100\", PostData. Serializable ( student)); Console. WriteLine ( response. Body); Console. WriteLine ( \"Indexing many students......\"); var studentArray = new object [] { new { index = new { _index = \"students\", _type = \"_doc\", _id = \"200\" }}, new { Id = 200, FirstName = \"Shirley\", LastName = \"Rodriguez\", Gpa = 3.91, GradYear = 2019 }, new { index = new { _index = \"students\", _type = \"_doc\", _id = \"300\" }}, new { Id = 300, FirstName = \"Nikki\", LastName = \"Wolf\", Gpa = 3.87, GradYear = 2020 } }; var manyResponse = client. Bulk &lt; StringResponse &gt;( PostData. MultiJson ( studentArray)); Console. WriteLine ( manyResponse. Body); Console. WriteLine ( \"Searching for students who graduated in 2019......\"); var searchResponseLow = client. Search &lt; StringResponse &gt;( \"students\", PostData. Serializable ( new { from = 0, size = 20, query = new { term = new { gradYear = new { value = 2019 } } } })); Console. WriteLine ( searchResponseLow. Body); Console. WriteLine ( \"Searching for a student with the last name Santos......\"); var searchResponse = client. Search &lt; StringResponse &gt;( \"students\", @\" {\n\"\"query\"\":\n{\n\"\"match\"\":\n{\n\"\"lastName\"\":\n{\n\"\"query\"\": \"\"Santos\"\"\n}\n}\n}\n}\"); Console. WriteLine ( searchResponse. Body); } } copy",
    "ancestors": [
      "Clients",
      ".NET clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/dot-net-conventions/",
    "title": ".NET client considerations",
    "content": "The following sections provide information regarding the considerations and best practices for using.NET clients.\nRegistering OpenSearch.Client as a singleton\nAs a rule, you should set up your OpenSearch.Client as a singleton. OpenSearch.Client manages connections to the server and the states of the nodes in a cluster. Additionally, each client uses a lot of configuration for its setup. Therefore, it is beneficial to create an OpenSearch.Client instance once and reuse it for all OpenSearch operations. The client is thread safe, so the same instance can be shared by multiple threads.\nExceptions\nThe following are the types of exceptions that may be thrown by.NET clients: OpenSearchClientException is a known exception that occurs either in the request pipeline (for example, timeout reached) or in OpenSearch (for example, malformed query). If it is an OpenSearch exception, the ServerError response property contains the error that OpenSearch returns. UnexpectedOpenSearchClientException is an unknown exception (for example, an error during deserialization) and is a subclass of OpenSearchClientException.\nSystem exceptions are thrown when the API is not used properly.\nNodes\nTo create a node, pass a Uri object into its constructor: var uri = new Uri ( \"http://example.org/opensearch\"); var node = new Node ( uri); copy When first created, a node is master eligible, and its HoldsData property is set to true.\nThe AbsolutePath property of the node created above is \"/opensearch/\": A trailing forward slash is appended so that the paths can be easily combined. If not specified, the default Port is 80.\nNodes are considered equal if they have the same endpoint. Metadata is not taken into account when checking nodes for equality.\nConnection pools\nConnection pools are instances of IConnectionPool and are responsible for managing the nodes in the OpenSearch cluster. We recommend creating a singleton client with a single ConnectionSettings object. The lifetime of both the client and its ConnectionSettings is the lifetime of the application.\nThe following are connection pool types. SingleNodeConnectionPool SingleNodeConnectionPool is the default connection pool that is used if no connection pool is passed to the ConnectionSettings constructor. Use SingleNodeConnectionPool if you have only one node in the cluster or if your cluster has a load balancer as an entry point. SingleNodeConnectionPool does not support sniffing or pinging and does not mark nodes as dead or alive. CloudConnectionPool CloudConnectionPool is a subclass of SingleNodeConnectionPool that takes a Cloud ID and credentials. Like SingleNodeConnectionPool, CloudConnectionPool does not support sniffing or pinging. StaticConnectionPool StaticConnectionPool is used for a small cluster when you do not want to turn on sniffing to learn about cluster topology. StaticConnectionPool does not support sniffing, but can support pinging. SniffingConnectionPool SniffingConnectionPool is a subclass of StaticConnectionPool. It is thread safe and supports sniffing and pinging. SniffingConnectionPool can be reseeded at run time, and you can specify node roles when seeding. StickyConnectionPool StickyConnectionPool is set up to return the first live node, which then persists between requests. It can be seeded using an enumerable of Uri or Node objects. StickyConnectionPool does not support sniffing but supports pinging. StickySniffingConnectionPool StickySniffingConnectionPool is a subclass of SniffingConnectionPool. Like StickyConnectionPool, it returns the first live node2, which then persists between requests. StickySniffingConnectionPool supports sniffing and sorting so that each instance of your application can favor a different node. Nodes have weights associated with them and can be sorted by weight.\nRetries\nIf a request does not succeed, it is automatically retried. By default, the number of retries is the number of nodes known to OpenSearch.Client in your cluster. The number of retries is also limited by the timeout parameter, so OpenSearch.Client retries requests as many times as possible within the timeout period.\nTo set the maximum number of retries, specify the number in the MaximumRetries property on the ConnectionSettings object. var settings = new ConnectionSettings ( connectionPool). MaximumRetries ( 5); copy You can also set a RequestTimeout that specifies a timeout for a single request and a MaxRetryTimeout that specifies the time limit for all retry attempts. In the example below, RequestTimeout is set to 4 seconds, and MaxRetryTimeout is set to 12 seconds, so the maximum number of attempts for a query is 3. var settings = new ConnectionSettings ( connectionPool). RequestTimeout ( TimeSpan. FromSeconds ( 4)). MaxRetryTimeout ( TimeSpan. FromSeconds ( 12)); copy Failover\nIf you are using a connection pool with multiple nodes, a request is retried if it returns a 502 (Bad Gateway), 503 (Service Unavailable), or 504 (Gateway Timeout) HTTP error response code. If the response code is an error code in the 400–501 or 505–599 ranges, the request is not retried.\nA response is considered valid if the response code is in the 2xx range or the response code has one of the expected values for this request. For example, 404 (Not Found) is a valid response for a request that checks whether an index exists.",
    "ancestors": [
      "Clients",
      ".NET clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/dot-net/",
    "title": ".NET clients",
    "content": "OpenSearch has two.NET clients: a low-level OpenSearch.Net client and a high-level OpenSearch.Client client. OpenSearch.Net is a low-level.NET client that provides the foundational layer of communication with OpenSearch. It is dependency free, and it can handle round-robin load balancing, transport, and the basic request/response cycle. OpenSearch.Net contains methods for all OpenSearch API endpoints. OpenSearch.Client is a high-level.NET client on top of OpenSearch.Net. It provides strongly typed requests and responses as well as Query DSL. It frees you from constructing raw JSON requests and parsing raw JSON responses by supplying models that parse and serialize/deserialize requests and responses automatically. OpenSearch.Client also exposes the OpenSearch.Net low-level client if you need it. OpenSearch.Client includes the following advanced functionality:\nAutomapping: Given a C# type, OpenSearch.Client can infer the correct mapping to send to OpenSearch.\nOperator overloading in queries.\nType and index inference.\nYou can use both.NET clients in a console program, a.NET core, an ASP.NET core, or in worker services.\nTo get started with OpenSearch.Client, follow the instructions in Getting started with the high-level.NET client or in More advanced features of the high-level.NET client, a slightly more advanced walkthrough.",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/go/",
    "title": "Go client",
    "content": "The OpenSearch Go client lets you connect your Go application with the data in your OpenSearch cluster. This getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client’s complete API documentation and additional examples, see the Go client API documentation.\nFor the client source code, see the opensearch-go repo.\nSetup\nIf you’re starting a new project, create a new module by running the following command: go mod init &lt; mymodulename &gt; copy To add the Go client to your project, import it like any other module: go get github. com / opensearch - project / opensearch - go copy Connecting to OpenSearch\nTo connect to the default OpenSearch host, create a client object with the address https://localhost:9200 if you are using the Security plugin: client, err:= opensearch. NewClient ( opensearch. Config { Transport: &amp; http. Transport { TLSClientConfig: &amp; tls. Config { InsecureSkipVerify: true }, }, Addresses: [] string { \"https://localhost:9200\" }, Username: \"admin\", // For testing only. Don't store credentials in code. Password: \"admin\", }) copy If you are not using the Security plugin, create a client object with the address http://localhost:9200: client, err:= opensearch. NewClient ( opensearch. Config { Transport: &amp; http. Transport { TLSClientConfig: &amp; tls. Config { InsecureSkipVerify: true }, }, Addresses: [] string { \"http://localhost:9200\" }, }) copy Connecting to Amazon OpenSearch Service\nThe following example illustrates connecting to Amazon OpenSearch Service: package main import ( \"context\" \"log\" \"github.com/aws/aws-sdk-go-v2/aws\" \"github.com/aws/aws-sdk-go-v2/config\" opensearch \"github.com/opensearch-project/opensearch-go/v2\" opensearchapi \"github.com/opensearch-project/opensearch-go/v2/opensearchapi\" requestsigner \"github.com/opensearch-project/opensearch-go/v2/signer/awsv2\") const endpoint = \"\" // e.g. https://opensearch-domain.region.com or Amazon OpenSearch Serverless endpoint func main () { ctx:= context. Background () awsCfg, err:= config. LoadDefaultConfig ( ctx, config. WithRegion ( \"&lt;AWS_REGION&gt;\"), config. WithCredentialsProvider ( getCredentialProvider ( \"&lt;AWS_ACCESS_KEY&gt;\", \"&lt;AWS_SECRET_ACCESS_KEY&gt;\", \"&lt;AWS_SESSION_TOKEN&gt;\"),),) if err!= nil { log. Fatal ( err) // Do not log.fatal in a production ready app. } // Create an AWS request Signer and load AWS configuration using default config folder or env vars. signer, err:= requestsigner. NewSignerWithService ( awsCfg, \"es\") if err!= nil { log. Fatal ( err) // Do not log.fatal in a production ready app. } // Create an opensearch client and use the request-signer client, err:= opensearch. NewClient ( opensearch. Config { Addresses: [] string { endpoint }, Signer: signer, }) if err!= nil { log. Fatal ( \"client creation err\", err) } } func getCredentialProvider ( accessKey, secretAccessKey, token string) aws. CredentialsProviderFunc { return func ( ctx context. Context) ( aws. Credentials, error) { c:= &amp; aws. Credentials { AccessKeyID: accessKey, SecretAccessKey: secretAccessKey, SessionToken: token, } return * c, nil } } copy Connecting to Amazon OpenSearch Serverless\nThe following example illustrates connecting to Amazon OpenSearch Serverless Service: package main import ( \"context\" \"log\" \"github.com/aws/aws-sdk-go-v2/aws\" \"github.com/aws/aws-sdk-go-v2/config\" opensearch \"github.com/opensearch-project/opensearch-go/v2\" opensearchapi \"github.com/opensearch-project/opensearch-go/v2/opensearchapi\" requestsigner \"github.com/opensearch-project/opensearch-go/v2/signer/awsv2\") const endpoint = \"\" // e.g. https://opensearch-domain.region.com or Amazon OpenSearch Serverless endpoint func main () { ctx:= context. Background () awsCfg, err:= config. LoadDefaultConfig ( ctx, config. WithRegion ( \"&lt;AWS_REGION&gt;\"), config. WithCredentialsProvider ( getCredentialProvider ( \"&lt;AWS_ACCESS_KEY&gt;\", \"&lt;AWS_SECRET_ACCESS_KEY&gt;\", \"&lt;AWS_SESSION_TOKEN&gt;\"),),) if err!= nil { log. Fatal ( err) // Do not log.fatal in a production ready app. } // Create an AWS request Signer and load AWS configuration using default config folder or env vars. signer, err:= requestsigner. NewSignerWithService ( awsCfg, \"aoss\") if err!= nil { log. Fatal ( err) // Do not log.fatal in a production ready app. } // Create an opensearch client and use the request-signer client, err:= opensearch. NewClient ( opensearch. Config { Addresses: [] string { endpoint }, Signer: signer, }) if err!= nil { log. Fatal ( \"client creation err\", err) } } func getCredentialProvider ( accessKey, secretAccessKey, token string) aws. CredentialsProviderFunc { return func ( ctx context. Context) ( aws. Credentials, error) { c:= &amp; aws. Credentials { AccessKeyID: accessKey, SecretAccessKey: secretAccessKey, SessionToken: token, } return * c, nil } } copy The Go client constructor takes an opensearch.Config{} type, which can be customized using options such as a list of OpenSearch node addresses or a username and password combination.\nTo connect to multiple OpenSearch nodes, specify them in the Addresses parameter: var ( urls = [] string { \"http://localhost:9200\", \"http://localhost:9201\", \"http://localhost:9202\" }) client, err:= opensearch. NewClient ( opensearch. Config { Transport: &amp; http. Transport { TLSClientConfig: &amp; tls. Config { InsecureSkipVerify: true }, }, Addresses: urls, }) copy The Go client retries requests for a maximum of three times by default. To customize the number of retries, set the MaxRetries parameter. Additionally, you can change the list of response codes for which a request is retried by setting the RetryOnStatus parameter. The following code snippet creates a new Go client with custom MaxRetries and RetryOnStatus values: client, err:= opensearch. NewClient ( opensearch. Config { Transport: &amp; http. Transport { TLSClientConfig: &amp; tls. Config { InsecureSkipVerify: true }, }, Addresses: [] string { \"http://localhost:9200\" }, MaxRetries: 5, RetryOnStatus: [] int { 502, 503, 504 }, }) copy Creating an index\nTo create an OpenSearch index, use the IndicesCreateRequest method. You can use the following code to construct a JSON object with custom settings: settings:= strings. NewReader ( `{\n'settings': {\n'index': {\n'number_of_shards': 1,\n'number_of_replicas': 0\n}\n}\n}`) res:= opensearchapi. IndicesCreateRequest { Index: \"go-test-index1\", Body: settings, } copy Indexing a document\nYou can index a document into OpenSearch using the IndexRequest method: document:= strings. NewReader ( `{\n\"title\": \"Moneyball\",\n\"director\": \"Bennett Miller\",\n\"year\": \"2011\"\n}`) docId:= \"1\" req:= opensearchapi. IndexRequest { Index: \"go-test-index1\", DocumentID: docId, Body: document, } insertResponse, err:= req. Do ( context. Background (), client) copy Performing bulk operations\nYou can perform several operations at the same time by using the Bulk method of the client. The operations may be of the same type or of different types. blk, err:= client. Bulk ( strings. NewReader ( `\n{ \"index\": { \"_index\": \"go-test-index1\", \"_id\": \"2\" } }\n{ \"title\": \"Interstellar\", \"director\": \"Christopher Nolan\", \"year\": \"2014\"}\n{ \"create\": { \"_index\": \"go-test-index1\", \"_id\": \"3\" } }\n{ \"title\": \"Star Trek Beyond\", \"director\": \"Justin Lin\", \"year\": \"2015\"}\n{ \"update\": {\"_id\": \"3\", \"_index\": \"go-test-index1\" } }\n{ \"doc\": {\"year\": \"2016\"} }\n`),) copy Searching for documents\nThe easiest way to search for documents is to construct a query string. The following code uses a multi_match query to search for “miller” in the title and director fields. It boosts the documents where “miller” appears in the title field: content:= strings. NewReader ( `{\n\"size\": 5,\n\"query\": {\n\"multi_match\": {\n\"query\": \"miller\",\n\"fields\": [\"title^2\", \"director\"]\n}\n}\n}`) search:= opensearchapi. SearchRequest { Index: [] string { \"go-test-index1\" }, Body: content, } searchResponse, err:= search. Do ( context. Background (), client) copy Deleting a document\nYou can delete a document using the DeleteRequest method: delete:= opensearchapi. DeleteRequest { Index: \"go-test-index1\", DocumentID: \"1\", } deleteResponse, err:= delete. Do ( context. Background (), client) copy Deleting an index\nYou can delete an index using the IndicesDeleteRequest method: deleteIndex:= opensearchapi. IndicesDeleteRequest { Index: [] string { \"go-test-index1\" }, } deleteIndexResponse, err:= deleteIndex. Do ( context. Background (), client) copy Sample program\nThe following sample program creates a client, adds an index with non-default settings, inserts a document, performs bulk operations, searches for the document, deletes the document, and then deletes the index: package main import ( \"os\" \"context\" \"crypto/tls\" \"fmt\" opensearch \"github.com/opensearch-project/opensearch-go\" opensearchapi \"github.com/opensearch-project/opensearch-go/opensearchapi\" \"net/http\" \"strings\") const IndexName = \"go-test-index1\" func main () { // Initialize the client with SSL/TLS enabled. client, err:= opensearch. NewClient ( opensearch. Config { Transport: &amp; http. Transport { TLSClientConfig: &amp; tls. Config { InsecureSkipVerify: true }, }, Addresses: [] string { \"https://localhost:9200\" }, Username: \"admin\", // For testing only. Don't store credentials in code. Password: \"admin\", }) if err!= nil { fmt. Println ( \"cannot initialize\", err) os. Exit ( 1) } // Print OpenSearch version information on console. fmt. Println ( client. Info ()) // Define index settings. settings:= strings. NewReader ( `{\n'settings': {\n'index': {\n'number_of_shards': 1,\n'number_of_replicas': 2\n}\n}\n}`) // Create an index with non-default settings. res:= opensearchapi. IndicesCreateRequest { Index: IndexName, Body: settings, } fmt. Println ( \"Creating index\") fmt. Println ( res) // Add a document to the index. document:= strings. NewReader ( `{\n\"title\": \"Moneyball\",\n\"director\": \"Bennett Miller\",\n\"year\": \"2011\"\n}`) docId:= \"1\" req:= opensearchapi. IndexRequest { Index: IndexName, DocumentID: docId, Body: document, } insertResponse, err:= req. Do ( context. Background (), client) if err!= nil { fmt. Println ( \"failed to insert document \", err) os. Exit ( 1) } fmt. Println ( \"Inserting a document\") fmt. Println ( insertResponse) defer insertResponse. Body. Close () // Perform bulk operations. blk, err:= client. Bulk ( strings. NewReader ( `\n{ \"index\": { \"_index\": \"go-test-index1\", \"_id\": \"2\" } }\n{ \"title\": \"Interstellar\", \"director\": \"Christopher Nolan\", \"year\": \"2014\"}\n{ \"create\": { \"_index\": \"go-test-index1\", \"_id\": \"3\" } }\n{ \"title\": \"Star Trek Beyond\", \"director\": \"Justin Lin\", \"year\": \"2015\"}\n{ \"update\": {\"_id\": \"3\", \"_index\": \"go-test-index1\" } }\n{ \"doc\": {\"year\": \"2016\"} }\n`),) if err!= nil { fmt. Println ( \"failed to perform bulk operations\", err) os. Exit ( 1) } fmt. Println ( \"Performing bulk operations\") fmt. Println ( blk) // Search for the document. content:= strings. NewReader ( `{\n\"size\": 5,\n\"query\": {\n\"multi_match\": {\n\"query\": \"miller\",\n\"fields\": [\"title^2\", \"director\"]\n}\n}\n}`) search:= opensearchapi. SearchRequest { Index: [] string { IndexName }, Body: content, } searchResponse, err:= search. Do ( context. Background (), client) if err!= nil { fmt. Println ( \"failed to search document \", err) os. Exit ( 1) } fmt. Println ( \"Searching for a document\") fmt. Println ( searchResponse) defer searchResponse. Body. Close () // Delete the document. delete:= opensearchapi. DeleteRequest { Index: IndexName, DocumentID: docId, } deleteResponse, err:= delete. Do ( context. Background (), client) if err!= nil { fmt. Println ( \"failed to delete document \", err) os. Exit ( 1) } fmt. Println ( \"Deleting a document\") fmt. Println ( deleteResponse) defer deleteResponse. Body. Close () // Delete the previously created index. deleteIndex:= opensearchapi. IndicesDeleteRequest { Index: [] string { IndexName }, } deleteIndexResponse, err:= deleteIndex. Do ( context. Background (), client) if err!= nil { fmt. Println ( \"failed to delete index \", err) os. Exit ( 1) } fmt. Println ( \"Deleting the index\") fmt. Println ( deleteIndexResponse) defer deleteIndexResponse. Body. Close () } copy",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/index/",
    "title": "Language clients",
    "content": "OpenSearch provides clients in JavaScript, Python, Ruby, Java, PHP,.NET, Go and Rust.\nOpenSearch clients\nOpenSearch provides clients for the following programming languages and platforms: Python OpenSearch high-level Python client OpenSearch low-level Python client opensearch-py-ml client Java OpenSearch Java client JavaScript OpenSearch JavaScript (Node.js) client Go OpenSearch Go client Ruby OpenSearch Ruby client PHP OpenSearch PHP client.NET OpenSearch.NET clients Rust OpenSearch Rust client All clients are compatible with any version of OpenSearch.\nLegacy clients\nMost clients that work with Elasticsearch OSS 7.10.2 should work with OpenSearch, but the latest versions of those clients might include license or version checks that artificially break compatibility. This page includes recommendations around which versions of those clients to use for best compatibility with OpenSearch. Client Recommended version Elasticsearch Java low-level REST client 7.13.4 Elasticsearch Java high-level REST client 7.13.4 Elasticsearch Python client 7.13.4 Elasticsearch Node.js client 7.13.0 Elasticsearch Ruby client 7.13.0 If you test a legacy client and verify that it works, please submit a PR and add it to this table.",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/java-rest-high-level/",
    "title": "Java high-level REST client",
    "content": "The OpenSearch Java high-level REST client will be deprecated starting with OpenSearch version 3.0.0 and will be removed in a future release. We recommend switching to the Java client instead.\nThe OpenSearch Java high-level REST client lets you interact with your OpenSearch clusters and indices through Java methods and data structures rather than HTTP methods and JSON.\nSetup\nTo start using the OpenSearch Java high-level REST client, ensure that you have the following dependency in your project’s pom.xml file: &lt;dependency&gt;\n&lt;groupId&gt;org.opensearch.client&lt;/groupId&gt;\n&lt;artifactId&gt;opensearch-rest-high-level-client&lt;/artifactId&gt;\n&lt;version&gt;2.7.0&lt;/version&gt;\n&lt;/dependency&gt; You can now start your OpenSearch cluster. The OpenSearch 1.x high-level REST client works with the 1.x versions of OpenSearch.\nSecurity\nBefore using the REST client in your Java application, you must configure the application’s truststore to connect to the Security plugin. If you are using self-signed certificates or demo configurations, you can use the following command to create a custom truststore and add in root authority certificates.\nIf you’re using certificates from a trusted Certificate Authority (CA), you don’t need to configure the truststore. keytool -import &lt;path-to-cert&gt; -alias &lt;alias-to-call-cert&gt; -keystore &lt;truststore-name&gt; You can now point your Java client to the truststore and set basic authentication credentials that can access a secure cluster (refer to the sample code below on how to do so).\nIf you run into issues when configuring security, see common issues and troubleshoot TLS.\nSample program\nThis code example uses basic credentials that come with the default OpenSearch configuration. If you’re using the OpenSearch Java high-level REST client with your own OpenSearch cluster, be sure to change the code to use your own credentials. import org.apache.http.HttpHost; import org.apache.http.auth.AuthScope; import org.apache.http.auth.UsernamePasswordCredentials; import org.apache.http.client.CredentialsProvider; import org.apache.http.impl.client.BasicCredentialsProvider; import org.apache.http.impl.nio.client.HttpAsyncClientBuilder; import org.opensearch.action.admin.indices.delete.DeleteIndexRequest; import org.opensearch.action.delete.DeleteRequest; import org.opensearch.action.delete.DeleteResponse; import org.opensearch.action.get.GetRequest; import org.opensearch.action.get.GetResponse; import org.opensearch.action.index.IndexRequest; import org.opensearch.action.index.IndexResponse; import org.opensearch.action.support.master.AcknowledgedResponse; import org.opensearch.client.RequestOptions; import org.opensearch.client.RestClient; import org.opensearch.client.RestClientBuilder; import org.opensearch.client.RestHighLevelClient; import org.opensearch.client.indices.CreateIndexRequest; import org.opensearch.client.indices.CreateIndexResponse; import org.opensearch.common.settings.Settings; import java.io.IOException; import java.util.HashMap; public class RESTClientSample { public static void main ( String [] args) throws IOException { //Point to keystore with appropriate certificates for security. System. setProperty ( \"javax.net.ssl.trustStore\", \"/full/path/to/keystore\"); System. setProperty ( \"javax.net.ssl.trustStorePassword\", \"password-to-keystore\"); //Establish credentials to use basic authentication. //Only for demo purposes. Don't specify your credentials in code. final CredentialsProvider credentialsProvider = new BasicCredentialsProvider (); credentialsProvider. setCredentials ( AuthScope. ANY, new UsernamePasswordCredentials ( \"admin\", \"admin\")); //Create a client. RestClientBuilder builder = RestClient. builder ( new HttpHost ( \"localhost\", 9200, \"https\")). setHttpClientConfigCallback ( new RestClientBuilder. HttpClientConfigCallback () { @Override public HttpAsyncClientBuilder customizeHttpClient ( HttpAsyncClientBuilder httpClientBuilder) { return httpClientBuilder. setDefaultCredentialsProvider ( credentialsProvider); } }); RestHighLevelClient client = new RestHighLevelClient ( builder); //Create a non-default index with custom settings and mappings. CreateIndexRequest createIndexRequest = new CreateIndexRequest ( \"custom-index\"); createIndexRequest. settings ( Settings. builder () //Specify in the settings how many shards you want in the index.. put ( \"index.number_of_shards\", 4). put ( \"index.number_of_replicas\", 3)); //Create a set of maps for the index's mappings. HashMap &lt; String, String &gt; typeMapping = new HashMap &lt; String, String &gt;(); typeMapping. put ( \"type\", \"integer\"); HashMap &lt; String, Object &gt; ageMapping = new HashMap &lt; String, Object &gt;(); ageMapping. put ( \"age\", typeMapping); HashMap &lt; String, Object &gt; mapping = new HashMap &lt; String, Object &gt;(); mapping. put ( \"properties\", ageMapping); createIndexRequest. mapping ( mapping); CreateIndexResponse createIndexResponse = client. indices (). create ( createIndexRequest, RequestOptions. DEFAULT); //Adding data to the index. IndexRequest request = new IndexRequest ( \"custom-index\"); //Add a document to the custom-index we created. request. id ( \"1\"); //Assign an ID to the document. HashMap &lt; String, String &gt; stringMapping = new HashMap &lt; String, String &gt;(); stringMapping. put ( \"message:\", \"Testing Java REST client\"); request. source ( stringMapping); //Place your content into the index's source. IndexResponse indexResponse = client. index ( request, RequestOptions. DEFAULT); //Getting back the document GetRequest getRequest = new GetRequest ( \"custom-index\", \"1\"); GetResponse response = client. get ( getRequest, RequestOptions. DEFAULT); System. out. println ( response. getSourceAsString ()); //Delete the document DeleteRequest deleteDocumentRequest = new DeleteRequest ( \"custom-index\", \"1\"); //Index name followed by the ID. DeleteResponse deleteResponse = client. delete ( deleteDocumentRequest, RequestOptions. DEFAULT); //Delete the index DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest ( \"custom-index\"); //Index name. AcknowledgedResponse deleteIndexResponse = client. indices (). delete ( deleteIndexRequest, RequestOptions. DEFAULT); client. close (); } } Elasticsearch OSS Java high-level REST client\nWe recommend using the OpenSearch client to connect to OpenSearch clusters, but if you must use the Elasticsearch OSS Java high-level REST client, version 7.10.2 of the Elasticsearch OSS client also works with the 1.x versions of OpenSearch.\nMigrating to the OpenSearch Java high-level REST client\nMigrating from the Elasticsearch OSS client to the OpenSearch high-level REST client is as simple as changing your Maven dependency to one that references OpenSearch’s dependency.\nAfterward, change all references of org.elasticsearch to org.opensearch, and you’re ready to start submitting requests to your OpenSearch cluster.",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/java/",
    "title": "Java client",
    "content": "The OpenSearch Java client allows you to interact with your OpenSearch clusters through Java methods and data structures rather than HTTP methods and raw JSON. For example, you can submit requests to your cluster using objects to create indexes, add data to documents, or complete some other operation using the client’s built-in methods. For the client’s complete API documentation and additional examples, see the javadoc.\nThis getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-java repo.\nInstalling the client using Apache HttpClient 5 Transport\nTo start using the OpenSearch Java client, you need to provide a transport. The default ApacheHttpClient5TransportBuilder transport comes with the Java client. To use the OpenSearch Java client with the default transport, add it to your pom.xml file as a dependency: &lt;dependency&gt; &lt;groupId&gt; org.opensearch.client &lt;/groupId&gt; &lt;artifactId&gt; opensearch-java &lt;/artifactId&gt; &lt;version&gt; 2.4.0 &lt;/version&gt; &lt;/dependency&gt; copy If you’re using Gradle, add the following dependencies to your project: dependencies {\nimplementation 'org.opensearch.client:opensearch-java:2.4.0'\n} copy You can now start your OpenSearch cluster.\nInstalling the client using RestClient Transport\nAlternatively, you can create a Java client by using the RestClient -based transport. In this case, make sure that you have the following dependencies in your project’s pom.xml file: &lt;dependency&gt; &lt;groupId&gt; org.opensearch.client &lt;/groupId&gt; &lt;artifactId&gt; opensearch-rest-client &lt;/artifactId&gt; &lt;version&gt; 2.7.0 &lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt; org.opensearch.client &lt;/groupId&gt; &lt;artifactId&gt; opensearch-java &lt;/artifactId&gt; &lt;version&gt; 2.4.0 &lt;/version&gt; &lt;/dependency&gt; copy If you’re using Gradle, add the following dependencies to your project” dependencies {\nimplementation 'org.opensearch.client:opensearch-rest-client: 2.7.0'\nimplementation 'org.opensearch.client:opensearch-java:2.4.0'\n} copy You can now start your OpenSearch cluster.\nSecurity\nBefore using the REST client in your Java application, you must configure the application’s truststore to connect to the Security plugin. If you are using self-signed certificates or demo configurations, you can use the following command to create a custom truststore and add in root authority certificates.\nIf you’re using certificates from a trusted Certificate Authority (CA), you don’t need to configure the truststore. keytool -import &lt;path-to-cert&gt; -alias &lt;alias-to-call-cert&gt; -keystore &lt;truststore-name&gt; copy You can now point your Java client to the truststore and set basic authentication credentials that can access a secure cluster (refer to the sample code below on how to do so).\nIf you run into issues when configuring security, see common issues and troubleshoot TLS.\nSample data\nThis section uses a class called IndexData, which is a simple Java class that stores basic data and methods. For your own OpenSearch cluster, you might find that you need a more robust class to store your data.\nIndexData class static class IndexData { private String firstName; private String lastName; public IndexData ( String firstName, String lastName) { this. firstName = firstName; this. lastName = lastName; } public String getFirstName () { return firstName; } public void setFirstName ( String firstName) { this. firstName = firstName; } public String getLastName () { return lastName; } public void setLastName ( String lastName) { this. lastName = lastName; } @Override public String toString () { return String. format ( \"IndexData{first name='%s', last name='%s'}\", firstName, lastName); } } copy Initializing the client with SSL and TLS enabled using Apache HttpClient 5 Transport\nThis code example uses basic credentials that come with the default OpenSearch configuration. If you’re using the Java client with your own OpenSearch cluster, be sure to change the code so that it uses your own credentials.\nThe following sample code initializes a client with SSL and TLS enabled: import javax.net.ssl.SSLContext; import javax.net.ssl.SSLEngine; import org.apache.hc.client5.http.auth.AuthScope; import org.apache.hc.client5.http.auth.UsernamePasswordCredentials; import org.apache.hc.client5.http.impl.auth.BasicCredentialsProvider; import org.apache.hc.client5.http.impl.nio.PoolingAsyncClientConnectionManager; import org.apache.hc.client5.http.impl.nio.PoolingAsyncClientConnectionManagerBuilder; import org.apache.hc.client5.http.ssl.ClientTlsStrategyBuilder; import org.apache.hc.core5.function.Factory; import org.apache.hc.core5.http.HttpHost; import org.apache.hc.core5.http.nio.ssl.TlsStrategy; import org.apache.hc.core5.reactor.ssl.TlsDetails; import org.apache.hc.core5.ssl.SSLContextBuilder; import org.opensearch.client.opensearch.OpenSearchClient; import org.opensearch.client.transport.OpenSearchTransport; import org.opensearch.client.transport.httpclient5.ApacheHttpClient5TransportBuilder; public class OpenSearchClientExample { public static void main ( String [] args) throws Exception { System. setProperty ( \"javax.net.ssl.trustStore\", \"/full/path/to/keystore\"); System. setProperty ( \"javax.net.ssl.trustStorePassword\", \"password-to-keystore\"); final HttpHost host = new HttpHost ( \"https\", \"localhost\", 9200); final BasicCredentialsProvider credentialsProvider = new BasicCredentialsProvider (); // Only for demo purposes. Don't specify your credentials in code. credentialsProvider. setCredentials ( new AuthScope ( host), new UsernamePasswordCredentials ( \"admin\", \"admin\". toCharArray ())); final SSLContext sslcontext = SSLContextBuilder. create (). loadTrustMaterial ( null, ( chains, authType) -&gt; true). build (); final ApacheHttpClient5TransportBuilder builder = ApacheHttpClient5TransportBuilder. builder ( host); builder. setHttpClientConfigCallback ( httpClientBuilder -&gt; { final TlsStrategy tlsStrategy = ClientTlsStrategyBuilder. create (). setSslContext ( SSLContextBuilder. create (). build ()) // See https://issues.apache.org/jira/browse/HTTPCLIENT-2219. setTlsDetailsFactory ( new Factory &lt; SSLEngine, TlsDetails &gt;() { @Override public TlsDetails create ( final SSLEngine sslEngine) { return new TlsDetails ( sslEngine. getSession (), sslEngine. getApplicationProtocol ()); } }). build (); final PoolingAsyncClientConnectionManager connectionManager = PoolingAsyncClientConnectionManagerBuilder. create (). setTlsStrategy ( tlsStrategy). build (); return httpClientBuilder. setDefaultCredentialsProvider ( credentialsProvider). setConnectionManager ( connectionManager); }); final OpenSearchTransport transport = ApacheHttpClient5TransportBuilder. builder ( host). build (); OpenSearchClient client = new OpenSearchClient ( transport); } } Initializing the client with SSL and TLS enabled using RestClient Transport\nThis code example uses basic credentials that come with the default OpenSearch configuration. If you’re using the Java client with your own OpenSearch cluster, be sure to change the code so that it uses your own credentials.\nThe following sample code initializes a client with SSL and TLS enabled: import org.apache.hc.client5.http.auth.AuthScope; import org.apache.hc.client5.http.auth.UsernamePasswordCredentials; import org.apache.hc.client5.http.impl.async.HttpAsyncClientBuilder; import org.apache.hc.client5.http.impl.auth.BasicCredentialsProvider; import org.apache.hc.core5.http.HttpHost; import org.opensearch.client.RestClient; import org.opensearch.client.RestClientBuilder; import org.opensearch.client.json.jackson.JacksonJsonpMapper; import org.opensearch.client.opensearch.OpenSearchClient; import org.opensearch.client.transport.OpenSearchTransport; import org.opensearch.client.transport.rest_client.RestClientTransport; public class OpenSearchClientExample { public static void main ( String [] args) throws Exception { System. setProperty ( \"javax.net.ssl.trustStore\", \"/full/path/to/keystore\"); System. setProperty ( \"javax.net.ssl.trustStorePassword\", \"password-to-keystore\"); final HttpHost host = new HttpHost ( \"https\", \"localhost\", 9200); final BasicCredentialsProvider credentialsProvider = new BasicCredentialsProvider (); //Only for demo purposes. Don't specify your credentials in code. credentialsProvider. setCredentials ( new AuthScope ( host), new UsernamePasswordCredentials ( \"admin\", \"admin\". toCharArray ())); //Initialize the client with SSL and TLS enabled final RestClient restClient = RestClient. builder ( host). setHttpClientConfigCallback ( new RestClientBuilder. HttpClientConfigCallback () { @Override public HttpAsyncClientBuilder customizeHttpClient ( HttpAsyncClientBuilder httpClientBuilder) { return httpClientBuilder. setDefaultCredentialsProvider ( credentialsProvider); } }). build (); final OpenSearchTransport transport = new RestClientTransport ( restClient, new JacksonJsonpMapper ()); final OpenSearchClient client = new OpenSearchClient ( transport); } } copy Connecting to Amazon OpenSearch Service\nThe following example illustrates connecting to Amazon OpenSearch Service: SdkHttpClient httpClient = ApacheHttpClient. builder (). build (); OpenSearchClient client = new OpenSearchClient ( new AwsSdk2Transport ( httpClient, \"search-...us-west-2.es.amazonaws.com\", // OpenSearch endpoint, without https:// \"es\" Region. US_WEST_2, // signing service region AwsSdk2TransportOptions. builder (). build ())); InfoResponse info = client. info (); System. out. println ( info. version (). distribution () + \": \" + info. version (). number ()); httpClient. close (); copy Connecting to Amazon OpenSearch Serverless\nThe following example illustrates connecting to Amazon OpenSearch Serverless Service: SdkHttpClient httpClient = ApacheHttpClient. builder (). build (); OpenSearchClient client = new OpenSearchClient ( new AwsSdk2Transport ( httpClient, \"search-...us-west-2.aoss.amazonaws.com\", // OpenSearch endpoint, without https:// \"aoss\" Region. US_WEST_2, // signing service region AwsSdk2TransportOptions. builder (). build ())); InfoResponse info = client. info (); System. out. println ( info. version (). distribution () + \": \" + info. version (). number ()); httpClient. close (); copy Creating an index\nYou can create an index with non-default settings using the following code: String index = \"sample-index\"; CreateRequest createIndexRequest = new CreateRequest. Builder (). index ( index). build (); client. indices (). create ( createIndexRequest); IndexSettings indexSettings = new IndexSettings. Builder (). autoExpandReplicas ( \"0-all\"). build (); IndexSettingsBody settingsBody = new IndexSettingsBody. Builder (). settings ( indexSettings). build (); PutSettingsRequest putSettingsRequest = new PutSettingsRequest. Builder (). index ( index). value ( settingsBody). build (); client. indices (). putSettings ( putSettingsRequest); copy Indexing data\nYou can index data into OpenSearch using the following code: IndexData indexData = new IndexData ( \"first_name\", \"Bruce\"); IndexRequest &lt; IndexData &gt; indexRequest = new IndexRequest. Builder &lt; IndexData &gt;(). index ( index). id ( \"1\"). document ( indexData). build (); client. index ( indexRequest); copy Searching for documents\nYou can search for a document using the following code: SearchResponse &lt; IndexData &gt; searchResponse = client. search ( s -&gt; s. index ( index), IndexData. class); for ( int i = 0; i &lt; searchResponse. hits (). hits (). size (); i ++) { System. out. println ( searchResponse. hits (). hits (). get ( i). source ()); } copy Deleting a document\nThe following sample code deletes a document whose ID is 1: client. delete ( b -&gt; b. index ( index). id ( \"1\")); copy Deleting an index\nThe following sample code deletes an index: DeleteRequest deleteRequest = new DeleteRequest. Builder (). index ( index). build (); DeleteResponse deleteResponse = client. indices (). delete ( deleteRequest); } catch ( IOException e){ System. out. println ( e. toString ()); } finally { try { if ( restClient!= null) { restClient. close (); } } catch ( IOException e) { System. out. println ( e. toString ()); } } } } copy Sample program\nThe following sample program creates a client, adds an index with non-default settings, inserts a document, searches for the document, deletes the document, and then deletes the index: import org.apache.http.HttpHost; import org.apache.http.auth.AuthScope; import org.apache.http.auth.UsernamePasswordCredentials; import org.apache.http.client.CredentialsProvider; import org.apache.http.impl.client.BasicCredentialsProvider; import org.apache.http.impl.nio.client.HttpAsyncClientBuilder; import org.opensearch.client.RestClient; import org.opensearch.client.RestClientBuilder; import org.opensearch.client.base.RestClientTransport; import org.opensearch.client.base.Transport; import org.opensearch.client.json.jackson.JacksonJsonpMapper; import org.opensearch.client.opensearch.OpenSearchClient; import org.opensearch.client.opensearch._global.IndexRequest; import org.opensearch.client.opensearch._global.IndexResponse; import org.opensearch.client.opensearch._global.SearchResponse; import org.opensearch.client.opensearch.indices.*; import org.opensearch.client.opensearch.indices.put_settings.IndexSettingsBody; import java.io.IOException; public class OpenSearchClientExample { public static void main ( String [] args) { RestClient restClient = null; try { System. setProperty ( \"javax.net.ssl.trustStore\", \"/full/path/to/keystore\"); System. setProperty ( \"javax.net.ssl.trustStorePassword\", \"password-to-keystore\"); //Only for demo purposes. Don't specify your credentials in code. final CredentialsProvider credentialsProvider = new BasicCredentialsProvider (); credentialsProvider. setCredentials ( AuthScope. ANY, new UsernamePasswordCredentials ( \"admin\", \"admin\")); //Initialize the client with SSL and TLS enabled restClient = RestClient. builder ( new HttpHost ( \"localhost\", 9200, \"https\")). setHttpClientConfigCallback ( new RestClientBuilder. HttpClientConfigCallback () { @Override public HttpAsyncClientBuilder customizeHttpClient ( HttpAsyncClientBuilder httpClientBuilder) { return httpClientBuilder. setDefaultCredentialsProvider ( credentialsProvider); } }). build (); Transport transport = new RestClientTransport ( restClient, new JacksonJsonpMapper ()); OpenSearchClient client = new OpenSearchClient ( transport); //Create the index String index = \"sample-index\"; CreateRequest createIndexRequest = new CreateRequest. Builder (). index ( index). build (); client. indices (). create ( createIndexRequest); //Add some settings to the index IndexSettings indexSettings = new IndexSettings. Builder (). autoExpandReplicas ( \"0-all\"). build (); IndexSettingsBody settingsBody = new IndexSettingsBody. Builder (). settings ( indexSettings). build (); PutSettingsRequest putSettingsRequest = new PutSettingsRequest. Builder (). index ( index). value ( settingsBody). build (); client. indices (). putSettings ( putSettingsRequest); //Index some data IndexData indexData = new IndexData ( \"first_name\", \"Bruce\"); IndexRequest &lt; IndexData &gt; indexRequest = new IndexRequest. Builder &lt; IndexData &gt;(). index ( index). id ( \"1\"). document ( indexData). build (); client. index ( indexRequest); //Search for the document SearchResponse &lt; IndexData &gt; searchResponse = client. search ( s -&gt; s. index ( index), IndexData. class); for ( int i = 0; i &lt; searchResponse. hits (). hits (). size (); i ++) { System. out. println ( searchResponse. hits (). hits (). get ( i). source ()); } //Delete the document client. delete ( b -&gt; b. index ( index). id ( \"1\")); // Delete the index DeleteRequest deleteRequest = new DeleteRequest. Builder (). index ( index). build (); DeleteResponse deleteResponse = client. indices (). delete ( deleteRequest); } catch ( IOException e){ System. out. println ( e. toString ()); } finally { try { if ( restClient!= null) { restClient. close (); } } catch ( IOException e) { System. out. println ( e. toString ()); } } } } copy",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/javascript/helpers/",
    "title": "Helper methods",
    "content": "Helper methods simplify the use of complicated API tasks.\nBulk helper\nThe bulk helper simplifies making complex bulk API requests.\nUsage\nThe following code creates a bulk helper instance: const { Client } = require ( ' @opensearch-project/opensearch ') const documents = require ( './docs.json ') const client = new Client ({... }) const result = await client. helpers. bulk ({ datasource: documents, onDocument ( doc) { return { index: { _index: ' example-index ' } } } }) console. log ( result) copy Bulk helper operations return an object with the following fields: { total: number, failed: number, retry: number, successful: number, time: number, bytes: number, aborted: boolean } Bulk helper configuration options\nWhen creating a new bulk helper instance, you can use the following configuration options. Option Data type Required/Default Description datasource An array, async generator or a readable stream of strings or objects\nRequired\nRepresents the documents you need to create, delete, index, or update. onDocument Function\nRequired\nA function to be invoked with each document in the given datasource. It returns the operation to be executed for this document. Optionally, the document can be manipulated for create and index operations by returning a new document as part of the function’s result. concurrency Integer\nOptional. Default is 5.\nThe number of requests to be executed in parallel. flushBytes Integer\nOptional. Default is 5,000,000.\nMaximum bulk body size to send in bytes. flushInterval Integer\nOptional. Default is 30,000.\nTime in milliseconds to wait before flushing the body after the last document has been read. onDrop Function\nOptional. Default is noop.\nA function to be invoked for every document that can’t be indexed after reaching the maximum number of retries. refreshOnCompletion Boolean\nOptional. Default is false.\nWhether or not a refresh should be run on all affected indexes at the end of the bulk operation. retries Integer\nOptional. Defaults to the client’s maxRetries value.\nThe number of times an operation is retried before onDrop is called for that document. wait Integer\nOptional. Default is 5,000.\nTime in milliseconds to wait before retrying an operation. Examples\nThe following examples illustrate the index, create, update, and delete bulk helper operations.\nIndex\nThe index operation creates a new document if it doesn’t exist and recreates the document if it already exists.\nThe following bulk operation indexes documents into example-index: client. helpers. bulk ({ datasource: arrayOfDocuments, onDocument ( doc) { return { index: { _index: ' example-index ' } } } }) copy The following bulk operation indexes documents into example-index with document overwrite: client. helpers. bulk ({ datasource: arrayOfDocuments, onDocument ( doc) { return [ { index: { _index: ' example-index ' } }, {... doc, createdAt: new Date (). toISOString () }] } }) copy Create\nThe create operation creates a new document only if the document does not already exist.\nThe following bulk operation creates documents in the example-index: client. helpers. bulk ({ datasource: arrayOfDocuments, onDocument ( doc) { return { create: { _index: ' example-index ', _id: doc. id } } } }) copy The following bulk operation creates documents in the example-index with document overwrite: client. helpers. bulk ({ datasource: arrayOfDocuments, onDocument ( doc) { return [ { create: { _index: ' example-index ', _id: doc. id } }, {... doc, createdAt: new Date (). toISOString () }] } }) copy Update\nThe update operation updates the document with the fields being sent. The document must already exist in the index.\nThe following bulk operation updates documents in the arrayOfDocuments: client. helpers. bulk ({ datasource: arrayOfDocuments, onDocument ( doc) { // The update operation always requires a tuple to be returned, with the // first element being the action and the second being the update options. return [ { update: { _index: ' example-index ', _id: doc. id } }, { doc_as_upsert: true }] } }) copy The following bulk operation updates documents in the arrayOfDocuments with document overwrite: client. helpers. bulk ({ datasource: arrayOfDocuments, onDocument ( doc) { return [ { update: { _index: ' example-index ', _id: doc. id } }, { doc: {... doc, createdAt: new Date (). toISOString () }, doc_as_upsert: true }] } }) copy Delete\nThe delete operation deletes a document.\nThe following bulk operation deletes documents from the example-index: client. helpers. bulk ({ datasource: arrayOfDocuments, onDocument ( doc) { return { delete: { _index: ' example-index ', _id: doc. id } } } }) copy",
    "ancestors": [
      "Clients",
      "JavaScript client"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/javascript/index/",
    "title": "JavaScript client",
    "content": "The OpenSearch JavaScript (JS) client provides a safer and easier way to interact with your OpenSearch cluster. Rather than using OpenSearch from the browser and potentially exposing your data to the public, you can build an OpenSearch client that takes care of sending requests to your cluster. For the client’s complete API documentation and additional examples, see the JS client API documentation.\nThe client contains a library of APIs that let you perform different operations on your cluster and return a standard response body. The example here demonstrates some basic operations like creating an index, adding documents, and searching your data.\nSetup\nTo add the client to your project, install it from npm: npm install @opensearch-project/opensearch copy To install a specific major version of the client, run the following command: npm install @opensearch-project/opensearch@&lt;version&gt; copy If you prefer to add the client manually or just want to examine the source code, see opensearch-js on GitHub.\nThen require the client: const { Client } = require ( \" @opensearch-project/opensearch \"); copy Connecting to OpenSearch\nTo connect to the default OpenSearch host, create a client object with the address https://localhost:9200 if you are using the Security plugin: var host = \" localhost \"; var protocol = \" https \"; var port = 9200; var auth = \" admin:admin \"; // For testing only. Don't store credentials in code. var ca_certs_path = \" /full/path/to/root-ca.pem \"; // Optional client certificates if you don't want to use HTTP basic authentication. // var client_cert_path = '/full/path/to/client.pem' // var client_key_path = '/full/path/to/client-key.pem' // Create a client with SSL/TLS enabled. var { Client } = require ( \" @opensearch-project/opensearch \"); var fs = require ( \" fs \"); var client = new Client ({ node: protocol + \":// \" + auth + \" @ \" + host + \": \" + port, ssl: { ca: fs. readFileSync ( ca_certs_path), // You can turn off certificate verification (rejectUnauthorized: false) if you're using // self-signed certificates with a hostname mismatch. // cert: fs.readFileSync(client_cert_path), // key: fs.readFileSync(client_key_path) }, }); copy Authenticating with Amazon OpenSearch Service – AWS Sigv4\nUse the following code to authenticate with AWS V2 SDK: const AWS = require ( ' aws-sdk '); // V2 SDK. const { Client } = require ( ' @opensearch-project/opensearch '); const { AwsSigv4Signer } = require ( ' @opensearch-project/opensearch/aws '); const client = new Client ({... AwsSigv4Signer ({ region: ' us-west-2 ', service: ' es ', // Must return a Promise that resolve to an AWS.Credentials object. // This function is used to acquire the credentials when the client start and // when the credentials are expired. // The Client will refresh the Credentials only when they are expired. // With AWS SDK V2, Credentials.refreshPromise is used when available to refresh the credentials. // Example with AWS SDK V2: getCredentials: () =&gt; new Promise (( resolve, reject) =&gt; { // Any other method to acquire a new Credentials object can be used. AWS. config. getCredentials (( err, credentials) =&gt; { if ( err) { reject ( err); } else { resolve ( credentials); } }); }), }), node: ' https://search-xxx.region.es.amazonaws.com ', // OpenSearch domain URL }); copy AWS V2 SDK for Amazon OpenSearch Serverless const AWS = require ( ' aws-sdk '); // V2 SDK. const { Client } = require ( ' @opensearch-project/opensearch '); const { AwsSigv4Signer } = require ( ' @opensearch-project/opensearch/aws '); const client = new Client ({... AwsSigv4Signer ({ region: ' us-west-2 ', service: ' aoss ', // Must return a Promise that resolve to an AWS.Credentials object. // This function is used to acquire the credentials when the client start and // when the credentials are expired. // The Client will refresh the Credentials only when they are expired. // With AWS SDK V2, Credentials.refreshPromise is used when available to refresh the credentials. // Example with AWS SDK V2: getCredentials: () =&gt; new Promise (( resolve, reject) =&gt; { // Any other method to acquire a new Credentials object can be used. AWS. config. getCredentials (( err, credentials) =&gt; { if ( err) { reject ( err); } else { resolve ( credentials); } }); }), }), node: \" https://xxx.region.aoss.amazonaws.com \" // OpenSearch domain URL }); copy Use the following code to authenticate with AWS V3 SDK: const { defaultProvider } = require ( ' @aws-sdk/credential-provider-node '); // V3 SDK. const { Client } = require ( ' @opensearch-project/opensearch '); const { AwsSigv4Signer } = require ( ' @opensearch-project/opensearch/aws '); const client = new Client ({... AwsSigv4Signer ({ region: ' us-east-1 ', service: ' es ', // 'aoss' for OpenSearch Serverless // Must return a Promise that resolve to an AWS.Credentials object. // This function is used to acquire the credentials when the client start and // when the credentials are expired. // The Client will refresh the Credentials only when they are expired. // With AWS SDK V2, Credentials.refreshPromise is used when available to refresh the credentials. // Example with AWS SDK V3: getCredentials: () =&gt; { // Any other method to acquire a new Credentials object can be used. const credentialsProvider = defaultProvider (); return credentialsProvider (); }, }), node: ' https://search-xxx.region.es.amazonaws.com ', // OpenSearch domain URL // node: \"https://xxx.region.aoss.amazonaws.com\" for OpenSearch Serverless }); copy AWS V3 SDK for Amazon OpenSearch Serverless const { defaultProvider } = require ( ' @aws-sdk/credential-provider-node '); // V3 SDK. const { Client } = require ( ' @opensearch-project/opensearch '); const { AwsSigv4Signer } = require ( ' @opensearch-project/opensearch/aws '); const client = new Client ({... AwsSigv4Signer ({ region: ' us-east-1 ', service: ' aoss ', // Must return a Promise that resolve to an AWS.Credentials object. // This function is used to acquire the credentials when the client start and // when the credentials are expired. // The Client will refresh the Credentials only when they are expired. // With AWS SDK V2, Credentials.refreshPromise is used when available to refresh the credentials. // Example with AWS SDK V3: getCredentials: () =&gt; { // Any other method to acquire a new Credentials object can be used. const credentialsProvider = defaultProvider (); return credentialsProvider (); }, }), node: \" https://xxx.region.aoss.amazonaws.com \" // OpenSearch domain URL }); copy Creating an index\nTo create an OpenSearch index, use the indices.create() method. You can use the following code to construct a JSON object with custom settings: var index_name = \" books \"; var settings = { settings: { index: { number_of_shards: 4, number_of_replicas: 3, }, }, }; var response = await client. indices. create ({ index: index_name, body: settings, }); copy Indexing a document\nYou can index a document into OpenSearch using the client’s index method: var document = { title: \" The Outsider \", author: \" Stephen King \", year: \" 2018 \", genre: \" Crime fiction \", }; var id = \" 1 \"; var response = await client. index ({ id: id, index: index_name, body: document, refresh: true, }); copy Searching for documents\nThe easiest way to search for documents is to construct a query string. The following code uses a match query to search for “The Outsider” in the title field: var query = { query: { match: { title: { query: \" The Outsider \", }, }, }, }; var response = await client. search ({ index: index_name, body: query, }); copy Deleting a document\nYou can delete a document using the client’s delete method: var response = await client. delete ({ index: index_name, id: id, }); copy Deleting an index\nYou can delete an index using the indices.delete() method: var response = await client. indices. delete ({ index: index_name, }); copy Sample program\nThe following sample program creates a client, adds an index with non-default settings, inserts a document, searches for the document, deletes the document, and then deletes the index: \" use strict \"; var host = \" localhost \"; var protocol = \" https \"; var port = 9200; var auth = \" admin:admin \"; // For testing only. Don't store credentials in code. var ca_certs_path = \" /full/path/to/root-ca.pem \"; // Optional client certificates if you don't want to use HTTP basic authentication. // var client_cert_path = '/full/path/to/client.pem' // var client_key_path = '/full/path/to/client-key.pem' // Create a client with SSL/TLS enabled. var { Client } = require ( \" @opensearch-project/opensearch \"); var fs = require ( \" fs \"); var client = new Client ({ node: protocol + \":// \" + auth + \" @ \" + host + \": \" + port, ssl: { ca: fs. readFileSync ( ca_certs_path), // You can turn off certificate verification (rejectUnauthorized: false) if you're using // self-signed certificates with a hostname mismatch. // cert: fs.readFileSync(client_cert_path), // key: fs.readFileSync(client_key_path) }, }); async function search () { // Create an index with non-default settings. var index_name = \" books \"; var settings = { settings: { index: { number_of_shards: 4, number_of_replicas: 3, }, }, }; var response = await client. indices. create ({ index: index_name, body: settings, }); console. log ( \" Creating index: \"); console. log ( response. body); // Add a document to the index. var document = { title: \" The Outsider \", author: \" Stephen King \", year: \" 2018 \", genre: \" Crime fiction \", }; var id = \" 1 \"; var response = await client. index ({ id: id, index: index_name, body: document, refresh: true, }); console. log ( \" Adding document: \"); console. log ( response. body); // Search for the document. var query = { query: { match: { title: { query: \" The Outsider \", }, }, }, }; var response = await client. search ({ index: index_name, body: query, }); console. log ( \" Search results: \"); console. log ( response. body. hits); // Delete the document. var response = await client. delete ({ index: index_name, id: id, }); console. log ( \" Deleting document: \"); console. log ( response. body); // Delete the index. var response = await client. indices. delete ({ index: index_name, }); console. log ( \" Deleting index: \"); console. log ( response. body); } search (). catch ( console. log); copy Circuit breaker\nThe memoryCircuitBreaker option can be used to prevent errors caused by a response payload being too large to fit into the heap memory available to the client.\nThe memoryCircuitBreaker object contains two fields: enabled: A Boolean used to turn the circuit breaker on or off. Defaults to false. maxPercentage: The threshold that determines whether the circuit breaker engages. Valid values are floats in the [0, 1] range that represent percentages in decimal form. Any value that exceeds that range will correct to 1.0.\nThe following example instantiates a client with the circuit breaker enabled and its threshold set to 80% of the available heap size limit: var client = new Client ({ memoryCircuitBreaker: { enabled: true, maxPercentage: 0.8, }, }); copy",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/opensearch-py-ml/",
    "title": "Opensearch-py-ml",
    "content": "opensearch-py-ml is a Python client that provides a suite of data analytics and natural language processing (NLP) support tools for OpenSearch. It provides data analysts with the ability to:\nCall OpenSearch indexes and manipulate them using the opensearch-py-ml DataFrame APIs. The opensearch-py-ml DataFrame wraps an OpenSearch index into an API similar to pandas, giving you the ability to process large amounts of data from OpenSearch inside a Jupyter Notebook.\nUpload NLP SentenceTransformer models into OpenSearch using the ML Commons plugin.\nTrain and tune SentenceTransformer models with synthetic queries.\nPrerequisites\nTo use opensearch-py-ml, install the OpenSearch Python client. The Python client allows OpenSearch to use the Python syntax required to run DataFrames in opensearch-py-ml.\nInstall opensearch-py-ml To add the client to your project, install it using pip: pip install opensearch-py-ml copy Then import the client into OpenSearch like any other module: from opensearchpy import OpenSearch import openseach_py_ml as oml copy API reference\nFor information on all opensearch-py-ml objects, functions, and methods, see the opensearch-py-ml API reference.\nNext steps\nIf you want to track or contribute to the development of the opensearch-py-ml client, see the opensearch-py-ml GitHub repository.\nFor example Python notebooks to use with the client, see Examples.",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/php/",
    "title": "PHP client",
    "content": "The OpenSearch PHP client provides a safer and easier way to interact with your OpenSearch cluster. Rather than using OpenSearch from a browser and potentially exposing your data to the public, you can build an OpenSearch client that takes care of sending requests to your cluster. The client contains a library of APIs that let you perform different operations on your cluster and return a standard response body.\nThis getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-php repo.\nSetup\nTo add the client to your project, install it using composer: composer require opensearch-project/opensearch-php copy To install a specific major version of the client, run the following command: composer require opensearch-project/opensearch-php:&lt;version&gt; copy Then require the autload file from composer in your code: require __DIR__. '/vendor/autoload.php'; copy Connecting to OpenSearch\nTo connect to the default OpenSearch host, create a client object with the address https://localhost:9200 if you are using the Security plugin: $client = ( new \\ OpenSearch\\ClientBuilder ()) -&gt; setHosts ([ 'https://localhost:9200']) -&gt; setBasicAuthentication ( 'admin', 'admin') // For testing only. Don't store credentials in code. -&gt; setSSLVerification ( false) // For testing only. Use certificate for validation -&gt; build (); copy Connecting to Amazon OpenSearch Service\nThe following example illustrates connecting to Amazon OpenSearch Service: $client = ( new \\ OpenSearch\\ClientBuilder ()) -&gt; setSigV4Region ( 'us-east-2') -&gt; setSigV4Service ( 'es') // Default credential provider. -&gt; setSigV4CredentialProvider ( true) // Using a custom access key and secret -&gt; setSigV4CredentialProvider ([ 'key' =&gt; 'awskeyid', 'secret' =&gt; 'awssecretkey',]) -&gt; build (); copy Connecting to Amazon OpenSearch Serverless\nThe following example illustrates connecting to Amazon OpenSearch Serverless Service: $client = ( new \\ OpenSearch\\ClientBuilder ()) -&gt; setSigV4Region ( 'us-east-2') -&gt; setSigV4Service ( 'aoss') // Default credential provider. -&gt; setSigV4CredentialProvider ( true) // Using a custom access key and secret -&gt; setSigV4CredentialProvider ([ 'key' =&gt; 'awskeyid', 'secret' =&gt; 'awssecretkey',]) -&gt; build (); copy Creating an index\nTo create an OpenSearch index with custom settings, use the following code: $indexName = 'test-index-name'; // Create an index with non-default settings. $client -&gt; indices () -&gt; create ([ 'index' =&gt; $indexName, 'body' =&gt; [ 'settings' =&gt; [ 'index' =&gt; [ 'number_of_shards' =&gt; 4]]]]); copy Indexing a document\nYou can index a document into OpenSearch using the following code: $client -&gt; create ([ 'index' =&gt; $indexName, 'id' =&gt; 1, 'body' =&gt; [ 'title' =&gt; 'Moneyball', 'director' =&gt; 'Bennett Miller', 'year' =&gt; 2011]]); copy Searching for documents\nThe following code uses a multi_match query to search for “miller” in the title and director fields. It boosts the documents where “miller” appears in the title field: var_dump ( $client -&gt; search ([ 'index' =&gt; $indexName, 'body' =&gt; [ 'size' =&gt; 5, 'query' =&gt; [ 'multi_match' =&gt; [ 'query' =&gt; 'miller', 'fields' =&gt; [ 'title^2', 'director']]]]])); copy Deleting a document\nYou can delete a document using the following code: $client -&gt; delete ([ 'index' =&gt; $indexName, 'id' =&gt; 1,]); copy Deleting an index\nYou can delete an index using the following code: $client -&gt; indices () -&gt; delete ([ 'index' =&gt; $indexName]); copy Sample program\nThe following sample program creates a client, adds an index with non-default settings, inserts a document, searches for the document, deletes the document, and then deletes the index: &lt;?php require __DIR__. '/vendor/autoload.php'; $client = ( new \\ OpenSearch\\ClientBuilder ()) -&gt; setHosts ([ 'https://localhost:9200']) -&gt; setBasicAuthentication ( 'admin', 'admin') // For testing only. Don't store credentials in code. -&gt; setSSLVerification ( false) // For testing only. Use certificate for validation -&gt; build (); $indexName = 'test-index-name'; // Print OpenSearch version information on console. var_dump ( $client -&gt; info ()); // Create an index with non-default settings. $client -&gt; indices () -&gt; create ([ 'index' =&gt; $indexName, 'body' =&gt; [ 'settings' =&gt; [ 'index' =&gt; [ 'number_of_shards' =&gt; 4]]]]); $client -&gt; create ([ 'index' =&gt; $indexName, 'id' =&gt; 1, 'body' =&gt; [ 'title' =&gt; 'Moneyball', 'director' =&gt; 'Bennett Miller', 'year' =&gt; 2011]]); // Search for it var_dump ( $client -&gt; search ([ 'index' =&gt; $indexName, 'body' =&gt; [ 'size' =&gt; 5, 'query' =&gt; [ 'multi_match' =&gt; [ 'query' =&gt; 'miller', 'fields' =&gt; [ 'title^2', 'director']]]]])); // Delete a single document $client -&gt; delete ([ 'index' =&gt; $indexName, 'id' =&gt; 1,]); // Delete index $client -&gt; indices () -&gt; delete ([ 'index' =&gt; $indexName]);?&gt; copy",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/python-high-level/",
    "title": "High-level Python client",
    "content": "The OpenSearch high-level Python client ( opensearch-dsl-py) will be deprecated after version 2.1.0. We recommend switching to the Python client ( opensearch-py), which now includes the functionality of opensearch-dsl-py.\nThe OpenSearch high-level Python client ( opensearch-dsl-py) provides wrapper classes for common OpenSearch entities, like documents, so you can work with them as Python objects. Additionally, the high-level client simplifies writing queries and supplies convenient Python methods for common OpenSearch operations. The high-level Python client supports creating and indexing documents, searching with and without filters, and updating documents using queries.\nThis getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-dsl-py repo.\nSetup\nTo add the client to your project, install it using pip: pip install opensearch-dsl copy After installing the client, you can import it like any other module: from opensearchpy import OpenSearch from opensearch_dsl import Search copy Connecting to OpenSearch\nTo connect to the default OpenSearch host, create a client object with SSL enabled if you are using the Security plugin. You can use the default credentials for testing purposes: host = 'localhost' port = 9200 auth = ( 'admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA. # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch ( hosts = [{ 'host': host, 'port': port }], http_compress = True, # enables gzip compression for request bodies http_auth = auth, use_ssl = True, verify_certs = True, ssl_assert_hostname = False, ssl_show_warn = False, ca_certs = ca_certs_path) copy If you have your own client certificates, specify them in the client_cert_path and client_key_path parameters: host = 'localhost' port = 9200 auth = ( 'admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA. # Optional client certificates if you don't want to use HTTP basic authentication. client_cert_path = '/full/path/to/client.pem' client_key_path = '/full/path/to/client-key.pem' # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch ( hosts = [{ 'host': host, 'port': port }], http_compress = True, # enables gzip compression for request bodies http_auth = auth, client_cert = client_cert_path, client_key = client_key_path, use_ssl = True, verify_certs = True, ssl_assert_hostname = False, ssl_show_warn = False, ca_certs = ca_certs_path) copy If you are not using the Security plugin, create a client object with SSL disabled: host = 'localhost' port = 9200 # Create the client with SSL/TLS and hostname verification disabled. client = OpenSearch ( hosts = [{ 'host': host, 'port': port }], http_compress = True, # enables gzip compression for request bodies use_ssl = False, verify_certs = False, ssl_assert_hostname = False, ssl_show_warn = False) copy Creating an index\nTo create an OpenSearch index, use the client.indices.create() method. You can use the following code to construct a JSON object with custom settings: index_name = 'my-dsl-index' index_body = { 'settings': { 'index': { 'number_of_shards': 4 } } } response = client. indices. create ( index_name, body = index_body) copy Indexing a document\nYou can create a class to represent the documents that you’ll index in OpenSearch by extending the Document class: class Movie ( Document): title = Text ( fields = { 'raw': Keyword ()}) director = Text () year = Text () class Index: name = index_name def save ( self, ** kwargs): return super ( Movie, self). save ( ** kwargs) copy To index a document, create an object of the new class and call its save() method: # Set up the opensearch-py version of the document Movie. init ( using = client) doc = Movie ( meta = { 'id': 1 }, title = 'Moneyball', director = 'Bennett Miller', year = '2011') response = doc. save ( using = client) copy Performing bulk operations\nYou can perform several operations at the same time by using the bulk() method of the client. The operations may be of the same type or of different types. Note that the operations must be separated by a \\n and the entire string must be a single line: movies = '{ \"index\": { \"_index\": \"my-dsl-index\", \"_id\": \"2\" } } \\n { \"title\": \"Interstellar\", \"director\": \"Christopher Nolan\", \"year\": \"2014\"} \\n { \"create\": { \"_index\": \"my-dsl-index\", \"_id\": \"3\" } } \\n { \"title\": \"Star Trek Beyond\", \"director\": \"Justin Lin\", \"year\": \"2015\"} \\n { \"update\": {\"_id\": \"3\", \"_index\": \"my-dsl-index\" } } \\n { \"doc\": {\"year\": \"2016\"} }' client. bulk ( movies) copy Searching for documents\nYou can use the Search class to construct a query. The following code creates a Boolean query with a filter: s = Search ( using = client, index = index_name) \\. filter ( \"term\", year = \"2011\") \\. query ( \"match\", title = \"Moneyball\") response = s. execute () copy The preceding query is equivalent to the following query in OpenSearch domain-specific language (DSL): GET my-dsl-index/_search { \"query\": { \"bool\": { \"must\": { \"match\": { \"title\": \"Moneyball\" } }, \"filter\": { \"term\": { \"year\": 2011 } } } } } Deleting a document\nYou can delete a document using the client.delete() method: response = client. delete ( index = 'my-dsl-index', id = '1') copy Deleting an index\nYou can delete an index using the client.indices.delete() method: response = client. indices. delete ( index = 'my-dsl-index') copy Sample program\nThe following sample program creates a client, adds an index with non-default settings, inserts a document, performs bulk operations, searches for the document, deletes the document, and then deletes the index: from opensearchpy import OpenSearch from opensearch_dsl import Search, Document, Text, Keyword host = 'localhost' port = 9200 auth = ( 'admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = 'root-ca.pem' # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch ( hosts = [{ 'host': host, 'port': port }], http_compress = True, # enables gzip compression for request bodies # http_auth=auth, use_ssl = False, verify_certs = False, ssl_assert_hostname = False, ssl_show_warn = False, # ca_certs=ca_certs_path) index_name = 'my-dsl-index' index_body = { 'settings': { 'index': { 'number_of_shards': 4 } } } response = client. indices. create ( index_name, index_body) print ( ' \\n Creating index:') print ( response) # Create the structure of the document class Movie ( Document): title = Text ( fields = { 'raw': Keyword ()}) director = Text () year = Text () class Index: name = index_name def save ( self, ** kwargs): return super ( Movie, self). save ( ** kwargs) # Set up the opensearch-py version of the document Movie. init ( using = client) doc = Movie ( meta = { 'id': 1 }, title = 'Moneyball', director = 'Bennett Miller', year = '2011') response = doc. save ( using = client) print ( ' \\n Adding document:') print ( response) # Perform bulk operations movies = '{ \"index\": { \"_index\": \"my-dsl-index\", \"_id\": \"2\" } } \\n { \"title\": \"Interstellar\", \"director\": \"Christopher Nolan\", \"year\": \"2014\"} \\n { \"create\": { \"_index\": \"my-dsl-index\", \"_id\": \"3\" } } \\n { \"title\": \"Star Trek Beyond\", \"director\": \"Justin Lin\", \"year\": \"2015\"} \\n { \"update\": {\"_id\": \"3\", \"_index\": \"my-dsl-index\" } } \\n { \"doc\": {\"year\": \"2016\"} }' client. bulk ( movies) # Search for the document. s = Search ( using = client, index = index_name) \\. filter ( 'term', year = '2011') \\. query ( 'match', title = 'Moneyball') response = s. execute () print ( ' \\n Search results:') for hit in response: print ( hit. meta. score, hit. title) # Delete the document. print ( ' \\n Deleting document:') print ( response) # Delete the index. response = client. indices. delete ( index = index_name) print ( ' \\n Deleting index:') print ( response) copy",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/python-low-level/",
    "title": "Low-level Python client",
    "content": "The OpenSearch low-level Python client ( opensearch-py) provides wrapper methods for the OpenSearch REST API so that you can interact with your cluster more naturally in Python. Rather than sending raw HTTP requests to a given URL, you can create an OpenSearch client for your cluster and call the client’s built-in functions. For the client’s complete API documentation and additional examples, see the opensearch-py API documentation.\nThis getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-py repo.\nSetup\nTo add the client to your project, install it using pip: pip install opensearch-py copy After installing the client, you can import it like any other module: from opensearchpy import OpenSearch copy Connecting to OpenSearch\nTo connect to the default OpenSearch host, create a client object with SSL enabled if you are using the Security plugin. You can use the default credentials for testing purposes: host = 'localhost' port = 9200 auth = ( 'admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA. # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch ( hosts = [{ 'host': host, 'port': port }], http_compress = True, # enables gzip compression for request bodies http_auth = auth, use_ssl = True, verify_certs = True, ssl_assert_hostname = False, ssl_show_warn = False, ca_certs = ca_certs_path) copy If you have your own client certificates, specify them in the client_cert_path and client_key_path parameters: host = 'localhost' port = 9200 auth = ( 'admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA. # Optional client certificates if you don't want to use HTTP basic authentication. client_cert_path = '/full/path/to/client.pem' client_key_path = '/full/path/to/client-key.pem' # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch ( hosts = [{ 'host': host, 'port': port }], http_compress = True, # enables gzip compression for request bodies http_auth = auth, client_cert = client_cert_path, client_key = client_key_path, use_ssl = True, verify_certs = True, ssl_assert_hostname = False, ssl_show_warn = False, ca_certs = ca_certs_path) copy If you are not using the Security plugin, create a client object with SSL disabled: host = 'localhost' port = 9200 # Create the client with SSL/TLS and hostname verification disabled. client = OpenSearch ( hosts = [{ 'host': host, 'port': port }], http_compress = True, # enables gzip compression for request bodies use_ssl = False, verify_certs = False, ssl_assert_hostname = False, ssl_show_warn = False) copy Connecting to Amazon OpenSearch Service\nThe following example illustrates connecting to Amazon OpenSearch Service: from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth import boto3 host = '' # cluster endpoint, for example: my-test-domain.us-east-1.es.amazonaws.com region = 'us-west-2' service = 'es' credentials = boto3. Session (). get_credentials () auth = AWSV4SignerAuth ( credentials, region, service) client = OpenSearch ( hosts = [{ 'host': host, 'port': 443 }], http_auth = auth, use_ssl = True, verify_certs = True, connection_class = RequestsHttpConnection, pool_maxsize = 20) copy Connecting to Amazon OpenSearch Serverless\nThe following example illustrates connecting to Amazon OpenSearch Serverless Service: from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth import boto3 host = '' # cluster endpoint, for example: my-test-domain.us-east-1.aoss.amazonaws.com region = 'us-west-2' service = 'aoss' credentials = boto3. Session (). get_credentials () auth = AWSV4SignerAuth ( credentials, region, service) client = OpenSearch ( hosts = [{ 'host': host, 'port': 443 }], http_auth = auth, use_ssl = True, verify_certs = True, connection_class = RequestsHttpConnection, pool_maxsize = 20) copy Creating an index\nTo create an OpenSearch index, use the client.indices.create() method. You can use the following code to construct a JSON object with custom settings: index_name = 'python-test-index' index_body = { 'settings': { 'index': { 'number_of_shards': 4 } } } response = client. indices. create ( index_name, body = index_body) copy Indexing a document\nYou can index a document using the client.index() method: document = { 'title': 'Moneyball', 'director': 'Bennett Miller', 'year': '2011' } response = client. index ( index = 'python-test-index', body = document, id = '1', refresh = True) copy Performing bulk operations\nYou can perform several operations at the same time by using the bulk() method of the client. The operations may be of the same type or of different types. Note that the operations must be separated by a \\n and the entire string must be a single line: movies = '{ \"index\": { \"_index\": \"my-dsl-index\", \"_id\": \"2\" } } \\n { \"title\": \"Interstellar\", \"director\": \"Christopher Nolan\", \"year\": \"2014\"} \\n { \"create\": { \"_index\": \"my-dsl-index\", \"_id\": \"3\" } } \\n { \"title\": \"Star Trek Beyond\", \"director\": \"Justin Lin\", \"year\": \"2015\"} \\n { \"update\": {\"_id\": \"3\", \"_index\": \"my-dsl-index\" } } \\n { \"doc\": {\"year\": \"2016\"} }' client. bulk ( movies) copy Searching for documents\nThe easiest way to search for documents is to construct a query string. The following code uses a multi-match query to search for “miller” in the title and director fields. It boosts the documents that have “miller” in the title field: q = 'miller' query = { 'size': 5, 'query': { 'multi_match': { 'query': q, 'fields': [ 'title^2', 'director'] } } } response = client. search ( body = query, index = 'python-test-index') copy Deleting a document\nYou can delete a document using the client.delete() method: response = client. delete ( index = 'python-test-index', id = '1') copy Deleting an index\nYou can delete an index using the client.indices.delete() method: response = client. indices. delete ( index = 'python-test-index') copy Sample program\nThe following sample program creates a client, adds an index with non-default settings, inserts a document, performs bulk operations, searches for the document, deletes the document, and then deletes the index: from opensearchpy import OpenSearch host = 'localhost' port = 9200 auth = ( 'admin', 'admin') # For testing only. Don't store credentials in code. ca_certs_path = '/full/path/to/root-ca.pem' # Provide a CA bundle if you use intermediate CAs with your root CA. # Optional client certificates if you don't want to use HTTP basic authentication.\n# client_cert_path = '/full/path/to/client.pem'\n# client_key_path = '/full/path/to/client-key.pem' # Create the client with SSL/TLS enabled, but hostname verification disabled. client = OpenSearch ( hosts = [{ 'host': host, 'port': port }], http_compress = True, # enables gzip compression for request bodies http_auth = auth, # client_cert = client_cert_path, # client_key = client_key_path, use_ssl = True, verify_certs = True, ssl_assert_hostname = False, ssl_show_warn = False, ca_certs = ca_certs_path) # Create an index with non-default settings. index_name = 'python-test-index' index_body = { 'settings': { 'index': { 'number_of_shards': 4 } } } response = client. indices. create ( index_name, body = index_body) print ( ' \\n Creating index:') print ( response) # Add a document to the index. document = { 'title': 'Moneyball', 'director': 'Bennett Miller', 'year': '2011' } id = '1' response = client. index ( index = index_name, body = document, id = id, refresh = True) print ( ' \\n Adding document:') print ( response) # Perform bulk operations movies = '{ \"index\": { \"_index\": \"my-dsl-index\", \"_id\": \"2\" } } \\n { \"title\": \"Interstellar\", \"director\": \"Christopher Nolan\", \"year\": \"2014\"} \\n { \"create\": { \"_index\": \"my-dsl-index\", \"_id\": \"3\" } } \\n { \"title\": \"Star Trek Beyond\", \"director\": \"Justin Lin\", \"year\": \"2015\"} \\n { \"update\": {\"_id\": \"3\", \"_index\": \"my-dsl-index\" } } \\n { \"doc\": {\"year\": \"2016\"} }' client. bulk ( movies) # Search for the document. q = 'miller' query = { 'size': 5, 'query': { 'multi_match': { 'query': q, 'fields': [ 'title^2', 'director'] } } } response = client. search ( body = query, index = index_name) print ( ' \\n Search results:') print ( response) # Delete the document. response = client. delete ( index = index_name, id = id) print ( ' \\n Deleting document:') print ( response) # Delete the index. response = client. indices. delete ( index = index_name) print ( ' \\n Deleting index:') print ( response) copy",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/ruby/",
    "title": "Ruby client",
    "content": "The OpenSearch Ruby client allows you to interact with your OpenSearch clusters through Ruby methods rather than HTTP methods and raw JSON. For the client’s complete API documentation and additional examples, see the opensearch-transport, opensearch-api, opensearch-dsl, and opensearch-ruby gem documentation.\nThis getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-ruby repo.\nInstalling the Ruby client\nTo install the Ruby gem for the Ruby client, run the following command: gem install opensearch-ruby copy To use the client, import it as a module: require 'opensearch' copy Connecting to OpenSearch\nTo connect to the default OpenSearch host, create a client object, passing the default host address in the constructor: client = OpenSearch:: Client. new ( host: 'http://localhost:9200') copy The following example creates a client object with a custom URL and the log option set to true. It sets the retry_on_failure parameter to retry a failed request five times rather than the default three times. Finally, it increases the timeout by setting the request_timeout parameter to 120 seconds. It then returns the basic cluster health information: client = OpenSearch:: Client. new ( url: \"http://localhost:9200\", retry_on_failure: 5, request_timeout: 120, log: true) client. cluster. health copy The output is as follows: 2022-08-25 14:24:52 -0400: GET http://localhost:9200/ [ status:200, request:0.048s, query:n/a]\n2022-08-25 14:24:52 -0400: &lt; { \"name\": \"opensearch\", \"cluster_name\": \"docker-cluster\", \"cluster_uuid\": \"Aw0F5Pt9QF6XO9vXQHIs_w\", \"version\": { \"distribution\": \"opensearch\", \"number\": \"2.2.0\", \"build_type\": \"tar\", \"build_hash\": \"b1017fa3b9a1c781d4f34ecee411e0cdf930a515\", \"build_date\": \"2022-08-09T02:27:25.256769336Z\", \"build_snapshot\": false, \"lucene_version\": \"9.3.0\", \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } 2022-08-25 14:24:52 -0400: GET http://localhost:9200/_cluster/health [ status:200, request:0.018s, query:n/a]\n2022-08-25 14:24:52 -0400: &lt; { \"cluster_name\": \"docker-cluster\", \"status\": \"yellow\", \"timed_out\":false, \"number_of_nodes\":1, \"number_of_data_nodes\":1, \"discovered_master\":true, \"discovered_cluster_manager\":true, \"active_primary_shards\":10, \"active_shards\":10, \"relocating_shards\":0, \"initializing_shards\":0, \"unassigned_shards\":8, \"delayed_unassigned_shards\":0, \"number_of_pending_tasks\":0, \"number_of_in_flight_fetch\":0, \"task_max_waiting_in_queue_millis\":0, \"active_shards_percent_as_number\":55.55555555555556 } Connecting to Amazon OpenSearch Service\nTo connect to Amazon OpenSearch Service, first install the opensearch-aws-sigv4 gem: gem install opensearch-aws-sigv4 require 'opensearch-aws-sigv4' require 'aws-sigv4' signer = Aws:: Sigv4:: Signer. new ( service: 'es', region: 'us-west-2', # signing service region access_key_id: 'key_id', secret_access_key: 'secret') client = OpenSearch:: Aws:: Sigv4Client. new ({ host: 'https://your.amz-managed-opensearch.domain', log: true }, signer) # create an index and document index = 'prime' client. indices. create ( index: index) client. index ( index: index, id: '1', body: { name: 'Amazon Echo', msrp: '5999', year: 2011 }) # search for the document client. search ( body: { query: { match: { name: 'Echo' } } }) # delete the document client. delete ( index: index, id: '1') # delete the index client. indices. delete ( index: index) copy Connecting to Amazon OpenSearch Serverless\nTo connect to Amazon OpenSearch Serverless Service, first install the opensearch-aws-sigv4 gem: gem install opensearch-aws-sigv4 require 'opensearch-aws-sigv4' require 'aws-sigv4' signer = Aws:: Sigv4:: Signer. new ( service: 'aoss', region: 'us-west-2', # signing service region access_key_id: 'key_id', secret_access_key: 'secret') client = OpenSearch:: Aws:: Sigv4Client. new ({ host: 'https://your.amz-managed-opensearch.domain', # serverless endpoint for OpenSearch Serverless log: true }, signer) # create an index and document index = 'prime' client. indices. create ( index: index) client. index ( index: index, id: '1', body: { name: 'Amazon Echo', msrp: '5999', year: 2011 }) # search for the document client. search ( body: { query: { match: { name: 'Echo' } } }) # delete the document client. delete ( index: index, id: '1') # delete the index client. indices. delete ( index: index) copy Creating an index\nYou don’t need to create an index explicitly in OpenSearch. Once you upload a document into an index that does not exist, OpenSearch creates the index automatically. Alternatively, you can create an index explicitly to specify settings like the number of primary and replica shards. To create an index with non-default settings, create an index body hash with those settings: index_body = { 'settings': { 'index': { 'number_of_shards': 1, 'number_of_replicas': 2 } } } client. indices. create ( index: 'students', body: index_body) copy Mappings\nOpenSearch uses dynamic mapping to infer field types of the documents that are indexed. However, to have more control over the schema of your document, you can pass an explicit mapping to OpenSearch. You can define data types for some or all fields of your document in this mapping. To create a mapping for an index, use the put_mapping method: client. indices. put_mapping ( index: 'students', body: { properties: { first_name: { type: 'keyword' }, last_name: { type: 'keyword' } } }) copy By default, string fields are mapped as text, but in the mapping above, the first_name and last_name fields are mapped as keyword. This mapping signals to OpenSearch that these fields should not be analyzed and should support only full case-sensitive matches.\nYou can verify the index’s mappings using the get_mapping method: response = client. indices. get_mapping ( index: 'students') copy If you know the mapping of your documents in advance and want to avoid mapping errors (for example, misspellings of a field name), you can set the dynamic parameter to strict: client. indices. put_mapping ( index: 'students', body: { dynamic: 'strict', properties: { first_name: { type: 'keyword' }, last_name: { type: 'keyword' }, gpa: { type: 'float' }, grad_year: { type: 'integer' } } }) copy With strict mapping, you can index a document with a missing field, but you cannot index a document with a new field. For example, indexing the following document with a misspelled grad_yea field fails: document = { first_name: 'Connor', last_name: 'James', gpa: 3.93, grad_yea: 2021 } client. index ( index: 'students', body: document, id: 100, refresh: true) copy OpenSearch returns a mapping error: { \"error\": { \"root_cause\":[ { \"type\": \"strict_dynamic_mapping_exception\", \"reason\": \"mapping set to strict, dynamic introduction of [grad_yea] within [_doc] is not allowed\" }], \"type\": \"strict_dynamic_mapping_exception\", \"reason\": \"mapping set to strict, dynamic introduction of [grad_yea] within [_doc] is not allowed\" }, \"status\":400 } Indexing one document\nTo index one document, use the index method: document = { first_name: 'Connor', last_name: 'James', gpa: 3.93, grad_year: 2021 } client. index ( index: 'students', body: document, id: 100, refresh: true) copy Updating a document\nTo update a document, use the update method: client. update ( index: 'students', id: 100, body: { doc: { gpa: 3.25 } }, refresh: true) copy Deleting a document\nTo delete a document, use the delete method: client. delete ( index: 'students', id: 100, refresh: true) copy Bulk operations\nYou can perform several operations at the same time by using the bulk method. The operations may be of the same type or of different types.\nYou can index multiple documents using the bulk method: actions = [ { index: { _index: 'students', _id: '200' } }, { first_name: 'James', last_name: 'Rodriguez', gpa: 3.91, grad_year: 2019 }, { index: { _index: 'students', _id: '300' } }, { first_name: 'Nikki', last_name: 'Wolf', gpa: 3.87, grad_year: 2020 }] client. bulk ( body: actions, refresh: true) copy You can delete multiple documents as follows: # Deleting multiple documents. actions = [ { delete: { _index: 'students', _id: 200 } }, { delete: { _index: 'students', _id: 300 } }] client. bulk ( body: actions, refresh: true) copy You can perform different operations when using bulk as follows: actions = [ { index: { _index: 'students', _id: 100, data: { first_name: 'Paulo', last_name: 'Santos', gpa: 3.29, grad_year: 2022 } } }, { index: { _index: 'students', _id: 200, data: { first_name: 'Shirley', last_name: 'Rodriguez', gpa: 3.92, grad_year: 2020 } } }, { index: { _index: 'students', _id: 300, data: { first_name: 'Akua', last_name: 'Mansa', gpa: 3.95, grad_year: 2022 } } }, { index: { _index: 'students', _id: 400, data: { first_name: 'John', last_name: 'Stiles', gpa: 3.72, grad_year: 2019 } } }, { index: { _index: 'students', _id: 500, data: { first_name: 'Li', last_name: 'Juan', gpa: 3.94, grad_year: 2022 } } }, { index: { _index: 'students', _id: 600, data: { first_name: 'Richard', last_name: 'Roe', gpa: 3.04, grad_year: 2020 } } }, { update: { _index: 'students', _id: 100, data: { doc: { gpa: 3.73 } } } }, { delete: { _index: 'students', _id: 200 } }] client. bulk ( body: actions, refresh: true) copy In the above example, you pass the data and the header together and you denote the data with the data: key.\nSearching for a document\nTo search for a document, use the search method. The following example searches for a student whose first or last name is “James.” It uses a multi_match query to search for two fields ( first_name and last_name), and it is boosting the last_name field in relevance with a caret notation ( last_name^2). q = 'James' query = { 'size': 5, 'query': { 'multi_match': { 'query': q, 'fields': [ 'first_name', 'last_name^2'] } } } response = client. search ( body: query, index: 'students') copy If you omit the request body in the search method, your query becomes a match_all query and returns all documents in the index: client. search ( index: 'students') copy Boolean query\nThe Ruby client exposes full OpenSearch query capability. In addition to simple searches that use the match query, you can create a more complex Boolean query to search for students who graduated in 2022 and sort them by last name. In the example below, search is limited to 10 documents. query = { 'query': { 'bool': { 'filter': { 'term': { 'grad_year': 2022 } } } }, 'sort': { 'last_name': { 'order': 'asc' } } } response = client. search ( index: 'students', from: 0, size: 10, body: query) copy Multi-search\nYou can bulk several queries together and perform a multi-search using the msearch method. The following code searches for students whose GPAs are outside the 3.1–3.9 range: actions = [ {}, { query: { range: { gpa: { gt: 3.9 }}}}, {}, { query: { range: { gpa: { lt: 3.1 }}}}] response = client. msearch ( index: 'students', body: actions) copy Scroll\nYou can paginate your search results using the Scroll API: response = client. search ( index: index_name, scroll: '2m', size: 2) while response [ 'hits'][ 'hits']. size. positive? scroll_id = response [ '_scroll_id'] puts ( response [ 'hits'][ 'hits']. map { | doc | [ doc [ '_source'][ 'first_name'] + ' ' + doc [ '_source'][ 'last_name']] }) response = client. scroll ( scroll: '1m', body: { scroll_id: scroll_id }) end copy First, you issue a search query, specifying the scroll and size parameters. The scroll parameter tells OpenSearch how long to keep the search context. In this case, it is set to two minutes. The size parameter specifies how many documents you want to return in each request.\nThe response to the initial search query contains a _scroll_id that you can use to get the next set of documents. To do this, you use the scroll method, again specifying the scroll parameter and passing the _scroll_id in the body. You don’t need to specify the query or index to the scroll method. The scroll method returns the next set of documents and the _scroll_id. It’s important to use the latest _scroll_id when requesting the next batch of documents because _scroll_id can change between requests.\nDeleting an index\nYou can delete the index using the delete method: response = client. indices. delete ( index: index_name) copy Sample program\nThe following is a complete sample program that illustrates all of the concepts described in the preceding sections. The Ruby client’s methods return responses as Ruby hashes, which are hard to read. To display JSON responses in a pretty format, the sample program uses the MultiJson.dump method. require 'opensearch' client = OpenSearch:: Client. new ( host: 'http://localhost:9200') # Create an index with non-default settings index_name = 'students' index_body = { 'settings': { 'index': { 'number_of_shards': 1, 'number_of_replicas': 2 } } } client. indices. create ( index: index_name, body: index_body) # Create a mapping client. indices. put_mapping ( index: index_name, body: { properties: { first_name: { type: 'keyword' }, last_name: { type: 'keyword' } } }) # Get mappings response = client. indices. get_mapping ( index: index_name) puts 'Mappings for the students index:' puts MultiJson. dump ( response, pretty: \"true\") # Add one document to the index puts 'Adding one document:' document = { first_name: 'Connor', last_name: 'James', gpa: 3.93, grad_year: 2021 } id = 100 client. index ( index: index_name, body: document, id: id, refresh: true) response = client. search ( index: index_name) puts MultiJson. dump ( response, pretty: \"true\") # Update a document puts 'Updating a document:' client. update ( index: index_name, id: id, body: { doc: { gpa: 3.25 } }, refresh: true) response = client. search ( index: index_name) puts MultiJson. dump ( response, pretty: \"true\") print 'The updated gpa is ' puts response [ 'hits'][ 'hits']. map { | doc | doc [ '_source'][ 'gpa'] } # Add many documents in bulk documents = [ { index: { _index: index_name, _id: '200' } }, { first_name: 'James', last_name: 'Rodriguez', gpa: 3.91, grad_year: 2019 }, { index: { _index: index_name, _id: '300' } }, { first_name: 'Nikki', last_name: 'Wolf', gpa: 3.87, grad_year: 2020 }] client. bulk ( body: documents, refresh: true) # Get all documents in the index response = client. search ( index: index_name) puts 'All documents in the index after bulk upload:' puts MultiJson. dump ( response, pretty: \"true\") # Search for a document using a multi_match query puts 'Searching for documents that match \"James\":' q = 'James' query = { 'size': 5, 'query': { 'multi_match': { 'query': q, 'fields': [ 'first_name', 'last_name^2'] } } } response = client. search ( body: query, index: index_name) puts MultiJson. dump ( response, pretty: \"true\") # Delete the document response = client. delete ( index: index_name, id: id, refresh: true) response = client. search ( index: index_name) puts 'Documents in the index after one document was deleted:' puts MultiJson. dump ( response, pretty: \"true\") # Delete multiple documents actions = [ { delete: { _index: index_name, _id: 200 } }, { delete: { _index: index_name, _id: 300 } }] client. bulk ( body: actions, refresh: true) response = client. search ( index: index_name) puts 'Documents in the index after all documents were deleted:' puts MultiJson. dump ( response, pretty: \"true\") # Bulk several operations together actions = [ { index: { _index: index_name, _id: 100, data: { first_name: 'Paulo', last_name: 'Santos', gpa: 3.29, grad_year: 2022 } } }, { index: { _index: index_name, _id: 200, data: { first_name: 'Shirley', last_name: 'Rodriguez', gpa: 3.92, grad_year: 2020 } } }, { index: { _index: index_name, _id: 300, data: { first_name: 'Akua', last_name: 'Mansa', gpa: 3.95, grad_year: 2022 } } }, { index: { _index: index_name, _id: 400, data: { first_name: 'John', last_name: 'Stiles', gpa: 3.72, grad_year: 2019 } } }, { index: { _index: index_name, _id: 500, data: { first_name: 'Li', last_name: 'Juan', gpa: 3.94, grad_year: 2022 } } }, { index: { _index: index_name, _id: 600, data: { first_name: 'Richard', last_name: 'Roe', gpa: 3.04, grad_year: 2020 } } }, { update: { _index: index_name, _id: 100, data: { doc: { gpa: 3.73 } } } }, { delete: { _index: index_name, _id: 200 } }] client. bulk ( body: actions, refresh: true) puts 'All documents in the index after bulk operations with scrolling:' response = client. search ( index: index_name, scroll: '2m', size: 2) while response [ 'hits'][ 'hits']. size. positive? scroll_id = response [ '_scroll_id'] puts ( response [ 'hits'][ 'hits']. map { | doc | [ doc [ '_source'][ 'first_name'] + ' ' + doc [ '_source'][ 'last_name']] }) response = client. scroll ( scroll: '1m', body: { scroll_id: scroll_id }) end # Multi search actions = [ {}, { query: { range: { gpa: { gt: 3.9 }}}}, {}, { query: { range: { gpa: { lt: 3.1 }}}}] response = client. msearch ( index: index_name, body: actions) puts 'Multi search results:' puts MultiJson. dump ( response, pretty: \"true\") # Boolean query query = { 'query': { 'bool': { 'filter': { 'term': { 'grad_year': 2022 } } } }, 'sort': { 'last_name': { 'order': 'asc' } } } response = client. search ( index: index_name, from: 0, size: 10, body: query) puts 'Boolean query search results:' puts MultiJson. dump ( response, pretty: \"true\") # Delete the index puts 'Deleting the index:' response = client. indices. delete ( index: index_name) puts MultiJson. dump ( response, pretty: \"true\") copy The opensearch-aws-sigv4 gem provides the OpenSearch::Aws::Sigv4Client class, which has all features of OpenSearch::Client. The only difference between these two clients is that OpenSearch::Aws::Sigv4Client requires an instance of Aws::Sigv4::Signer during instantiation to authenticate with AWS: require 'opensearch-aws-sigv4' require 'aws-sigv4' signer = Aws:: Sigv4:: Signer. new ( service: 'es', region: 'us-west-2', access_key_id: 'key_id', secret_access_key: 'secret') client = OpenSearch:: Aws:: Sigv4Client. new ({ log: true }, signer) client. cluster. health client. transport. reload_connections! client. search q: 'test' copy",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/clients/rust/",
    "title": "Rust client",
    "content": "The OpenSearch Rust client lets you connect your Rust application with the data in your OpenSearch cluster. For the client’s complete API documentation and additional examples, see the OpenSearch docs.rs documentation.\nThis getting started guide illustrates how to connect to OpenSearch, index documents, and run queries. For the client source code, see the opensearch-rs repo.\nSetup\nIf you’re starting a new project, add the opensearch crate to Cargo.toml: [ dependencies] opensearch = \"1.0.0\" copy Additionally, you may want to add the following serde dependencies that help serialize types to JSON and deserialize JSON responses: serde = \"~1\" serde_json = \"~1\" copy The Rust client uses the higher-level reqwest HTTP client library for HTTP requests, and reqwest uses the tokio platform to support asynchronous requests. If you are planning to use asynchronous functions, you need to add the tokio dependency to Cargo.toml: tokio = { version = \"*\", features = [ \"full\"] } copy See the Sample program section for the complete Cargo.toml file.\nTo use the Rust client API, import the modules, structs, and enums you need: use opensearch:: OpenSearch; copy Connecting to OpenSearch\nTo connect to the default OpenSearch host, create a default client object that connects to OpenSearch at the address http://localhost:9200: let client = OpenSearch:: default (); copy To connect to an OpenSearch host that is running at a different address, create a client with the specified address: let transport = Transport:: single_node ( \"http://localhost:9200\")?; let client = OpenSearch:: new ( transport); copy Alternatively, you can customize the URL and use a connection pool by creating a TransportBuilder struct and passing it to OpenSearch::new to create a new instance of the client: let url = Url:: parse ( \"http://localhost:9200\")?; let conn_pool = SingleNodeConnectionPool:: new ( url); let transport = TransportBuilder:: new ( conn_pool).disable_proxy ().build ()?; let client = OpenSearch:: new ( transport); copy Connecting to Amazon OpenSearch Service\nThe following example illustrates connecting to Amazon OpenSearch Service: let url = Url:: parse ( \"https://...\"); let service_name = \"es\"; let conn_pool = SingleNodeConnectionPool:: new ( url?); let region_provider = RegionProviderChain:: default_provider ().or_else ( \"us-east-1\"); let aws_config = aws_config:: from_env ().region ( region_provider).load ().await.clone (); let transport = TransportBuilder:: new ( conn_pool).auth ( aws_config.clone ().try_into ()?).service_name ( service_name).build ()?; let client = OpenSearch:: new ( transport); copy Connecting to Amazon OpenSearch Serverless\nThe following example illustrates connecting to Amazon OpenSearch Serverless Service: let url = Url:: parse ( \"https://...\"); let service_name = \"aoss\"; let conn_pool = SingleNodeConnectionPool:: new ( url?); let region_provider = RegionProviderChain:: default_provider ().or_else ( \"us-east-1\"); let aws_config = aws_config:: from_env ().region ( region_provider).load ().await.clone (); let transport = TransportBuilder:: new ( conn_pool).auth ( aws_config.clone ().try_into ()?).service_name ( service_name).build ()?; let client = OpenSearch:: new ( transport); copy Creating an index\nTo create an OpenSearch index, use the create function of the opensearch::indices::Indices struct. You can use the following code to construct a JSON object with custom mappings: let response = client.indices ().create ( IndicesCreateParts:: Index ( \"movies\")).body ( json! ({ \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\" } } } })).send ().await?; copy Indexing a document\nYou can index a document into OpenSearch using the client’s index function: let response = client.index ( IndexParts:: IndexId ( \"movies\", \"1\")).body ( json! ({ \"id\": 1, \"title\": \"Moneyball\", \"director\": \"Bennett Miller\", \"year\": \"2011\" })).send ().await?; copy Performing bulk operations\nYou can perform several operations at the same time by using the client’s bulk function. First, create the JSON body of a Bulk API call, and then pass it to the bulk function: let mut body: Vec &lt; JsonBody &lt; _ &gt;&gt; = Vec:: with_capacity ( 4); // add the first operation and document body.push ( json! ({ \"index\": { \"_id\": \"2\" }}).into ()); body.push ( json! ({ \"id\": 2, \"title\": \"Interstellar\", \"director\": \"Christopher Nolan\", \"year\": \"2014\" }).into ()); // add the second operation and document body.push ( json! ({ \"index\": { \"_id\": \"3\" }}).into ()); body.push ( json! ({ \"id\": 3, \"title\": \"Star Trek Beyond\", \"director\": \"Justin Lin\", \"year\": \"2015\" }).into ()); let response = client.bulk ( BulkParts:: Index ( \"movies\")).body ( body).send ().await?; copy Searching for documents\nThe easiest way to search for documents is to construct a query string. The following code uses a multi_match query to search for “miller” in the title and director fields. It boosts the documents where “miller” appears in the title field: response = client.search ( SearchParts:: Index ( &amp; [ \"movies\"])).from ( 0).size ( 10).body ( json! ({ \"query\": { \"multi_match\": { \"query\": \"miller\", \"fields\": [ \"title^2\", \"director\"] } } })).send ().await?; copy You can then read the response body as JSON and iterate over the hits array to read all the _source documents: let response_body = response.json:: &lt; Value &gt; ().await?; for hit in response_body [ \"hits\"][ \"hits\"].as_array ().unwrap () { // print the source document println! ( \"{}\", serde_json:: to_string_pretty ( &amp; hit [ \"_source\"]).unwrap ()); } copy Deleting a document\nYou can delete a document using the client’s delete function: let response = client.delete ( DeleteParts:: IndexId ( \"movies\", \"2\")).send ().await?; copy Deleting an index\nYou can delete an index using the delete function of the opensearch::indices::Indices struct: let response = client.indices ().delete ( IndicesDeleteParts:: Index ( &amp; [ \"movies\"])).send ().await?; copy Sample program\nThe sample program uses the following Cargo.toml file with all dependencies described in the Setup section: [ package] name = \"os_rust_project\" version = \"0.1.0\" edition = \"2021\" # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html [ dependencies] opensearch = \"1.0.0\" tokio = { version = \"*\", features = [ \"full\"] } serde = \"~1\" serde_json = \"~1\" copy The following sample program creates a client, adds an index with non-default mappings, inserts a document, performs bulk operations, searches for the document, deletes the document, and then deletes the index: use opensearch::{ DeleteParts, OpenSearch, IndexParts, http:: request:: JsonBody, BulkParts, SearchParts }; use opensearch::{ indices::{ IndicesDeleteParts, IndicesCreateParts }}; use serde_json::{ json, Value }; #[tokio::main] async fn main () -&gt; Result &lt; (), Box &lt; dyn std:: error:: Error &gt;&gt; { let client = OpenSearch:: default (); // Create an index let mut response = client.indices ().create ( IndicesCreateParts:: Index ( \"movies\")).body ( json! ({ \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\" } } } })).send ().await?; let mut successful = response.status_code ().is_success (); if successful { println! ( \"Successfully created an index\"); } else { println! ( \"Could not create an index\"); } // Index a single document println! ( \"Indexing a single document...\"); response = client.index ( IndexParts:: IndexId ( \"movies\", \"1\")).body ( json! ({ \"id\": 1, \"title\": \"Moneyball\", \"director\": \"Bennett Miller\", \"year\": \"2011\" })).send ().await?; successful = response.status_code ().is_success (); if successful { println! ( \"Successfully indexed a document\"); } else { println! ( \"Could not index document\"); } // Index multiple documents using the bulk operation println! ( \"Indexing multiple documents...\"); let mut body: Vec &lt; JsonBody &lt; _ &gt;&gt; = Vec:: with_capacity ( 4); // add the first operation and document body.push ( json! ({ \"index\": { \"_id\": \"2\" }}).into ()); body.push ( json! ({ \"id\": 2, \"title\": \"Interstellar\", \"director\": \"Christopher Nolan\", \"year\": \"2014\" }).into ()); // add the second operation and document body.push ( json! ({ \"index\": { \"_id\": \"3\" }}).into ()); body.push ( json! ({ \"id\": 3, \"title\": \"Star Trek Beyond\", \"director\": \"Justin Lin\", \"year\": \"2015\" }).into ()); response = client.bulk ( BulkParts:: Index ( \"movies\")).body ( body).send ().await?; let mut response_body = response.json:: &lt; Value &gt; ().await?; successful = response_body [ \"errors\"].as_bool ().unwrap () == false; if successful { println! ( \"Successfully performed bulk operations\"); } else { println! ( \"Could not perform bulk operations\"); } // Search for a document println! ( \"Searching for a document...\"); response = client.search ( SearchParts:: Index ( &amp; [ \"movies\"])).from ( 0).size ( 10).body ( json! ({ \"query\": { \"multi_match\": { \"query\": \"miller\", \"fields\": [ \"title^2\", \"director\"] } } })).send ().await?; response_body = response.json:: &lt; Value &gt; ().await?; for hit in response_body [ \"hits\"][ \"hits\"].as_array ().unwrap () { // print the source document println! ( \"{}\", serde_json:: to_string_pretty ( &amp; hit [ \"_source\"]).unwrap ()); } // Delete a document response = client.delete ( DeleteParts:: IndexId ( \"movies\", \"2\")).send ().await?; successful = response.status_code ().is_success (); if successful { println! ( \"Successfully deleted a document\"); } else { println! ( \"Could not delete document\"); } // Delete the index response = client.indices ().delete ( IndicesDeleteParts:: Index ( &amp; [ \"movies\"])).send ().await?; successful = response.status_code ().is_success (); if successful { println! ( \"Successfully deleted the index\"); } else { println! ( \"Could not delete the index\"); } Ok (()) } copy",
    "ancestors": [
      "Clients"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/common-use-cases/common-use-cases/",
    "title": "Common use cases",
    "content": "You can use Data Prepper for several different purposes, including trace analytics, log analytics, Amazon S3 log analytics, and metrics ingestion.",
    "ancestors": [
      "Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/common-use-cases/log-analytics/",
    "title": "Log analytics",
    "content": "Data Prepper is an extendable, configurable, and scalable solution for log ingestion into OpenSearch and Amazon OpenSearch Service. Data Prepper supports receiving logs from Fluent Bit through the HTTP Source and processing those logs with a Grok Processor before ingesting them into OpenSearch through the OpenSearch sink.\nThe following image shows all of the components used for log analytics with Fluent Bit, Data Prepper, and OpenSearch. In the application environment, run Fluent Bit. Fluent Bit can be containerized through Kubernetes, Docker, or Amazon Elastic Container Service (Amazon ECS). You can also run Fluent Bit as an agent on Amazon Elastic Compute Cloud (Amazon EC2). Configure the Fluent Bit http output plugin to export log data to Data Prepper. Then deploy Data Prepper as an intermediate component and configure it to send the enriched log data to your OpenSearch cluster. From there, use OpenSearch Dashboards to perform more intensive visualization and analysis.\nLog analytics pipeline\nLog analytics pipelines in Data Prepper are extremely customizable. The following image shows a simple pipeline. HTTP source\nThe HTTP Source accepts log data from Fluent Bit. This source accepts log data in a JSON array format and supports industry-standard encryption in the form of TLS/HTTPS and HTTP basic authentication.\nProcessor\nData Prepper 1.2 and above come with a Grok Processor. The Grok Processor is an invaluable tool for structuring and extracting important fields from your logs, making them more queryable.\nThe Grok Processor comes with a wide variety of default patterns that match common log formats like Apache logs or syslogs, but it can easily accept any custom patterns that cater to your specific log format.\nFor more information about Grok features, see the documentation.\nSink\nThere is a generic sink that writes data to OpenSearch as the destination. The OpenSearch sink has configuration options related to an OpenSearch cluster, like endpoint, SSL/username, index name, index template, and index state management.\nPipeline configuration\nThe following sections discuss pipeline configuration.\nExample pipeline with SSL and basic authentication enabled\nThis example pipeline configuration comes with SSL and basic authentication enabled for the http-source: log-pipeline: source: http: ssl_certificate_file: \" /full/path/to/certfile.crt\" ssl_key_file: \" /full/path/to/keyfile.key\" authentication: http_basic: username: \" myuser\" password: \" mys3cret\" processor: - grok: match: # This will match logs with a \"log\" key against the COMMONAPACHELOG pattern (ex: { \"log\": \"actual apache log...\" }) # You should change this to match what your logs look like. See the grok documenation to get started. log: [ \" %{COMMONAPACHELOG}\"] sink: - opensearch: hosts: [ \" https://localhost:9200\"] # Change to your credentials username: \" admin\" password: \" admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 # Since we are grok matching for apache logs, it makes sense to send them to an OpenSearch index named apache_logs. # You should change this to correspond with how your OpenSearch indexes are set up. index: apache_logs This pipeline configuration is an example of Apache log ingestion. Don’t forget that you can easily configure the Grok Processor for your own custom logs. You will need to modify the configuration for your OpenSearch cluster.\nThe following are the main changes you need to make: hosts – Set to your hosts. index – Change this to the OpenSearch index to which you want to send logs. username – Provide your OpenSearch username. password – Provide your OpenSearch password. aws_sigv4 – If you use Amazon OpenSearch Service with AWS signing, set this to true. It will sign requests with the default AWS credentials provider. aws_region – If you use Amazon OpenSearch Service with AWS signing, set this value to the AWS Region in which your cluster is hosted.\nFluent Bit\nYou will need to run Fluent Bit in your service environment. See Getting Started with Fluent Bit for installation instructions. Ensure that you can configure the Fluent Bit http output plugin to your Data Prepper HTTP source. The following is an example fluent-bit.conf that tails a log file named test.log and forwards it to a locally running Data Prepper HTTP source, which runs by default on port 2021.\nNote that you should adjust the file path, output Host, and Port according to how and where you have Fluent Bit and Data Prepper running.\nExample: Fluent Bit file without SSL and basic authentication enabled\nThe following is an example fluent-bit.conf file without SSL and basic authentication enabled on the HTTP source: [INPUT]\nname tail\nrefresh_interval 5\npath test.log\nread_from_head true\n[OUTPUT]\nName http\nMatch *\nHost localhost\nPort 2021\nURI /log/ingest\nFormat json If your HTTP source has SSL and basic authentication enabled, you will need to add the details of http_User, http_Passwd, tls.crt_file, and tls.key_file to the fluent-bit.conf file, as shown in the following example.\nExample: Fluent Bit file with SSL and basic authentication enabled\nThe following is an example fluent-bit.conf file with SSL and basic authentication enabled on the HTTP source: [INPUT]\nname tail\nrefresh_interval 5\npath test.log\nread_from_head true\n[OUTPUT]\nName http\nMatch *\nHost localhost\nhttp_User myuser\nhttp_Passwd mys3cret\ntls On\ntls.crt_file /full/path/to/certfile.crt\ntls.key_file /full/path/to/keyfile.key\nPort 2021\nURI /log/ingest\nFormat json See the Data Prepper Log Ingestion Demo Guide for a specific example of Apache log ingestion from FluentBit -&gt; Data Prepper -&gt; OpenSearch running through Docker.\nIn the future, Data Prepper will offer additional sources and processors that will make more complex log analytics pipelines available. Check out the Data Prepper Project Roadmap to see what is coming.\nIf there is a specific source, processor, or sink that you would like to include in your log analytics workflow and is not currently on the roadmap, please bring it to our attention by creating a GitHub issue. Additionally, if you are interested in contributing to Data Prepper, see our Contributing Guidelines as well as our developer guide and plugin development guide.",
    "ancestors": [
      "Data Prepper",
      "Common use cases"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/common-use-cases/s3-logs/",
    "title": "S3 logs",
    "content": "Data Prepper allows you to load logs from Amazon Simple Storage Service (Amazon S3), including traditional logs, JSON documents, and CSV logs.\nArchitecture\nData Prepper can read objects from S3 buckets using an Amazon Simple Queue Service (SQS) (Amazon SQS) queue and Amazon S3 Event Notifications.\nData Prepper polls the Amazon SQS queue for S3 event notifications. When Data Prepper receives a notification that an S3 object was created, Data Prepper reads and parses that S3 object.\nThe following diagram shows the overall architecture of the components involved. The flow of data is as follows.\nA system produces logs into the S3 bucket.\nS3 creates an S3 event notification in the SQS queue.\nData Prepper polls Amazon SQS for messages and then receives a message.\nData Prepper downloads the content from the S3 object.\nData Prepper sends a document to OpenSearch for the content in the S3 object.\nPipeline overview\nData Prepper supports reading data from S3 using the s3 source.\nThe following diagram shows a conceptual outline of a Data Prepper pipeline reading from S3. Prerequisites\nBefore Data Prepper can read log data from S3, you need the following prerequisites:\nAn S3 bucket.\nA log producer that writes logs to S3. The exact log producer will vary depending on your specific use case, but could include writing logs to S3 or a service such as Amazon CloudWatch.\nGetting started\nUse the following steps to begin loading logs from S3 with Data Prepper.\nCreate an SQS standard queue for your S3 event notifications.\nConfigure bucket notifications for SQS. Use the s3:ObjectCreated:* event type.\nGrant AWS IAM permissions to Data Prepper for accessing SQS and S3.\n(Recommended) Create an SQS dead-letter queue (DLQ).\n(Recommended) Configure an SQS re-drive policy to move failed messages into the DLQ.\nSetting permissions for Data Prepper\nTo view S3 logs, Data Prepper needs access to Amazon SQS and S3.\nUse the following example to set up permissions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"s3-access\", \"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::&lt;YOUR-BUCKET&gt;/*\" }, { \"Sid\": \"sqs-access\", \"Effect\": \"Allow\", \"Action\": [ \"sqs:DeleteMessage\", \"sqs:ReceiveMessage\"], \"Resource\": \"arn:aws:sqs:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:&lt;YOUR-SQS-QUEUE&gt;\" }, { \"Sid\": \"kms-access\", \"Effect\": \"Allow\", \"Action\": \"kms:Decrypt\", \"Resource\": \"arn:aws:kms:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:key/&lt;YOUR-KMS-KEY&gt;\" }] } If your S3 objects or SQS queues do not use KMS, you can remove the kms:Decrypt permission.\nSQS dead-letter queue\nThe are two options for how to handle errors resulting from processing S3 objects.\nUse an SQS dead-letter queue (DLQ) to track the failure. This is the recommended approach.\nDelete the message from SQS. You must manually find the S3 object and correct the error.\nThe following diagram shows the system architecture when using SQS with DLQ. To use an SQS dead-letter queue, perform the following steps:\nCreate a new SQS standard queue to act as your DLQ.\nConfigure your SQS’s redrive policy to use your DLQ. Consider using a low value such as 2 or 3 for the “Maximum Receives” setting.\nConfigure the Data Prepper s3 source to use retain_messages for on_error. This is the default behavior.\nPipeline design\nCreate a pipeline to read logs from S3, starting with an s3 source plugin. Use the following example for guidance. s3-log-pipeline: source: s3: notification_type: sqs compression: gzip codec: newline: sqs: # Change this value to your SQS Queue URL queue_url: \" arn:aws:sqs:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:&lt;YOUR-SQS-QUEUE&gt;\" visibility_timeout: \" 2m\" Configure the following options according to your use case: queue_url: This the SQS queue URL and is always unique to your pipeline. codec: The codec determines how to parse the incoming data. visibility_timeout: Configure this value to be large enough for Data Prepper to process 10 S3 objects. However, if you make this value too large, messages that fail to process will take at least as long as the specified value before Data Prepper retries.\nThe default values for each option work for the majority of use cases. For all available options for the S3 source, see s3. s3-log-pipeline: source: s3: notification_type: sqs compression: gzip codec: newline: sqs: # Change this value to your SQS Queue URL queue_url: \" arn:aws:sqs:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:&lt;YOUR-SQS-QUEUE&gt;\" visibility_timeout: \" 2m\" aws: # Specify the correct region region: \" &lt;YOUR-REGION&gt;\" # This shows using an STS role, but you can also use your system's default permissions. sts_role_arn: \" arn:aws:iam::&lt;123456789012&gt;:role/&lt;DATA-PREPPER-ROLE&gt;\" processor: # You can configure a grok pattern to enrich your documents in OpenSearch. #- grok: # match: # message: [ \"%{COMMONAPACHELOG}\"] sink: - opensearch: hosts: [ \" https://localhost:9200\"] # Change to your credentials username: \" admin\" password: \" admin\" index: s3_logs Multiple Data Prepper pipelines\nWe recommend that you have one SQS queue per Data Prepper pipeline. In addition, you can have multiple nodes in the same cluster reading from the same SQS queue, which doesn’t require additional configuration with Data Prepper.\nIf you have multiple pipelines, you must create multiple SQS queues for each pipeline, even if both pipelines use the same S3 bucket.",
    "ancestors": [
      "Data Prepper",
      "Common use cases"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/common-use-cases/trace-analytics/",
    "title": "Trace analytics",
    "content": "Trace analytics allows you to collect trace data and customize a pipeline that ingests and transforms the data for use in OpenSearch. The following provides an overview of the trace analytics workflow in Data Prepper, how to configure it, and how to visualize trace data.\nIntroduction\nWhen using Data Prepper as a server-side component to collect trace data, you can customize a Data Prepper pipeline to ingest and transform the data for use in OpenSearch. Upon transformation, you can visualize the transformed trace data for use with the Observability plugin inside of OpenSearch Dashboards. Trace data provides visibility into your application’s performance, and helps you gain more information about individual traces.\nThe following flowchart illustrates the trace analytics workflow, from running OpenTelemetry Collector to using OpenSearch Dashboards for visualization. To monitor trace analytics, you need to set up the following components in your service environment:\nAdd instrumentation to your application so it can generate telemetry data and send it to an OpenTelemetry collector.\nRun an OpenTelemetry collector as a sidecar or daemonset for Amazon Elastic Kubernetes Service (Amazon EKS), a sidecar for Amazon Elastic Container Service (Amazon ECS), or an agent on Amazon Elastic Compute Cloud (Amazon EC2). You should configure the collector to export trace data to Data Prepper.\nDeploy Data Prepper as the ingestion collector for OpenSearch. Configure it to send the enriched trace data to your OpenSearch cluster or to the Amazon OpenSearch Service domain.\nUse OpenSearch Dashboards to visualize and detect problems in your distributed applications.\nTrace analytics pipeline\nTo monitor trace analytics in Data Prepper, we provide three pipelines: entry-pipeline, raw-trace-pipeline, and service-map-pipeline. The following image provides an overview of how the pipelines work together to monitor trace analytics. OpenTelemetry trace source\nThe OpenTelemetry source accepts trace data from the OpenTelemetry Collector. The source follows the OpenTelemetry Protocol and officially supports transport over gRPC and the use of industry-standard encryption (TLS/HTTPS).\nProcessor\nThere are three processors for the trace analytics feature: otel_trace_raw - The otel_trace_raw processor receives a collection of span records from otel-trace-source, and performs stateful processing, extraction, and completion of trace-group-related fields. otel_trace_group - The otel_trace_group processor fills in the missing trace-group-related fields in the collection of span records by looking up the OpenSearch backend. service_map_stateful – The service_map_stateful processor performs the required preprocessing for trace data and builds metadata to display the service-map dashboards.\nOpenSearch sink\nOpenSearch provides a generic sink that writes data to OpenSearch as the destination. The OpenSearch sink has configuration options related to the OpenSearch cluster, such as endpoint, SSL, username/password, index name, index template, and index state management.\nThe sink provides specific configurations for the trace analytics feature. These configurations allow the sink to use indexes and index templates specific to trace analytics. The following OpenSearch indexes are specific to trace analytics: otel-v1-apm-span – The otel-v1-apm-span index stores the output from the otel_trace_raw processor. otel-v1-apm-service-map – The otel-v1-apm-service-map index stores the output from the service_map_stateful processor.\nTrace tuning\nStarting with version 0.8.x, Data Prepper supports both vertical and horizontal scaling for trace analytics. You can adjust the size of a single Data Prepper instance to meet your workload’s demands and scale vertically.\nYou can scale horizontally by using the core peer forwarder to deploy multiple Data Prepper instances to form a cluster. This enables Data Prepper instances to communicate with instances in the cluster and is required for horizontally scaling deployments.\nScaling recommendations\nUse the following recommended configurations to scale Data Prepper. We recommend that you modify parameters based on the requirements. We also recommend that you monitor the Data Prepper host metrics and OpenSearch metrics to ensure that the configuration works as expected.\nBuffer\nThe total number of trace requests processed by Data Prepper is equal to the sum of the buffer_size values in otel-trace-pipeline and raw-pipeline. The total number of trace requests sent to OpenSearch is equal to the product of batch_size and workers in raw-trace-pipeline. For more information about raw-pipeline, see Trace analytics pipeline.\nWe recommend the following when making changes to buffer settings:\nThe buffer_size value in otel-trace-pipeline and raw-pipeline should be the same.\nThe buffer_size should be greater than or equal to workers * batch_size in the raw-pipeline.\nWorkers\nThe workers setting determines the number of threads that are used by Data Prepper to process requests from the buffer. We recommend that you set workers based on the CPU utilization. This value can be higher than the number of available processors because Data Prepper uses significant input/output time when sending data to OpenSearch.\nHeap\nConfigure the Data Prepper heap by setting the JVM_OPTS environment variable. We recommend that you set the heap value to a minimum value of 4 * batch_size * otel_send_batch_size * maximum size of indvidual span.\nAs mentioned in the OpenTelemetry Collector section, set otel_send_batch_size to a value of 50 in your OpenTelemetry Collector configuration.\nLocal disk\nData Prepper uses the local disk to store metadata required for service map processing, so we recommend storing only the following key fields: traceId, spanId, parentSpanId, spanKind, spanName, and serviceName. The service-map plugin stores only two files, each of which stores window_duration seconds of data. As an example, testing with a throughput of 3000 spans/second resulted in the total disk usage of 4 MB.\nData Prepper also uses the local disk to write logs. In the most recent version of Data Prepper, you can redirect the logs to your preferred path.\nAWS CloudFormation template and Kubernetes/Amazon EKS configuration files\nThe AWS CloudFormation template provides a user-friendly mechanism for configuring the scaling attributes described in the Trace tuning section.\nThe Kubernetes configuration files and Amazon EKS configuration files are available for configuring these attributes in a cluster deployment.\nBenchmark tests\nThe benchmark tests were performed on an r5.xlarge EC2 instance with the following configuration: buffer_size: 4096 batch_size: 256 workers: 8 Heap: 10 GB\nThis setup was able to handle a throughput of 2100 spans/second at 20 percent CPU utilization.\nPipeline configuration\nThe following sections provide examples of different types of pipelines and how to configure each type.\nExample: Trace analytics pipeline\nThe following example demonstrates how to build a pipeline that supports the OpenSearch Dashboards Observability plugin. This pipeline takes data from the OpenTelemetry Collector and uses two other pipelines as sinks. These two separate pipelines serve two different purposes and write to different OpenSearch indexes. The first pipeline prepares trace data for OpenSearch and enriches and ingests the span documents into a span index within OpenSearch. The second pipeline aggregates traces into a service map and writes service map documents into a service map index within OpenSearch.\nStarting with Data Prepper version 2.0, Data Prepper no longer supports the otel_trace_raw_prepper processor. The otel_trace_raw processor replaces the otel_trace_raw_prepper processor and supports some of Data Prepper’s recent data model changes. Instead, you should use the otel_trace_raw processor. See the following YAML file example: entry-pipeline: delay: \" 100\" source: otel_trace_source: ssl: false buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 sink: - pipeline: name: \" raw-trace-pipeline\" - pipeline: name: \" service-map-pipeline\" raw-pipeline: source: pipeline: name: \" entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - otel_trace_raw: sink: - opensearch: hosts: [ \" https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-raw service-map-pipeline: delay: \" 100\" source: pipeline: name: \" entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - service_map_stateful: sink: - opensearch: hosts: [ \" https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-service-map To maintain similar ingestion throughput and latency, scale the buffer_size and batch_size by the estimated maximum batch size in the client request payload. {:.tip}\nExample: otel trace The following is an example otel-trace-source.yaml file with SSL and basic authentication enabled. Note that you will need to modify your otel-collector-config.yaml file so that it uses your own credentials. source: otel_trace_source: #record_type: event # Add this when using Data Prepper 1.x. This option is removed in 2.0 ssl: true sslKeyCertChainFile: \" /full/path/to/certfile.crt\" sslKeyFile: \" /full/path/to/keyfile.key\" authentication: http_basic: username: \" my-user\" password: \" my_s3cr3t\" Example: pipeline.yaml\nThe following is an example pipeline.yaml file without SSL and basic authentication enabled for the otel-trace-pipeline pipeline: otel-trace-pipeline: # workers is the number of threads processing data in each pipeline. # We recommend same value for all pipelines. # default value is 1, set a value based on the machine you are running Data Prepper workers: 8 # delay in milliseconds is how often the worker threads should process data. # Recommend not to change this config as we want the entry-pipeline to process as quick as possible # default value is 3_000 ms delay: \" 100\" source: otel_trace_source: #record_type: event # Add this when using Data Prepper 1.x. This option is removed in 2.0 ssl: false # Change this to enable encryption in transit authentication: unauthenticated: buffer: bounded_blocking: # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory. # We recommend to keep the same buffer_size for all pipelines. # Make sure you configure sufficient heap # default value is 512 buffer_size: 512 # This is the maximum number of request each worker thread will process within the delay. # Default is 8. # Make sure buffer_size &gt;= workers * batch_size batch_size: 8 sink: - pipeline: name: \" raw-trace-pipeline\" - pipeline: name: \" entry-pipeline\" raw-pipeline: # Configure same as the otel-trace-pipeline workers: 8 # We recommend using the default value for the raw-pipeline. delay: \" 3000\" source: pipeline: name: \" entry-pipeline\" buffer: bounded_blocking: # Configure the same value as in entry-pipeline # Make sure you configure sufficient heap # The default value is 512 buffer_size: 512 # The raw processor does bulk request to your OpenSearch sink, so configure the batch_size higher. # If you use the recommended otel-collector setup each ExportTraceRequest could contain max 50 spans. https://github.com/opensearch-project/data-prepper/tree/v0.7.x/deployment/aws # With 64 as batch size each worker thread could process upto 3200 spans (64 * 50) batch_size: 64 processor: - otel_trace_raw: - otel_trace_group: hosts: [ \" https://localhost:9200\"] # Change to your credentials username: \" admin\" password: \" admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 sink: - opensearch: hosts: [ \" https://localhost:9200\"] index_type: trace-analytics-raw # Change to your credentials username: \" admin\" password: \" admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 service-map-pipeline: workers: 8 delay: \" 100\" source: pipeline: name: \" entry-pipeline\" processor: - service_map_stateful: # The window duration is the maximum length of time the data prepper stores the most recent trace data to evaluvate service-map relationships. # The default is 3 minutes, this means we can detect relationships between services from spans reported in last 3 minutes. # Set higher value if your applications have higher latency. window_duration: 180 buffer: bounded_blocking: # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory. # We recommend to keep the same buffer_size for all pipelines. # Make sure you configure sufficient heap # default value is 512 buffer_size: 512 # This is the maximum number of request each worker thread will process within the delay. # Default is 8. # Make sure buffer_size &gt;= workers * batch_size batch_size: 8 sink: - opensearch: hosts: [ \" https://localhost:9200\"] index_type: trace-analytics-service-map # Change to your credentials username: \" admin\" password: \" admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 You need to modify the preceding configuration for your OpenSearch cluster so that the configuration matches your environment. Note that it has two opensearch sinks that need to be modified.\nYou must make the following changes: hosts – Set to your hosts. username – Provide your OpenSearch username. password – Provide your OpenSearch password. aws_sigv4 – If you are using Amazon OpenSearch Service with AWS signing, set this value to true. It will sign requests with the default AWS credentials provider. aws_region – If you are using Amazon OpenSearch Service with AWS signing, set this value to your AWS Region.\nFor other configurations available for OpenSearch sinks, see Data Prepper OpenSearch sink.\nOpenTelemetry Collector\nYou need to run OpenTelemetry Collector in your service environment. Follow Getting Started to install an OpenTelemetry collector. Ensure that you configure the collector with an exporter configured for your Data Prepper instance. The following example otel-collector-config.yaml file receives data from various instrumentations and exports it to Data Prepper.\nExample otel-collector-config.yaml file\nThe following is an example otel-collector-config.yaml file: receivers:\njaeger:\nprotocols:\ngrpc:\notlp:\nprotocols:\ngrpc:\nzipkin:\nprocessors:\nbatch/traces:\ntimeout: 1s\nsend_batch_size: 50\nexporters:\notlp/data-prepper:\nendpoint: localhost:21890\ntls:\ninsecure: true\nservice:\npipelines:\ntraces:\nreceivers: [jaeger, otlp, zipkin]\nprocessors: [batch/traces]\nexporters: [otlp/data-prepper] After you run OpenTelemetry in your service environment, you must configure your application to use the OpenTelemetry Collector. The OpenTelemetry Collector typically runs alongside your application.\nNext steps and more information\nThe OpenSearch Dashboards Observability plugin documentation provides additional information about configuring OpenSearch to view trace analytics in OpenSearch Dashboards.\nFor more information about how to tune and scale Data Prepper for trace analytics, see Trace tuning.\nMigrating to Data Prepper 2.0\nStarting with Data Prepper version 1.4, trace processing uses Data Prepper’s event model. This allows pipeline authors to configure other processors to modify spans or traces. To provide a migration path, Data Prepper version 1.4 introduced the following changes: otel_trace_source has an optional record_type parameter that can be set to event. When configured, it will output event objects. otel_trace_raw replaces otel_trace_raw_prepper for event-based spans. otel_trace_group replaces otel_trace_group_prepper for event-based spans.\nIn Data Prepper version 2.0, otel_trace_source will only output events. Data Prepper version 2.0 also removes otel_trace_raw_prepper and otel_trace_group_prepper entirely. To migrate to Data Prepper version 2.0, you can configure your trace pipeline using the event model.",
    "ancestors": [
      "Data Prepper",
      "Common use cases"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/getting-started/",
    "title": "Getting started",
    "content": "Data Prepper is an independent component, not an OpenSearch plugin, that converts data for use with OpenSearch. It’s not bundled with the all-in-one OpenSearch installation packages.\nIf you are migrating from Open Distro Data Prepper, see Migrating from Open Distro.\n1. Installing Data Prepper\nThere are two ways to install Data Prepper: you can run the Docker image or build from source.\nThe easiest way to use Data Prepper is by running the Docker image. We suggest that you use this approach if you have Docker available. Run the following command: docker pull opensearchproject/data-prepper:latest copy If you have special requirements that require you to build from source, or if you want to contribute, see the Developer Guide.\n2. Configuring Data Prepper\nTwo configuration files are required to run a Data Prepper instance. Optionally, you can configure a Log4j 2 configuration file. See Configuring Log4j for more information. The following list describes the purpose of each configuration file: pipelines.yaml: This file describes which data pipelines to run, including sources, processors, and sinks. data-prepper-config.yaml: This file contains Data Prepper server settings that allow you to interact with exposed Data Prepper server APIs. log4j2-rolling.properties (optional): This file contains Log4j 2 configuration options and can be a JSON, YAML, XML, or.properties file type.\nFor Data Prepper versions earlier than 2.0, the.jar file expects the pipeline configuration file path to be followed by the server configuration file path. See the following configuration path example: java -jar data-prepper-core-$VERSION.jar pipelines.yaml data-prepper-config.yaml Optionally, you can add \"-Dlog4j.configurationFile=config/log4j2.properties\" to the command to pass a custom Log4j 2 configuration file. If you don’t provide a properties file, Data Prepper defaults to the log4j2.properties file in the shared-config directory.\nStarting with Data Prepper 2.0, you can launch Data Prepper by using the following data-prepper script that does not require any additional command line arguments: bin/data-prepper Configuration files are read from specific subdirectories in the application’s home directory: pipelines/: Used for pipeline configurations. Pipeline configurations can be written in one or more YAML files. config/data-prepper-config.yaml: Used for the Data Prepper server configuration.\nYou can supply your own pipeline configuration file path followed by the server configuration file path. However, this method will not be supported in a future release. See the following example: bin/data-prepper pipelines.yaml data-prepper-config.yaml The Log4j 2 configuration file is read from the config/log4j2.properties file located in the application’s home directory.\nTo configure Data Prepper, see the following information for each use case: Trace analytics: Learn how to collect trace data and customize a pipeline that ingests and transforms that data. Log analytics: Learn how to set up Data Prepper for log observability.\n3. Defining a pipeline\nCreate a Data Prepper pipeline file named pipelines.yaml using the following configuration: simple-sample-pipeline: workers: 2 delay: \" 5000\" source: random: sink: - stdout: copy 4. Running Data Prepper\nRun the following command with your pipeline configuration YAML. docker run --name data-prepper \\ -v / ${ PWD } /pipelines.yaml:/usr/share/data-prepper/pipelines/pipelines.yaml \\ opensearchproject/data-prepper:latest copy The example pipeline configuration above demonstrates a simple pipeline with a source ( random) sending data to a sink ( stdout). For examples of more advanced pipeline configurations, see Pipelines.\nAfter starting Data Prepper, you should see log output and some UUIDs after a few seconds: 2021-09-30T20:19:44,147 [main] INFO com.amazon.dataprepper.pipeline.server.DataPrepperServer - Data Prepper server running at:4900 2021-09-30T20:19:44,681 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:45,183 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:45,687 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:46,191 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:46,694 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:47,200 [random-source-pool-0] INFO com.amazon.dataprepper.plugins.source.RandomStringSource - Writing to buffer 2021-09-30T20:19:49,181 [simple-test-pipeline-processor-worker-1-thread-1] INFO com.amazon.dataprepper.pipeline.ProcessWorker - simple-test-pipeline Worker: Processing 6 records from buffer 07dc0d37-da2c-447e-a8df-64792095fb72 5ac9b10a-1d21-4306-851a-6fb12f797010 99040c79-e97b-4f1d-a70b-409286f2a671 5319a842-c028-4c17-a613-3ef101bd2bdd e51e700e-5cab-4f6d-879a-1c3235a77d18 b4ed2d7e-cf9c-4e9d-967c-b18e8af35c90 The remainder of this page provides examples for running Data Prepper from the Docker image. If you\nbuilt it from source, refer to the Developer Guide for more information.\nHowever you configure your pipeline, you’ll run Data Prepper the same way. You run the Docker\nimage and modify both the pipelines.yaml and data-prepper-config.yaml files.\nFor Data Prepper 2.0 or later, use this command: docker run --name data-prepper -p 4900:4900 -v ${PWD}/pipelines.yaml:/usr/share/data-prepper/pipelines/pipelines.yaml -v ${PWD}/data-prepper-config.yaml:/usr/share/data-prepper/config/data-prepper-config.yaml opensearchproject/data-prepper:latest copy For Data Prepper versions earlier than 2.0, use this command: docker run --name data-prepper -p 4900:4900 -v ${PWD}/pipelines.yaml:/usr/share/data-prepper/pipelines.yaml -v ${PWD}/data-prepper-config.yaml:/usr/share/data-prepper/data-prepper-config.yaml opensearchproject/data-prepper:1.x copy Once Data Prepper is running, it processes data until it is shut down. Once you are done, shut it down with the following command: POST /shutdown copy Additional configurations\nFor Data Prepper 2.0 or later, the Log4j 2 configuration file is read from config/log4j2.properties in the application’s home directory. By default, it uses log4j2-rolling.properties in the shared-config directory.\nFor Data Prepper 1.5 or earlier, optionally add \"-Dlog4j.configurationFile=config/log4j2.properties\" to the command if you want to pass a custom log4j2 properties file. If no properties file is provided, Data Prepper defaults to the log4j2.properties file in the shared-config directory.\nNext steps\nTrace analytics is an important Data Prepper use case. If you haven’t yet configured it, see Trace analytics.\nLog ingestion is also an important Data Prepper use case. To learn more, see Log analytics.\nTo learn how to run Data Prepper with a Logstash configuration, see Migrating from Logstash.\nFor information on how to monitor Data Prepper, see Monitoring.\nMore examples\nFor more examples of Data Prepper, see examples in the Data Prepper repo.",
    "ancestors": [
      "Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/index/",
    "title": "Data Prepper",
    "content": "Data Prepper is a server-side data collector capable of filtering, enriching, transforming, normalizing, and aggregating data for downstream analytics and visualization.\nData Prepper lets users build custom pipelines to improve the operational view of applications. Two common uses for Data Prepper are trace and log analytics. Trace analytics can help you visualize the flow of events and identify performance problems, and log analytics can improve searching, analyzing and provide insights into your application.\nConcepts\nData Prepper includes one or more pipelines that collect and filter data based on the components set within the pipeline. Each component is pluggable, enabling you to use your own custom implementation of each component. These components include the following:\nOne source One or more sinks (Optional) One buffer (Optional) One or more processors A single instance of Data Prepper can have one or more pipelines.\nEach pipeline definition contains two required components: source and sink. If buffers and processors are missing from the Data Prepper pipeline, Data Prepper uses the default buffer and a no-op processor.\nSource\nSource is the input component that defines the mechanism through which a Data Prepper pipeline will consume events. A pipeline can have only one source. The source can consume events either by receiving the events over HTTP or HTTPS or by reading from external endpoints like OTeL Collector for traces and metrics and Amazon Simple Storage Service (Amazon S3). Sources have their own configuration options based on the format of the events (such as string, JSON, Amazon CloudWatch logs, or open telemetry trace). The source component consumes events and writes them to the buffer component.\nBuffer\nThe buffer component acts as the layer between the source and the sink. Buffer can be either in-memory or disk based. The default buffer uses an in-memory queue called bounded_blocking that is bounded by the number of events. If the buffer component is not explicitly mentioned in the pipeline configuration, Data Prepper uses the default bounded_blocking.\nSink\nSink is the output component that defines the destination(s) to which a Data Prepper pipeline publishes events. A sink destination could be a service, such as OpenSearch or Amazon S3, or another Data Prepper pipeline. When using another Data Prepper pipeline as the sink, you can chain multiple pipelines together based on the needs of the data. Sink contains its own configuration options based on the destination type.\nProcessor\nProcessors are units within the Data Prepper pipeline that can filter, transform, and enrich events using your desired format before publishing the record to the sink component. The processor is not defined in the pipeline configuration; the events publish in the format defined in the source component. You can have more than one processor within a pipeline. When using multiple processors, the processors are run in the order they are defined inside the pipeline specification.\nSample pipeline configurations\nTo understand how all pipeline components function within a Data Prepper configuration, see the following examples. Each pipeline configuration uses a yaml file format.\nMinimal component\nThis pipeline configuration reads from the file source and writes to another file in the same path. It uses the default options for the buffer and processor. sample-pipeline: source: file: path: &lt;path/to/input-file&gt; sink: - file: path: &lt;path/to/output-file&gt; All components\nThe following pipeline uses a source that reads string events from the input-file. The source then pushes the data to the buffer, bounded by a max size of 1024. The pipeline is configured to have 4 workers, each of them reading a maximum of 256 events from the buffer for every 100 milliseconds. Each worker runs the string_converter processor and writes the output of the processor to the output-file. sample-pipeline: workers: 4 #Number of workers delay: 100 # in milliseconds, how often the workers should run source: file: path: &lt;path/to/input-file&gt; buffer: bounded_blocking: buffer_size: 1024 # max number of events the buffer will accept batch_size: 256 # max number of events the buffer will drain for each read processor: - string_converter: upper_case: true sink: - file: path: &lt;path/to/output-file&gt; Next steps\nTo get started building your own custom pipelines with Data Prepper, see Getting started.",
    "ancestors": [
      "Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/managing-data-prepper/configuring-data-prepper/",
    "title": "Configuring Data Prepper",
    "content": "This is the reference for Data Prepper configuration files (data-prepper-config.yaml). These are general Data Prepper configurations independent from pipelines.\nData Prepper server options Option Required Type Description ssl\nNo\nBoolean\nIndicates whether TLS should be used for server APIs. Defaults to true.\nkeyStoreFilePath\nNo\nString\nPath to a.jks or.p12 keystore file. Required if ssl is true.\nkeyStorePassword\nNo\nString\nPassword for keystore. Optional, defaults to empty string.\nprivateKeyPassword\nNo\nString\nPassword for private key within keystore. Optional, defaults to empty string.\nserverPort\nNo\nInteger\nPort number to use for server APIs. Defaults to 4900.\nmetricRegistries\nNo\nList\nMetrics registries for publishing the generated metrics. Currently supports Prometheus and CloudWatch. Defaults to Prometheus.\nmetricTags\nNo\nMap\nKey-value pairs as common metric tags to metric registries. The maximum number of pairs is three. Note that serviceName is a reserved tag key with DataPrepper as default tag value. Alternatively, administrators can set this value through the environment variable DATAPREPPER_SERVICE_NAME. If serviceName is defined in metricTags, that value overwrites those set through the above methods.\nauthentication\nNo\nObject\nAuthentication configuration. Valid option is http_basic with username and password properties. If not defined, the server does not perform authentication.\nprocessorShutdownTimeout\nNo\nDuration\nTime given to processors to clear any in-flight data and gracefully shut down. Default is 30s.\nsinkShutdownTimeout\nNo\nDuration\nTime given to sinks to clear any in-flight data and gracefully shut down. Default is 30s.\npeer_forwarder\nNo\nObject\nPeer forwarder configurations. See Peer forwarder options for more details. Peer forwarder options\nThe following section details various configuration options for peer forwarder.\nGeneral options for peer forwarding Option Required Type Description port\nNo\nInteger\nThe peer forwarding server port. Valid options are between 0 and 65535. Defaults is 4994.\nrequest_timeout\nNo\nInteger\nRequest timeout for the peer forwarder HTTP server in milliseconds. Default is 10000.\nserver_thread_count\nNo\nInteger\nNumber of threads used by the peer forwarder server. Default is 200.\nclient_thread_count\nNo\nInteger\nNumber of threads used by the peer forwarder client. Default is 200.\nmax_connection_count\nNo\nInteger\nMaximum number of open connections for the peer forwarder server. Default is 500.\nmax_pending_requests\nNo\nInteger\nMaximum number of allowed tasks in ScheduledThreadPool work queue. Default is 1024.\ndiscovery_mode\nNo\nString\nPeer discovery mode to use. Valid options are local_node, static, dns, or aws_cloud_map. Defaults to local_node, which processes events locally.\nstatic_endpoints\nConditionally\nList\nA list containing endpoints of all Data Prepper instances. Required if discovery_mode is set to static.\ndomain_name\nConditionally\nString\nA single domain name to query DNS against. Typically, used by creating multiple DNS A Records for the same domain. Required if discovery_mode is set to dns.\naws_cloud_map_namespace_name\nConditionally\nString\nCloud Map namespace when using AWS Cloud Map service discovery. Required if discovery_mode is set to aws_cloud_map.\naws_cloud_map_service_name\nConditionally\nString\nCloud Map service name when using AWS Cloud Map service discovery. Required if discovery_mode is set to aws_cloud_map.\naws_cloud_map_query_parameters\nNo\nMap\nKey-value pairs to filter the results based on the custom attributes attached to an instance. Only instances that match all the specified key-value pairs are returned.\nbuffer_size\nNo\nInteger\nMax number of unchecked records the buffer accepts. Number of unchecked records is the sum of the number of records written into the buffer and the num of in-flight records not yet checked by the Checkpointing API. Default is 512.\nbatch_size\nNo\nInteger\nMax number of records the buffer returns on read. Default is 48.\naws_region\nConditionally\nString\nAWS region to use ACM, S3 or AWS Cloud Map. Required if use_acm_certificate_for_ssl is set to true or ssl_certificate_file and ssl_key_file is AWS S3 path or discovery_mode is set to aws_cloud_map.\ndrain_timeout\nNo\nDuration\nWait time for the peer forwarder to complete processing data before shutdown. Default is 10s. TLS/SSL options for peer forwarder Option Required Type Description ssl\nNo\nBoolean\nEnables TLS/SSL. Default is true.\nssl_certificate_file\nConditionally\nString\nSSL certificate chain file path or AWS S3 path. S3 path example s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is true and use_acm_certificate_for_ssl is false. Defaults to config/default_certificate.pem which is the default certificate file. Read more about how the certificate file is generated here.\nssl_key_file\nConditionally\nString\nSSL key file path or AWS S3 path. S3 path example s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is true and use_acm_certificate_for_ssl is false. Defaults to config/default_private_key.pem which is the default private key file. Read more about how the default private key file is generated here.\nssl_insecure_disable_verification\nNo\nBoolean\nDisables the verification of server’s TLS certificate chain. Default is false.\nssl_fingerprint_verification_only\nNo\nBoolean\nDisables the verification of server’s TLS certificate chain and instead verifies only the certificate fingerprint. Default is false.\nuse_acm_certificate_for_ssl\nNo\nBoolean\nEnables TLS/SSL using certificate and private key from AWS Certificate Manager (ACM). Default is false.\nacm_certificate_arn\nConditionally\nString\nACM certificate ARN. The ACM certificate takes preference over S3 or a local file system certificate. Required if use_acm_certificate_for_ssl is set to true.\nacm_private_key_password\nNo\nString\nACM private key password that decrypts the private key. If not provided, Data Prepper generates a random password.\nacm_certificate_timeout_millis\nNo\nInteger\nTimeout in milliseconds for ACM to get certificates. Default is 120000.\naws_region\nConditionally\nString\nAWS region to use ACM, S3 or AWS Cloud Map. Required if use_acm_certificate_for_ssl is set to true or ssl_certificate_file and ssl_key_file is AWS S3 path or discovery_mode is set to aws_cloud_map. Authentication options for peer forwarder Option Required Type Description authentication\nNo\nMap\nAuthentication method to use. Valid options are mutual_tls (use mTLS) or unauthenticated (no authentication). Default is unauthenticated.",
    "ancestors": [
      "Data Prepper",
      "Managing Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/managing-data-prepper/configuring-log4j/",
    "title": "Configuring Log4j",
    "content": "You can configure logging using Log4j in Data Prepper.\nLogging\nData Prepper uses SLF4J with a Log4j 2 binding.\nFor Data Prepper versions 2.0 and later, the Log4j 2 configuration file can be found and edited in config/log4j2.properties in the application’s home directory. The default properties for Log4j 2 can be found in log4j2-rolling.properties in the shared-config directory.\nFor Data Prepper versions before 2.0, the Log4j 2 configuration file can be overridden by setting the log4j.configurationFile system property when running Data Prepper. The default properties for Log4j 2 can be found in log4j2.properties in the shared-config directory.\nExample\nWhen running Data Prepper, the following command can be overridden by setting the system property -Dlog4j.configurationFile={property_value}, where {property_value} is a path to the Log4j 2 configuration file: java \"-Dlog4j.configurationFile=config/custom-log4j2.properties\" -jar data-prepper-core-$VERSION.jar pipelines.yaml data-prepper-config.yaml See the Log4j 2 configuration documentation for more information about Log4j 2 configuration.",
    "ancestors": [
      "Data Prepper",
      "Managing Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/managing-data-prepper/core-apis/",
    "title": "Core APIs",
    "content": "All Data Prepper instances expose a server with some control APIs. By default, this server runs on port 4900. Some plugins, especially source plugins, may expose other servers that run on different ports. Configurations for these plugins are independent of the core API. For example, to shut down Data Prepper, you can run the following curl request: curl -X POST http://localhost:4900/shutdown APIs\nThe following table lists the available APIs. Name Description GET /list POST /list Returns a list of running pipelines. POST /shutdown Starts a graceful shutdown of Data Prepper. GET /metrics/prometheus POST /metrics/prometheus Returns a scrape of Data Prepper metrics in Prometheus text format. This API is available as a metricsRegistries parameter in the Data Prepper configuration file data-prepper-config.yaml and contains Prometheus as part of the registry. GET /metrics/sys POST /metrics/sys Returns JVM metrics in Prometheus text format. This API is available as a metricsRegistries parameter in the Data Prepper configuration file data-prepper-config.yaml and contains Prometheus as part of the registry. Configuring the server\nYou can configure your Data Prepper core APIs through the data-prepper-config.yaml file.\nSSL/TLS connection\nMany of the getting started guides for this project disable SSL on the endpoint: ssl: false To enable SSL on your Data Prepper endpoint, configure your data-prepper-config.yaml file with the following options: ssl: true keyStoreFilePath: \" /usr/share/data-prepper/keystore.p12\" keyStorePassword: \" secret\" privateKeyPassword: \" secret\" For more information about configuring your Data Prepper server with SSL, see Server Configuration. If you are using a self-signed certificate, you can add the -k flag to the request to quickly test core APIs with SSL. Use the following shutdown request to test core APIs with SSL: curl -k -X POST https://localhost:4900/shutdown Authentication\nThe Data Prepper core APIs support HTTP basic authentication. You can set the username and password with the following configuration in the data-prepper-config.yaml file: authentication: http_basic: username: \" myuser\" password: \" mys3cr3t\" You can disable authentication of core endpoints using the following configuration. Use this with caution because the shutdown API and others will be accessible to anybody with network access to your Data Prepper instance. authentication: unauthenticated: Peer Forwarder\nPeer Forwarder can be configured to enable stateful aggregation across multiple Data Prepper nodes. For more information about configuring Peer Forwarder, see Peer forwarder. It is supported by the service_map_stateful, otel_trace_raw, and aggregate processors.\nShutdown timeouts\nWhen you run the Data Prepper shutdown API, the process gracefully shuts down and clears any remaining data for both the ExecutorService sink and ExecutorService processor. The default timeout for shutdown of both processes is 10 seconds. You can configure the timeout with the following optional data-prepper-config.yaml file parameters: processorShutdownTimeout: \" PT15M\" sinkShutdownTimeout: 30s The values for these parameters are parsed into a Duration object through the Data Prepper Duration Deserializer.",
    "ancestors": [
      "Data Prepper",
      "Managing Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/managing-data-prepper/managing-data-prepper/",
    "title": "Managing Data Prepper",
    "content": "You can perform administrator functions for Data Prepper, including system configuration, interacting with core APIs, Log4j configuration, and monitoring. You can set up peer forwarding to coordinate multiple Data Prepper nodes when using stateful aggregation.",
    "ancestors": [
      "Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/managing-data-prepper/monitoring/",
    "title": "Monitoring",
    "content": "You can monitor Data Prepper with metrics using Micrometer. There are two types of metrics: JVM/system metrics and plugin metrics. Prometheus is used as the default metrics backend.\nJVM and system metrics\nJVM and system metrics are runtime metrics that are used to monitor Data Prepper instances. They include metrics for classloaders, memory, garbage collection, threads, and others. For more information, see JVM and system metrics.\nNaming\nJVM and system metrics follow predefined names in Micrometer. For example, the Micrometer metrics name for memory usage is jvm.memory.used. Micrometer changes the name to match the metrics system. Following the same example, jvm.memory.used is reported to Prometheus as jvm_memory_used, and is reported to Amazon CloudWatch as jvm.memory.used.value.\nServing\nBy default, metrics are served from the /metrics/sys endpoint on the Data Prepper server in Prometheus scrape format. You can configure Prometheus to scrape from the Data Prepper URL. Prometheus then polls Data Prepper for metrics and stores them in its database. To visualize the data, you can set up any frontend that accepts Prometheus metrics, such as Grafana. You can update the configuration to serve metrics to other registries like Amazon CloudWatch, which does not require or host the endpoint but publishes the metrics directly to CloudWatch.\nPlugin metrics\nPlugins report their own metrics. Data Prepper uses a naming convention to help with consistency in the metrics. Plugin metrics do not use dimensions.\nAbstractBuffer\nCounter recordsWritten: The number of records written into a buffer recordsRead: The number of records read from a buffer recordsProcessed: The number of records read from a buffer and marked as processed writeTimeouts: The count of write timeouts in a buffer\nGaugefir recordsInBuffer: The number of records in a buffer recordsInFlight: The number of records read from a buffer and being processed by data-prepper downstreams (for example, processor, sink)\nTimer readTimeElapsed: The time elapsed while reading from a buffer checkpointTimeElapsed: The time elapsed while checkpointing\nAbstractProcessor\nCounter recordsIn: The number of records ingressed into a processor recordsOut: The number of records egressed from a processor\nTimer timeElapsed: The time elapsed during initiation of a processor\nAbstractSink\nCounter recordsIn: The number of records ingressed into a sink\nTimer timeElapsed: The time elapsed during execution of a sink\nNaming\nMetrics follow a naming convention of PIPELINE_NAME_PLUGIN_NAME_METRIC_NAME. For example, a recordsIn metric for the opensearch-sink plugin in a pipeline named output-pipeline has a qualified name of output-pipeline_opensearch_sink_recordsIn.\nServing\nBy default, metrics are served from the /metrics/sys endpoint on the Data Prepper server in a Prometheus scrape format. You can configure Prometheus to scrape from the Data Prepper URL. The Data Prepper server port has a default value of 4900 that you can modify, and this port can be used for any frontend that accepts Prometheus metrics, such as Grafana. You can update the configuration to serve metrics to other registries like CloudWatch, that does not require or host the endpoint, but publishes the metrics directly to CloudWatch.",
    "ancestors": [
      "Data Prepper",
      "Managing Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/managing-data-prepper/peer-forwarder/",
    "title": "Peer forwarder",
    "content": "Peer forwarder is an HTTP service that performs peer forwarding of an event between Data Prepper nodes for aggregation. This HTTP service uses a hash-ring approach to aggregate events and determine which Data Prepper node it should handle on a given trace before rerouting it to that node. Currently, peer forwarder is supported by the aggregate, service_map_stateful, and otel_trace_raw processors.\nPeer forwarder groups events based on the identification keys provided by the supported processors. For service_map_stateful and otel_trace_raw, the identification key is traceId by default and cannot be configured. The aggregate processor is configured using the identification_keys configuration option. From here, you can specify which keys to use for peer forwarder. See Aggregate Processor page for more information about identification keys.\nPeer discovery allows Data Prepper to find other nodes that it will communicate with. Currently, peer discovery is provided by a static list, a DNS record lookup, or AWS Cloud Map.\nDiscovery modes\nThe following sections provide information about discovery modes.\nStatic\nStatic discovery mode allows a Data Prepper node to discover nodes using a list of IP addresses or domain names. See the following YAML file for an example of static discovery mode: peer_forwarder:4 discovery_mode: static static_endpoints: [ \" data-prepper1\", \" data-prepper2\"] DNS lookup\nDNS discovery is preferred over static discovery when scaling out a Data Prepper cluster. DNS discovery configures a DNS provider to return a list of Data Prepper hosts when given a single domain name. This list consists of a DNS A record, and a list of IP addresses of a given domain. See the following YAML file for an example of DNS lookup: peer_forwarder: discovery_mode: dns domain_name: \" data-prepper-cluster.my-domain.net\" AWS Cloud Map AWS Cloud Map provides API-based service discovery as well as DNS-based service discovery.\nPeer forwarder can use the API-based service discovery in AWS Cloud Map. To support this, you must have an existing namespace configured for API instance discovery. You can create a new one by following the instructions provided by the AWS Cloud Map documentation.\nYour Data Prepper configuration needs to include the following: aws_cloud_map_namespace_name – Set to your AWS Cloud Map namespace name. aws_cloud_map_service_name – Set to the service name within your specified namespace. aws_region – Set to the AWS Region in which your namespace exists. discovery_mode – Set to aws_cloud_map.\nYour Data Prepper configuration can optionally include the following: aws_cloud_map_query_parameters – Key-value pairs are used to filter the results based on the custom attributes attached to an instance. Results include only those instances that match all of the specified key-value pairs.\nExample configuration\nSee the following YAML file example of AWS Cloud Map configuration: peer_forwarder: discovery_mode: aws_cloud_map aws_cloud_map_namespace_name: \" my-namespace\" aws_cloud_map_service_name: \" data-prepper-cluster\" aws_cloud_map_query_parameters: instance_type: \" r5.xlarge\" aws_region: \" us-east-1\" IAM policy with necessary permissions\nData Prepper must also be running with the necessary permissions. The following AWS Identity and Access Management (IAM) policy shows the necessary permissions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CloudMapPeerForwarder\", \"Effect\": \"Allow\", \"Action\": \"servicediscovery:DiscoverInstances\", \"Resource\": \"*\" }] } Configuration\nThe following table provides optional configuration values. Value Type Description port Integer\nA value between 0 and 65535 that represents the port that the peer forwarder server is running on. Default value is 4994. request_timeout Integer\nRepresents the request timeout duration in milliseconds for the peer forwarder HTTP server. Default value is 10000. server_thread_count Integer\nRepresents the number of threads used by the peer forwarder server. Default value is 200. client_thread_count Integer\nRepresents the number of threads used by the peer forwarder client. Default value is 200. maxConnectionCount Integer\nRepresents the maximum number of open connections for the peer forwarder server. Default value is 500. discovery_mode String\nRepresents the peer discovery mode to be used. Allowable values are local_node, static, dns, and aws_cloud_map. Defaults to local_node, which processes events locally. static_endpoints List\nContains the endpoints of all Data Prepper instances. Required if discovery_mode is set to static. domain_name String\nRepresents the single domain name to query DNS against. Typically used by creating multiple DNS A records for the same domain. Required if discovery_mode is set to dns. aws_cloud_map_namespace_name String\nRepresents the AWS Cloud Map namespace when using AWS Cloud Map service discovery. Required if discovery_mode is set to aws_cloud_map. aws_cloud_map_service_name String\nRepresents the AWS Cloud Map service when using AWS Cloud Map service discovery. Required if discovery_mode is set to aws_cloud_map. aws_cloud_map_query_parameters Map\nKey-value pairs used to filter the results based on the custom attributes attached to an instance. Only instances that match all the specified key-value pairs are returned. buffer_size Integer\nRepresents the maximum number of unchecked records the buffer accepts (the number of unchecked records equals the number of records written into the buffer plus the number of records that are still processing and not yet checked by the Checkpointing API). Default is 512. batch_size Integer\nRepresents the maximum number of records that the buffer returns on read. Default is 48. aws_region String\nRepresents the AWS Region that uses ACM, Amazon S3, or AWS Cloud Map and is required when any of the following conditions are met: - The use_acm_certificate_for_ssl setting is set to true. - Either ssl_certificate_file or ssl_key_file specifies an Amazon Simple Storage Service (Amazon S3) URI (for example, s3://mybucket/path/to/public.cert). - The discovery_mode is set to aws_cloud_map. drain_timeout Duration\nRepresents the amount of time that peer forwarder will wait to complete data processing before shutdown. SSL configuration\nThe following table provides optional SSL configuration values that allow you to set up a trust manager for the peer forwarder client in order to connect to other Data Prepper instances. Value Type Description ssl Boolean\nEnables TLS/SSL. Default value is true. ssl_certificate_file String\nRepresents the SSL certificate chain file path or Amazon S3 path. The following is an example of an Amazon S3 path: s3://&lt;bucketName&gt;/&lt;path&gt;. Defaults to the default certificate file, config/default_certificate.pem. See Default Certificates for more information about how the certificate is generated. ssl_key_file String\nRepresents the SSL key file path or Amazon S3 path. Amazon S3 path example: s3://&lt;bucketName&gt;/&lt;path&gt;. Defaults to config/default_private_key.pem which is the default private key file. See Default Certificates for more information about how the private key file is generated. ssl_insecure_disable_verification Boolean\nDisables the verification of the server’s TLS certificate chain. Default value is false. ssl_fingerprint_verification_only Boolean\nDisables the verification of the server’s TLS certificate chain and instead verifies only the certificate fingerprint. Default value is false. use_acm_certificate_for_ssl Boolean\nEnables TLS/SSL using the certificate and private key from AWS Certificate Manager (ACM). Default value is false. acm_certificate_arn String\nRepresents the ACM certificate Amazon Resource Name (ARN). The ACM certificate takes precedence over Amazon S3 or the local file system certificate. Required if use_acm_certificate_for_ssl is set to true. acm_private_key_password String\nRepresents the ACM private key password that will be used to decrypt the private key. If it’s not provided, a random password will be generated. acm_certificate_timeout_millis Integer\nRepresents the timeout in milliseconds required for ACM to get certificates. Default value is 120000. aws_region String\nRepresents the AWS Region that uses ACM, Amazon S3, or AWS Cloud Map. Required if use_acm_certificate_for_ssl is set to true or ssl_certificate_file. Also required when the ssl_key_file is set to use the Amazon S3 path or if discovery_mode is set to aws_cloud_map. Example configuration\nThe following YAML file provides an example configuration: peer_forwarder: ssl: true ssl_certificate_file: \" &lt;cert-file-path&gt;\" ssl_key_file: \" &lt;private-key-file-path&gt;\" Authentication Authentication is optional and is a Map that enables mutual TLS (mTLS). It can either be mutual_tls or unauthenticated. The default value is unauthenticated. The following YAML file provides an example of authentication: peer_forwarder: authentication: mutual_tls: Metrics\nCore peer forwarder introduces the following custom metrics. All the metrics are prefixed by core.peerForwarder.\nTimer\nPeer forwarder’s timer capability provides the following information: requestForwardingLatency: Measures latency of requests forwarded by the peer forwarder client. requestProcessingLatency: Measures latency of requests processed by the peer forwarder server.\nCounter\nThe following table provides counter metric options. Value Description requests Measures the total number of forwarded requests. requestsFailed Measures the total number of failed requests. Applies to requests with an HTTP response code other than 200. requestsSuccessful Measures the total number of successful requests. Applies to requests with HTTP response code 200. requestsTooLarge Measures the total number of requests that are too large to be written to the peer forwarder buffer. Applies to requests with HTTP response code 413. requestTimeouts Measures the total number of requests that time out while writing content to the peer forwarder buffer. Applies to requests with HTTP response code 408. requestsUnprocessable Measures the total number of requests that fail due to an unprocessable entity. Applies to requests with HTTP response code 422. badRequests Measures the total number of requests with a bad request format. Applies to requests with HTTP response code 400. recordsSuccessfullyForwarded Measures the total number of successfully forwarded records. recordsFailedForwarding Measures the total number of records that fail to be forwarded. recordsToBeForwarded Measures the total number of records to be forwarded. recordsToBeProcessedLocally Measures the total number of records to be processed locally. recordsActuallyProcessedLocally Measures the total number of records actually processed locally. This value is the sum of recordsToBeProcessedLocally and recordsFailedForwarding. recordsReceivedFromPeers Measures the total number of records received from remote peers. Gauge peerEndpoints Measures the number of dynamically discovered peer Data Prepper endpoints. For static mode, the size is fixed.",
    "ancestors": [
      "Data Prepper",
      "Managing Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/migrate-open-distro/",
    "title": "Migrating from Open Distro",
    "content": "Existing users can migrate from the Open Distro Data Prepper to OpenSearch Data Prepper. Beginning with Data Prepper version 1.1, there is only one distribution of OpenSearch Data Prepper.\nChange your pipeline configuration\nThe elasticsearch sink has changed to opensearch. Therefore, change your existing pipeline to use the opensearch plugin instead of elasticsearch.\nWhile the Data Prepper plugin is titled opensearch, it remains compatible with Open Distro and ElasticSearch 7.x.\nUpdate Docker image\nIn your Data Prepper Docker configuration, adjust amazon/opendistro-for-elasticsearch-data-prepper to opensearchproject/data-prepper. This change will download the latest Data Prepper Docker image.\nNext steps\nFor more information about Data Prepper configurations, see Getting Started with Data Prepper.",
    "ancestors": [
      "Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/migrating-from-logstash-data-prepper/",
    "title": "Migrating from Logstash",
    "content": "You can run Data Prepper with a Logstash configuration.\nAs mentioned in Getting started with Data Prepper, you’ll need to configure Data Prepper with a pipeline using a pipelines.yaml file.\nAlternatively, if you have a Logstash configuration logstash.conf to configure Data Prepper instead of pipelines.yaml.\nSupported plugins\nAs of the Data Prepper 1.2 release, the following plugins from the Logstash configuration are supported:\nHTTP Input plugin\nGrok Filter plugin\nElasticsearch Output plugin\nAmazon Elasticsearch Output plugin\nLimitations\nApart from the supported plugins, all other plugins from the Logstash configuration will throw an Exception and fail to run.\nConditionals in the Logstash configuration are not supported as of the Data Prepper 1.2 release.\nRunning Data Prepper with a Logstash configuration\nTo install Data Prepper’s Docker image, see Installing Data Prepper in Getting Started with Data Prepper.\nRun the Docker image installed in Step 1 by supplying your logstash.conf configuration. docker run --name data-prepper -p 4900:4900 -v ${PWD}/logstash.conf:/usr/share/data-prepper/pipelines.conf opensearchproject/data-prepper:latest pipelines.conf The logstash.conf file is converted to logstash.yaml by mapping the plugins and attributes in the Logstash configuration to the corresponding plugins and attributes in Data Prepper.\nYou can find the converted logstash.yaml file in the same directory where you stored logstash.conf.\nThe following output in your terminal indicates that Data Prepper is running correctly: INFO org.opensearch.dataprepper.pipeline.ProcessWorker - log-pipeline Worker: No records received from buffer",
    "ancestors": [
      "Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/buffers/bounded-blocking/",
    "title": "Bounded blocking",
    "content": "Overview Bounded blocking is the default buffer and is memory based. The following table describes the Bounded blocking parameters. Option Required Type Description buffer_size\nNo\nInteger\nThe maximum number of records the buffer accepts. Default value is 12800.\nbatch_size\nNo\nInteger\nThe maximum number of records the buffer drains after each read. Default value is 200. <!--- ## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section. --->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Buffers"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/buffers/buffers/",
    "title": "Buffers",
    "content": "Buffers store data as it passes through the pipeline. If you implement a custom buffer, it can be memory based, which provides better performance, or disk based, which is larger in size.",
    "ancestors": [
      "Data Prepper",
      "Pipelines"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/add-entries/",
    "title": "add_entries",
    "content": "The add_entries processor adds entries to an event.\nConfiguration\nYou can configure the add_entries processor with the following options. Option Required Description entries Yes\nA list of entries to add to an event. key Yes\nThe key of the new entry to be added. Some examples of keys include my_key, myKey, and object/sub_Key. value Yes\nThe value of the new entry to be added. You can use the following data types: strings, Booleans, numbers, null, nested objects, and arrays. overwrite_if_key_exists No\nWhen set to true, the existing value is overwritten if key already exists in the event. The default value is false. Usage\nTo get started, create the following pipeline.yaml file: pipeline: source:....... processor: - add_entries: entries: - key: \" newMessage\" value: 3 overwrite_if_key_exists: true sink: copy For example, when your source contains the following event record: { \"message\": \"hello\" } And then you run the add_entries processor using the example pipeline, it adds a new entry, {\"newMessage\": 3}, to the existing event, {\"message\": \"hello\"}, so that the new event contains two entries in the final output: { \"message\": \"hello\", \"newMessage\": 3 } If newMessage already exists, its existing value is overwritten with a value of 3.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/aggregate/",
    "title": "aggregate",
    "content": "The aggregate processor groups events based on the values of identification_keys. Then, the processor performs an action on each group, helping reduce unnecessary log volume and creating aggregated logs over time. You can use existing actions or create your own custom aggregations using Java code.\nConfiguration\nThe following table describes the options you can use to configure the aggregate processor. Option Required Type Description identification_keys\nYes\nList\nAn unordered list by which to group events. Events with the same values as these keys are put into the same group. If an event does not contain one of the identification_keys, then the value of that key is considered to be equal to null. At least one identification_key is required (for example, [\"sourceIp\", \"destinationIp\", \"port\"]).\naction\nYes\nAggregateAction\nThe action to be performed on each group. One of the available aggregate actions must be provided, or you can create custom aggregate actions. remove_duplicates and put_all are the available actions. For more information, see Creating New Aggregate Actions.\ngroup_duration\nNo\nString\nThe amount of time that a group should exist before it is concluded automatically. Supports ISO_8601 notation strings (“PT20.345S”, “PT15M”, etc.) as well as simple notation for seconds ( \"60s\") and milliseconds ( \"1500ms\"). Default value is 180s. Available aggregate actions\nUse the following aggregate actions to determine how the aggregate processor processes events in each group.\nremove_duplicates\nThe remove_duplicates action processes the first event for a group immediately and drops any events that duplicate the first event from the source. For example, when using identification_keys: [\"sourceIp\", \"destination_ip\"]:\nThe remove_duplicates action processes { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 }, the first event in the source.\nData Prepper drops the { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 } event because the sourceIp and destinationIp match the first event in the source.\nThe remove_duplicates action processes the next event, { \"sourceIp\": \"127.0.0.2\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 }. Because the sourceIp is different from the first event of the group, Data Prepper creates a new group based on the event.\nput_all\nThe put_all action combines events belonging to the same group by overwriting existing keys and adding new keys, similarly to the Java Map.putAll. The action drops all events that make up the combined event. For example, when using identification_keys: [\"sourceIp\", \"destination_ip\"], the put_all action processes the following three events: { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 }\n{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 }\n{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"http_verb\": \"GET\" } Then the action combines the events into one. The pipeline then uses the following combined event: { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200, \"bytes\": 1000, \"http_verb\": \"GET\" } count\nThe count event counts events that belong to the same group and generates a new event with values of the identification_keys and the count, which indicates the number of new events. You can customize the processor with the following configuration options: count_key: Key used for storing the count. Default name is aggr._count. start_time_key: Key used for storing the start time. Default name is aggr._start_time. output_format: Format of the aggregated event. otel_metrics: Default output format. Outputs in OTel metrics SUM type with count as value. raw - Generates a JSON object with the count_key field as a count value and the start_time_key field with aggregation start time as value.\nFor an example, when using identification_keys: [\"sourceIp\", \"destination_ip\"], the count action counts and processes the following events: { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 503 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 400 } The processor creates the following event: { \"isMonotonic\": true, \"unit\": \"1\", \"aggregationTemporality\": \"AGGREGATION_TEMPORALITY_DELTA\", \"kind\": \"SUM\", \"name\": \"count\", \"description\": \"Number of events\", \"startTime\": \"2022-12-02T19:29:51.245358486Z\", \"time\": \"2022-12-02T19:30:15.247799684Z\", \"value\": 3.0, \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\" } histogram\nThe histogram action aggregates events belonging to the same group and generates a new event with values of the identification_keys and histogram of the aggregated events based on a configured key. The histogram contains the number of events, sum, buckets, bucket counts, and optionally min and max of the values corresponding to the key. The action drops all events that make up the combined event.\nYou can customize the processor with the following configuration options: key: Name of the field in the events the histogram generates. generated_key_prefix: key_prefix used by all the fields created in the aggregated event. Having a prefix ensures that the names of the histogram event do not conflict with the field names in the event. units: The units for the values in the key. record_minmax: A Boolean value indicating whether the histogram should include the min and max of the values in the aggregation. buckets: A list of buckets (values of type double) indicating the buckets in the histogram. output_format: Format of the aggregated event. otel_metrics: Default output format. Outputs in OTel metrics SUM type with count as value. raw: Generates a JSON object with count_key field with count as value and start_time_key field with aggregation start time as value.\nFor example, when using identification_keys: [\"sourceIp\", \"destination_ip\", \"request\"], key: latency, and buckets: [0.0, 0.25, 0.5], the histogram action processes the following events: { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\": \"/index.html\", \"latency\": 0.2 }\n{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\": \"/index.html\", \"latency\": 0.55}\n{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\": \"/index.html\", \"latency\": 0.25 }\n{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"request\": \"/index.html\", \"latency\": 0.15 } Then the processor creates the following event: { \"max\": 0.55, \"kind\": \"HISTOGRAM\", \"buckets\":[{ \"min\": -3.4028234663852886E38, \"max\": 0.0, \"count\": 0 },{ \"min\": 0.0, \"max\": 0.25, \"count\": 2 },{ \"min\": 0.25, \"max\": 0.50, \"count\": 1 },{ \"min\": 0.50, \"max\": 3.4028234663852886E38, \"count\": 1 }], \"count\": 4, \"bucketCountsList\":[ 0, 2, 1, 1], \"description\": \"Histogram of latency in the events\", \"sum\": 1.15, \"unit\": \"seconds\", \"aggregationTemporality\": \"AGGREGATION_TEMPORALITY_DELTA\", \"min\": 0.15, \"bucketCounts\": 4, \"name\": \"histogram\", \"startTime\": \"2022-12-14T06:43:40.848762215Z\", \"explicitBoundsCount\": 3, \"time\": \"2022-12-14T06:44:04.852564623Z\", \"explicitBounds\":[ 0.0, 0.25, 0.5], \"request\": \"/index.html\", \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"key\": \"latency\" } rate_limiter\nThe rate_limiter action controls the number of events aggregated per second. By default, rate_limiter blocks the aggregate processor from running if it receives more events than the configured number allowed. You can overwrite the number events that triggers the rate_limited by using the when_exceeds configuration option.\nYou can customize the processor with the following configuration options: events_per_second: The number of events allowed per second. when_exceeds: Indicates what action the rate_limiter takes when the number of events received is greater than the number of events allowed per second. Default value is block, which blocks the processor from running after the maximum number of events allowed per second is reached until the next second. Alternatively, the drop option drops the excess events received in that second.\nFor example, if events_per_second is set to 1 and when_exceeds is set to drop, the action tries to process the following events when received during the one second time interval: { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"http_verb\": \"GET\" } The following event is processed, but all other events are ignored because the rate_limiter blocks them: { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"status\": 200 } If when_exceeds is set to drop, all three events are processed.\npercent_sampler\nThe percent_sampler action controls the number of events aggregated based on a percentage of events. The action drops any events not included in the percentage.\nYou can set the percentage of events using the percent configuration, which indicates the percentage of events processed during a one second interval (0%–100%).\nFor example, if percent is set to 50, the action tries to process the following events in the one-second interval: { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 2500 }\n{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 500 }\n{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 1000 }\n{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 3100 } The pipeline processes 50% of the events, drops the other events, and does not generate a new event: { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 500 }\n{ \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"bytes\": 3100 } Metrics\nThe following table describes common Abstract processor metrics. Metric name Type Description recordsIn Counter\nMetric representing the ingress of records to a pipeline component. recordsOut Counter\nMetric representing the egress of records from a pipeline component. timeElapsed Timer\nMetric representing the time elapsed during execution of a pipeline component. The aggregate processor includes the following custom metrics. Counter actionHandleEventsOut: The number of events that have been returned from the handleEvent call to the configured action. actionHandleEventsDropped: The number of events that have not been returned from the handleEvent call to the configured action. actionHandleEventsProcessingErrors: The number of calls made to handleEvent for the configured action that resulted in an error. actionConcludeGroupEventsOut: The number of events that have been returned from the concludeGroup call to the configured action. actionConcludeGroupEventsDropped: The number of events that have not been returned from the condludeGroup call to the configured action. actionConcludeGroupEventsProcessingErrors: The number of calls made to concludeGroup for the configured action that resulted in an error. Gauge currentAggregateGroups: The current number of groups. This gauge decreases when a group concludes and increases when an event initiates the creation of a new group.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/anomaly-detector/",
    "title": "anomaly_detector",
    "content": "The anomaly detector processor takes structured data and runs anomaly detection algorithms on fields that you can configure in that data. The data must be either an integer or a real number for the anomaly detection algorithm to detect anomalies. Deploying the aggregate processor in a pipeline before the anomaly detector processor can help you achieve the best results, as the aggregate processor automatically aggregates events by key and keeps them on the same host. For example, if you are searching for an anomaly in latencies from a specific IP address and if all the events go to the same host, then the host has more data for these events. This additional data results in better training of the machine learning (ML) algorithm, which results in better anomaly detection.\nConfiguration\nYou can configure the anomaly detector processor by specifying a key and the options for the selected mode. You can use the following options to configure the anomaly detector processor. Name Required Description keys Yes\nA non-ordered List&lt;String&gt; that is used as input to the ML algorithm to detect anomalies in the values of the keys in the list. At least one key is required. mode Yes\nThe ML algorithm (or model) used to detect anomalies. You must provide a mode. See random_cut_forest mode. Keys\nKeys that are used in the anomaly detector processor are present in the input event. For example, if the input event is {\"key1\":value1, \"key2\":value2, \"key3\":value3}, then any of the keys (such as key1, key2, key3) in that input event can be used as anomaly detector keys as long as their value (such as value1, value2, value3) is an integer or real number.\nrandom_cut_forest mode\nThe random cut forest (RCF) ML algorithm is an unsupervised algorithm for detecting anomalous data points within a dataset. To detect anomalies, the anomaly detector processor uses the random_cut_forest mode. Name Description random_cut_forest Processes events using the RCF ML algorithm to detect anomalies. RCF is an unsupervised ML algorithm for detecting anomalous data points within a dataset. Data Prepper uses RCF to detect anomalies in data by passing the values of the configured key to RCF. For example, when an event with a latency value of 11.5 is sent, the following anomaly event is generated: { \"latency\": 11.5, \"deviation_from_expected\":[ 10.469302736820003], \"grade\": 1.0 } In this example, deviation_from_expected is a list of deviations for each of the keys from their corresponding expected values, and grade is the anomaly grade that indicates the anomaly severity.\nYou can configure random_cut_forest mode with the following options. Name Default value Range Description shingle_size 4 1–60\nThe shingle size used in the ML algorithm. sample_size 256 100–2500\nThe sample size used in the ML algorithm. time_decay 0.1 0–1.0\nThe time decay value used in the ML algorithm. Used as the mathematical expression timeDecay divided by SampleSize in the ML algorithm. type metrics N/A\nThe type of data sent to the algorithm. version 1.0 N/A\nThe algorithm version number. Usage\nTo get started, create the following pipeline.yaml file. You can use the following pipeline configuration to look for anomalies in the latency field in events that are passed to the processor. Then you can use the following YAML configuration file random_cut_forest mode to detect anomalies: ad-pipeline: source:....... processor: - anomaly_detector: keys: [ \" latency\"] mode: random_cut_forest: When you run the anomaly detector processor, the processor extracts the value for the latency key, and then passes the value through the RCF ML algorithm. You can configure any key that comprises integers or real numbers as values. In the following example, you can configure bytes or latency as the key for an anomaly detector. {\"ip\":\"1.2.3.4\", \"bytes\":234234, \"latency\":0.2}",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/convert_entry_type/",
    "title": "convert_entry_type",
    "content": "The convert_entry_type processor converts a value type associated with the specified key in a event to the specified type. It is a casting processor that changes the types of some fields in events. Some data must be converted to a different type, such as an integer to a double, or a string to an integer, so that it will pass the events through condition-based processors or perform conditional routing.\nConfiguration\nYou can configure the convert_entry_type processor with the following options. Option Required Description key Yes\nKeys whose value needs to be converted to a different type. type No\nTarget type for the key-value pair. Possible values are integer, double, string, and Boolean. Default value is integer. Usage\nTo get started, create the following pipeline.yaml file: type-conv-pipeline: source:....... processor: - convert_entry_type_type: key: \" response_status\" type: \" integer\" copy Next, create a log file named logs_json.log and replace the path in the file source of your pipeline.yaml file with that filepath. For more information, see Configuring Data Prepper.\nFor example, before you run the convert_entry_type processor, if the logs_json.log file contains the following event record: { \"message\": \"value\", \"response_status\": \"200\" } The convert_entry_type processor converts the output received to the following output, where the type of response_status value changes from a string to an integer: { \"message\": \"value\", \"response_status\": 200 }",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/copy-values/",
    "title": "copy_values",
    "content": "The copy_values processor copies values within an event and is a mutate event processor.\nConfiguration\nYou can configure the copy_values processor with the following options. Option Required Description entries Yes\nA list of entries to be copied in an event. from_key Yes\nThe key of the entry to be copied. to_key Yes\nThe key of the new entry to be added. overwrite_if_key_exists No\nWhen set to true, the existing value is overwritten if key already exists in the event. The default value is false. Usage\nTo get started, create the following pipeline.yaml file: pipeline: source:....... processor: - copy_values: entries: - from_key: \" message\" to_key: \" newMessage\" overwrite_if_to_key_exists: true sink: copy Next, create a log file named logs_json.log and replace the path in the file source of your pipeline.yaml file with that filepath. For more information, see Configuring Data Prepper.\nFor example, before you run the copy_values processor, if the logs_json.log file contains the following event record: { \"message\": \"hello\" } When you run this processor, it parses the message into the following output: { \"message\": \"hello\", \"newMessage\": \"hello\" } If newMessage already exists, its existing value is overwritten with value.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/csv/",
    "title": "csv",
    "content": "The csv processor parses comma-separated values (CSVs) from the event into columns.\nConfiguration\nThe following table describes the options you can use to configure the csv processor. Option Required Type Description source\nNo\nString\nThe field in the event that will be parsed. Default value is message.\nquote_character\nNo\nString\nThe character used as a text qualifier for a single column of data. Default value is \".\ndelimiter\nNo\nString\nThe character separating each column. Default value is,.\ndelete_header\nNo\nBoolean\nIf specified, the event header ( column_names_source_key) is deleted after the event is parsed. If there is no event header, no action is taken. Default value is true.\ncolumn_names_source_key\nNo\nString\nThe field in the event that specifies the CSV column names, which will be automatically detected. If there need to be extra column names, the column names are automatically generated according to their index. If column_names is also defined, the header in column_names_source_key can also be used to generate the event fields. If too few columns are specified in this field, the remaining column names are automatically generated. If too many column names are specified in this field, the CSV processor omits the extra column names.\ncolumn_names\nNo\nList\nUser-specified names for the CSV columns. Default value is [column1, column2,..., columnN] if there are no columns of data in the CSV record and column_names_source_key is not defined. If column_names_source_key is defined, the header in column_names_source_key generates the event fields. If too few columns are specified in this field, the remaining column names are automatically generated. If too many column names are specified in this field, the CSV processor omits the extra column names. <!---## Configuration\nContent will be added to this section.--->\nMetrics\nThe following table describes common Abstract processor metrics. Metric name Type Description recordsIn Counter\nMetric representing the ingress of records to a pipeline component. recordsOut Counter\nMetric representing the egress of records from a pipeline component. timeElapsed Timer\nMetric representing the time elapsed during execution of a pipeline component. The csv processor includes the following custom metrics. Counter csvInvalidEvents: The number of invalid events. An exception is thrown when an invalid event is parsed. An unclosed quote usually causes this exception.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/date/",
    "title": "date",
    "content": "The date processor adds a default timestamp to an event, parses timestamp fields, and converts timestamp information to the International Organization for Standardization (ISO) 8601 format. This timestamp information can be used as an event timestamp.\nConfiguration\nThe following table describes the options you can use to configure the date processor. Option Required Type Description match\nConditionally\nList\nList of key and patterns where patterns is a list. The list of match can have exactly one key and patterns. There is no default value. This option cannot be defined at the same time as from_time_received. Include multiple date processors in your pipeline if both options should be used.\nfrom_time_received\nConditionally\nBoolean\nA boolean that is used for adding default timestamp to event data from event metadata which is the time when source receives the event. Default value is false. This option cannot be defined at the same time as match. Include multiple date processors in your pipeline if both options should be used.\ndestination\nNo\nString\nField to store the timestamp parsed by date processor. It can be used with both match and from_time_received. Default value is @timestamp.\nsource_timezone\nNo\nString\nTime zone used to parse dates. It is used in case the zone or offset cannot be extracted from the value. If the zone or offset are part of the value, then timezone is ignored. Find all the available timezones the list of database time zones in the TZ database name column.\ndestination_timezone\nNo\nString\nTimezone used for storing timestamp in destination field. The available timezone values are the same as source_timestamp.\nlocale\nNo\nString\nLocale is used for parsing dates. It’s commonly used for parsing month names( MMM). It can have language, country and variant fields using IETF BCP 47 or String representation of Locale object. For example en-US for IETF BCP 47 and en_US for string representation of Locale. Full list of locale fields which includes language, country and variant can be found the language subtag registry. Default value is Locale.ROOT. <!---## Configuration\nContent will be added to this section.--->\nMetrics\nThe following table describes common Abstract processor metrics. Metric name Type Description recordsIn Counter\nMetric representing the ingress of records to a pipeline component. recordsOut Counter\nMetric representing the egress of records from a pipeline component. timeElapsed Timer\nMetric representing the time elapsed during execution of a pipeline component. The date processor includes the following custom metrics. dateProcessingMatchSuccessCounter: Returns the number of records that match with at least one pattern specified by the match configuration option. dateProcessingMatchFailureCounter: Returns the number of records that did not match any of the patterns specified by the patterns match configuration option.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/delete-entries/",
    "title": "delete_entries",
    "content": "The delete_entries processor deletes entries, such as key-value pairs, from an event. You can define the keys you want to delete in the with-keys field following delete_entries in the YAML configuration file. Those keys and their values are deleted.\nConfiguration\nYou can configure the delete_entries processor with the following options. Option Required Description with_keys Yes\nAn array of keys for the entries to be deleted. Usage\nTo get started, create the following pipeline.yaml file: pipeline: source:....... processor: - delete_entries: with_keys: [ \" message\"] sink: copy Next, create a log file named logs_json.log and replace the path in the file source of your pipeline.yaml file with that filepath. For more information, see Configuring Data Prepper.\nFor example, before you run the delete_entries processor, if the logs_json.log file contains the following event record: { \"message\": \"hello\", \"message2\": \"goodbye\" } When you run the delete_entries processor, it parses the message into the following output: { \"message2\": \"goodbye\" } If message does not exist in the event, then no action occurs.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/drop-events/",
    "title": "drop_events",
    "content": "The drop_events processor drops all the events that are passed into it. The following table describes when events are dropped and how exceptions for dropping events are handled. Option Required Type Description drop_when\nYes\nString\nAccepts a Data Prepper expression string following the Data Prepper Expression Syntax. Configuring drop_events with drop_when: true drops all the events received.\nhandle_failed_events\nNo\nEnum\nSpecifies how exceptions are handled when an exception occurs while evaluating an event. Default value is drop, which drops the event so that it is not sent to OpenSearch. Available options are drop, drop_silently, skip, and skip_silently. For more information, see handle_failed_events. <!---## Configuration\nContent will be added to this section.--->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/grok/",
    "title": "grok",
    "content": "The Grok processor takes unstructured data and utilizes pattern matching to structure and extract important keys.\nConfiguration\nThe following table describes options you can use with the Grok processor to structure your data and make your data easier to query. Option Required Type Description match\nNo\nMap\nSpecifies which keys to match specific patterns against. Default value is an empty body.\nkeep_empty_captures\nNo\nBoolean\nEnables preserving null captures. Default value is false.\nnamed_captures_only\nNo\nBoolean\nSpecifies whether to keep only named captures. Default value is true.\nbreak_on_match\nNo\nBoolean\nSpecifies whether to match all patterns or stop once the first successful match is found. Default value is true.\nkeys_to_overwrite\nNo\nList\nSpecifies which existing keys will be overwritten if there is a capture with the same key value. Default value is [].\npattern_definitions\nNo\nMap\nAllows for custom pattern use inline. Default value is an empty body.\npatterns_directories\nNo\nList\nSpecifies the path of directories that contain customer pattern files. Default value is an empty list.\npattern_files_glob\nNo\nString\nSpecifies which pattern files to use from the directories specified for pattern_directories. Default value is *.\ntarget_key\nNo\nString\nSpecifies a parent-level key used to store all captures. Default value is null.\ntimeout_millis\nNo\nInteger\nThe maximum amount of time during which matching occurs. Setting to 0 disables the timeout. Default value is 30,000. <!---## Configuration\nContent will be added to this section.--->\nMetrics\nThe following table describes common Abstract processor metrics. Metric name Type Description recordsIn Counter\nMetric representing the ingress of records to a pipeline component. recordsOut Counter\nMetric representing the egress of records from a pipeline component. timeElapsed Timer\nMetric representing the time elapsed during execution of a pipeline component. The Grok processor includes the following custom metrics.\nCounter grokProcessingMismatch: Records the number of records that did not match any of the patterns specified in the match field. grokProcessingMatch: Records the number of records that matched at least one pattern from the match field. grokProcessingErrors: Records the total number of record processing errors. grokProcessingTimeouts: Records the total number of records that timed out while matching.\nTimer grokProcessingTime: The time taken by individual records to match against patterns from match. The avg metric is the most useful metric for this timer because it provides you with an average value of the time it takes records to match.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/key-value/",
    "title": "key_value",
    "content": "You can use the key_value processor to parse the specified field into key-value pairs. You can customize the key_value processor to parse field information with the following options. The type for each of the following options is string. Option Description Example source\nThe message field to be parsed. Optional. Default value is message.\nIf source is \"message1\", {\"message1\": {\"key1=value1\"}, \"message2\": {\"key2=value2\"}} parses into {\"message1\": {\"key1=value1\"}, \"message2\": {\"key2=value2\"}, \"parsed_message\": {\"key1\": \"value1\"}}.\ndestination\nThe destination field for the parsed source. The parsed source overwrites the preexisting data for that key. Optional. Default value is parsed_message.\nIf destination is \"parsed_data\", {\"message\": {\"key1=value1\"}} parses into {\"message\": {\"key1=value1\"}, \"parsed_data\": {\"key1\": \"value1\"}}.\nfield_delimiter_regex\nA regular expression specifying the delimiter that separates key-value pairs. Special regular expression characters such as [ and] must be escaped with \\\\. Cannot be defined at the same time as field_split_characters. Optional. If this option is not defined, field_split_characters is used.\nIf field_delimiter_regex is \"&amp;\\\\{2\\\\}\", {\"key1=value1&amp;&amp;key2=value2\"} parses into {\"key1\": \"value1\", \"key2\": \"value2\"}.\nfield_split_characters\nA string of characters specifying the delimeter that separates key-value pairs. Special regular expression characters such as [ and] must be escaped with \\\\. Cannot be defined at the same time as field_delimiter_regex. Optional. Default value is &amp;.\nIf field_split_characters is \"&amp;&amp;\", {\"key1=value1&amp;&amp;key2=value2\"} parses into {\"key1\": \"value1\", \"key2\": \"value2\"}.\nkey_value_delimiter_regex\nA regular expression specifying the delimiter that separates the key and value within a key-value pair. Special regular expression characters such as [ and] must be escaped with \\\\. This option cannot be defined at the same time as value_split_characters. Optional. If this option is not defined, value_split_characters is used.\nIf key_value_delimiter_regex is \"=\\\\{2\\\\}\", {\"key1==value1\"} parses into {\"key1\": \"value1\"}.\nvalue_split_characters\nA string of characters specifying the delimiter that separates the key and value within a key-value pair. Special regular expression characters such as [ and] must be escaped with \\\\. Cannot be defined at the same time as key_value_delimiter_regex. Optional. Default value is =.\nIf value_split_characters is \"==\", {\"key1==value1\"} parses into {\"key1\": \"value1\"}.\nnon_match_value\nWhen a key-value pair cannot be successfully split, the key-value pair is placed in the key field, and the specified value is placed in the value field. Optional. Default value is null. key1value1&amp;key2=value2 parses into {\"key1value1\": null, \"key2\": \"value2\"}.\nprefix\nA prefix to append before all keys. Optional. Default value is an empty string.\nIf prefix is \"custom\", {\"key1=value1\"} parses into {\"customkey1\": \"value1\"}.\ndelete_key_regex\nA regular expression specifying the characters to delete from the key. Special regular expression characters such as [ and] must be escaped with \\\\. Cannot be an empty string. Optional. No default value.\nIf delete_key_regex is \"\\s\", {\"key1 =value1\"} parses into {\"key1\": \"value1\"}.\ndelete_value_regex\nA regular expression specifying the characters to delete from the value. Special regular expression characters such as [ and] must be escaped with \\\\. Cannot be an empty string. Optional. No default value.\nIf delete_value_regex is \"\\s\", {\"key1=value1 \"} parses into {\"key1\": \"value1\"}. <!--- ## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section. --->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/list-to-map/",
    "title": "list_to_map",
    "content": "The list_to_map processor converts a list of objects from an event, where each object contains a key field, into a map of target keys.\nConfiguration\nThe following table describes the configuration options used to generate target keys for the mappings. Option Required Type Description key Yes\nString\nThe key of the fields to be extracted as keys in the generated mappings. source Yes\nString\nThe list of objects with key fields to be converted into keys for the generated map. target No\nString\nThe target for the generated map. When not specified, the generated map will be placed in the root node. value_key No\nString\nWhen specified, values given a value_key in objects contained in the source list will be extracted and converted into the value specified by this option based on the generated map. When not specified, objects contained in the source list retain their original value when mapped. flatten No\nBoolean\nWhen true, values in the generated map output flatten into single items based on the flattened_element. Otherwise, objects mapped to values from the generated map appear as lists. flattened_element Conditionally\nString\nThe element to keep, either first or last, when flatten is set to true. Usage\nThe following example shows how to test the usage of the list_to_map processor before using the processor on your own source.\nCreate a source file named logs_json.log. Because the file source reads each line in the.log file as an event, the object list appears as one line even though it contains multiple objects: { \"mylist\":[{ \"name\": \"a\", \"value\": \"val-a\" },{ \"name\": \"b\", \"value\": \"val-b1\" },{ \"name\": \"b\", \"value\": \"val-b2\" },{ \"name\": \"c\", \"value\": \"val-c\" }]} copy Next, create a pipeline.yaml file that uses the logs_json.log file as the source by pointing to the.log file’s correct path: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - list_to_map: key: \" name\" source: \" mylist\" value_key: \" value\" flatten: true sink: - stdout: copy Run the pipeline. If successful, the processor returns the generated map with objects mapped according to their value_key. Similar to the original source, which contains one line and therefore one event, the processor returns the following JSON as one line. For readability, the following example and all subsequent JSON examples have been adjusted to span multiple lines: { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" }], \"a\": \"val-a\", \"b\": \"val-b1\", \"c\": \"val-c\" } Example: Maps set to target The following example pipeline.yaml file shows the list_to_map processor when set to a specified target, mymap: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - list_to_map: key: \" name\" source: \" mylist\" target: \" mymap\" value_key: \" value\" flatten: true sink: - stdout: copy The generated map appears under the target key: { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" }], \"mymap\": { \"a\": \"val-a\", \"b\": \"val-b1\", \"c\": \"val-c\" } } Example: No value_key specified\nThe follow example pipeline.yaml file shows the list_to_map processor with no value_key specified. Because key is set to name, the processor extracts the object names to use as keys in the map. pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - list_to_map: key: \" name\" source: \" mylist\" flatten: true sink: - stdout: copy The values from the generated map appear as original objects from the.log source, as shown in the following example response: { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" }], \"a\": { \"name\": \"a\", \"value\": \"val-a\" }, \"b\": { \"name\": \"b\", \"value\": \"val-b1\" }, \"c\": { \"name\": \"c\", \"value\": \"val-c\" } } Example: flattened_element set to last The following example pipeline.yaml file sets the flattened_element to last, therefore flattening the processor output based on each value’s last element: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - list_to_map: key: \" name\" source: \" mylist\" target: \" mymap\" value_key: \" value\" flatten: true flattened_element: \" last\" sink: - stdout: copy The processor maps object b to value val-b2 because val-b2 is the last element in object b, as shown in the following output: { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" }], \"a\": \"val-a\", \"b\": \"val-b2\", \"c\": \"val-c\" } Example: flatten set to false\nThe following example pipeline.yaml file sets flatten to false, causing the processor to output values from the generated map as a list: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - list_to_map: key: \" name\" source: \" mylist\" target: \" mymap\" value_key: \" value\" flatten: false sink: - stdout: copy Some objects in the response may have more than one element in their values, as shown in the following response: { \"mylist\": [ { \"name\": \"a\", \"value\": \"val-a\" }, { \"name\": \"b\", \"value\": \"val-b1\" }, { \"name\": \"b\", \"value\": \"val-b2\" }, { \"name\": \"c\", \"value\": \"val-c\" }], \"a\": [ \"val-a\"], \"b\": [ \"val-b1\", \"val-b2\"], \"c\": [ \"val-c\"] }",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/lowercase-string/",
    "title": "lowercase_string",
    "content": "The lowercase_string processor converts a string to its lowercase counterpart and is a mutate string processor. The following table describes options for configuring the lowercase_string processor to convert strings to a lowercase format. Option Required Type Description with_keys\nYes\nList\nA list of keys to convert to lowercase. <!---## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section.--->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/mutate-event/",
    "title": "Mutate event",
    "content": "Mutate event processors allow you to modify events in Data Prepper. The following processors are available: add_entries allows you to add entries to an event. copy_values allows you to copy values within an event. delete_entries allows you to delete entries from an event. rename_keys allows you to rename keys in an event. convert_entry_type allows you to convert value types in an event. list_to_map allows you to convert list of objects from an event where each object contains a key field into a map of target keys.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/mutate-string/",
    "title": "Mutate string",
    "content": "You can change the way that a string appears by using a mutate string processesor. For example, you can use the uppercase_string processor to convert a string to uppercase, and you can use the lowercase_string processor to convert a string to lowercase. The following is a list of processors that allow you to mutate a string: substitute_string split_string uppercase_string lowercase_string trim_string substitute_string\nThe substitute_string processor matches a key’s value against a regular expression (regex) and replaces all returned matches with a replacement string.\nConfiguration\nYou can configure the substitute_string processor with the following options. Option Required Description entries Yes\nA list of entries to add to an event. source Yes\nThe key to be modified. from Yes\nThe regex string to be replaced. Special regex characters such as [ and] must be escaped using \\\\ when using double quotes and \\ when using single quotes. For more information, see Class Pattern in the Java documentation. to Yes\nThe string that replaces each match of from. Usage\nTo get started, create the following pipeline.yaml file: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - substitute_string: entries: - source: \" message\" from: \":\" to: \" -\" sink: - stdout: copy Next, create a log file named logs_json.log. After that, replace the path of the file source in your pipeline.yaml file with your file path. For more detailed information, see Configuring Data Prepper.\nBefore you run Data Prepper, the source appears in the following format: { \"message\": \"ab:cd:ab:cd\" } After you run Data Prepper, the source is converted to the following format: { \"message\": \"ab-cd-ab-cd\" } from defines which string is replaced, and to defines the string that replaces the from string. In the preceding example, string ab:cd:ab:cd becomes ab-cd-ab-cd. If the from regex string does not return a match, the key is returned without any changes.\nsplit_string\nThe split_string processor splits a field into an array using a delimiter character.\nConfiguration\nYou can configure the split_string processor with the following options. Option Required Description entries Yes\nA list of entries to add to an event. source Yes\nThe key to be split. delimiter No\nThe separator character responsible for the split. Cannot be defined at the same time as delimiter_regex. At least delimiter or delimiter_regex must be defined. delimiter_regex No\nA regex string responsible for the split. Cannot be defined at the same time as delimiter. Either delimiter or delimiter_regex must be defined. Usage\nTo get started, create the following pipeline.yaml file: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - split_string: entries: - source: \" message\" delimiter: \",\" sink: - stdout: copy Next, create a log file named logs_json.log. After that, replace the path in the file source of your pipeline.yaml file with your file path. For more detailed information, see Configuring Data Prepper.\nBefore you run Data Prepper, the source appears in the following format: { \"message\": \"hello,world\" } After you run Data Prepper, the source is converted to the following format: { \"message\":[ \"hello\", \"world\"]} uppercase_string\nThe uppercase_string processor converts the value (a string) of a key from its current case to uppercase.\nConfiguration\nYou can configure the uppercase_string processor with the following options. Option Required Description with_keys Yes\nA list of keys to convert to uppercase. Usage\nTo get started, create the following pipeline.yaml file: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - uppercase_string: with_keys: - \" uppercaseField\" sink: - stdout: copy Next, create a log file named logs_json.log. After that, replace the path in the file source of your pipeline.yaml file with the correct file path. For more detailed information, see Configuring Data Prepper.\nBefore you run Data Prepper, the source appears in the following format: { \"uppercaseField\": \"hello\" } After you run Data Prepper, the source is converted to the following format: { \"uppercaseField\": \"HELLO\" } lowercase_string\nThe lowercase string processor converts a string to lowercase.\nConfiguration\nYou can configure the lowercase string processor with the following options. Option Required Description with_keys Yes\nA list of keys to convert to lowercase. Usage\nTo get started, create the following pipeline.yaml file: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - lowercase_string: with_keys: - \" lowercaseField\" sink: - stdout: copy Next, create a log file named logs_json.log. After that, replace the path in the file source of your pipeline.yaml file with the correct file path. For more detailed information, see Configuring Data Prepper.\nBefore you run Data Prepper, the source appears in the following format: { \"lowercaseField\": \"TESTmeSSage\" } After you run Data Prepper, the source is converted to the following format: { \"lowercaseField\": \"testmessage\" } trim_string\nThe trim_string processor removes whitespace from the beginning and end of a key.\nConfiguration\nYou can configure the trim_string processor with the following options. Option Required Description with_keys Yes\nA list of keys from which to trim the whitespace. Usage\nTo get started, create the following pipeline.yaml file: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - trim_string: with_keys: - \" trimField\" sink: - stdout: copy Next, create a log file named logs_json.log. After that, replace the path in the file source of your pipeline.yaml file with the correct file path. For more detailed information, see Configuring Data Prepper.\nBefore you run Data Prepper, the source appears in the following format: { \"trimField\": \" Space Ship \" } After you run Data Prepper, the source is converted to the following format: { \"trimField\": \"Space Ship\" }",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/otel-metrics/",
    "title": "otel_metrics",
    "content": "The otel_metrics processor serializes a collection of ExportMetricsServiceRequest records sent from the OTel metrics source into a collection of string records.\nUsage\nTo get started, add the following processor to your pipeline.yaml configuration file: processor: - otel_metrics_raw_processor: copy Configuration\nYou can use the following optional parameters to configure histogram buckets and their default values. A histogram displays numerical data by grouping data into buckets. You can use histogram buckets to view sets of events that are organized by the total event count and aggregate sum for all events. For more detailed information, see OpenTelemetry Histograms. Parameter Default value Description calculate_histogram_buckets True Whether or not to calculate histogram buckets. calculate_exponential_histogram_buckets True Whether or not to calculate exponential histogram buckets. exponential_histogram_max_allowed_scale 10 Maximum allowed scale in exponential histogram calculation. flatten_attributes False Whether or not to flatten the attributes field in the JSON data. calculate_histogram_buckets\nIf calculate_histogram_buckets is not set to false, then the following JSON file will be added to every histogram JSON. If flatten_attributes is set to false, the JSON string format of the metrics does not change the attributes field. If flatten_attributes is set to true, the values in the attributes field are placed in the parent JSON object. The default value is true. See the following JSON example: \"buckets\": [ { \"min\": 0.0, \"max\": 5.0, \"count\": 2 }, { \"min\": 5.0, \"max\": 10.0, \"count\": 5 }] You can create detailed representations of histogram buckets and their boundaries. You can control this feature by using the following parameters in your pipeline.yaml file: processor: - otel_metrics_raw_processor: calculate_histogram_buckets: true calculate_exponential_histogram_buckets: true exponential_histogram_max_allowed_scale: 10 flatten_attributes: false copy Each array element describes one bucket. Each bucket contains the lower boundary, upper boundary, and its value count. This is a specific form of more detailed OpenTelemetry representation that is a part of the JSON output created by the otel_metrics processor. See the following JSON file, which is added to each JSON histogram by the otel_metrics processor: \"explicitBounds\": [ 5.0, 10.0], \"bucketCountsList\": [ 2, 5] calculate_exponential_histogram_buckets\nIf calculate_exponential_histogram_buckets is set to true (the default setting), the following JSON values are added to each JSON histogram: \"negativeBuckets\": [ { \"min\": 0.0, \"max\": 5.0, \"count\": 2 }, { \"min\": 5.0, \"max\": 10.0, \"count\": 5 }],... \"positiveBuckets\": [ { \"min\": 0.0, \"max\": 5.0, \"count\": 2 }, { \"min\": 5.0, \"max\": 10.0, \"count\": 5 }], The following JSON file is a more detailed form of OpenTelemetry representation that consists of negative and positive buckets, a scale parameter, an offset, and a list of bucket counts: \"negative\": [ 1, 2, 3], \"positive\": [ 1, 2, 3], \"scale\": -3, \"negativeOffset\": 0, \"positiveOffset\": 1 exponential_histogram_max_allowed_scale\nThe exponential_histogram_max_allowed_scale parameter defines the maximum allowed scale for an exponential histogram. If you increase this parameter, you will increase potential memory consumption. See the OpenTelemetry specifications for more information on exponential histograms and their computational complexity.\nAll exponential histograms that have a scale that is above the configured parameter (by default, a value of 10) are discarded and logged with an error level. You can check the log that Data Prepper creates to see the ERROR log message.\nThe absolute scale value is used for comparison, so a scale of -11 that is treated equally to 11 exceeds the configured value of 10 and can be discarded.\nMetrics\nThe following table describes metrics that are common to all processors. Metric name Type Description recordsIn Counter\nMetric representing the number of ingress records. recordsOut Counter\nMetric representing the number of egress records. timeElapsed Timer\nMetric representing the time elapsed during execution of records.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/otel-trace-group/",
    "title": "otel_trace_group",
    "content": "The otel_trace_group processor completes missing trace-group-related fields in the collection of span records by looking up the OpenSearch backend. The otel_trace_group processor identifies the missing trace group information for a spanId by looking up the relevant fields in its root span stored in OpenSearch.\nOpenSearch\nWhen you connect to an OpenSearch cluster using your username and password, use the following example pipeline.yaml file to configure the otel_trace_group processor: pipeline:...\nprocessor:\n- otel_trace_group:\nhosts: [\"https://localhost:9200\"]\ncert: path/to/cert\nusername: YOUR_USERNAME_HERE\npassword: YOUR_PASSWORD_HERE See OpenSearch security for a more detailed explanation of which OpenSearch credentials and permissions are required and how to configure those credentials for the OTel trace group processor.\nAmazon OpenSearch Service\nWhen you use Amazon OpenSearch Service, use the following example pipeline.yaml file to configure the otel_trace_group processor: pipeline:...\nprocessor:\n- otel_trace_group:\nhosts: [\"https://your-amazon-opensearch-service-endpoint\"]\naws_sigv4: true\ncert: path/to/cert\ninsecure: false Configuration\nYou can configure the otel_trace_group processor with the following options. Name Description Default value hosts A list of IP addresses of OpenSearch nodes. Required.\nNo default value. cert A certificate authority (CA) certificate that is PEM encoded. Accepts both.pem or.crt. This enables the client to trust the CA that has signed the certificate that OpenSearch is using. null aws_sigv4 A Boolean flag used to sign the HTTP request with AWS credentials. Only applies to Amazon OpenSearch Service. See OpenSearch security for details. false. aws_region A string that represents the AWS Region of the Amazon OpenSearch Service domain, for example, us-west-2. Only applies to Amazon OpenSearch Service. us-east-1 aws_sts_role_arn An AWS Identity and Access Management (IAM) role that the sink plugin assumes to sign the request to Amazon OpenSearch Service. If not provided, the plugin uses the default credentials. null aws_sts_header_overrides A map of header overrides that the IAM role assumes for the sink plugin. null insecure A Boolean flag used to turn off SSL certificate verification. If set to true, CA certificate verification is turned off and insecure HTTP requests are sent. false username A string that contains the username and is used in the internal users YAML configuration file of your OpenSearch cluster. null password A string that contains the password and is used in the internal users YAML configuration file of your OpenSearch cluster. null Configuration option examples\nYou can define the configuration option values in the aws_sts_header_overrides option. See the following example: aws_sts_header_overrides:\nx-my-custom-header-1: my-custom-value-1\nx-my-custom-header-2: my-custom-value-2 Metrics\nThe following table describes custom metrics specific to the otel_trace_group processor. Metric name Type Description recordsInMissingTraceGroup Counter\nThe number of ingress records missing trace group fields. recordsOutFixedTraceGroup Counter\nThe number of egress records with successfully completed trace group fields. recordsOutMissingTraceGroup Counter\nThe number of egress records missing trace group fields.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/otel-trace-raw/",
    "title": "otel_trace",
    "content": "The otel_trace processor completes trace-group-related fields in all incoming Data Prepper span records by state caching the root span information for each tradeId.\nParameters\nThis processor includes the following parameters. traceGroup: Root span name endTime: End time of the entire trace in International Organization for Standardization (ISO) 8601 format durationInNanos: Duration of the entire trace in nanoseconds statusCode: Status code for the entire trace in nanoseconds\nConfiguration\nThe following table describes the options you can use to configure the otel_trace processor. Option Required Type Description trace_flush_interval\nNo\nInteger\nRepresents the time interval in seconds to flush all the descendant spans without any root span. Default is 180. Metrics\nThe following table describes common Abstract processor metrics. Metric name Type Description recordsIn Counter\nMetric representing the ingress of records to a pipeline component. recordsOut Counter\nMetric representing the egress of records from a pipeline component. timeElapsed Timer\nMetric representing the time elapsed during execution of a pipeline component. The otel_trace processor includes the following custom metrics: traceGroupCacheCount: The number of trace groups in the trace group cache. spanSetCount: The number of span sets in the span set collection.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/parse-json/",
    "title": "parse_json",
    "content": "The parse_json processor parses JSON data for an event, including any nested fields. The processor extracts the JSON pointer data and adds the input event to the extracted fields.\nConfiguration\nYou can configure the parse_json processor with the following options. Option Required Type Description source No\nString\nThe field in the event that will be parsed. Default value is message. destination No\nString\nThe destination field of the parsed JSON. Defaults to the root of the event. Cannot be \"\", /, or any whitespace-only string because these are not valid event fields. pointer No\nString\nA JSON pointer to the field to be parsed. There is no pointer by default, meaning the entire source is parsed. The pointer can access JSON array indexes as well. If the JSON pointer is invalid then the entire source data is parsed into the outgoing event. If the key that is pointed to already exists in the event and the destination is the root, then the pointer uses the entire path of the key. Usage\nTo get started, create the following pipeline.yaml file: parse-json-pipeline: source:....... processor: - parse_json: Basic example\nTo test the parse_json processor with the previous configuration, run the pipeline and paste the following line into your console, then enter exit on a new line: {\"outer_key\": {\"inner_key\": \"inner_value\"}} copy The parse_json processor parses the message into the following format: {\"message\": {\"outer_key\": {\"inner_key\": \"inner_value\"}}\", \"outer_key\":{\"inner_key\":\"inner_value\"}}} Example with a JSON pointer\nYou can use a JSON pointer to parse a selection of the JSON data by specifying the pointer option in the configuration. To get started, create the following pipeline.yaml file: parse-json-pipeline: source:....... processor: - parse_json: pointer: \" outer_key/inner_key\" To test the parse_json processor with the pointer option, run the pipeline, paste the following line into your console, and then enter exit on a new line: {\"outer_key\": {\"inner_key\": \"inner_value\"}} copy The processor parses the message into the following format: {\"message\": {\"outer_key\": {\"inner_key\": \"inner_value\"}}\", \"inner_key\": \"inner_value\"}",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/processors/",
    "title": "Processors",
    "content": "Processors perform an action on your data, such as filtering, transforming, or enriching.\nPrior to Data Prepper 1.3, processors were named preppers. Starting in Data Prepper 1.3, the term prepper is deprecated in favor of the term processor. Data Prepper will continue to support the term prepper until 2.0, where it will be removed.",
    "ancestors": [
      "Data Prepper",
      "Pipelines"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/rename-keys/",
    "title": "rename_keys",
    "content": "The rename_keys processor renames keys in an event.\nConfiguration\nYou can configure the rename_keys processor with the following options. Option Required Description entries Yes\nA list of event entries to rename. from_key Yes\nThe key of the entry to be renamed. to_key Yes\nThe new key of the entry. overwrite_if_to_key_exists No\nWhen set to true, the existing value is overwritten if key already exists in the event. The default value is false. Usage\nTo get started, create the following pipeline.yaml file: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - rename_keys: entries: - from_key: \" message\" to_key: \" newMessage\" overwrite_if_to_key_exists: true sink: - stdout: copy Next, create a log file named logs_json.log and replace the path in the file source of your pipeline.yaml file with that filepath. For more information, see Configuring Data Prepper.\nFor example, before you run the rename_keys processor, if the logs_json.log file contains the following event record: { \"message\": \"hello\" } When you run the rename_keys processor, it parses the message into the following “newMessage” output: { \"newMessage\": \"hello\" } If newMessage already exists, its existing value is overwritten with value.\nSpecial considerations\nRenaming operations occur in the order that the key-value pair entries are listed in the pipeline.yaml file. This means that chaining (where key-value pairs are renamed in sequence) is implicit in the rename_keys processor. See the following example pipline.yaml file: pipeline: source: file: path: \" /full/path/to/logs_json.log\" record_type: \" event\" format: \" json\" processor: - rename_keys: entries: - from_key: \" message\" to_key: \" message2\" - from_key: \" message2\" to_key: \" message3\" sink: - stdout: Add the following contents to the logs_json.log file: { \"message\": \"hello\" } copy After the rename_keys processor runs, the following output appears: { \"message3\": \"hello\" }",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/routes/",
    "title": "routes",
    "content": "Routes define conditions that can be used in sinks for conditional routing. Routes are specified at the same level as processors and sinks under the name route and consist of a list of key-value pairs, where the key is the name of a route and the value is a Data Prepper expression representing the routing condition.\n<!---## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section.--->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/service-map-stateful/",
    "title": "service_map",
    "content": "The service_map processor uses OpenTelemetry data to create a distributed service map for visualization in OpenSearch Dashboards.\nConfiguration\nThe following table describes the option you can use to configure the service_map processor. Option Required Type Description window_duration\nNo\nInteger\nRepresents the fixed time window, in seconds, during which service map relationships are evaluated. Default value is 180. <!---## Configuration\nContent will be added to this section.--->\nMetrics\nThe following table describes common Abstract processor metrics. Metric name Type Description recordsIn Counter\nMetric representing the ingress of records to a pipeline component. recordsOut Counter\nMetric representing the egress of records from a pipeline component. timeElapsed Timer\nMetric representing the time elapsed during execution of a pipeline component. The service-map-stateful processor includes following custom metrics: traceGroupCacheCount: The number of trace groups in the trace group cache. spanSetCount: The number of span sets in the span set collection.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/split-string/",
    "title": "split_string",
    "content": "The split_string processor splits a field into an array using a delimiting character and is a mutate string processor. The following table describes the options you can use to configure the split_string processor. Option Required Type Description entries\nYes\nList\nList of entries. Valid values are source, delimiter, and delimiter_regex.\nsource\nN/A\nN/A\nThe key to split.\ndelimiter\nNo\nN/A\nThe separator character responsible for the split. Cannot be defined at the same time as delimiter_regex. At least delimiter or delimiter_regex must be defined.\ndelimiter_regex\nNo\nN/A\nThe regex string responsible for the split. Cannot be defined at the same time as delimiter. At least delimiter or delimiter_regex must be defined. <!---## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section.--->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/string-converter/",
    "title": "string_converter",
    "content": "The string_converter processor converts a string to uppercase or lowercase. You can use it as an example for developing your own processor. The following table describes the option you can use to configure the string_converter processor. Option Required Type Description upper_case\nNo\nBoolean\nWhether to convert to uppercase ( true) or lowercase ( false). <!---## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section.--->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/substitute-string/",
    "title": "substitute_string",
    "content": "The substitute_string processor matches a key’s value against a regular expression and replaces all matches with a replacement string. substitute_string is a mutate string processor.\nConfiguration\nThe following table describes the options you can use to configure the substitue_string processor. Option Required Type Description entries\nYes\nList\nList of entries. Valid values are source, from, and to.\nsource\nN/A\nN/A\nThe key to modify.\nfrom\nN/A\nN/A\nThe Regex String to be replaced. Special regex characters such as [ and] must be escaped using \\\\ when using double quotes and \\ when using single quotes. See Java Patterns for more information.\nto\nN/A\nN/A\nThe String to be substituted for each match of from. <!---## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section.--->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/trace-peer-forwarder/",
    "title": "trace_peer_forwarder",
    "content": "The trace_peer_forwarder processor is used with peer forwarder to reduce by half the number of events forwarded in a Trace Analytics pipeline. In Trace Analytics, each event is typically duplicated when it is sent from otel-trace-pipeline to raw-pipeline and service-map-pipeline. When pipelines forward events, this causes the core peer forwarder to send multiple HTTP requests for the same event. You can use trace peer forwarder to forward an event once through the otel-trace-pipeline instead of raw-pipeline and service-map-pipeline, which prevents unnecessary HTTP requests.\nYou should use trace_peer_forwarder for Trace Analytics pipelines when you have multiple nodes.\nUsage\nTo get started with trace_peer_forwarder, first configure peer forwarder. Then create a pipeline.yaml file and specify trace peer forwarder as the processor. You can configure peer forwarder in your data-prepper-config.yaml file. For more detailed information, see Configuring Data Prepper.\nSee the following example pipeline.yaml file: otel-trace-pipeline: delay: \" 100\" source: otel_trace_source: processor: - trace_peer_forwarder: sink: - pipeline: name: \" raw-pipeline\" - pipeline: name: \" service-map-pipeline\" raw-pipeline: source: pipeline: name: \" entry-pipeline\" processor: - otel_trace_raw: sink: - opensearch: service-map-pipeline: delay: \" 100\" source: pipeline: name: \" entry-pipeline\" processor: - service_map_stateful: sink: - opensearch: In the preceding pipeline.yaml file, events are forwarded in the otel-trace-pipeline to the target peer, and no forwarding is performed in raw-pipeline or service-map-pipeline. This process helps improve network performance by forwarding events (as HTTP requests) once instead of twice.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/trim-string/",
    "title": "trim_string",
    "content": "The trim_string processor removes whitespace from the beginning and end of a key and is a mutate string processor. The following table describes the option you can use to configure the trim_string processor. Option Required Type Description with_keys\nYes\nList\nA list of keys to trim the whitespace from. <!---## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section.--->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/processors/uppercase-string/",
    "title": "uppercase_string",
    "content": "The uppercase_string processor converts an entire string to uppercase and is a mutate string processor. The following table describes the option you can use to configure the uppercase_string processor. Option Required Type Description with_keys\nYes\nList\nA list of keys to convert to uppercase. <!---## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section.--->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Processors"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sinks/file/",
    "title": "file sink",
    "content": "Overview\nYou can use the file sink to create a flat file output. The following table describes options you can configure for the file sink. Option Required Type Description path\nYes\nString\nPath for the output file (e.g. logs/my-transformed-log.log). <!--- ## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section. --->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Sinks"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sinks/opensearch/",
    "title": "OpenSearch sink",
    "content": "You can use the opensearch sink plugin to send data to an OpenSearch cluster, a legacy Elasticsearch cluster, or an Amazon OpenSearch Service domain.\nThe plugin supports OpenSearch 1.0 and later and Elasticsearch 7.3 and later.\nUsage\nTo configure an opensearch sink, specify the opensearch option within the pipeline configuration: pipeline:... sink: opensearch: hosts: [ \" https://localhost:9200\"] cert: path/to/cert username: YOUR_USERNAME password: YOUR_PASSWORD index_type: trace-analytics-raw dlq_file: /your/local/dlq-file max_retries: 20 bulk_size: 4 To configure an Amazon OpenSearch Service sink, specify the domain endpoint as the hosts option: pipeline:... sink: opensearch: hosts: [ \" https://your-amazon-opensearch-service-endpoint\"] aws_sigv4: true cert: path/to/cert insecure: false index_type: trace-analytics-service-map bulk_size: 4 Configuration options\nThe following table describes options you can configure for the opensearch sink. Option Required Type Description hosts\nYes\nList\nList of OpenSearch hosts to write to (for example, [\"https://localhost:9200\", \"https://remote-cluster:9200\"]).\ncert\nNo\nString\nPath to the security certificate (for example, \"config/root-ca.pem\") if the cluster uses the OpenSearch Security plugin.\nusername\nNo\nString\nUsername for HTTP basic authentication.\npassword\nNo\nString\nPassword for HTTP basic authentication.\naws_sigv4\nNo\nBoolean\nDefault value is false. Whether to use AWS Identity and Access Management (IAM) signing to connect to an Amazon OpenSearch Service domain. For your access key, secret key, and optional session token, Data Prepper uses the default credential chain (environment variables, Java system properties, ~/.aws/credential, etc.).\naws_region\nNo\nString\nThe AWS region (for example, \"us-east-1\") for the domain if you are connecting to Amazon OpenSearch Service.\naws_sts_role_arn\nNo\nString\nIAM role that the plugin uses to sign requests sent to Amazon OpenSearch Service. If this information is not provided, the plugin uses the default credentials. max_retries No\nInteger\nThe maximum number of times the OpenSearch sink should try to push data to the OpenSearch server before considering it to be a failure. Defaults to Integer.MAX_VALUE. If not provided, the sink will try to push data to the OpenSearch server indefinitely because the default value is high and exponential backoff would increase the waiting time before retry.\nsocket_timeout\nNo\nInteger\nThe timeout, in milliseconds, waiting for data to return (or the maximum period of inactivity between two consecutive data packets). A timeout value of zero is interpreted as an infinite timeout. If this timeout value is negative or not set, the underlying Apache HttpClient would rely on operating system settings for managing socket timeouts.\nconnect_timeout\nNo\nInteger\nThe timeout in milliseconds used when requesting a connection from the connection manager. A timeout value of zero is interpreted as an infinite timeout. If this timeout value is negative or not set, the underlying Apache HttpClient would rely on operating system settings for managing connection timeouts.\ninsecure\nNo\nBoolean\nWhether or not to verify SSL certificates. If set to true, certificate authority (CA) certificate verification is disabled and insecure HTTP requests are sent instead. Default value is false.\nproxy\nNo\nString\nThe address of a forward HTTP proxy server. The format is “&lt;host name or IP&gt;:&lt;port&gt;”. Examples: “example.com:8100”, “http://example.com:8100”, “112.112.112.112:8100”. Port number cannot be omitted.\nindex\nConditionally\nString\nName of the export index. Applicable and required only when the index_type is custom.\nindex_type\nNo\nString\nThis index type tells the Sink plugin what type of data it is handling. Valid values: custom, trace-analytics-raw, trace-analytics-service-map, management-disabled. Default value is custom.\ntemplate_file\nNo\nString\nPath to a JSON index template file (for example, /your/local/template-file.json) if index_type is custom. See otel-v1-apm-span-index-template.json for an example.\ndocument_id_field\nNo\nString\nThe field from the source data to use for the OpenSearch document ID (for example, \"my-field\") if index_type is custom.\ndlq_file\nNo\nString\nThe path to your preferred dead letter queue file (for example, /your/local/dlq-file). Data Prepper writes to this file when it fails to index a document on the OpenSearch cluster.\ndlq\nNo\nN/A\nDLQ configurations. See Dead Letter Queues for details. If the dlq_file option is also available, the sink will fail.\nbulk_size\nNo\nInteger (long)\nThe maximum size (in MiB) of bulk requests sent to the OpenSearch cluster. Values below 0 indicate an unlimited size. If a single document exceeds the maximum bulk request size, Data Prepper sends it individually. Default value is 5.\nism_policy_file\nNo\nString\nThe absolute file path for an ISM (Index State Management) policy JSON file. This policy file is effective only when there is no built-in policy file for the index type. For example, custom index type is currently the only one without a built-in policy file, thus it would use the policy file here if it’s provided through this parameter. For more information, see ISM policies.\nnumber_of_shards\nNo\nInteger\nThe number of primary shards that an index should have on the destination OpenSearch server. This parameter is effective only when template_file is either explicitly provided in Sink configuration or built-in. If this parameter is set, it would override the value in index template file. For more information, see Create index.\nnumber_of_replicas\nNo\nInteger\nThe number of replica shards each primary shard should have on the destination OpenSearch server. For example, if you have 4 primary shards and set number_of_replicas to 3, the index has 12 replica shards. This parameter is effective only when template_file is either explicitly provided in Sink configuration or built-in. If this parameter is set, it would override the value in index template file. For more information, see Create index. Configure max_retries\nYou can include the max_retries option in your pipeline configuration to control the number of times the source tries to write to sinks with exponential backoff. If you don’t include this option, pipelines keep retrying forever.\nIf you specify max_retries and a pipeline has a dead-letter queue (DLQ) configured, the pipeline will keep trying to write to sinks until it reaches the maximum number of retries, at which point it starts to send failed data to the DLQ.\nIf you don’t specify max_retries, only data that is rejected by sinks is written to the DLQ. Pipelines continue to try to write all other data to the sinks.\nOpenSearch cluster security\nIn order to send data to an OpenSearch cluster using the opensearch sink plugin, you must specify your username and password within the pipeline configuration. The following example pipelines.yaml file demonstrates how to specify admin security credentials: sink: - opensearch: username: \" admin\" password: \" admin\"... Alternately, rather than admin credentials, you can specify the credentials of a user mapped to a role with the minimum permissions listed in the following sections.\nCluster permissions cluster_all indices:admin/template/get indices:admin/template/put Index permissions\nIndex: otel-v1*; Index permission: indices_all Index:.opendistro-ism-config; Index permission: indices_all Index: *; Index permission: manage_aliases For instructions on how to map users to roles, see Map users to roles.\nAmazon OpenSearch Service domain security\nThe opensearch sink plugin can send data to an Amazon OpenSearch Service domain, which uses IAM for security. The plugin uses the default credential chain. Run aws configure using the AWS Command Line Interface (AWS CLI) to set your credentials.\nMake sure the credentials that you configure have the required IAM permissions. The following domain access policy demonstrates the minimum required permissions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::&lt;AccountId&gt;:user/data-prepper-user\" }, \"Action\": \"es:ESHttp*\", \"Resource\": [ \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/otel-v1*\", \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/_template/otel-v1*\", \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/_plugins/_ism/policies/raw-span-policy\", \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/_alias/otel-v1*\", \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/_alias/_bulk\"] }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::&lt;AccountId&gt;:user/data-prepper-user\" }, \"Action\": \"es:ESHttpGet\", \"Resource\": \"arn:aws:es:us-east-1:&lt;AccountId&gt;:domain/&lt;domain-name&gt;/_cluster/settings\" }] } For instructions on how to configure the domain access policy, see Resource-based policies in the Amazon OpenSearch Service documentation.\nFine-grained access control\nIf your OpenSearch Service domain uses fine-grained access control, the opensearch sink plugin requires some additional configuration.\nIAM ARN as master user\nIf you’re using an IAM Amazon Resource Name (ARN) as the master user, include the aws_sigv4 option in your sink configuration:... sink: opensearch: hosts: [ \" https://your-fgac-amazon-opensearch-service-endpoint\"] aws_sigv4: true Run aws configure using the AWS CLI to use the master IAM user credentials. If you don’t want to use the master user, you can specify a different IAM role using the aws_sts_role_arn option. The plugin will then use this role to sign requests sent to the domain sink. The ARN that you specify must be included in the domain access policy.\nMaster user in the internal user database\nIf your domain uses a master user in the internal user database, specify the master username and password as well as the aws_sigv4 option: sink: opensearch: hosts: [ \" https://your-fgac-amazon-opensearch-service-endpoint\"] aws_sigv4: false username: \" master-username\" password: \" master-password\" For more information, see Recommended configurations in the Amazon OpenSearch Service documentation. Note: You can create a new IAM role or internal user database user with the all_access permission and use it instead of the master user.\nOpenSearch Serverless collection security\nThe opensearch sink plugin can send data to an Amazon OpenSearch Serverless collection.\nOpenSearch Serverless collection sinks have the following limitations:\nYou can’t write to a collection that uses virtual private cloud (VPC) access. The collection must be accessible from public networks.\nThe OTel trace group processor doesn’t currently support collection sinks.\nCreating a pipeline role\nFirst, create an IAM role that the pipeline will assume in order to write to the collection. The role must have the following minimum permissions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"aoss:BatchGetCollection\"], \"Resource\": \"*\" }] } The role must have the following trust relationship, which allows the pipeline to assume it: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::&lt;AccountId&gt;:root\" }, \"Action\": \"sts:AssumeRole\" }] } Creating a collection\nNext, create a collection with the following settings:\nPublic network access to both the OpenSearch endpoint and OpenSearch Dashboards.\nThe following data access policy, which grants the required permissions to the pipeline role: [ { \"Rules\":[ { \"Resource\":[ \"index/collection-name/*\"], \"Permission\":[ \"aoss:CreateIndex\", \"aoss:UpdateIndex\", \"aoss:DescribeIndex\", \"aoss:WriteDocument\"], \"ResourceType\": \"index\" }], \"Principal\":[ \"arn:aws:iam::&lt;AccountId&gt;:role/PipelineRole\"], \"Description\": \"Pipeline role access\" }] Important: Make sure to replace the ARN in the Principal element with the ARN of the pipeline role that you created in the preceding step.\nFor instructions on how to create collections, see Creating collections in the Amazon OpenSearch Service documentation.\nCreating a pipeline\nWithin your pipelines.yaml file, specify the OpenSearch Serverless collection endpoint as the hosts option. In addition, you must set the serverless option to true. Specify the pipeline role in the sts_role_arn option: log-pipeline: source: http: processor: - date: from_time_received: true destination: \" @timestamp\" sink: - opensearch: hosts: [ \" https://&lt;serverless-public-collection-endpoint&gt;\"] index: \" my-serverless-index\" aws: serverless: true sts_role_arn: \" arn:aws:iam::&lt;AccountId&gt;:role/PipelineRole\" region: \" us-east-1\"",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Sinks"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sinks/pipeline/",
    "title": "Pipeline sink",
    "content": "Overview\nYou can use the pipeline sink to write to another pipeline. Option Required Type Description name\nYes\nString\nName of the pipeline to write to. <!--- ## Configuration\nContent will be added to this section.\n## Metrics\nContent will be added to this section. --->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Sinks"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sinks/sinks/",
    "title": "Sinks",
    "content": "Sinks define where Data Prepper writes your data to.\nGeneral options for all sink types\nThe following table describes options you can use to configure the sinks sink. Option Required Type Description routes\nNo\nList\nList of routes that the sink accepts. If not specified, the sink accepts all upstream events.",
    "ancestors": [
      "Data Prepper",
      "Pipelines"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sinks/stdout/",
    "title": "stdout sink",
    "content": "Overview\nYou can use the stdout sink for console output and testing. It has no configurable options.\n<!---\n## Metrics\nContent will be added to this section. --->",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Sinks"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sources/http-source/",
    "title": "http_source",
    "content": "http_source is a source plugin that supports HTTP. Currently, http_source only supports the JSON UTF-8 codec for incoming requests, such as [{\"key1\": \"value1\"}, {\"key2\": \"value2\"}]. The following table describes options you can use to configure the http_source source. Option Required Type Description port\nNo\nInteger\nThe port that the source is running on. Default value is 2021. Valid options are between 0 and 65535.\nhealth_check_service\nNo\nBoolean\nEnables the health check service on the /health endpoint on the defined port. Default value is false.\nunauthenticated_health_check\nNo\nBoolean\nDetermines whether or not authentication is required on the health check endpoint. Data Prepper ignores this option if no authentication is defined. Default value is false.\nrequest_timeout\nNo\nInteger\nThe request timeout, in milliseconds. Default value is 10000.\nthread_count\nNo\nInteger\nThe number of threads to keep in the ScheduledThreadPool. Default value is 200.\nmax_connection_count\nNo\nInteger\nThe maximum allowed number of open connections. Default value is 500.\nmax_pending_requests\nNo\nInteger\nThe maximum allowed number of tasks in the ScheduledThreadPool work queue. Default value is 1024.\nauthentication\nNo\nObject\nAn authentication configuration. By default, this creates an unauthenticated server for the pipeline. This uses pluggable authentication for HTTPS. To use basic authentication define the http_basic plugin with a username and password. To provide customer authentication, use or create a plugin that implements ArmeriaHttpAuthenticationProvider.\nssl\nNo\nBoolean\nEnables TLS/SSL. Default value is false.\nssl_certificate_file\nConditionally\nString\nSSL certificate chain file path or Amazon Simple Storage Service (Amazon S3) path. Amazon S3 path example s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is set to true and use_acm_certificate_for_ssl is set to false.\nssl_key_file\nConditionally\nString\nSSL key file path or Amazon S3 path. Amazon S3 path example s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is set to true and use_acm_certificate_for_ssl is set to false.\nuse_acm_certificate_for_ssl\nNo\nBoolean\nEnables a TLS/SSL using certificate and private key from AWS Certificate Manager (ACM). Default value is false.\nacm_certificate_arn\nConditionally\nString\nThe ACM certificate Amazon Resource Name (ARN). The ACM certificate takes preference over Amazon S3 or a local file system certificate. Required if use_acm_certificate_for_ssl is set to true.\nacm_private_key_password\nNo\nString\nACM private key password that decrypts the private key. If not provided, Data Prepper generates a random password.\nacm_certificate_timeout_millis\nNo\nInteger\nTimeout, in milliseconds, that ACM takes to get certificates. Default value is 120000.\naws_region\nConditionally\nString\nAWS region used by ACM or Amazon S3. Required if use_acm_certificate_for_ssl is set to true or ssl_certificate_file and ssl_key_file is the Amazon S3 path. <!--- ## Configuration\nContent will be added to this section.--->\nMetrics\nThe http_source source includes the following metrics.\nCounters requestsReceived: Measures the total number of requests received by the /log/ingest endpoint. requestsRejected: Measures the total number of requests rejected (429 response status code) by the HTTP Source plugin. successRequests: Measures the total number of requests successfully processed (200 response status code) the by HTTP Source plugin. badRequests: Measures the total number of requests with either an invalid content type or format processed by the HTTP Source plugin (400 response status code). requestTimeouts: Measures the total number of requests that time out in the HTTP source server (415 response status code). requestsTooLarge: Measures the total number of requests where the size of the event is larger than the buffer capacity (413 response status code). internalServerError: Measures the total number of requests processed by the HTTP Source with a custom exception type (500 response status code).\nTimers requestProcessDuration: Measures the latency of requests processed by the HTTP Source plugin in seconds.\nDistribution summaries payloadSize: Measures the incoming request payload size in bytes.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Sources"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sources/otel-logs-source/",
    "title": "otel_logs_source",
    "content": "The otel_logs_source source is an OpenTelemetry source that follows the OpenTelemetry Protocol Specification and receives logs from the OTel Collector in the form of ExportLogsServiceRequest records.\nThis source supports the OTLP/gRPC protocol.\nConfiguration\nYou can configure the otel_logs_source source with the following options. Option Type Description port\nint\nRepresents the port that the otel_logs_source source is running on. Default value is 21892.\npath\nstring\nRepresents the path for sending unframed HTTP requests. You can use this option to support an unframed gRPC request with an HTTP idiomatic path to a configurable path. The path should start with /, and its length should be at least 1. The /opentelemetry.proto.collector.logs.v1.LogsService/Export endpoint is disabled for both gRPC and HTTP requests if the path is configured. The path can contain a ${pipelineName} placeholder, which is replaced with the pipeline name. If the value is empty and unframed_requests is true, then the path that the source provides is /opentelemetry.proto.collector.logs.v1.LogsService/Export.\nrequest_timeout\nint\nRepresents the request timeout duration in milliseconds. Default value is 10000.\nhealth_check_service\nBoolean\nEnables the gRPC health check service under grpc.health.v1/Health/Check. Default value is false.\nproto_reflection_service\nBoolean\nEnables a reflection service for Protobuf services (see ProtoReflectionService and gRPC reflection). Default value is false.\nunframed_requests\nBoolean\nEnables requests that are not framed using the gRPC wire protocol. Default value is false.\nthread_count\nint\nThe number of threads to keep in the ScheduledThreadPool. Default value is 500.\nmax_connection_count\nint\nThe maximum number of open connections allowed. Default value is 500. SSL\nYou can configure SSL in the otel_logs_source source with the following options. Option Type Description ssl\nBoolean\nEnables TLS/SSL. Default value is true.\nsslKeyCertChainFile\nstring\nRepresents the SSL certificate chain file path or Amazon Simple Storage Service (Amazon S3) path. For example, see the Amazon S3 path s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is set to true.\nsslKeyFile\nstring\nRepresents the SSL key file path or Amazon S3 path. For example, see the Amazon S3 path s3://&lt;bucketName&gt;/&lt;path&gt;. Required if ssl is set to true.\nuseAcmCertForSSL\nBoolean\nEnables TLS/SSL using a certificate and private key from AWS Certificate Manager (ACM). Default value is false.\nacmCertificateArn\nstring\nRepresents the ACM certificate Amazon Resource Name (ARN). ACM certificates take precedence over Amazon S3 or local file system certificates. Required if useAcmCertForSSL is set to true.\nawsRegion\nstring\nRepresents the AWS Region used by ACM or Amazon S3. Required if useAcmCertForSSL is set to true or sslKeyCertChainFile or sslKeyFile is the Amazon S3 path. Usage\nTo get started, create a pipeline.yaml file and add otel_logs_source as the source: source:\n- otel_logs_source: Metrics\nYou can use the following metrics with the otel_logs_source source. Option Type Description requestTimeouts Counter\nMeasures the total number of requests that time out. requestsReceived Counter\nMeasures the total number of requests received by the otel_logs_source source. badRequests Counter\nMeasures the total number of requests that could not be parsed. requestsTooLarge Counter\nMeasures the total number of requests that exceed the maximum allowed size. Indicates that the size of the data being written into the buffer is beyond the buffer’s maximum capacity. internalServerError Counter\nMeasures the total number of requests that are erroneous due to errors other than requestTimeouts or requestsTooLarge. successRequests Counter\nMeasures the total number of requests successfully written to the buffer. payloadSize Distribution summary\nMeasures the distribution of all incoming payload sizes. requestProcessDuration Timer\nMeasures the duration of request processing.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Sources"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sources/otel-metrics-source/",
    "title": "otel_metrics_source",
    "content": "otel_metrics_source is an OpenTelemetry Collector source that collects metric data. The following table describes options you can use to configure the otel_metrics_source source. Option Required Type Description port\nNo\nInteger\nThe port that the OpenTelemtry metrics source runs on. Default value is 21891.\nrequest_timeout\nNo\nInteger\nThe request timeout, in milliseconds. Default value is 10000.\nhealth_check_service\nNo\nBoolean\nEnables a gRPC health check service under grpc.health.v1/Health/Check. Default value is false.\nproto_reflection_service\nNo\nBoolean\nEnables a reflection service for Protobuf services (see gRPC reflection and gRPC Server Reflection Tutorial docs). Default value is false.\nunframed_requests\nNo\nBoolean\nEnables requests not framed using the gRPC wire protocol.\nthread_count\nNo\nInteger\nThe number of threads to keep in the ScheduledThreadPool. Default value is 200.\nmax_connection_count\nNo\nInteger\nThe maximum allowed number of open connections. Default value is 500.\nssl\nNo\nBoolean\nEnables connections to the OpenTelemetry source port over TLS/SSL. Default value is true.\nsslKeyCertChainFile\nConditionally\nString\nFile-system path or Amazon Simple Storage Service (Amazon S3) path to the security certificate (for example, \"config/demo-data-prepper.crt\" or \"s3://my-secrets-bucket/demo-data-prepper.crt\"). Required if ssl is set to true.\nsslKeyFile\nConditionally\nString\nFile-system path or Amazon S3 path to the security key (for example, \"config/demo-data-prepper.key\" or \"s3://my-secrets-bucket/demo-data-prepper.key\"). Required if ssl is set to true.\nuseAcmCertForSSL\nNo\nBoolean\nWhether to enable TLS/SSL using a certificate and private key from AWS Certificate Manager (ACM). Default value is false.\nacmCertificateArn\nConditionally\nString\nRepresents the ACM certificate ARN. ACM certificate take preference over S3 or local file system certificates. Required if useAcmCertForSSL is set to true.\nawsRegion\nConditionally\nString\nRepresents the AWS Region used by ACM or Amazon S3. Required if useAcmCertForSSL is set to true or sslKeyCertChainFile and sslKeyFile is the Amazon S3 path.\nauthentication\nNo\nObject\nAn authentication configuration. By default, an unauthenticated server is created for the pipeline. This uses pluggable authentication for HTTPS. To use basic authentication, define the http_basic plugin with a username and password. To provide customer authentication, use or create a plugin that implements GrpcAuthenticationProvider. <!--- ## Configuration\nContent will be added to this section.--->\nMetrics\nThe otel_metrics_source source includes the following metrics.\nCounters requestTimeouts: Measures the total number of requests that time out. requestsReceived: Measures the total number of requests received by the OpenTelemetry metrics source.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Sources"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sources/otel-trace/",
    "title": "otel_trace_source source",
    "content": "Overview\nThe otel_trace source is a source for the OpenTelemetry Collector. The following table describes options you can use to configure the otel_trace source.\n<!--- What does otel_trace_source do? Other plugins include that in the overview section.---> Option Required Type Description port\nNo\nInteger\nThe port that the otel_trace source runs on. Default value is 21890.\nrequest_timeout\nNo\nInteger\nThe request timeout, in milliseconds. Default value is 10000.\nhealth_check_service\nNo\nBoolean\nEnables a gRPC health check service under grpc.health.v1/Health/Check. Default value is false.\nunauthenticated_health_check\nNo\nBoolean\nDetermines whether or not authentication is required on the health check endpoint. Data Prepper ignores this option if no authentication is defined. Default value is false.\nproto_reflection_service\nNo\nBoolean\nEnables a reflection service for Protobuf services (see gRPC reflection and gRPC Server Reflection Tutorial docs). Default value is false.\nunframed_requests\nNo\nBoolean\nEnable requests not framed using the gRPC wire protocol.\nthread_count\nNo\nInteger\nThe number of threads to keep in the ScheduledThreadPool. Default value is 200.\nmax_connection_count\nNo\nInteger\nThe maximum allowed number of open connections. Default value is 500.\nssl\nNo\nBoolean\nEnables connections to the OTel source port over TLS/SSL. Defaults to true.\nsslKeyCertChainFile\nConditionally\nString\nFile system path or Amazon Simple Storage Service (Amazon S3) path to the security certificate (for example, \"config/demo-data-prepper.crt\" or \"s3://my-secrets-bucket/demo-data-prepper.crt\"). Required if ssl is set to true.\nsslKeyFile\nConditionally\nString\nFile system path or Amazon S3 path to the security key (for example, \"config/demo-data-prepper.key\" or \"s3://my-secrets-bucket/demo-data-prepper.key\"). Required if ssl is set to true.\nuseAcmCertForSSL\nNo\nBoolean\nWhether to enable TLS/SSL using a certificate and private key from AWS Certificate Manager (ACM). Default value is false.\nacmCertificateArn\nConditionally\nString\nRepresents the ACM certificate ARN. ACM certificate take preference over S3 or local file system certificate. Required if useAcmCertForSSL is set to true.\nawsRegion\nConditionally\nString\nRepresents the AWS region used by ACM or Amazon S3. Required if useAcmCertForSSL is set to true or sslKeyCertChainFile and sslKeyFile are Amazon S3 paths.\nauthentication\nNo\nObject\nAn authentication configuration. By default, an unauthenticated server is created for the pipeline. This parameter uses pluggable authentication for HTTPS. To use basic authentication, define the http_basic plugin with a username and password. To provide customer authentication, use or create a plugin that implements GrpcAuthenticationProvider. <!--- ## Configuration\nContent will be added to this section.--->\nMetrics\nCounters requestTimeouts: Measures the total number of requests that time out. requestsReceived: Measures the total number of requests received by the otel_trace source. successRequests: Measures the total number of requests successfully processed by the otel_trace source plugin. badRequests: Measures the total number of requests with an invalid format processed by the otel_trace source plugin. requestsTooLarge: Measures the total number of requests whose number of spans exceeds the buffer capacity. internalServerError: Measures the total number of requests processed by the otel_trace source with a custom exception type.\nTimers requestProcessDuration: Measures the latency of requests processed by the otel_trace source plugin in seconds.\nDistribution summaries payloadSize: Measures the incoming request payload size distribution in bytes.",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Sources"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sources/s3/",
    "title": "s3 source",
    "content": "s3 is a source plugin that reads events from Amazon Simple Storage Service (Amazon S3) objects. It requires an Amazon Simple Queue Service (Amazon SQS) queue that receives S3 Event Notifications. After Amazon SQS is configured, the s3 source receives messages from Amazon SQS. When the SQS message indicates that an S3 object was created, the s3 source loads the S3 objects and then parses them using the configured codec. You can also configure the s3 source to use Amazon S3 Select instead of Data Prepper to parse S3 objects.\nIAM permissions\nIn order to use the s3 source, configure your AWS Identity and Access Management (IAM) permissions to grant Data Prepper access to Amazon S3. You can use a configuration similar to the following JSON configuration: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"s3-access\", \"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::&lt;YOUR-BUCKET&gt;/*\" }, { \"Sid\": \"sqs-access\", \"Effect\": \"Allow\", \"Action\": [ \"sqs:DeleteMessage\", \"sqs:ReceiveMessage\"], \"Resource\": \"arn:aws:sqs:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:&lt;YOUR-SQS-QUEUE&gt;\" }, { \"Sid\": \"kms-access\", \"Effect\": \"Allow\", \"Action\": \"kms:Decrypt\", \"Resource\": \"arn:aws:kms:&lt;YOUR-REGION&gt;:&lt;123456789012&gt;:key/&lt;YOUR-KMS-KEY&gt;\" }] } If your S3 objects or Amazon SQS queues do not use AWS Key Management Service (AWS KMS), remove the kms:Decrypt permission.\nConfiguration\nYou can use the following options to configure the s3 source. Option Required Type Description notification_type\nYes\nString\nMust be sqs.\ncompression\nNo\nString\nThe compression algorithm to apply: none, gzip, or automatic. Default value is none.\ncodec\nYes\nCodec\nThe codec to apply.\nsqs\nYes\nsqs\nThe SQS configuration. See sqs for details.\naws\nYes\naws\nThe AWS configuration. See aws for details.\non_error\nNo\nString\nDetermines how to handle errors in Amazon SQS. Can be either retain_messages or delete_messages. If retain_messages, then Data Prepper will leave the message in the Amazon SQS queue and try again. This is recommended for dead-letter queues. If delete_messages, then Data Prepper will delete failed messages. Default value is retain_messages.\nbuffer_timeout\nNo\nDuration\nThe amount of time allowed for for writing events to the Data Prepper buffer before timeout occurs. Any events that the Amazon S3 source cannot write to the buffer in this time will be discarded. Default value is 10 seconds.\nrecords_to_accumulate\nNo\nInteger\nThe number of messages that accumulate before writing to the buffer. Default value is 100.\nmetadata_root_key\nNo\nString\nBase key for adding S3 metadata to each Event. The metadata includes the key and bucket for each S3 object. Defaults to s3/.\ndisable_bucket_ownership_validation\nNo\nBoolean\nIf true, the S3Source will not attempt to validate that the bucket is owned by the expected account. The expected account is the same account that owns the Amazon SQS queue. Defaults to false.\nacknowledgments\nNo\nBoolean\nIf true, enables s3 sources to receive end-to-end acknowledgments when events are received by OpenSearch sinks. sqs\nThe following parameters allow you to configure usage for Amazon SQS in the s3 source plugin. Option Required Type Description queue_url\nYes\nString\nThe URL of the Amazon SQS queue from which messages are received.\nmaximum_messages\nNo\nInteger\nThe maximum number of messages to receive from the Amazon SQS queue in any single request. Default value is 10.\nvisibility_timeout\nNo\nDuration\nThe visibility timeout to apply to messages read from the Amazon SQS queue. This should be set to the amount of time that Data Prepper may take to read all the S3 objects in a batch. Default value is 30s.\nwait_time\nNo\nDuration\nThe amount of time to wait for long polling on the Amazon SQS API. Default value is 20s.\npoll_delay\nNo\nDuration\nA delay to place between reading/processing a batch of Amazon SQS messages and making a subsequent request. Default value is 0s. aws Option Required Type Description region\nNo\nString\nThe AWS Region to use for credentials. Defaults to standard SDK behavior to determine the Region.\nsts_role_arn\nNo\nString\nThe AWS Security Token Service (AWS STS) role to assume for requests to Amazon SQS and Amazon S3. Defaults to null, which will use the standard SDK behavior for credentials.\naws_sts_header_overrides\nNo\nMap\nA map of header overrides that the IAM role assumes for the sink plugin. codec\nThe codec determines how the s3 source parses each S3 object.\nnewline codec\nThe newline codec parses each single line as a single log event. This is ideal for most application logs because each event parses per single line. It can also be suitable for S3 objects that have individual JSON objects on each line, which matches well when used with the parse_json processor to parse each line.\nUse the following options to configure the newline codec. Option Required Type Description skip_lines\nNo\nInteger\nThe number of lines to skip before creating events. You can use this configuration to skip common header rows. Default is 0.\nheader_destination\nNo\nString\nA key value to assign to the header line of the S3 object. If this option is specified, then each event will contain a header_destination field. json codec\nThe json codec parses each S3 object as a single JSON object from a JSON array and then creates a Data Prepper log event for each object in the array.\ncsv codec\nThe csv codec parses objects in comma-separated value (CSV) format, with each row producing a Data Prepper log event. Use the following options to configure the csv codec. Option Required Type Description delimiter\nYes\nInteger\nThe delimiter separating columns. Default is,.\nquote_character\nYes\nString\nThe character used as a text qualifier for CSV data. Default is \".\nheader\nNo\nString list\nThe header containing the column names used to parse CSV data.\ndetect_header\nNo\nBoolean\nWhether the first line of the S3 object should be interpreted as a header. Default is true. Using s3_select with the s3 source\nWhen configuring s3_select to parse S3 objects, use the following options. Option Required Type Description expression\nYes, when using s3_select String\nThe expression used to query the object. Maps directly to the expression property.\nexpression_type\nNo\nString\nThe type of the provided expression. Default value is SQL. Maps directly to the ExpressionType.\ninput_serialization\nYes, when using s3_select String\nProvides the S3 Select file format. Amazon S3 uses this format to parse object data into records and returns only records that match the specified SQL expression. May be csv, json, or parquet.\ncompression_type\nNo\nString\nSpecifies an object’s compression format. Maps directly to the CompressionType.\ncsv\nNo csv Provides the CSV configuration for processing CSV data.\njson\nNo json Provides the JSON configuration for processing JSON data. csv Use the following options in conjunction with the csv configuration for s3_select to determine how your parsed CSV file should be formatted.\nThese options map directly to options available in the S3 Select CSVInput data type. Option Required Type Description file_header_info\nNo\nString\nDescribes the first line of input. Maps directly to the FileHeaderInfo property.\nquote_escape\nNo\nString\nA single character used for escaping the quotation mark character inside an already escaped value. Maps directly to the QuoteEscapeCharacter property.\ncomments\nNo\nString\nA single character used to indicate that a row should be ignored when the character is present at the start of that row. Maps directly to the Comments property. json Use the following option in conjunction with json for s3_select to determine how S3 Select processes the JSON file. Option Required Type Description type\nNo\nString\nThe type of JSON array. May be either DOCUMENT or LINES. Maps directly to the Type property. Metrics\nThe s3 source includes the following metrics.\nCounters s3ObjectsFailed: The number of S3 objects that the s3 source failed to read. s3ObjectsNotFound: The number of S3 objects that the s3 source failed to read due to an S3 “Not Found” error. These are also counted toward s3ObjectsFailed. s3ObjectsAccessDenied: The number of S3 objects that the s3 source failed to read due to an “Access Denied” or “Forbidden” error. These are also counted toward s3ObjectsFailed. s3ObjectsSucceeded: The number of S3 objects that the s3 source successfully read. sqsMessagesReceived: The number of Amazon SQS messages received from the queue by the s3 source. sqsMessagesDeleted: The number of Amazon SQS messages deleted from the queue by the s3 source. sqsMessagesFailed: The number of Amazon SQS messages that the s3 source failed to parse.\nTimers s3ObjectReadTimeElapsed: Measures the amount of time the s3 source takes to perform a request to GET an S3 object, parse it, and write events to the buffer. sqsMessageDelay: Measures the time elapsed from when S3 creates an object to when it is fully parsed.\nDistribution summaries s3ObjectSizeBytes: Measures the size of S3 objects as reported by the S3 Content-Length. For compressed objects, this is the compressed size. s3ObjectProcessedBytes: Measures the bytes processed by the s3 source for a given object. For compressed objects, this is the uncompressed size. s3ObjectsEvents: Measures the number of events (sometimes called records) produced by an S3 object.\nExample: Uncompressed logs\nThe following pipeline.yaml file shows the minimum configuration for reading uncompressed newline-delimited logs: source:\ns3:\nnotification_type: sqs\ncodec:\nnewline:\ncompression: none\nsqs:\nqueue_url: \"https://sqs.us-east-1.amazonaws.com/123456789012/MyQueue\"\naws:\nregion: \"us-east-1\"\nsts_role_arn: \"arn:aws:iam::123456789012:role/Data-Prepper\"",
    "ancestors": [
      "Data Prepper",
      "Pipelines",
      "Sources"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/configuration/sources/sources/",
    "title": "Sources",
    "content": "Sources define where your data comes from within a Data Prepper pipeline.",
    "ancestors": [
      "Data Prepper",
      "Pipelines"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/dlq/",
    "title": "Dead-letter queues",
    "content": "Data Prepper pipelines support dead-letter queues (DLQs) for offloading failed events and making them accessible for analysis.\nAs of Data Prepper 2.3, only the s3 source supports DLQs.\nConfigure a DLQ writer\nTo configure a DLQ writer for the s3 source, add the following to your pipeline.yaml file: sink: opensearch: dlq: s3: bucket: \" my-dlq-bucket\" key_path_prefix: \" dlq-files/\" region: \" us-west-2\" sts_role_arn: \" arn:aws:iam::123456789012:role/dlq-role\" The resulting DLQ file outputs as a JSON array of DLQ objects. Any file written to the S3 DLQ contains the following name pattern: dlq-v${version}-${pipelineName}-${pluginId}-${timestampIso8601}-${uniqueId} The following information is replaced in the name pattern: version: The Data Prepper version. pipelineName: The pipeline name indicated in pipeline.yaml. pluginId: The ID of the plugin associated with the DLQ event.\nConfiguration\nDLQ supports the following configuration options. Option Required Type Description bucket\nYes\nString\nThe name of the bucket into which the DLQ outputs failed records.\nkey_path_prefix\nNo\nString\nThe key_prefix used in the S3 bucket. Defaults to \"\". Supports time value pattern variables, such as /%{yyyy}/%{MM}/%{dd}, including any variables listed in the Java DateTimeFormatter. For example, when using the /%{yyyy}/%{MM}/%{dd} pattern, you can set key_prefix as /2023/01/24.\nregion\nNo\nString\nThe AWS Region of the S3 bucket. Defaults to us-east-1.\nsts_role_arn\nNo\nString\nThe STS role the DLQ assumes in order to write to an AWS S3 bucket. Default is null, which uses the standard SDK behavior for credentials. To use this option, the S3 bucket must have the S3:PutObject permission configured. When using DLQ with an OpenSearch sink, you can configure the max_retries option to send failed data to the DLQ when the sink reaches the maximum number of retries.\nMetrics\nDLQ supports the following metrics.\nCounter dlqS3RecordsSuccess: Measures the number of successful records sent to S3. dlqS3RecordsFailed: Measures the number of records that failed to be sent to S3. dlqS3RequestSuccess: Measures the number of successful S3 requests. dlqS3RequestFailed: Measures the number of failed S3 requests.\nDistribution summary dlqS3RequestSizeBytes: Measures the distribution of the S3 request’s payload size in bytes.\nTimer dlqS3RequestLatency: Measures latency when sending each S3 request, including retries.\nDLQ objects\nDLQ supports the following DLQ objects: pluginId: The ID of the plugin that originated the event sent to the DLQ. pluginName: The name of the plugin. failedData: An object that contains the failed object and its options. This object is unique to each plugin. pipelineName: The name of the Data Prepper pipeline in which the event failed. timestamp: The timestamp of the failures in an ISO8601 format.",
    "ancestors": [
      "Data Prepper",
      "Pipelines"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/expression-syntax/",
    "title": "Expression syntax",
    "content": "The following sections provide information about expression syntax in Data Prepper.\nSupported operators\nOperators are listed in order of precedence (top to bottom, left to right). Operator Description Associativity () Priority Expression\nleft-to-right not + - Unary Logical NOT Unary Positive Unary negative\nright-to-left &lt;, &lt;=, &gt;, &gt;= Relational Operators\nleft-to-right ==,!= Equality Operators\nleft-to-right and, or Conditional Expression\nleft-to-right Reserved for possible future functionality\nReserved symbol set: ^, *, /, %, +, -, xor, =, +=, -=, *=, /=, %=, ++, --, ${&lt;text&gt;} Set initializer\nThe set initializer defines a set or term and/or expressions.\nExamples\nThe following are examples of set initializer syntax.\nHTTP status codes {200, 201, 202} HTTP response payloads {\"Created\", \"Accepted\"} Handle multiple event types with different keys {/request_payload, /request_message} Priority expression\nA priority expression identifies an expression that will be evaluated at the highest priority level. A priority expression must contain an expression or value; empty parentheses are not supported.\nExample /is_cool == (/name == \"Steven\") Relational operators\nRelational operators are used to test the relationship of two numeric values. The operands must be numbers or JSON Pointers that resolve to numbers.\nSyntax &lt;Number | JSON Pointer&gt; &lt; &lt;Number | JSON Pointer&gt;\n&lt;Number | JSON Pointer&gt; &lt;= &lt;Number | JSON Pointer&gt;\n&lt;Number | JSON Pointer&gt; &gt; &lt;Number | JSON Pointer&gt;\n&lt;Number | JSON Pointer&gt; &gt;= &lt;Number | JSON Pointer&gt; Example /status_code &gt;= 200 and /status_code &lt; 300 Equality operators\nEquality operators are used to test whether two values are equivalent.\nSyntax &lt;Any&gt; == &lt;Any&gt;\n&lt;Any&gt;!= &lt;Any&gt; Examples /is_cool == true\n3.14!= /status_code\n{1, 2} == /event/set_property Using equality operators to check for a JSON Pointer\nEquality operators can also be used to check whether a JSON Pointer exists by comparing the value with null.\nSyntax &lt;JSON Pointer&gt; == null\n&lt;JSON Pointer&gt;!= null\nnull == &lt;JSON Pointer&gt;\nnull!= &lt;JSON Pointer&gt; Example /response == null\nnull!= /response Conditional expression\nA conditional expression is used to chain together multiple expressions and/or values.\nSyntax &lt;Any&gt; and &lt;Any&gt;\n&lt;Any&gt; or &lt;Any&gt;\nnot &lt;Any&gt; Example /status_code == 200 and /message == \"Hello world\"\n/status_code == 200 or /status_code == 202\nnot /status_code in {200, 202}\n/response == null\n/response!= null Definitions\nThis section provides expression definitions.\nLiteral\nA literal is a fundamental value that has no children:\nFloat: Supports values from 3.40282347 × 10 38 to 1.40239846 × 10 −45.\nInteger: Supports values from −2,147,483,648 to 2,147,483,647.\nBoolean: Supports true or false.\nJSON Pointer: See the JSON Pointer section for details.\nString: Supports valid Java strings.\nNull: Supports null check to see whether a JSON Pointer exists.\nExpression string\nAn expression string takes the highest priority in a Data Prepper expression and only supports one expression string resulting in a return value. An expression string is not the same as an expression.\nStatement\nA statement is the highest-priority component of an expression string.\nExpression\nAn expression is a generic component that contains a Primary or an Operator. Expressions may contain expressions. An expression’s imminent children can contain 0–1 Operators.\nPrimary Set Priority Expression Literal Operator\nAn operator is a hardcoded token that identifies the operation used in an expression.\nJSON Pointer\nA JSON Pointer is a literal used to reference a value within an event and provided as context for an expression string. JSON Pointers are identified by a leading / containing alphanumeric characters or underscores, delimited by /. JSON Pointers can use an extended character set if wrapped in double quotes ( \") using the escape character \\. Note that JSON Pointers require ~ and / characters, which should be used as part of the path and not as a delimiter that needs to be escaped.\nThe following are examples of JSON Pointers: ~0 representing ~ ~1 representing / Shorthand syntax (Regex, \\w = [A-Za-z_]) /\\w+(/\\w+)* Example of shorthand\nThe following is an example of shorthand: /Hello/World/0 Example of escaped syntax\nThe following is an example of escaped syntax: \"/&lt;Valid String Characters | Escaped Character&gt;(/&lt;Valid String Characters | Escaped Character&gt;)*\" Example of an escaped JSON Pointer\nThe following is an example of an escaped JSON Pointer: # Path\n# { \"Hello - 'world/\": [{ \"\\\"JsonPointer\\\"\": true }] }\n\"/Hello - 'world\\//0/\\\"JsonPointer\\\"\" White space\nWhite space is optional surrounding relational operators, regex equality operators, equality operators, and commas.\nWhite space is required surrounding set initializers, priority expressions, set operators, and conditional expressions. Operator Description White space required ✅ Valid examples ❌ Invalid examples {} Set initializer\nYes /status in {200} /status in{200} () Priority expression\nYes /a==(/b==200) /a in ({200}) /status in({200}) in, not in Set operators\nYes /a in {200} /a not in {400} /a in{200, 202} /a not in{400} &lt;, &lt;=, &gt;, &gt;= Relational operators\nNo /status &lt; 300 /status&gt;=300   =~,!~ Regex equality pperators\nNo /msg =~ \"^\\w*$\" /msg=~\"^\\w*$\"   ==,!= Equality operators\nNo /status == 200 /status_code==200   and, or, not Conditional operators\nYes /a&lt;300 and /b&gt;200 /b&lt;300and/b&gt;200, Set value delimiter\nNo /a in {200, 202} /a in {200,202} /a in {200, 202} /a in {200,}",
    "ancestors": [
      "Data Prepper",
      "Pipelines"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/pipelines-configuration-options/",
    "title": "Pipeline options",
    "content": "This page provides information about pipeline configuration options in Data Prepper.\nGeneral pipeline options Option Required Type Description workers\nNo\nInteger\nEssentially the number of application threads. As a starting point for your use case, try setting this value to the number of CPU cores on the machine. Default is 1.\ndelay\nNo\nInteger\nAmount of time in milliseconds workers wait between buffer read attempts. Default is 3,000.",
    "ancestors": [
      "Data Prepper",
      "Pipelines"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/data-prepper/pipelines/pipelines/",
    "title": "Pipelines",
    "content": "The following image illustrates how a pipeline works. To use Data Prepper, you define pipelines in a configuration YAML file. Each pipeline is a combination of a source, a buffer, zero or more processors, and one or more sinks. For example: simple-sample-pipeline: workers: 2 # the number of workers delay: 5000 # in milliseconds, how long workers wait between read attempts source: random: buffer: bounded_blocking: buffer_size: 1024 # max number of records the buffer accepts batch_size: 256 # max number of records the buffer drains after each read processor: - string_converter: upper_case: true sink: - stdout: Sources define where your data comes from. In this case, the source is a random UUID generator ( random).\nBuffers store data as it passes through the pipeline.\nBy default, Data Prepper uses its one and only buffer, the bounded_blocking buffer, so you can omit this section unless you developed a custom buffer or need to tune the buffer settings.\nProcessors perform some action on your data: filter, transform, enrich, etc.\nYou can have multiple processors, which run sequentially from top to bottom, not in parallel. The string_converter processor transform the strings by making them uppercase.\nSinks define where your data goes. In this case, the sink is stdout.\nStarting from Data Prepper 2.0, you can define pipelines across multiple configuration YAML files, where each file contains the configuration for one or more pipelines. This gives you more freedom to organize and chain complex pipeline configurations. For Data Prepper to load your pipeline configuration properly, place your configuration YAML files in the pipelines folder under your application’s home directory (e.g. /usr/share/data-prepper).\nEnd-to-end acknowledgments\nData Prepper ensures the durability and reliability of data written from sources and delivered to sinks through end-to-end (E2E) acknowledgments. An E2E acknowledgment begins at the source, which monitors a batch of events set inside pipelines and waits for a positive acknowledgment when those events are successfully pushed to sinks. When a pipeline contains multiple sinks, including sinks set as additional Data Prepper pipelines, the E2E acknowledgment sends when events are received by the final sink in a pipeline chain.\nAlternatively, the source sends a negative acknowledgment when an event cannot be delivered to a sink for any reason.\nWhen any component of a pipeline fails and is unable to send an event, the source receives no acknowledgment. In the case of a failure, the pipeline’s source times out. This gives you the ability to take any necessary actions to address the source failure, including rerunning the pipeline or logging the failure.\nAs of Data Prepper 2.2, only the s3 source and opensearch sink support E2E acknowledgments.\nConditional routing\nPipelines also support conditional routing which allows you to route Events to different sinks based on specific conditions. To add conditional routing to a pipeline, specify a list of named routes under the route component and add specific routes to sinks under the routes property. Any sink with the routes property will only accept Events that match at least one of the routing conditions.\nIn the following example, application-logs is a named route with a condition set to /log_type == \"application\". The route uses Data Prepper expressions to define the conditions. Data Prepper only routes events that satisfy the condition to the first OpenSearch sink. By default, Data Prepper routes all Events to a sink which does not define a route. In the example, all Events route into the third OpenSearch sink. conditional-routing-sample-pipeline: source: http: processor: route: - application-logs: ' /log_type == \"application\"' - http-logs: ' /log_type == \"apache\"' sink: - opensearch: hosts: [ \" https://opensearch:9200\"] index: application_logs routes: [ application-logs] - opensearch: hosts: [ \" https://opensearch:9200\"] index: http_logs routes: [ http-logs] - opensearch: hosts: [ \" https://opensearch:9200\"] index: all_logs Examples\nThis section provides some pipeline examples that you can use to start creating your own pipelines. For more pipeline configurations, select from the following options for each component: Buffers Processors Sinks Sources The Data Prepper repository has several sample applications to help you get started.\nLog ingestion pipeline\nThe following example pipeline.yaml file with SSL and basic authentication enabled for the http-source demonstrates how to use the HTTP Source and Grok Prepper plugins to process unstructured log data: log-pipeline: source: http: ssl_certificate_file: \" /full/path/to/certfile.crt\" ssl_key_file: \" /full/path/to/keyfile.key\" authentication: http_basic: username: \" myuser\" password: \" mys3cret\" processor: - grok: match: # This will match logs with a \"log\" key against the COMMONAPACHELOG pattern (ex: { \"log\": \"actual apache log...\" }) # You should change this to match what your logs look like. See the grok documenation to get started. log: [ \" %{COMMONAPACHELOG}\"] sink: - opensearch: hosts: [ \" https://localhost:9200\"] # Change to your credentials username: \" admin\" password: \" admin\" # Add a certificate file if you are accessing an OpenSearch cluster with a self-signed certificate #cert: /path/to/cert # If you are connecting to an Amazon OpenSearch Service domain without # Fine-Grained Access Control, enable these settings. Comment out the # username and password above. #aws_sigv4: true #aws_region: us-east-1 # Since we are grok matching for apache logs, it makes sense to send them to an OpenSearch index named apache_logs. # You should change this to correspond with how your OpenSearch indices are set up. index: apache_logs This example uses weak security. We strongly recommend securing all plugins which open external ports in production environments.\nTrace analytics pipeline\nThe following example demonstrates how to build a pipeline that supports the Trace Analytics OpenSearch Dashboards plugin. This pipeline takes data from the OpenTelemetry Collector and uses two other pipelines as sinks. These two separate pipelines index trace and the service map documents for the dashboard plugin.\nStarting from Data Prepper 2.0, Data Prepper no longer supports otel_trace_raw_prepper processor due to the Data Prepper internal data model evolution.\nInstead, users should use otel_trace_raw. entry-pipeline: delay: \" 100\" source: otel_trace_source: ssl: false buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 sink: - pipeline: name: \" raw-pipeline\" - pipeline: name: \" service-map-pipeline\" raw-pipeline: source: pipeline: name: \" entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - otel_trace_raw: sink: - opensearch: hosts: [ \" https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-raw service-map-pipeline: delay: \" 100\" source: pipeline: name: \" entry-pipeline\" buffer: bounded_blocking: buffer_size: 10240 batch_size: 160 processor: - service_map_stateful: sink: - opensearch: hosts: [ \" https://localhost:9200\"] insecure: true username: admin password: admin index_type: trace-analytics-service-map To maintain similar ingestion throughput and latency, scale the buffer_size and batch_size by the estimated maximum batch size in the client request payload.\nMetrics pipeline\nData Prepper supports metrics ingestion using OTel. It currently supports the following metric types:\nGauge\nSum\nSummary\nHistogram\nOther types are not supported. Data Prepper drops all other types, including Exponential Histogram and Summary. Additionally, Data Prepper does not support Scope instrumentation.\nTo set up a metrics pipeline: metrics-pipeline: source: otel_metrics_source: processor: - otel_metrics_raw_processor: sink: - opensearch: hosts: [ \" https://localhost:9200\"] username: admin password: admin S3 log ingestion pipeline\nThe following example demonstrates how to use the S3Source and Grok Processor plugins to process unstructured log data from Amazon Simple Storage Service (Amazon S3). This example uses application load balancer logs. As the application load balancer writes logs to S3, S3 creates notifications in Amazon SQS. Data Prepper monitors those notifications and reads the S3 objects to get the log data and process it. log-pipeline: source: s3: notification_type: \" sqs\" compression: \" gzip\" codec: newline: sqs: queue_url: \" https://sqs.us-east-1.amazonaws.com/12345678910/ApplicationLoadBalancer\" aws: region: \" us-east-1\" sts_role_arn: \" arn:aws:iam::12345678910:role/Data-Prepper\" processor: - grok: match: message: [ \" %{DATA:type} %{TIMESTAMP_ISO8601:time} %{DATA:elb} %{DATA:client} %{DATA:target} %{BASE10NUM:request_processing_time} %{DATA:target_processing_time} %{BASE10NUM:response_processing_time} %{BASE10NUM:elb_status_code} %{DATA:target_status_code} %{BASE10NUM:received_bytes} %{BASE10NUM:sent_bytes} \\\" %{DATA:request} \\\" \\\" %{DATA:user_agent} \\\" %{DATA:ssl_cipher} %{DATA:ssl_protocol} %{DATA:target_group_arn} \\\" %{DATA:trace_id} \\\" \\\" %{DATA:domain_name} \\\" \\\" %{DATA:chosen_cert_arn} \\\" %{DATA:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \\\" %{DATA:actions_executed} \\\" \\\" %{DATA:redirect_url} \\\" \\\" %{DATA:error_reason} \\\" \\\" %{DATA:target_list} \\\" \\\" %{DATA:target_status_code_list} \\\" \\\" %{DATA:classification} \\\" \\\" %{DATA:classification_reason}\"] - grok: match: request: [ \" (%{NOTSPACE:http_method})? (%{NOTSPACE:http_uri})? (%{NOTSPACE:http_version})?\"] - grok: match: http_uri: [ \" (%{WORD:protocol})?(://)?(%{IPORHOST:domain})?(:)?(%{INT:http_port})?(%{GREEDYDATA:request_uri})?\"] - date: from_time_received: true destination: \" @timestamp\" sink: - opensearch: hosts: [ \" https://localhost:9200\"] username: \" admin\" password: \" admin\" index: alb_logs Migrating from Logstash\nData Prepper supports Logstash configuration files for a limited set of plugins. Simply use the logstash config to run Data Prepper. docker run --name data-prepper \\ -v /full/path/to/logstash.conf:/usr/share/data-prepper/pipelines/pipelines.conf \\ opensearchproject/opensearch-data-prepper:latest This feature is limited by feature parity of Data Prepper. As of Data Prepper 1.2 release, the following plugins from the Logstash configuration are supported:\nHTTP Input plugin\nGrok Filter plugin\nElasticsearch Output plugin\nAmazon Elasticsearch Output plugin\nConfigure the Data Prepper server\nData Prepper itself provides administrative HTTP endpoints such as /list to list pipelines and /metrics/prometheus to provide Prometheus-compatible metrics data. The port that has these endpoints has a TLS configuration and is specified by a separate YAML file. By default, these endpoints are secured by Data Prepper docker images. We strongly recommend providing your own configuration file for securing production environments. Here is an example data-prepper-config.yaml: ssl: true keyStoreFilePath: \" /usr/share/data-prepper/keystore.jks\" keyStorePassword: \" password\" privateKeyPassword: \" other_password\" serverPort: 1234 To configure the Data Prepper server, run Data Prepper with the additional yaml file. docker run --name data-prepper \\ -v /full/path/to/my-pipelines.yaml:/usr/share/data-prepper/pipelines/my-pipelines.yaml \\ -v /full/path/to/data-prepper-config.yaml:/usr/share/data-prepper/data-prepper-config.yaml \\ opensearchproject/data-prepper:latest Configure peer forwarder\nData Prepper provides an HTTP service to forward Events between Data Prepper nodes for aggregation. This is required for operating Data Prepper in a clustered deployment. Currently, peer forwarding is supported in aggregate, service_map_stateful, and otel_trace_raw processors. Peer forwarder groups events based on the identification keys provided by the processors. For service_map_stateful and otel_trace_raw it’s traceId by default and can not be configured. For aggregate processor, it is configurable using identification_keys option.\nPeer forwarder supports peer discovery through one of three options: a static list, a DNS record lookup, or AWS Cloud Map. Peer discovery can be configured using discovery_mode option. Peer forwarder also supports SSL for verification and encryption, and mTLS for mutual authentication in a peer forwarding service.\nTo configure peer forwarder, add configuration options to data-prepper-config.yaml mentioned in the Configure the Data Prepper server section: peer_forwarder: discovery_mode: dns domain_name: \" data-prepper-cluster.my-domain.net\" ssl: true ssl_certificate_file: \" &lt;cert-file-path&gt;\" ssl_key_file: \" &lt;private-key-file-path&gt;\" authentication: mutual_tls:",
    "ancestors": [
      "Data Prepper"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tools/cli/",
    "title": "OpenSearch CLI",
    "content": "The OpenSearch CLI command line interface (opensearch-cli) lets you manage your OpenSearch cluster from the command line and automate tasks.\nCurrently, opensearch-cli supports the Anomaly Detection and k-NN plugins, along with arbitrary REST API paths. Among other things, you can use opensearch-cli to create and delete detectors, start and stop them, and check k-NN statistics.\nProfiles let you easily access different clusters or sign requests with different credentials. opensearch-cli supports unauthenticated requests, HTTP basic signing, and IAM signing for Amazon Web Services.\nThis example moves a detector ( ecommerce-count-quantity) from a staging cluster to a production cluster: opensearch-cli ad get ecommerce-count-quantity --profile staging &gt; ecommerce-count-quantity.json\nopensearch-cli ad create ecommerce-count-quantity.json --profile production\nopensearch-cli ad start ecommerce-count-quantity.json --profile production\nopensearch-cli ad stop ecommerce-count-quantity --profile staging\nopensearch-cli ad delete ecommerce-count-quantity --profile staging Install Download and extract the appropriate installation package for your computer.\nMake the opensearch-cli file executable: chmod +x./opensearch-cli Add the command to your path: export PATH = $PATH: $( pwd) Confirm the CLI is working properly: opensearch-cli --version Profiles\nProfiles let you easily switch between different clusters and user credentials. To get started, run opensearch-cli profile create with the --auth-type, --endpoint, and --name options: opensearch-cli profile create --auth-type basic --endpoint https://localhost:9200 --name docker-local Alternatively, save a configuration file to ~/.opensearch-cli/config.yaml: profiles: - name: docker-local endpoint: https://localhost:9200 user: admin password: foobar - name: aws endpoint: https://some-cluster.us-east-1.es.amazonaws.com aws_iam: profile: \" \" service: es Usage\nopensearch-cli commands use the following syntax: opensearch-cli &lt; command &gt; &lt;subcommand&gt; &lt;flags&gt; For example, the following command retrieves information about a detector: opensearch-cli ad get my-detector --profile docker-local For a request to the OpenSearch CAT API, try the following command: opensearch-cli curl get --path _cat/plugins --profile aws Use the -h or --help flag to see all supported commands, subcommands, or usage for a specific command: opensearch-cli -h opensearch-cli ad -h opensearch-cli ad get -h",
    "ancestors": [
      "Tools"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tools/grafana/",
    "title": "Grafana",
    "content": "Grafana has a data source plugin that lets you explore and visualize your OpenSearch data. For information on getting started with the plugin, see the Grafana overview page.",
    "ancestors": [
      "Tools"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tools/index/",
    "title": "Tools",
    "content": "This section provides documentation for OpenSearch-supported tools, including: Agents and ingestion tools OpenSearch CLI OpenSearch Kubernetes operator Agents and ingestion tools\nHistorically, many multiple popular agents and ingestion tools have worked with Elasticsearch OSS, such as Beats, Logstash, Fluentd, FluentBit, and OpenTelemetry. OpenSearch aims to continue to support a broad set of agents and ingestion tools, but not all have been tested or have explicitly added OpenSearch compatibility.\nAs an intermediate compatibility solution, OpenSearch has a setting that instructs the cluster to return version 7.10.2 rather than its actual version.\nIf you use clients that include a version check, such as versions of Logstash OSS or Filebeat OSS between 7.x - 7.12.x, enable the setting: PUT _cluster/settings { \"persistent\": { \"compatibility\": { \"override_main_response_version\": true } } } Just like any other setting, the alternative is to add the following line to opensearch.yml on each node and then restart the node: compatibility.override_main_response_version: true Logstash OSS 8.0 introduces a breaking change where all plugins run in ECS compatibility mode by default. If you use a compatible OSS client you must override the default value to maintain legacy behavior: ecs_compatibility =&gt; disabled Downloads\nYou can download the OpenSearch output plugin for Logstash from OpenSearch downloads. The Logstash output plugin is compatible with OpenSearch and Elasticsearch OSS (7.10.2 or lower).\nThese are the latest versions of Beats OSS with OpenSearch compatibility. For more information, see the compatibility matrices. Filebeat OSS 7.12.1 Metricbeat OSS 7.12.1 Packetbeat OSS 7.12.1 Heartbeat OSS 7.12.1 Winlogbeat OSS 7.12.1 Auditbeat OSS 7.12.1 Some users report compatibility issues with ingest pipelines on these versions of Beats. If you use ingest pipelines with OpenSearch, consider using the 7.10.2 versions of Beats instead.\nCompatibility Matrices Italicized cells are untested, but indicate what a value theoretically should be based on existing information.\nCompatibility Matrix for Logstash   Logstash OSS 7.0.0 to 7.11.x Logstash OSS 7.12.x* Logstash 7.13.x-7.16.x without OpenSearch output plugin Logstash 7.13.x-7.16.x with OpenSearch output plugin Logstash 8.x+ with OpenSearch output plugin Elasticsearch OSS 7.0.0 to 7.9.x Yes Yes No Yes Yes Elasticsearch OSS 7.10.2 Yes Yes No Yes Yes ODFE 1.0 to 1.12 Yes Yes No Yes Yes ODFE 1.13 Yes Yes No Yes Yes OpenSearch 1.x to 2.x\nYes via version setting\nYes via version setting No Yes Yes, with Elastic Common Schema Setting * Most current compatible version with Elasticsearch OSS.\nCompatibility Matrix for Beats   Beats OSS 7.0.0 to 7.11.x** Beats OSS 7.12.x* Beats 7.13.x Elasticsearch OSS 7.0.0 to 7.9.x Yes Yes No\nElasticsearch OSS 7.10.2 Yes Yes No\nODFE 1.0 to 1.12 Yes Yes No\nODFE 1.13 Yes Yes No\nOpenSearch 1.x to 2.x\nYes via version setting\nYes via version setting\nNo\nLogstash OSS 7.0.0 to 7.11.x Yes Yes Yes Logstash OSS 7.12.x* Yes Yes Yes Logstash 7.13.x with OpenSearch output plugin Yes Yes Yes * Most current compatible version with Elasticsearch OSS.\n** Beats OSS includes all Apache 2.0 Beats agents (i.e. Filebeat, Metricbeat, Auditbeat, Heartbeat, Winlogbeat, Packetbeat).\nBeats versions newer than 7.12.x are not supported by OpenSearch. If you must update the Beats agent(s) in your environment to a newer version, you can work around the incompatibility by directing traffic from Beats to Logstash and using the Logstash Output plugin to ingest the data to OpenSearch.\nOpenSearch CLI\nThe OpenSearch CLI command line interface (opensearch-cli) lets you manage your OpenSearch cluster from the command line and automate tasks. For more information on OpenSearch CLI, see OpenSearch CLI.\nOpenSearch Kubernetes operator\nThe OpenSearch Kubernetes (K8s) Operator is an open-source kubernetes operator that helps automate the deployment and provisioning of OpenSearch and OpenSearch Dashboards in a containerized environment. For information on how to use the K8s operator, see OpenSearch Kubernetes operator",
    "ancestors": [
      "Tools"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tools/k8s-operator/",
    "title": "OpenSearch Kubernetes Operator",
    "content": "The OpenSearch Kubernetes Operator is an open-source kubernetes operator that helps automate the deployment and provisioning of OpenSearch and OpenSearch Dashboards in a containerized environment. The operator can manage multiple OpenSearch clusters that can be scaled up and down depending on your needs.\nInstallation\nThere are two ways to get started with the operator: Use a Helm chart. Use a local installation.\nUse a Helm chart\nIf you use Helm to manage your Kubernetes cluster, you can use the OpenSearch Kubernetes Operator’s Cloud Native Computing Foundation (CNCF) project stored in Artifact Hub, a web-based application for finding, installing, and publishing CNCF packages.\nTo begin, log in to your Kubernetes cluster and add the Helm repository (repo) from Artifact Hub. helm repo add opensearch-operator https://opster.github.io/opensearch-k8s-operator/ Make sure that the repo is included in your Kubernetes cluster. helm repo list | grep opensearch Both the opensearch and opensearch-operator repos appear in the list of repos.\nInstall the manager that operates all of the OpenSearch Kubernetes Operator’s actions. helm install opensearch-operator opensearch-operator/opensearch-operator After the installation completes, the operator returns information on the deployment with STATUS: deployed. Then you can configure and start your OpenSearch cluster.\nUse a local installation\nIf you want to create a new Kubernetes cluster on your existing machine, use a local installation.\nIf this is your first time running Kubernetes and you intend to run through these instructions on your laptop, make sure that you have the following installed: Kubernetes Docker minikube Before running through the installation steps, make sure that you have a Kubernetes environment running locally. When using minikube, open a new terminal window and enter minikube start. Kubernetes will now use a containerized minikube cluster with a namespace called default.\nThen install the OpenSearch Kubernetes Operator using the following steps:\nIn your preferred directory, clone the OpenSearch Kubernetes Operator repo. Navigate into repo’s directory using cd.\nGo to the opensearch-operator folder.\nEnter make build manifests.\nStart a Kubernetes cluster. When using minikube, open a new terminal window and enter minikube start. Kubernetes will now use a containerized minikube cluster with a namespace called default. Make sure that ~/.kube/config points to the cluster. apiVersion: v1 clusters: - cluster: certificate-authority: /Users/naarcha/.minikube/ca.crt extensions: - extension: last-update: Mon, 29 Aug 2022 10:11:47 CDT provider: minikube.sigs.k8s.io version: v1.26.1 name: cluster_info server: https://127.0.0.1:61661 name: minikube contexts: - context: cluster: minikube extensions: - extension: last-update: Mon, 29 Aug 2022 10:11:47 CDT provider: minikube.sigs.k8s.io version: v1.26.1 name: context_info namespace: default user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /Users/naarcha/.minikube/profiles/minikube/client.crt client-key: /Users/naarcha/.minikube/profiles/minikube/client.key Enter make install to create the CustomResourceDefinition that runs in your Kubernetes cluster.\nStart the OpenSearch Kubernetes Operator. Enter make run.\nVerify Kubernetes deployment\nTo ensure that Kubernetes recognizes the OpenSearch Kubernetes Operator as a namespace, enter k get ns | grep opensearch. Both opensearch and opensearch-operator-system should appear as Active.\nWith the operator active, use k get pod -n opensearch-operator-system to make sure that the operator’s pods are running. NAME READY STATUS RESTARTS AGE\nopensearch-operator-controller-manager-&lt;pod-id&gt; 2/2 Running 0 25m With the Kubernetes cluster running, you can now run OpenSearch inside the cluster.\nDeploy a new OpenSearch cluster\nFrom your cloned OpenSearch Kubernetes Operator repo, navigate to the opensearch-operator/examples directory. There you’ll find the opensearch-cluster.yaml file, which can be customized to the needs of your cluster, including the clusterName that acts as the namespace in which your new OpenSearch cluster will reside.\nWith your cluster configured, run the kubectl apply command. kubectl apply -f opensearch-cluster.yaml The operator creates several pods, including a bootstrap pod, three OpenSearch cluster pods, and one Dashboards pod. To connect to your cluster, use the port-forward command. kubectl port-forward svc/my-cluster-dashboards 5601 Open http://localhost:5601 in your preferred browser and log in with the default demo credentials admin / admin. You can also run curl commands against the OpenSearch REST API by forwarding to port 9200. kubectl port-forward svc/my-cluster 9200 In order to delete the OpenSearch cluster, delete the cluster resources. The following command deletes the cluster namespace and all its resources. kubectl delete -f opensearch-cluster.yaml Next steps\nTo learn more about how to customize your Kubernetes OpenSearch cluster, including data persistence, authentication methods, and scaling, see the OpenSearch Kubernetes Operator User Guide.\nIf you want to contribute to the development of the OpenSearch Kubernetes Operator, see the repo design documents.",
    "ancestors": [
      "Tools"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tools/logstash/advanced-config/",
    "title": "Advanced configurations",
    "content": "This section describes how to set up advanced configuration options, like referencing field values and conditional statements, for Logstash.\nReferencing field values\nTo get access to a field, use the - field syntax.\nYou can also surround the field name by square brackets - [field] which makes it more explicit that you’re referring to a field.\nFor example, if you have the following event: { \"request\": \"/products/view/123\", \"verb\": \"GET\", \"response\": 200, \"headers\": { \"request_path\" =&gt; \"/\" } } To access the request field, use - request or - [request].\nIf you want to reference nested fields, use the square brackets syntax and specify the path to the field. With each level being enclosed within square brackets: - [headers][request_path].\nYou can reference fields using the sprintf format. This is also called string expansion. You need to add a % sign and then wrap the field reference within curly brackets.\nYou need to reference field values when using conditional statements.\nFor example, you can make the file name dynamic and contain the type of the processed events - either access or error. The type option is mainly used for conditionally applying filter plugins based on the type of events being processed.\nLet’s add a type option and specify a value of access. input { file { path =&gt; \"\" start_position =&gt; \"beginning\" type =&gt; \"access\" } http { type =&gt; \"access\" } } filter { mutate { remove_field =&gt; {\"host\"} } } output { stdout { codec =&gt; rubydebug } file { path =&gt; \"%{[type]}.log\" } } Start Logstash and send an HTTP request. The processed event is output in the terminal. The event now includes a field named type.\nYou’ll see the access.log file created within the Logstash directory.\nConditional statements\nYou can use conditional statements to control the flow of code execution based on some conditions.\nSyntax: if EXPR {... } else if EXPR {... } else {... } EXPR is any valid Logstash syntax that evaluates to a boolean value.\nFor example, you can check if an event type is set to access or error and perform some action based on that: if [type] == \"access\" {... } else if [type] == \"error\" { file {.. } } else {... } You can compare a field value to some arbitrary value: if [headers][content_length] &gt;= 1000 {... } You can regex: if [some_field =~ /[0-9]+/ { //some field only contains digits } You can use arrays: if [some_field] in [\"one\", \"two\", \"three\"] { some field is either \"one\", \"two\", or \"three\" } You can use boolean operators: if [type] == \"access\" or [type] == \"error\" {... } Formatting dates\nYou can use the sprintf format or string expansion to format dates.\nFor example, you might want the current date to be part of the filename.\nTo format the date, add a plus sign in curly brackets followed by the date format - %{+yyyy-MM-dd}. file { path =&gt; \"%{[type]}_%{+yyyy_MM_dd}.log\" } This is the date stored within the @timestamp fields, which is the time and date of the event.\nSend a request to the pipeline and verify that a filename is outputted that contains the events date.\nYou can embed the date in other outputs as well, for example into the index name in OpenSearch.\nSending time information\nYou can set the time of events.\nLogstash already sets the time when the event is received by the input plugin within the @timestamp field.\nIn some scenarios, you might need to use a different timestamp.\nFor example, if you have an eCommerce store and you process the orders daily at midnight. When Logstash receives the events at midnight, it sets the timestamp to the current time.\nBut you want it to be the time when the order is placed and not when Logstash received the event.\nLet’s change the event timestamp to the date the request is received by the web server. You can do this using a filter plugin named dates.\nThe dates filter passes a date or datetime value from a field and uses the results as the event timestamp.\nAdd the date plugin at the bottom of the filter block: date { match =&gt; [ \"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\"] } timestamp is the field that the grok pattern creates. Z is the timezone. i.e., UTC offsets.\nStart Logstash and send an HTTP request.\nYou can see that the filename contains the date of the request instead of the present date.\nIf the passing of the date fails, the filter plugin adds a tag named _datepassfailure to the text field.\nAfter you have set the @timestamp field to a new value, you don’t really need the other timestamp field anymore. You can remove it with the remove_field option. date { match =&gt; [ \"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\"] remove_field =&gt; [ \"timestamp\"] } Parsing user agents\nThe user agent is the last part of a log entry that consists of the name of the browser, the browser version, and the OS of the device.\nUsers might be using a wide range of browsers, devices, and OS’s. Doing this manually is hard.\nYou can’t use grok patterns because the grok pattern only matches the usage in the string as whole and doesn’t figure out which browser the visitor used for instance.\nLogstash ships with a file containing regular expressions for this purpose. This makes it really easy to extract user agent information, which you could send to OpenSearch and run aggregations on.\nTo do this, add a source option that contains the name of the field. In this case, that’s the agent field.\nBy default the user agent plugin, adds a number of fields at the top-level of the event.\nSince that can get pretty confusing, we can add an option named target with a value of ua, short for user agent. What this does is that it nests the fields within an object named ua, making things more organized. useragent { source =&gt; \"agent\" target =&gt; \"ua\" } Start Logstash and send an HTTP request.\nYou can see a field named ua with a number of keys including the browser name and version, the OS, and the device.\nYou could OpenSearch Dashboards to create a pie chart that shows how many visitors are from mobile devices and how many are desktop users. Or, you could get statistics on which browser versions are popular.\nEnriching geographical data\nYou can take an IP address and perform geographical lookup to resolve the geographical location of the user using the geoip filter.\nThe geoip filter plugin ships with a database called geolite 2, which is provided by a company named MaxMind. geolite 2 is a popular source of geographical data and it’s available for free.\nAdd the geoip plugin at the bottom of the else block.\nThe value of the source option is the name of the field containing the IP address, in this case that’s clientip. You can make this field available using the grok pattern. geoip { source =&gt; \"clientip\" } Start Logstash and send an HTTP request.\nWithin the terminal, you see a new field named geoip that contains information such as the timezone, country, continent, city, postal code, and the latitude / longitude pair.\nIf you only need the country name for instance, include an option named fields with an array of the field names that you want the geoip plugin to return.\nSome of the fields are not always available such as city name and region because translating IP addresses into geographical locations is generally not that accurate. If the geoip plugin fails to look up the geographical location, it adds a tag named geoip_lookup_failure.\nYou can use the geoip plugin with the OpenSearch output because location object within the geoip object, is a standard format for representing geospatial data in JSON. This is the same format as OpenSearch uses for its geo_point data type.\nYou can use the powerful geospatial queries of OpenSearch for working with geographical data.",
    "ancestors": [
      "Tools",
      "Logstash"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tools/logstash/common-filters/",
    "title": "Common filter plugins",
    "content": "This page contains a list of common filter plugins.\nmutate\nYou can use the mutate filter to change the data type of a field. For example, you can use the mutate filter if you’re sending events to OpenSearch and you need to change the data type of a field to match any existing mappings.\nTo convert the quantity field from a string type to an integer type: input { http { host =&gt; \"127.0.0.1\" port =&gt; 8080 } } filter { mutate { convert =&gt; {\"quantity\" =&gt; \"integer\"} } } output { file { path =&gt; \"output.txt\" } } Sample output\nYou can see that the type of the quantity field is changed from a string to an integer. { \" quantity\" =&gt; 3, \" host\" =&gt; \"127.0.0.1\", \" @timestamp\" =&gt; 2021-05-23T19: 02: 08.026Z, \" amount\" =&gt; 10, \" @version\" =&gt; \"1\", \" headers\" =&gt; { \" request_path\" =&gt; \"/\", \" connection\" =&gt; \"keep-alive\", \" content_length\" =&gt; \"41\", \" http_user_agent\" =&gt; \"PostmanRuntime/7.26.8\", \" request_method\" =&gt; \"PUT\", \" cache_control\" =&gt; \"no-cache\", \" http_accept\" =&gt; \"*/*\", \" content_type\" =&gt; \"application/json\", \" http_version\" =&gt; \"HTTP/1.1\", \" http_host\" =&gt; \"127.0.0.1: 8080\", \" accept_encoding\" =&gt; \"gzip, deflate, br\", \" postman_token\" =&gt; \"ffd1cdcb-7a1d-4d63-90f8-0f2773069205\" } } Other data types you can convert to are float, string, and boolean values. If you pass in an array, the mutate filter converts all the elements in the array. If you pass a string like “world” to cast to an integer type, the result is 0 and Logstash continues processing events.\nLogstash supports a few common options for all filter plugins: Option Description add_field Adds one or more fields to the event. remove_field Removes one or more events from the field. add_tag Adds one or more tags to the event. You can use tags to perform conditional processing on events depending on which tags they contain. remove_tag Removes one or more tags from the event. For example, you can remove the host field from the event: input { http { host =&gt; \"127.0.0.1\" port =&gt; 8080 } } filter { mutate { remove_field =&gt; {\"host\"} } } output { file { path =&gt; \"output.txt\" } } grok\nWith the grok filter, you can parse unstructured data and and structure it into fields. The grok filter uses text patterns to match text in your logs. You can think of text patterns as variables containing regular expressions.\nThe format of a text pattern is as follows: % { SYNTAX:SEMANTIC } SYNTAX is the format a piece of text should be in for the pattern to match. You can enter any of grok ’s predefined patterns. For example, you can use the email identifier to match an email address from a given piece of text. SEMANTIC is an arbitrary name for the matched text. For example, if you’re using the email identifier syntax, you can name it “email.”\nThe following request consists of the IP address of the visitor, name of the visitor, the timestamp of the request, the HTTP verb and URL, the HTTP status code, and the number of bytes: 184.252.108.229 - joe [ 20/Sep/2017:13:22:22 +0200] GET /products/view/123 200 12798 To split this request into different fields: filter { grok { match =&gt; { \"message\" =&gt; \" %{IP: ip_address} %{USER:identity} %{USER:auth} \\[%{HTTPDATE:reg_ts}\\] \\\"%{WORD:http_verb} %{URIPATHPARAM: req_path} \\\" %{INT:http_status:int} %{INT:num_bytes:int}\"} } } where: IP: matches the IP address field. USER: matches the user name. WORD: matches the HTTP verb. URIPATHPARAM: matches the URI path. INT: matches the HTTP status field. INT: matches the number of bytes.\nThis is what the event looks like after the grok filter breaks it down into individual fields: ip_address: 184.252.108.229 identity: joe reg_ts: 20/Sep/2017:13:22:22 +0200 http_verb:GET req_path: /products/view/123 http_status: 200 num_bytes: 12798 For common log formats, you use the predefined patterns defined here⁠— Logstash patterns. You can make any adjustments to the results with the mutate filter.",
    "ancestors": [
      "Tools",
      "Logstash"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tools/logstash/execution-model/",
    "title": "Logstash execution model",
    "content": "Here’s a brief introduction to how Logstash processes events internally.\nHandling events concurrently\nYou can configure Logstash to have a number of inputs listening for events. Each input runs in its own thread to avoid inputs blocking each other. If you have two incoming events at the same time, Logstash handles both events concurrently.\nAfter receiving an event and possibly applying an input codec, Logstash sends the event to a work queue. Pipeline workers or batchers perform the rest of the work involving filters and outputs along with any codec used at the output. Each pipeline worker also runs within its own thread meaning that Logstash processes multiple events simultaneously.\nProcessing events in batches\nA pipeline worker consumes events from the work queue in batches to optimize the throughput of the pipeline as a whole.\nOne reason why Logstash works in batches is that some code needs to be executed regardless of how many events are processed at a time within the pipeline worker. Instead of executing that code 100 times for 100 events, it’s more efficient to execute it once for a batch of 100 events.\nAnother reason is that a few output plugins group together events as batches. For example, if you send 100 requests to OpenSearch, the OpenSearch output plugin uses the bulk API to send a single request that groups together the 100 requests.\nLogstash determines the batch size by two configuration options⁠—a number representing the maximum batch size and the batch delay. The batch delay is how long Logstash waits before processing the unprocessed batch of events.\nIf you set the maximum batch size to 50 and the batch delay to 100 ms, Logstash processes a batch if they’re either 50 unprocessed events in the work queue or if one hundred milliseconds have elapsed.\nThe reason that a batch is processed, even if the maximum batch size isn’t reached, is to reduce the delay in processing and to continue to process events in a timely manner. This works well for pipelines that process a low volume of events.\nImagine that you’ve a pipeline that processes error logs from web servers and pushes them to OpenSearch. You’re using OpenSearch Dashboards to analyze the error logs. Because you’re possibly dealing with a fairly low number of events, it might take a long time to reach 50 events. Logstash processes the events before reaching this threshold because otherwise there would be a long delay before we see the errors appear in OpenSearch Dashboards.\nThe default batch size and batch delay work for most cases. You don’t need to change the default values unless you need to minutely optimize the performance.\nOptimizing based on CPU cores\nThe number of pipeline workers are proportional to the number of CPU cores on the nodes.\nIf you have 5 workers running on a server with 2 CPU cores, the 5 workers won’t be able to process events concurrently. On the other hand, running 5 workers on a server running 10 CPU cores limits the throughput of a Logstash instance.\nInstead of running a fixed number of workers, which results in poor performance in some cases, Logstash examines the number of CPU cores of the instance and selects the number of pipeline workers to optimize its performance for the platform on which its running. For instance, your local development machine might not have the same processing power as a production server. So you don’t need to manually configure Logstash for different machines.",
    "ancestors": [
      "Tools",
      "Logstash"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tools/logstash/index/",
    "title": "Logstash",
    "content": "Logstash is a real-time event processing engine. It’s part of the OpenSearch stack which includes OpenSearch, Beats, and OpenSearch Dashboards.\nYou can send events to Logstash from many different sources. Logstash processes the events and sends it one or more destinations. For example, you can send access logs from a web server to Logstash. Logstash extracts useful information from each log and sends it to a destination like OpenSearch.\nSending events to Logstash lets you decouple event processing from your app. Your app only needs to send events to Logstash and doesn’t need to know anything about what happens to the events afterwards.\nThe open-source community originally built Logstash for processing log data but now you can process any type of events, including events in XML or JSON format.\nStructure of a pipeline\nThe way that Logstash works is that you configure a pipeline that has three phases⁠—inputs, filters, and outputs.\nEach phase uses one or more plugins. Logstash has over 200 built-in plugins so chances are that you’ll find what you need. Apart from the built-in plugins, you can use plugins from the community or even write your own.\nThe structure of a pipeline is as follows: input { input_plugin =&gt; {} } filter { filter_plugin =&gt; {} } output { output_plugin =&gt; {} } where: input receives events like logs from multiple sources simultaneously. Logstash supports a number of input plugins for TCP/UDP, files, syslog, Microsoft Windows EventLogs, stdin, HTTP, and so on. You can also use an open source collection of input tools called Beats to gather events. The input plugin sends the events to a filter. filter parses and enriches the events in one way or the other. Logstash has a large collection of filter plugins that modify events and pass them on to an output. For example, a grok filter parses unstructured events into fields and a mutate filter changes fields. Filters are executed sequentially. output ships the filtered events to one or more destinations. Logstash supports a wide range of output plugins for destinations like OpenSearch, TCP/UDP, emails, files, stdout, HTTP, Nagios, and so on.\nBoth the input and output phases support codecs to process events as they enter or exit the pipeline.\nSome of the popular codecs are json and multiline. The json codec processes data that’s in JSON format and the multiline codec merges multiple line events into a single line.\nYou can also write conditional statements within pipeline configurations to perform certain actions, if a certain criteria is met.\nInstall Logstash\nThe OpenSearch Logstash plugin has two installation options at this time: Linux (ARM64/X64) and Docker (ARM64/X64).\nMake sure you have Java Development Kit (JDK) version 8 or 11 installed.\nIf you’re migrating from an existing Logstash installation, you can install the OpenSearch output plugin manually and update pipeline.conf. We include this plugin by default in our tarball and Docker downloads.\nTarball\nDownload the Logstash tarball from OpenSearch downloads.\nNavigate to the downloaded folder in the terminal and extract the files: tar -zxvf logstash-oss-with-opensearch-output-plugin-7.16.2-linux-x64.tar.gz Navigate to the logstash-7.16.2 directory.\nYou can add your pipeline configurations to the config directory. Logstash saves any data from the plugins in the data directory. The bin directory contains the binaries for starting Logstash and managing plugins.\nDocker\nPull the Logstash oss package with the OpenSearch output plugin image: docker pull opensearchproject/logstash-oss-with-opensearch-output-plugin:7.16.2 Create a Docker network: docker network create test Start OpenSearch with this network: docker run -p 9200:9200 -p 9600:9600 --name opensearch --net test -e \"discovery.type=single-node\" opensearchproject/opensearch:1.2.0 Start Logstash: docker run -it --rm --name logstash --net test opensearchproject/logstash-oss-with-opensearch-output-plugin:7.16.2 -e 'input { stdin { } } output {\nopensearch {\nhosts =&gt; [\"https://opensearch:9200\"]\nindex =&gt; \"opensearch-logstash-docker-%{+YYYY.MM.dd}\"\nuser =&gt; \"admin\"\npassword =&gt; \"admin\"\nssl =&gt; true\nssl_certificate_verification =&gt; false\n}\n}' Process text from the terminal\nYou can define a pipeline that listens for events on stdin and outputs events on stdout. stdin and stdout refer to the terminal in which you’re running Logstash.\nTo enter some text in the terminal and see the event data in the output:\nUse the -e argument to pass a pipeline configuration directly to the Logstash binary. In this case, stdin is the input plugin and stdout is the output plugin: bin/logstash -e \"input { stdin { } } output { stdout { } }\" Add the —debug flag to see a more detailed output.\nEnter “hello world” in your terminal. Logstash processes the text and outputs it back to the terminal: { \" message\" =&gt; \"hello world\", \" host\" =&gt; \"a483e711a548.ant.amazon.com\", \" @timestamp\" =&gt; 2021-05-30T05: 15: 56.816Z, \" @version\" =&gt; \"1\" } The message field contains your raw input. The host field is an IP address when you don’t run Logstash locally. @timestamp shows the date and time for when the event is processed. Logstash uses the @version field for internal processing.\nPress Ctrl + C to shut down Logstash.\nTroubleshooting\nIf you already have a Logstash process running, you’ll get an error. To fix this issue:\nDelete the.lock file from the data directory: cd data rm -rf.lock Restart Logstash.\nProcess JSON or HTTP input and output it to a file\nTo define a pipeline that handles JSON requests:\nOpen the config/pipeline.conf file in any text editor you like. You can create a pipeline configuration file with any extension, the.conf extension is a Logstash convention. Add the json codec to accept JSON as the input and the file plugin to output the processed events to a.txt file: input { stdin { codec =&gt; json } } output { file { path =&gt; \"output.txt\" } } To process inputs from a file, add an input file to the events-data directory and then pass its path to the file plugin at the input: input { file { path =&gt; \"events-data/input_data.log\" } } Start Logstash: $ bin/logstash -f config/pipeline.conf config/pipeline.conf is a relative path to the pipeline.conf file. You can use an absolute path as well.\nAdd a JSON object in the terminal: { \"amount\": 10, \"quantity\": 2 } The pipeline only handles a single line of input. If you paste some JSON that spans multiple lines, you’ll get an error.\nCheck that the fields from the JSON object are added to the output.txt file: $ cat output.txt { \"@version\": \"1\", \"@timestamp\": \"2021-05-30T05:52:52.421Z\", \"host\": \"a483e711a548.ant.amazon.com\", \"amount\": 10, \"quantity\": 2 } If you type in some invalid JSON as the input, you’ll see a JSON parsing error. Logstash doesn’t discard the invalid JSON because you still might want to do something with it. For example, you can trigger an email or send a notification to a Slack channel.\nTo define a pipeline that handles HTTP requests:\nUse the http plugin to send events to Logstash through HTTP: input { http { host =&gt; \"127.0.0.1\" port =&gt; 8080 } } output { file { path =&gt; \"output.txt\" } } If you don’t specify any options, the http plugin binds to localhost and listens on port 8080.\nStart Logstash: $ bin/logstash -f config/pipeline.conf Use Postman to send an HTTP request. Set Content-Type to an HTTP header with a value of application/json: PUT 127.0. 0.1: 8080 { \"amount\": 10, \"quantity\": 2 } Or, you can use the curl command: curl -XPUT -H \"Content-Type: application/json\" -d ' {\"amount\": 7, \"quantity\": 3 }' http://localhost:8080 ( http://localhost:8080/) Even though we haven’t added the json plugin to the input, the pipeline configuration still works because the HTTP plugin automatically applies the appropriate codec based on the Content-Type header.\nIf you specify a value of applications/json, Logstash parses the request body as JSON.\nThe headers field contains the HTTP headers that Logstash receives: { \"host\": \"127.0.0.1\", \"quantity\": \"3\", \"amount\": 10, \"@timestamp\": \"2021-05-30T06:05:48.135Z\", \"headers\": { \"http_version\": \"HTTP/1.1\", \"request_method\": \"PUT\", \"http_user_agent\": \"PostmanRuntime/7.26.8\", \"connection\": \"keep-alive\", \"postman_token\": \"c6cd29cf-1b37-4420-8db3-9faec66b9e7e\", \"http_host\": \"127.0.0.1:8080\", \"cache_control\": \"no-cache\", \"request_path\": \"/\", \"content_type\": \"application/json\", \"http_accept\": \"*/*\", \"content_length\": \"41\", \"accept_encoding\": \"gzip, deflate, br\" }, \"@version\": \"1\" } Automatically reload the pipeline configuration\nYou can configure Logstash to detect any changes to the pipeline configuration file or the input log file and automatically reload the configuration.\nThe stdin plugin doesn’t supporting automatic reloading.\nAdd an option named start_position with a value of beginning to the input plugin: input { file { path =&gt; \"/Users/&lt;user&gt;/Desktop/logstash7-12.1/events-data/input_file.log\" start_position =&gt; \"beginning\" } } Logstash only processes any new events added to the input file and ignores the ones that it has already processed to avoid processing the same event more than once on restart.\nLogstash records its progress in a file that’s referred to as a sinceDB file. Logstash creates a sinceDB file for each file that it watches for changes.\nOpen the sinceDB file to check how much of the input files are processed: cd data/plugins/inputs/file/ ls -al -rw-r--r-- 1 user staff 0 Jun 13 10:50.sincedb_9e484f2a9e6c0d1bdfe6f23ac107ffc5 cat.sincedb_9e484f2a9e6c0d1bdfe6f23ac107ffc5\n51575938 1 4 7727 The last number in the sinceDB file (7727) is the byte offset of the last known event processed.\nTo process the input file from the beginning, delete the sinceDB file: rm.sincedb_* Start Logstash with a —-config.reload.automatic argument: bin/logstash -f config/pipeline.conf --config.reload.automatic The reload option only reloads if you add a new line at the end of the pipeline configuration file.\nSample output: { \" message\" =&gt; \"216.243.171.38 - - [ 20/Sep/2017: 19: 11: 52 +0200] \\\"GET /products/view/123 HTTP/1.1\\\" 200 12798 \\\"https: //codingexplained.com/products\\\" \\\"Mozilla/5.0 (compatible; YandexBot/3.0; +http: //yandex.com/bots)\\\"\", \" @version\" =&gt; \"1\", \" host\" =&gt; \"a483e711a548.ant.amazon.com\", \" path\" =&gt; \"/Users/kumarjao/Desktop/odfe1/logstash-7.12.1/events-data/input_file.log\", \" @timestamp\" =&gt; 2021-06-13T18: 03: 30.423Z } { \" message\" =&gt; \"91.59.108.75 - - [ 20/Sep/2017: 20: 11: 43 +0200] \\\"GET /js/main.js HTTP/1.1\\\" 200 588 \\\"https: //codingexplained.com/products/view/863\\\" \\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv: 45.0) Gecko/20100101 Firefox/45.0\\\"\", \" @version\" =&gt; \"1\", \" host\" =&gt; \"a483e711a548.ant.amazon.com\", \" path\" =&gt; \"/Users/kumarjao/Desktop/odfe1/logstash-7.12.1/events-data/input_file.log\", \" @timestamp\" =&gt; 2021-06-13T18: 03: 30.424Z } Add a new line to the input file.\nLogstash immediately detects the change and processes the new line as an event.\nMake a change to the pipeline.conf file.\nLogstash immediately detects the change and reloads the modified pipeline.",
    "ancestors": [
      "Tools"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tools/logstash/read-from-opensearch/",
    "title": "Read from OpenSearch",
    "content": "As we ship Logstash events to an OpenSearch cluster using the OpenSearch output plugin, we can also perform read operations on an OpenSearch cluster and load data into Logstash using the OpenSearch input plugin.\nThe OpenSearch input plugin reads the search query results performed on an OpenSearch cluster and loads them into Logstash. This lets you replay test logs, reindex, and perform other operations based on the loaded data. You can schedule ingestions to run periodically by using cron expressions, or manually load data into Logstash by running the query once.\nOpenSearch input plugin\nTo run the OpenSearch input plugin, add the configuration to the pipeline.conf file within your Logstash’s config folder. The example below runs the match_all query filter and loads in data once. input { opensearch { hosts =&gt; \"https://hostname:port\" user =&gt; \"admin\" password =&gt; \"admin\" index =&gt; \"logstash-logs-%{+YYYY.MM.dd}\" query =&gt; \"{ \"query\": { \" match_all\": {}} } \" } } filter { } output { } To ingest data according to a schedule, use a cron expression that specifies the schedule you want. For example, to load in data every minute, add schedule =&gt; \"* * * * *\" to the input section of your pipeline.conf file.\nLike the output plugin, after adding your configuration to the pipeline.conf file, start Logstash by providing the path to this file: $ bin/logstash -f config/pipeline.conf --config.reload.automatic config/pipeline.conf is a relative path to the pipeline.conf file. You can use an absolute path as well.\nAdding stdout{} to the output{} section of your pipeline.conf file prints the query results to the console.\nTo reindex the data into an OpenSearch domain, add the destination domain configuration in the output{} section like shown here.",
    "ancestors": [
      "Tools",
      "Logstash"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/tools/logstash/ship-to-opensearch/",
    "title": "Ship events to OpenSearch",
    "content": "You can Ship Logstash events to an OpenSearch cluster and then visualize your events with OpenSearch Dashboards.\nMake sure you have Logstash, OpenSearch, and OpenSearch Dashboards.\nOpenSearch output plugin\nTo run the OpenSearch output plugin, add the following configuration in your pipeline.conf file: output { opensearch { hosts =&gt; \"https://localhost:9200\" user =&gt; \"admin\" password =&gt; \"admin\" index =&gt; \"logstash-logs-%{+YYYY.MM.dd}\" ssl_certificate_verification =&gt; false } } Sample walkthrough\nOpen the config/pipeline.conf file and add in the following configuration: input { stdin { codec =&gt; json } } output { opensearch { hosts =&gt; \"https://localhost:9200\" user =&gt; \"admin\" password =&gt; \"admin\" index =&gt; \"logstash-logs-%{+YYYY.MM.dd}\" ssl_certificate_verification =&gt; false } } This Logstash pipeline accepts JSON input through the terminal and ships the events to an OpenSearch cluster running locally. Logstash writes the events to an index with the logstash-logs-%{+YYYY.MM.dd} naming convention.\nStart Logstash: $ bin/logstash -f config/pipeline.conf --config.reload.automatic config/pipeline.conf is a relative path to the pipeline.conf file. You can use an absolute path as well.\nAdd a JSON object in the terminal: { \"amount\": 10, \"quantity\": 2 } Start OpenSearch Dashboards and choose Dev Tools: GET _cat/indices?v health | status | index | uuid | pri | rep | docs.count | docs.deleted | store.size | pri.store.size green | open | logstash-logs -2021.07. 01 | iuh 648 LYSnmQrkGf 70 pplA | 1 | 1 | 1 | 0 | 10.3 kb | 5.1 kb Adding different Authentication mechanisms in the Output plugin\nauth_type to support different authentication mechanisms\nIn addition to the existing authentication mechanisms, if we want to add new authentication then we will be adding them in the configuration by using auth_type\nExample Configuration for basic authentication: output { opensearch { hosts =&gt; [\"https://hostname:port\"] auth_type =&gt; { type =&gt; 'basic' user =&gt; 'admin' password =&gt; 'admin' } index =&gt; \"logstash-logs-%{+YYYY.MM.dd}\" } } Parameters inside auth_type\ntype (string) - We should specify the type of authentication\nWe should add credentials required for that authentication like ‘user’ and ‘password’ for ‘basic’ authentication\nWe should also add other parameters required for that authentication mechanism like we added ‘region’ for ‘aws_iam’ authentication\nConfiguration for AWS IAM Authentication\nTo run the Logstash Output Opensearch plugin using aws_iam authentication, simply add a configuration following the below documentation.\nExample Configuration: output { opensearch { hosts =&gt; [\"https://hostname:port\"] auth_type =&gt; { type =&gt; 'aws_iam' aws_access_key_id =&gt; 'ACCESS_KEY' aws_secret_access_key =&gt; 'SECRET_KEY' region =&gt; 'us-west-2' service_name =&gt; 'es' } index =&gt; \"logstash-logs-%{+YYYY.MM.dd}\" } } Required Parameters\nhosts (array of string) - AmazonOpensearchService domain endpoint: port number\nauth_type (Json object) - Which holds other parameters required for authentication\ntype (string) - “aws_iam”\naws_access_key_id (string) - AWS access key\naws_secret_access_key (string) - AWS secret access key\nregion (string,:default =&gt; “us-east-1”) - region in which the domain is located\nif we want to pass other optional parameters like profile, session_token,etc. They needs to be added in auth_type\nport (string) - AmazonOpensearchService listens on port 443 for HTTPS\nprotocol (string) - The protocol used to connect to AmazonOpensearchService is ‘https’\nOptional Parameters\nThe credential resolution logic can be described as follows:\nUser passed aws_access_key_id and aws_secret_access_key in configuration\nEnvironment variables - AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY (RECOMMENDED since they are recognized by all the AWS SDKs and CLI except for.NET), or AWS_ACCESS_KEY and AWS_SECRET_KEY (only recognized by Java SDK)\nCredential profiles file at the default location (~/.aws/credentials) shared by all AWS SDKs and the AWS CLI\nInstance profile credentials delivered through the Amazon EC2 metadata service\ntemplate (path) - You can set the path to your own template here. If no template is specified, the plugin uses the default template.\ntemplate_name (string, default =&gt; “logstash”) - Defines how the template is named inside Opensearch\nservice_name (string, default =&gt; “es”) - Defines the service name to be used for aws_iam authentication.\nlegacy_template (boolean, default =&gt; true) - Selects the OpenSearch template API. When true, uses legacy templates via the _template API. When false, uses composable templates via the _index_template API.\ndefault_server_major_version (number) - The OpenSearch server major version to use when it’s not available from the OpenSearch root URL. If not set, the plugin throws an exception when the version can’t be fetched.\nData streams\nThe OpenSearch output plugin can store both time series datasets (such as logs, events, and metrics) and non-time series data in OpenSearch.\nThe data stream is recommended to index time series datasets (such as logs, metrics, and events) into OpenSearch.\nTo know more about data streams, refer to this documentation.\nWe can ingest data into a data stream through logstash. We need to create the data stream and specify the name of data stream and the op_type of create in the output configuration. The sample configuration is shown below: output { opensearch { hosts =&gt; [\"https://hostname:port\"] auth_type =&gt; { type =&gt; 'basic' user =&gt; 'admin' password =&gt; 'admin' } index =&gt; \"my-data-stream\" action =&gt; \"create\" } }",
    "ancestors": [
      "Tools",
      "Logstash"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/alias/",
    "title": "Alias",
    "content": "Introduced 1.0\nAn alias is a virtual pointer that you can use to reference one or more indexes. Creating and updating aliases are atomic operations, so you can reindex your data and point an alias at it without any downtime.\nExample POST _aliases { \"actions\": [ { \"add\": { \"index\": \"movies\", \"alias\": \"movies-alias1\" } }, { \"remove\": { \"index\": \"old-index\", \"alias\": \"old-index-alias\" } }] } copy Path and HTTP methods POST _aliases URL parameters\nAll alias parameters are optional. Parameter Data Type Description master_timeout\nTime\nThe amount of time to wait for a response from the master node. Default is 30s.\ntimeout\nTime\nThe amount of time to wait for a response from the cluster. Default is 30s. Request body\nIn your request body, you need to specify what action to take, the alias name, and the index you want to associate with the alias. Other fields are optional. Field Data Type Description Required actions\nArray\nSet of actions you want to perform on the index. Valid options are: add, remove, and remove_index. You must have at least one action in the array.\nYes\nadd\nN/A\nAdds an alias to the specified index.\nNo\nremove\nN/A\nRemoves an alias from the specified index.\nNo\nremove_index\nN/A\nDeletes an index.\nNo\nindex\nString\nName of the index you want to associate with the alias. Supports wildcard expressions.\nYes if you don’t supply an indices field in the body.\nindices\nArray\nArray of index names you want to associate with the alias.\nYes if you don’t supply an index field in the body.\nalias\nString\nThe name of the alias.\nYes if you don’t supply an aliases field in the body.\naliases\nArray\nArray of alias names.\nYes if you don’t supply an alias field in the body.\nfilter\nObject\nA filter to use with the alias, so the alias points to a filtered part of the index.\nNo\nis_hidden\nBoolean\nSpecifies whether the alias should be hidden from results that include wildcard expressions\nNo\nmust_exist\nBoolean\nSpecifies whether the alias to remove must exist.\nNo\nis_write_index\nBoolean\nSpecifies whether the index should be a write index. An alias can only have one write index at a time. If a write request is submitted to a alias that links to multiple indexes, OpenSearch executes the request only on the write index.\nNo\nrouting\nString\nUsed to assign a custom value to a shard for specific operations.\nNo\nindex_routing\nString\nAssigns a custom value to a shard only for index operations.\nNo\nsearch_routing\nString\nAssigns a custom value to a shard only for search operations.\nNo Response { \"acknowledged\": true } For more alias API operations, see Index aliases.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/analyze-apis/index/",
    "title": "Analyze API",
    "content": "The analyze API allows you to perform text analysis, which is the process of converting unstructured text into individual tokens (usually words) that are optimized for search.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/analyze-apis/perform-text-analysis/",
    "title": "Perform text analysis",
    "content": "The perform text analysis API analyzes a text string and returns the resulting tokens.\nIf you use the Security plugin, you must have the manage index privilege. If you simply want to analyze text, you must have the manager cluster privilege.\nPath and HTTP methods GET /_analyze\nGET /{index}/_analyze\nPOST /_analyze\nPOST /{index}/_analyze Although you can issue an analyzer request via both GET and POST requests, the two have important distinctions. A GET request causes data to be cached in the index so that the next time the data is requested, it is retrieved faster. A POST request sends a string that does not already exist to the analyzer to be compared to data that is already in the index. POST requests are not cached.\nPath parameter\nYou can include the following optional path parameter in your request. Parameter Data type Description index\nString\nIndex that is used to derive the analyzer. Query parameters\nYou can include the following optional query parameters in your request. Field Data type Description analyzer\nString\nThe name of the analyzer to apply to the text field. The analyzer can be built in or configured in the index. If analyzer is not specified, the analyze API uses the analyzer defined in the mapping of the field field. If the field field is not specified, the analyze API uses the default analyzer for the index. If no index is specified or the index does not have a default analyzer, the analyze API uses the standard analyzer.\nattributes\nArray of Strings\nArray of token attributes for filtering the output of the explain field.\nchar_filter\nArray of Strings\nArray of character filters for preprocessing characters before the tokenizer field.\nexplain\nBoolean\nIf true, causes the response to include token attributes and additional details. Defaults to false.\nfield\nString\nField for deriving the analyzer. If you specify field, you must also specify the index path parameter. If you specify the analyzer field, it overrides the value of field. If you do not specify field, the analyze API uses the default analyzer for the index. If you do not specify the index field, or the index does not have a default analyzer, the analyze API uses the standard analyzer.\nfilter\nArray of Strings\nArray of token filters to apply after the tokenizer field.\nnormalizer\nString\nNormalizer for converting text into a single token.\ntokenizer\nString\nTokenizer for converting the text field into tokens. The following query parameter is required. Field Data type Description text\nString or Array of Strings\nText to analyze. If you provide an array of strings, the text is analyzed as a multi-value field. Example requests Analyze array of text strings Apply a built-in analyzer Apply a custom analyzer Apply a custom transient analyzer Specify an index Derive the analyzer from an index field Specify a normalizer Get token details Set a token limit Analyze array of text strings\nWhen you pass an array of strings to the text field, it is analyzed as a multi-value field. GET /_analyze { \"analyzer\": \"standard\", \"text\": [ \"first array element\", \"second array element\"] } copy The previous request returns the following fields: { \"tokens\": [ { \"token\": \"first\", \"start_offset\": 0, \"end_offset\": 5, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"array\", \"start_offset\": 6, \"end_offset\": 11, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"element\", \"start_offset\": 12, \"end_offset\": 19, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }, { \"token\": \"second\", \"start_offset\": 20, \"end_offset\": 26, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 3 }, { \"token\": \"array\", \"start_offset\": 27, \"end_offset\": 32, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 4 }, { \"token\": \"element\", \"start_offset\": 33, \"end_offset\": 40, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 5 }] } Apply a built-in analyzer\nIf you omit the index path parameter, you can apply any of the built-in analyzers to the text string.\nThe following request analyzes text using the standard built-in analyzer: GET /_analyze { \"analyzer\": \"standard\", \"text\": \"OpenSearch text analysis\" } copy The previous request returns the following fields: { \"tokens\": [ { \"token\": \"opensearch\", \"start_offset\": 0, \"end_offset\": 10, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"text\", \"start_offset\": 11, \"end_offset\": 15, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"analysis\", \"start_offset\": 16, \"end_offset\": 24, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }] } Apply a custom analyzer\nYou can create your own analyzer and specify it in an analyze request.\nIn this scenario, a custom analyzer lowercase_ascii_folding has been created and associated with the books2 index. The analyzer converts text to lowercase and converts non-ASCII characters to ASCII.\nThe following request applies the custom analyzer to the provided text: GET /books 2 /_analyze { \"analyzer\": \"lowercase_ascii_folding\", \"text\": \"Le garçon m'a SUIVI.\" } copy The previous request returns the following fields: { \"tokens\": [ { \"token\": \"le\", \"start_offset\": 0, \"end_offset\": 2, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"garcon\", \"start_offset\": 3, \"end_offset\": 9, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"m'a\", \"start_offset\": 10, \"end_offset\": 13, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }, { \"token\": \"suivi\", \"start_offset\": 14, \"end_offset\": 19, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 3 }] } Apply a custom transient analyzer\nYou can build a custom transient analyzer from tokenizers, token filters, or character filters. Use the filter parameter to specify token filters.\nThe following request uses the uppercase character filter to convert the text to uppercase: GET /_analyze { \"tokenizer\": \"keyword\", \"filter\": [ \"uppercase\"], \"text\": \"OpenSearch filter\" } copy The previous request returns the following fields: { \"tokens\": [ { \"token\": \"OPENSEARCH FILTER\", \"start_offset\": 0, \"end_offset\": 17, \"type\": \"word\", \"position\": 0 }] } The following request uses the html_strip filter to remove HTML characters from the text: GET /_analyze { \"tokenizer\": \"keyword\", \"filter\": [ \"lowercase\"], \"char_filter\": [ \"html_strip\"], \"text\": \"&lt;b&gt;Leave&lt;/b&gt; right now!\" } copy The previous request returns the following fields: { \"tokens\": [ { \"token\": \"leave right now!\", \"start_offset\": 3, \"end_offset\": 23, \"type\": \"word\", \"position\": 0 }] } You can combine filters using an array.\nThe following request combines a lowercase translation with a stop filter that removes the words in the stopwords array: GET /_analyze { \"tokenizer\": \"whitespace\", \"filter\": [ \"lowercase\", { \"type\": \"stop\", \"stopwords\": [ \"to\", \"in\"]}], \"text\": \"how to train your dog in five steps\" } copy The previous request returns the following fields: { \"tokens\": [ { \"token\": \"how\", \"start_offset\": 0, \"end_offset\": 3, \"type\": \"word\", \"position\": 0 }, { \"token\": \"train\", \"start_offset\": 7, \"end_offset\": 12, \"type\": \"word\", \"position\": 2 }, { \"token\": \"your\", \"start_offset\": 13, \"end_offset\": 17, \"type\": \"word\", \"position\": 3 }, { \"token\": \"dog\", \"start_offset\": 18, \"end_offset\": 21, \"type\": \"word\", \"position\": 4 }, { \"token\": \"five\", \"start_offset\": 25, \"end_offset\": 29, \"type\": \"word\", \"position\": 6 }, { \"token\": \"steps\", \"start_offset\": 30, \"end_offset\": 35, \"type\": \"word\", \"position\": 7 }] } Specify an index\nYou can analyze text using an index’s default analyzer, or you can specify a different analyzer.\nThe following request analyzes the provided text using the default analyzer associated with the books index: GET /books/_analyze { \"text\": \"OpenSearch analyze test\" } copy The previous request returns the following fields: \"tokens\": [ { \"token\": \"opensearch\", \"start_offset\": 0, \"end_offset\": 10, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"analyze\", \"start_offset\": 11, \"end_offset\": 18, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"test\", \"start_offset\": 19, \"end_offset\": 23, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }] } The following request analyzes the provided text using the keyword analyzer, which returns the entire text value as a single token: GET /books/_analyze { \"analyzer\": \"keyword\", \"text\": \"OpenSearch analyze test\" } copy The previous request returns the following fields: { \"tokens\": [ { \"token\": \"OpenSearch analyze test\", \"start_offset\": 0, \"end_offset\": 23, \"type\": \"word\", \"position\": 0 }] } Derive the analyzer from an index field\nYou can pass text and a field in the index. The API looks up the field’s analyzer and uses it to analyze the text.\nIf the mapping does not exist, the API uses the standard analyzer, which converts all text to lowercase and tokenizes based on white space.\nThe following request causes the analysis to be based on the mapping for name: GET /books 2 /_analyze { \"field\": \"name\", \"text\": \"OpenSearch analyze test\" } copy The previous request returns the following fields: { \"tokens\": [ { \"token\": \"opensearch\", \"start_offset\": 0, \"end_offset\": 10, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"analyze\", \"start_offset\": 11, \"end_offset\": 18, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"test\", \"start_offset\": 19, \"end_offset\": 23, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }] } Specify a normalizer\nInstead of using a keyword field, you can use the normalizer associated with the index. A normalizer causes the analysis change to produce a single token.\nIn this example, the books2 index includes a normalizer called to_lower_fold_ascii that converts text to lowercase and translates non-ASCII text to ASCII.\nThe following request applies to_lower_fold_ascii to the text: GET /books 2 /_analyze { \"normalizer\": \"to_lower_fold_ascii\", \"text\": \"C'est le garçon qui m'a suivi.\" } copy The previous request returns the following fields: { \"tokens\": [ { \"token\": \"c'est le garcon qui m'a suivi.\", \"start_offset\": 0, \"end_offset\": 30, \"type\": \"word\", \"position\": 0 }] } You can create a custom transient normalizer with token and character filters.\nThe following request uses the uppercase character filter to convert the given text to all uppercase: GET /_analyze { \"filter\": [ \"uppercase\"], \"text\": \"That is the boy who followed me.\" } copy The previous request returns the following fields: { \"tokens\": [ { \"token\": \"THAT IS THE BOY WHO FOLLOWED ME.\", \"start_offset\": 0, \"end_offset\": 32, \"type\": \"word\", \"position\": 0 }] } Get token details\nYou can obtain additional details for all tokens by setting the explain attribute to true.\nThe following request provides detailed token information for the reverse filter used with the standard tokenizer: GET /_analyze { \"tokenizer\": \"standard\", \"filter\": [ \"reverse\"], \"text\": \"OpenSearch analyze test\", \"explain\": true, \"attributes\": [ \"keyword\"] } copy The previous request returns the following fields: { \"detail\": { \"custom_analyzer\": true, \"charfilters\": [], \"tokenizer\": { \"name\": \"standard\", \"tokens\": [ { \"token\": \"OpenSearch\", \"start_offset\": 0, \"end_offset\": 10, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"analyze\", \"start_offset\": 11, \"end_offset\": 18, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"test\", \"start_offset\": 19, \"end_offset\": 23, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }] }, \"tokenfilters\": [ { \"name\": \"reverse\", \"tokens\": [ { \"token\": \"hcraeSnepO\", \"start_offset\": 0, \"end_offset\": 10, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"ezylana\", \"start_offset\": 11, \"end_offset\": 18, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"tset\", \"start_offset\": 19, \"end_offset\": 23, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 }] }] } } Set a token limit\nYou can set a limit to the number of tokens generated. Setting a lower value reduces a node’s memory usage. The default value is 10000.\nThe following request limits the tokens to four: PUT /books 2 { \"settings\": { \"index.analyze.max_token_count\": 4 } } copy The preceding request is an index API rather than an analyze API. See DYNAMIC INDEX SETTINGS for additional details.\nResponse fields\nThe text analysis endpoints return the following response fields. Field Data type Description tokens\nArray\nArray of tokens derived from the text. See token object.\ndetail\nObject\nDetails about the analysis and each token. Included only when you request token details. See detail object. Token object Field Data type Description token\nString\nThe token’s text.\nstart_offset\nInteger\nThe token’s starting position within the original text string. Offsets are zero-based.\nend_offset\nInteger\nThe token’s ending position within the original text string.\ntype\nString\nClassification of the token: &lt;ALPHANUM&gt;, &lt;NUM&gt;, and so on. The tokenizer usually sets the type, but some filters define their own types. For example, the synonym filter defines the &lt;SYNONYM&gt; type.\nposition\nInteger\nThe token’s position within the tokens array. Detail object Field Data type Description custom_analyzer\nBoolean\nWhether the analyzer applied to the text is custom or built in.\ncharfilters\nArray\nList of character filters applied to the text.\ntokenizer\nObject\nName of the tokenizer applied to the text and a list of tokens * with content before the token filters were applied.\ntokenfilters\nArray\nList of token filters applied to the text. Each token filter includes the filter’s name and a list of tokens * with content after the filters were applied. Token filters are listed in the order they are specified in the request. See token object for token field descriptions.",
    "ancestors": [
      "API reference",
      "Analyze API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/analyze-apis/terminology/",
    "title": "Analysis API Terminology",
    "content": "The following sections provide descriptions of important text analysis terms.\nAnalyzers\nAnalyzers tell OpenSearch how to index and search text. An analyzer is composed of three components: a tokenizer, zero or more token filters, and zero or more character filters.\nOpenSearch provides built-in analyzers. For example, the standard built-in analyzer converts text to lowercase and breaks text into tokens based on word boundaries such as carriage returns and white space. The standard analyzer is also called the default analyzer and is used when no analyzer is specified in the text analysis request.\nIf needed, you can combine tokenizers, token filters, and character filters to create a custom analyzer.\nTokenizers\nTokenizers break unstuctured text into tokens and maintain metadata about tokens, such as their start and ending positions in the text.\nCharacter filters\nCharacter filters examine text and perform translations, such as changing, removing, and adding characters.\nToken filters\nToken filters modify tokens, performing operations such as converting a token’s characters to uppercase and adding or removing tokens.\nNormalizers\nSimilar to analyzers, normalizers tokenize text but return a single token only. Normalizers do not employ tokenizers; they make limited use of character and token filters, such as those that operate on one character at a time.\nBy default, OpenSearch does not apply normalizers. To apply normalizers, you must add them to your data before creating an index.",
    "ancestors": [
      "API reference",
      "Analyze API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-aliases/",
    "title": "CAT aliases",
    "content": "Introduced 1.0\nThe CAT aliases operation lists the mapping of aliases to indexes, plus routing and filtering information.\nExample GET _cat/aliases?v copy To limit the information to a specific alias, add the alias name after your query: GET _cat/aliases/&lt;alias&gt;?v copy If you want to get information for more than one alias, separate the alias names with commas: GET _cat/aliases/alias 1,alias 2,alias 3 copy Path and HTTP methods GET _cat/aliases/&lt;alias&gt;\nGET _cat/aliases URL parameters\nAll CAT aliases URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description local\nBoolean\nWhether to return information from the local node only instead of from the master node. Default is false.\nexpand_wildcards\nEnum\nExpands wildcard expressions to concrete indices. Combine multiple values with commas. Supported values are all, open, closed, hidden, and none. Default is open. Response\nThe following response shows that alias1 refers to a movies index and has a configured filter: alias | index | filter | routing.index | routing.search | is_write_index alias 1 | movies | * | - | - | -.opensearch-dashboards |.opensearch-dashboards_ 1 | - | - | - | - To learn more about index aliases, see Index aliases.",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-allocation/",
    "title": "CAT allocation",
    "content": "Introduced 1.0\nThe CAT allocation operation lists the allocation of disk space for indexes and the number of shards on each node.\nExample GET _cat/allocation?v copy To limit the information to a specific node, add the node name after your query: GET _cat/allocation/&lt;node_name&gt; copy If you want to get information for more than one node, separate the node names with commas: GET _cat/allocation/node_name_ 1,node_name_ 2,node_name_ 3 copy Path and HTTP methods GET _cat/allocation?v\nGET _cat/allocation/&lt;node_name&gt; URL parameters\nAll CAT allocation URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description bytes\nByte size\nSpecify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units.\nlocal\nBoolean\nWhether to return information from the local node only instead of from the cluster_manager node. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster_manager node. Default is 30 seconds. Response\nThe following response shows that eight shards are allocated to each of the two nodes available: shards | disk.indices | disk.used | disk.avail | disk.total | disk.percent host | ip | node 8 | 989.4 kb | 25.9 gb | 32.4 gb | 58.4 gb | 44 172.18. 0.4 | 172.18. 0.4 | odfe-node 1 8 | 962.4 kb | 25.9 gb | 32.4 gb | 58.4 gb | 44 172.18. 0.3 | 172.18. 0.3 | odfe-node 2",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-cluster_manager/",
    "title": "CAT cluster manager",
    "content": "Introduced 1.0\nThe CAT cluster manager operation lists information that helps identify the elected cluster manager node.\nExample GET _cat/cluster_manager?v copy Path and HTTP methods GET _cat/cluster_manager URL parameters\nAll CAT cluster manager URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters:\nParameter | Type | Description:— |:— |:—\ncluster_manager_timeout | Time | The amount of time to wait for a connection to the cluster manager node. Default is 30 seconds.\nResponse id | host | ip | node ZaIkkUd 4 TEiAihqJGkp 5 CA | 172.18. 0.3 | 172.18. 0.3 | opensearch-node 2",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-count/",
    "title": "CAT count",
    "content": "Introduced 1.0\nThe CAT count operation lists the number of documents in your cluster.\nExample GET _cat/count?v copy To see the number of documents in a specific index or alias, add the index or alias name after your query: GET _cat/count/&lt;index_or_alias&gt;?v copy If you want to get information for more than one index or alias, separate the index or alias names with commas: GET _cat/count/index_or_alias_ 1,index_or_alias_ 2,index_or_alias_ 3 copy Path and HTTP methods GET _cat/count?v\nGET _cat/count/&lt;index&gt;?v URL parameters\nAll CAT count URL parameters are optional. You can specify any of the common URL parameters.\nResponse\nThe following response shows the overall document count as 1625: epoch | timestamp | count 1624237738 | 01: 08: 58 | 1625",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-field-data/",
    "title": "CAT field data",
    "content": "Introduced 1.0\nThe CAT fielddata operation lists the memory size used by each field per node.\nExample GET _cat/fielddata?v copy To limit the information to a specific field, add the field name after your query: GET _cat/fielddata/&lt;field_name&gt;?v copy If you want to get information for more than one field, separate the field names with commas: GET _cat/fielddata/field_name_ 1,field_name_ 2,field_name_ 3 copy Path and HTTP methods GET _cat/fielddata?v\nGET _cat/fielddata/&lt;field_name&gt;?v URL parameters\nAll CAT fielddata URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameter: Parameter Type Description bytes\nByte size\nSpecify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units. Response\nThe following response shows the memory size for all fields as 284 bytes: id host ip node field size 1 vo 54 NuxSxOrbPEYdkSF 0 w 172.18. 0.4 172.18. 0.4 odfe-node 1 _id 284 b ZaIkkUd 4 TEiAihqJGkp 5 CA 172.18. 0.3 172.18. 0.3 odfe-node 2 _id 284 b",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-health/",
    "title": "CAT health",
    "content": "Introduced 1.0\nThe CAT health operation lists the status of the cluster, how long the cluster has been up, the number of nodes, and other useful information that helps you analyze the health of your cluster.\nExample GET _cat/health?v copy Path and HTTP methods GET _cat/health?v copy URL parameters\nAll CAT health URL parameters are optional. Parameter Type Description time\nTime\nSpecify the units for time. For example, 5d or 7h. For more information, see Supported units.\nts\nBoolean\nIf true, returns HH:MM:SS and Unix epoch timestamps. Default is true. Response GET _cat/health?v&amp;time= 5 d epoch | timestamp | cluster | status | node.total | node.data | shards | pri | relo | init | unassign | pending_tasks | max_task_wait_time | active_shards_percent 1624248112 | 04: 01: 52 | odfe-cluster | green | 2 | 2 | 16 | 8 | 0 | 0 | 0 | 0 | - | 100.0 %",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-indices/",
    "title": "CAT indices operation",
    "content": "Introduced 1.0\nThe CAT indices operation lists information related to indexes, that is, how much disk space they are using, how many shards they have, their health status, and so on.\nExample GET _cat/indices?v copy To limit the information to a specific index, add the index name after your query. GET _cat/indices/&lt;index&gt;?v copy If you want to get information for more than one index, separate the indices with commas: GET _cat/indices/index 1,index 2,index 3 copy Path and HTTP methods GET _cat/indices/&lt;index&gt;\nGET _cat/indices URL parameters\nAll CAT indices URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description bytes\nByte size\nSpecify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units.\nhealth\nString\nLimit indices based on their health status. Supported values are green, yellow, and red.\ninclude_unloaded_segments\nBoolean\nWhether to include information from segments not loaded into memory. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster manager node. Default is 30 seconds.\npri\nBoolean\nWhether to return information only from the primary shards. Default is false.\ntime\nTime\nSpecify the units for time. For example, 5d or 7h. For more information, see Supported units.\nexpand_wildcards\nEnum\nExpands wildcard expressions to concrete indices. Combine multiple values with commas. Supported values are all, open, closed, hidden, and none. Default is open. Response health | status | index | uuid | pri | rep | docs.count | docs.deleted | store.size | pri.store.size green | open | movies | UZbpfERBQ 1-3 GSH 2 bnM 3 sg | 1 | 1 | 1 | 0 | 7.7 kb | 3.8 kb",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-nodeattrs/",
    "title": "CAT nodeattrs",
    "content": "Introduced 1.0\nThe CAT nodeattrs operation lists the attributes of custom nodes.\nExample GET _cat/nodeattrs?v copy Path and HTTP methods GET _cat/nodeattrs URL parameters\nAll CAT nodeattrs URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description local\nBoolean\nWhether to return information from the local node only instead of from the cluster_manager node. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. Response node | host | ip | attr | value odfe-node 2 | 172.18. 0.3 | 172.18. 0.3 | testattr | test",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-nodes/",
    "title": "CAT nodes operation",
    "content": "Introduced 1.0\nThe CAT nodes operation lists node-level information, including node roles and load metrics.\nA few important node metrics are pid, name, cluster_manager, ip, port, version, build, jdk, along with disk, heap, ram, and file_desc.\nExample GET _cat/nodes?v copy Path and HTTP methods GET _cat/nodes URL parameters\nAll CAT nodes URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description bytes\nByte size\nSpecify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units.\nfull_id\nBoolean\nIf true, return the full node ID. If false, return the shortened node ID. Defaults to false.\nlocal\nBoolean\nWhether to return information from the local node only instead of from the cluster_manager node. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster manager node. Default is 30 seconds.\ntime\nTime\nSpecify the units for time. For example, 5d or 7h. For more information, see Supported units.\ninclude_unloaded_segments\nBoolean\nWhether to include information from segments not loaded into memory. Default is false. Response ip | heap.percent | ram.percent | cpu load_ 1 m | load_ 5 m | load_ 15 m | node.role | node.roles | cluster_manager | name 10.11. 1.225 | 31 | 32 | 0 | 0.00 | 0.00 | di | data,ingest,ml | - | data-e 5 b 89 ad 7",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-pending-tasks/",
    "title": "CAT pending tasks",
    "content": "Introduced 1.0\nThe CAT pending tasks operation lists the progress of all pending tasks, including task priority and time in queue.\nExample GET _cat/pending_tasks?v copy Path and HTTP methods GET _cat/pending_tasks URL parameters\nAll CAT nodes URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description local\nBoolean\nWhether to return information from the local node only instead of from the cluster_manager node. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster manager node. Default is 30 seconds.\ntime\nTime\nSpecify the units for time. For example, 5d or 7h. For more information, see Supported units. Response insertOrder | timeInQueue | priority | source 1786 | 1.8 s | URGENT | shard-started",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-plugins/",
    "title": "CAT plugins",
    "content": "Introduced 1.0\nThe CAT plugins operation lists the names, components, and versions of the installed plugins.\nExample GET _cat/plugins?v copy Path and HTTP methods GET _cat/plugins URL parameters\nAll CAT plugins URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description local\nBoolean\nWhether to return information from the local node only instead of from the cluster_manager node. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster_manager node. Default is 30 seconds. Response name component version odfe-node 2 opendistro-alerting 1.13. 1.0 odfe-node 2 opendistro-anomaly-detection 1.13. 0.0 odfe-node 2 opendistro-asynchronous-search 1.13. 0.1 odfe-node 2 opendistro-index-management 1.13. 2.0 odfe-node 2 opendistro-job-scheduler 1.13. 0.0 odfe-node 2 opendistro-knn 1.13. 0.0 odfe-node 2 opendistro-performance-analyzer 1.13. 0.0 odfe-node 2 opendistro-reports-scheduler 1.13. 0.0 odfe-node 2 opendistro-sql 1.13. 2.0 odfe-node 2 opendistro_security 1.13. 1.0 odfe-node 1 opendistro-alerting 1.13. 1.0 odfe-node 1 opendistro-anomaly-detection 1.13. 0.0 odfe-node 1 opendistro-asynchronous-search 1.13. 0.1 odfe-node 1 opendistro-index-management 1.13. 2.0 odfe-node 1 opendistro-job-scheduler 1.13. 0.0 odfe-node 1 opendistro-knn 1.13. 0.0 odfe-node 1 opendistro-performance-analyzer 1.13. 0.0 odfe-node 1 opendistro-reports-scheduler 1.13. 0.0 odfe-node 1 opendistro-sql 1.13. 2.0 odfe-node 1 opendistro_security 1.13. 1.0",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-recovery/",
    "title": "CAT recovery",
    "content": "Introduced 1.0\nThe CAT recovery operation lists all completed and ongoing index and shard recoveries.\nExample GET _cat/recovery?v copy To see only the recoveries of a specific index, add the index name after your query. GET _cat/recovery/&lt;index&gt;?v copy If you want to get information for more than one index, separate the indices with commas: GET _cat/recovery/index 1,index 2,index 3 copy Path and HTTP methods GET _cat/recovery URL parameters\nAll CAT recovery URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description active_only\nBoolean\nWhether to only include ongoing shard recoveries. Default is false.\nbytes\nByte size\nSpecify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units.\ndetailed\nBoolean\nWhether to include detailed information about shard recoveries. Default is false.\ntime\nTime\nSpecify the units for time. For example, 5d or 7h. For more information, see Supported units. Response index | shard | time | type | stage | source_host | source_node | target_host | target_node | repository | snapshot | files | files_recovered | files_percent | files_total | bytes | bytes_recovered | bytes_percent | bytes_total | translog_ops | translog_ops_recovered | translog_ops_percent movies | 0 | 117 ms | empty_store | done | n/a | n/a | 172.18. 0.4 | odfe-node 1 | n/a | n/a | 0 | 0 | 0.0 % | 0 | 0 | 0 | 0.0 % | 0 | 0 | 0 | 100.0 % movies | 0 | 382 ms | peer | done | 172.18. 0.4 | odfe-node 1 | 172.18. 0.3 | odfe-node 2 | n/a | n/a | 1 | 1 | 100.0 % | 1 | 208 | 208 | 100.0 % | 208 | 1 | 1 | 100.0 %",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-repositories/",
    "title": "CAT repositories",
    "content": "Introduced 1.0\nThe CAT repositories operation lists all completed and ongoing index and shard recoveries.\nExample GET _cat/repositories?v copy Path and HTTP methods GET _cat/repositories URL parameters\nAll CAT repositories URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description local\nBoolean\nWhether to return information from the local node only instead of from the cluster_manager node. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster_manager node. Default is 30 seconds. Response id type repo 1 fs repo 2 s 3",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-segment-replication/",
    "title": "CAT segment replication",
    "content": "Introduced 2.7\nThe CAT segment replication operation returns information about active and last completed segment replication events on each replica shard, including related shard-level metrics. These metrics provide information about how far behind the primary shard the replicas are lagging.\nCall the CAT Segment Replication API only on indexes with segment replication enabled.\nPath and HTTP methods GET /_cat/segment_replication GET /_cat/segment_replication/&lt;index&gt; Path parameters\nThe following table lists the available optional path parameter. Parameter Type Description index String\nThe name of the index, or a comma-separated list or wildcard expression of index names used to filter results. If this parameter is not provided, the response contains information about all indexes in the cluster. Query parameters\nThe CAT segment replication API operation supports the following optional query parameters. Parameter Data type Description active_only Boolean\nIf true, the response only includes active segment replications. Defaults to false. detailed String\nIf true, the response includes additional metrics for each stage of a segment replication event. Defaults to false. shards String\nA comma-separated list of shards to display. format String\nA short version of the HTTP accept header. Valid values include JSON and YAML. h String\nA comma-separated list of column names to display. help Boolean\nIf true, the response includes help information. Defaults to false. time Time value Units used to display time values. Defaults to ms (milliseconds). v Boolean\nIf true, the response includes column headings. Defaults to false. s String\nSpecifies to sort the results. For example, s=shardId:desc sorts by shardId in descending order. Examples\nThe following examples illustrate various segment replication responses.\nExample 1: No active segment replication events\nThe following query requests segment replication metrics with column headings for all indexes: GET /_cat/segment_replication?v= true copy The response contains the metrics for the preceding request: shardId target_node target_host checkpoints_behind bytes_behind current_lag last_completed_lag rejected_requests [ index-1][0] runTask-1 127.0.0.1 0 0b 0s 7ms 0 Example 2: Shard ID specified\nThe following query requests segment replication metrics with column headings for shards with the ID 0 from indexes index1 and index2: GET /_cat/segment_replication/index 1,index 2?v= true &amp;shards= 0 copy The response contains the metrics for the preceding request. The column headings correspond to the metric names: shardId target_node target_host checkpoints_behind bytes_behind current_lag last_completed_lag rejected_requests [ index-1][0] runTask-1 127.0.0.1 0 0b 0s 3ms 0 [ index-2][0] runTask-1 127.0.0.1 0 0b 0s 5ms 0 Example 3: Detailed response\nThe following query requests detailed segment replication metrics with column headings for all indexes: GET /_cat/segment_replication?v= true &amp;detailed= true copy The response contains additional metrics about the files and stages of a segment replication event: shardId target_node target_host checkpoints_behind bytes_behind current_lag last_completed_lag rejected_requests stage time files_fetched files_percent bytes_fetched bytes_percent start_time stop_time files files_total bytes bytes_total replicating_stage_time_taken get_checkpoint_info_stage_time_taken file_diff_stage_time_taken get_files_stage_time_taken finalize_replication_stage_time_taken [ index-1][0] runTask-1 127.0.0.1 0 0b 0s 3ms 0 done 10ms 6 100.0% 4753 100.0% 2023-03-16T13:46:16.802Z 2023-03-16T13:46:16.812Z 6 6 4.6kb 4.6kb 0s 2ms 0s 3ms 3ms [ index-2][0] runTask-1 127.0.0.1 0 0b 0s 5ms 0 done 7ms 3 100.0% 3664 100.0% 2023-03-16T13:53:33.466Z 2023-03-16T13:53:33.474Z 3 3 3.5kb 3.5kb 0s 1ms 0s 2ms 2ms Example 4: Sorting the results\nThe following query requests segment replication metrics with column headings for all indexes, sorted by shard ID in descending order: GET /_cat/segment_replication?v&amp;s=shardId:desc copy The response contains the sorted results: shardId target_node target_host checkpoints_behind bytes_behind current_lag last_completed_lag rejected_requests [ test6][1] runTask-2 127.0.0.1 0 0b 0s 5ms 0 [ test6][0] runTask-2 127.0.0.1 0 0b 0s 4ms 0 Example 5: Using a metric alias\nIn a request, you can either use a metric’s full name or one of its aliases. The following query is the same as the preceding query, but it uses the alias s instead of shardID for sorting: GET /_cat/segment_replication?v&amp;s=s:desc copy Response metrics\nThe following table lists the response metrics that are returned for all requests. When referring to a metric in a query parameter, you can provide either the metric’s full name or any of its aliases, as shown in the previous example. Metric Alias Description shardId s The ID of a specific shard. target_host thost The target host IP address. target_node tnode The target node name. checkpoints_behind cpb The number of checkpoints by which the replica shard is behind the primary shard. bytes_behind bb The number of bytes by which the replica shard is behind the primary shard. current_lag clag The time elapsed while waiting for a replica shard to catch up to the primary shard. last_completed_lag lcl The time taken for a replica shard to catch up to the latest primary shard refresh. rejected_requests rr The number of rejected requests for the replication group. Additional detailed response metrics\nThe following table lists the additional response fields returned if detailed is set to true. Metric Alias Description stage st The current stage of a segment replication event. time t, ti The amount of time a segment replication event took to complete, in milliseconds. files_fetched ff The number of files fetched so far for a segment replication event. files_percent fp The percentage of files fetched so far for a segment replication event. bytes_fetched bf The number of bytes fetched so far for a segment replication event. bytes_percent bp The number of bytes fetched so far for a segment replication event as a percentage. start_time start The segment replication start time. stop_time stop The segment replication stop time. files f The number of files that needs to be fetched for a segment replication event. files_total tf The total number of files that are part of this recovery, including both reused and recovered files. bytes b The number of bytes that needs to be fetched for a segment replication event. bytes_total tb The total number of bytes in the shard. replicating_stage_time_taken rstt The amount of time the replicating stage of a segment replication event took to complete. get_checkpoint_info_stage_time_taken gcistt The amount of time the get checkpoint info stage of a segment replication event took to complete. file_diff_stage_time_taken fdstt The amount of time the file diff stage of a segment replication event took to complete. get_files_stage_time_taken gfstt The amount of time the get files stage of a segment replication event took to complete. finalize_replication_stage_time_taken frstt The amount of time the finalize replication stage of a segment replication event took to complete.",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-segments/",
    "title": "CAT segments",
    "content": "Introduced 1.0\nThe cat segments operation lists Lucene segment-level information for each index.\nExample GET _cat/segments?v copy To see only the information about segments of a specific index, add the index name after your query. GET _cat/segments/&lt;index&gt;?v copy If you want to get information for more than one index, separate the indices with commas: GET _cat/segments/index1,index2,index3 copy Path and HTTP methods GET _cat/segments URL parameters\nAll CAT segments URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description bytes\nByte size\nSpecify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units..\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. Response index | shard | prirep | ip | segment | generation | docs.count | docs.deleted | size | size.memory | committed | searchable | version | compound movies | 0 | p | 172.18. 0.4 | _ 0 | 0 | 1 | 0 | 3.5 kb | 1364 | true | true | 8.7. 0 | true movies | 0 | r | 172.18. 0.3 | _ 0 | 0 | 1 | 0 | 3.5 kb | 1364 | true | true | 8.7. 0 | true",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-shards/",
    "title": "CAT shards",
    "content": "Introduced 1.0\nThe CAT shards operation lists the state of all primary and replica shards and how they are distributed.\nExample GET _cat/shards?v copy To see only the information about shards of a specific index, add the index name after your query. GET _cat/shards/&lt;index&gt;?v copy If you want to get information for more than one index, separate the indices with commas: GET _cat/shards/index1,index2,index3 copy Path and HTTP methods GET _cat/shards URL parameters\nAll cat shards URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description bytes\nByte size\nSpecify the units for byte size. For example, 7kb or 6gb. For more information, see Supported units.\nlocal\nBoolean\nWhether to return information from the local node only instead of from the cluster_manager node. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster_manager node. Default is 30 seconds.\ntime\nTime\nSpecify the units for time. For example, 5d or 7h. For more information, see Supported units. Response index | shard | prirep | state | docs | store | ip | | node plugins | 0 | p | STARTED | 0 | 208 b | 172.18. 0.4 | odfe-node 1 plugins | 0 | r | STARTED | 0 | 208 b | 172.18. 0.3 | odfe-node 2",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-snapshots/",
    "title": "CAT snapshots",
    "content": "Introduced 1.0\nThe CAT snapshots operation lists all snapshots for a repository.\nExample GET _cat/snapshots?v copy Path and HTTP methods GET _cat/snapshots URL parameters\nAll CAT snapshots URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameter: Parameter Type Description cluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster manager node. Default is 30 seconds.\ntime\nTime\nSpecify the units for time. For example, 5d or 7h. For more information, see Supported units. Response index | shard | prirep | state | docs | store | ip | | node plugins | 0 | p | STARTED | 0 | 208 b | 172.18. 0.4 | odfe-node 1 plugins | 0 | r | STARTED | 0 | 208 b | 172.18. 0.3 | odfe-node 2",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-tasks/",
    "title": "CAT tasks",
    "content": "Introduced 1.0\nThe CAT tasks operation lists the progress of all tasks currently running on your cluster.\nExample GET _cat/tasks?v copy Path and HTTP methods GET _cat/tasks URL parameters\nAll CAT tasks URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description nodes\nList\nA comma-separated list of node IDs or names to limit the returned information. Use _local to return information from the node you’re connecting to, specify the node name to get information from specific nodes, or keep the parameter empty to get information from all nodes.\ndetailed\nBoolean\nReturns detailed task information. (Default: false)\nparent_task_id\nString\nReturns tasks with a specified parent task ID (node_id:task_number). Keep empty or set to -1 to return all.\ntime\nTime\nSpecify the units for time. For example, 5d or 7h. For more information, see Supported units. Response action | task_id | parent_task_id | type | start_time | timestamp | running_time | ip | node cluster:monitor/tasks/lists | 1 vo 54 NuxSxOrbPEYdkSF 0 w: 168062 | - | transport | 1624337809471 | 04: 56: 49 | 489.5 ms | 172.18. 0.4 | odfe-node 1",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-templates/",
    "title": "CAT templates",
    "content": "Introduced 1.0\nThe CAT templates operation lists the names, patterns, order numbers, and version numbers of index templates.\nExample GET _cat/templates?v copy If you want to get information for a specific template or pattern: GET _cat/templates/&lt;template_name_or_pattern&gt; copy Path and HTTP methods GET _cat/templates copy URL parameters\nAll CAT templates URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description local\nBoolean\nWhether to return information from the local node only instead of from the cluster manager node. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster manager node. Default is 30 seconds. Response name | index_patterns order version composed_of\ntenant_template | [opensearch-dashboards*] | 0 | To learn more about index templates, see Index templates.",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/cat-thread-pool/",
    "title": "CAT thread pool",
    "content": "Introduced 1.0\nThe CAT thread pool operation lists the active, queued, and rejected threads of different thread pools on each node.\nExample GET _cat/thread_pool?v copy If you want to get information for more than one thread pool, separate the thread pool names with commas: GET _cat/thread_pool/thread_pool_name_1,thread_pool_name_2,thread_pool_name_3 copy If you want to limit the information to a specific thread pool, add the thread pool name after your query: GET _cat/thread_pool/&lt;thread_pool_name&gt;?v copy Path and HTTP methods GET _cat/thread_pool URL parameters\nAll CAT thread pool URL parameters are optional.\nIn addition to the common URL parameters, you can specify the following parameters: Parameter Type Description local\nBoolean\nWhether to return information from the local node only instead of from the cluster_manager node. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster_manager node. Default is 30 seconds. Response node_name name active queue rejected odfe-node 2 ad-batch-task-threadpool 0 0 0 odfe-node 2 ad-threadpool 0 0 0 odfe-node 2 analyze 0 0 0 s",
    "ancestors": [
      "API reference",
      "CAT API"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cat/index/",
    "title": "CAT API",
    "content": "You can get essential statistics about your cluster in an easy-to-understand, tabular format using the compact and aligned text (CAT) API. The CAT API is a human-readable interface that returns plain text instead of traditional JSON.\nUsing the CAT API, you can answer questions like which node is the elected master, what state is the cluster in, how many documents are in each index, and so on.\nExample\nTo see the available operations in the CAT API, use the following command: GET _cat copy Optional query parameters\nYou can use the following query parameters with any CAT API to filter your results. Parameter Description v Provides verbose output by adding headers to the columns. It also adds some formatting to help align each of the columns together. All examples in this section include the v parameter. help Lists the default and other available headers for a given operation. h Limits the output to specific headers. format Returns the result in JSON, YAML, or CBOR formats. sort Sorts the output by the specified columns. Query parameter usage examples\nYou can specify a query parameter to any CAT operation to obtain more specific results.\nGet verbose output\nTo query aliases and get verbose output that includes all column headings in the response, use the v query parameter. GET _cat/aliases?v copy The response provides more details, such as names of each column in the response. alias index filter routing.index routing.search is_write_index.kibana.kibana_1 - - - -\nsample-alias1 sample-index-1 - - - - Without the verbose parameter, v, the response simply returns the alias names:.kibana.kibana_1 - - - -\nsample-alias1 sample-index-1 - - - - Get all available headers\nTo see all the available headers, use the help parameter: GET _cat/&lt;operation_name&gt;?help Get a subset of headers\nTo limit the output to a subset of headers, use the h parameter: GET _cat/&lt;operation_name&gt;?h=&lt;header_name_1&gt;,&lt;header_name_2&gt;&amp;v Typically, for any operation you can find out what headers are available using the help parameter, and then use the h parameter to limit the output to only the headers that you care about.\nIf you use the Security plugin, make sure you have the appropriate permissions.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cluster-api/cluster-allocation/",
    "title": "Cluster allocation explain",
    "content": "Introduced 1.0\nThe most basic cluster allocation explain request finds an unassigned shard and explains why it can’t be allocated to a node.\nIf you add some options, you can instead get information on a specific shard, including why OpenSearch assigned it to its current node.\nExample GET _cluster/allocation/explain?include_yes_decisions= true { \"index\": \"movies\", \"shard\": 0, \"primary\": true } copy Path and HTTP methods GET _cluster/allocation/explain\nPOST _cluster/allocation/explain URL parameters\nAll cluster allocation explain parameters are optional. Parameter Type Description include_yes_decisions\nBoolean\nOpenSearch makes a series of yes or no decisions when trying to allocate a shard to a node. If this parameter is true, OpenSearch includes the (generally more numerous) “yes” decisions in its response. Default is false.\ninclude_disk_info\nBoolean\nWhether to include information about disk usage in the response. Default is false. Request body\nAll cluster allocation explain fields are optional. Field Type Description current_node\nString\nIf you only want an explanation if the shard happens to be on a particular node, specify that node name here.\nindex\nString\nThe name of the shard’s index.\nprimary\nBoolean\nWhether to provide an explanation for the primary shard (true) or its first replica (false), which share the same shard ID.\nshard\nInteger\nThe shard ID that you want an explanation for. Response { \"index\": \"movies\", \"shard\": 0, \"primary\": true, \"current_state\": \"started\", \"current_node\": { \"id\": \"d8jRZcW1QmCBeVFlgOJx5A\", \"name\": \"opensearch-node1\", \"transport_address\": \"172.24.0.4:9300\", \"weight_ranking\": 1 }, \"can_remain_on_current_node\": \"yes\", \"can_rebalance_cluster\": \"yes\", \"can_rebalance_to_other_node\": \"no\", \"rebalance_explanation\": \"cannot rebalance as no target node exists that can both allocate this shard and improve the cluster balance\", \"node_allocation_decisions\": [{ \"node_id\": \"vRxi4uPcRt2BtHlFoyCyTQ\", \"node_name\": \"opensearch-node2\", \"transport_address\": \"172.24.0.3:9300\", \"node_decision\": \"no\", \"weight_ranking\": 1, \"deciders\": [{ \"decider\": \"max_retry\", \"decision\": \"YES\", \"explanation\": \"shard has no previous failures\" }, { \"decider\": \"replica_after_primary_active\", \"decision\": \"YES\", \"explanation\": \"shard is primary and can be allocated\" }, { \"decider\": \"enable\", \"decision\": \"YES\", \"explanation\": \"all allocations are allowed\" }, { \"decider\": \"node_version\", \"decision\": \"YES\", \"explanation\": \"can relocate primary shard from a node with version [1.0.0] to a node with equal-or-newer version [1.0.0]\" }, { \"decider\": \"snapshot_in_progress\", \"decision\": \"YES\", \"explanation\": \"no snapshots are currently running\" }, { \"decider\": \"restore_in_progress\", \"decision\": \"YES\", \"explanation\": \"ignored as shard is not being recovered from a snapshot\" }, { \"decider\": \"filter\", \"decision\": \"YES\", \"explanation\": \"node passes include/exclude/require filters\" }, { \"decider\": \"same_shard\", \"decision\": \"NO\", \"explanation\": \"a copy of this shard is already allocated to this node [[movies][0], node[vRxi4uPcRt2BtHlFoyCyTQ], [R], s[STARTED], a[id=x8w7QxWdQQa188HKGn0iMQ]]\" }, { \"decider\": \"disk_threshold\", \"decision\": \"YES\", \"explanation\": \"enough disk for shard on node, free: [35.9gb], shard size: [15.1kb], free after allocating shard: [35.9gb]\" }, { \"decider\": \"throttling\", \"decision\": \"YES\", \"explanation\": \"below shard recovery limit of outgoing: [0 &lt; 2] incoming: [0 &lt; 2]\" }, { \"decider\": \"shards_limit\", \"decision\": \"YES\", \"explanation\": \"total shard limits are disabled: [index: -1, cluster: -1] &lt;= 0\" }, { \"decider\": \"awareness\", \"decision\": \"YES\", \"explanation\": \"allocation awareness is not enabled, set cluster setting [cluster.routing.allocation.awareness.attributes] to enable it\" }] }] }",
    "ancestors": [
      "API reference",
      "Cluster APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cluster-api/cluster-awareness/",
    "title": "Cluster routing and awareness",
    "content": "To control the distribution of search or HTTP traffic, you can use the weights per awareness attribute to control the distribution of search or HTTP traffic across zones. This is commonly used for zonal deployments, heterogeneous instances, and routing traffic away from zones during zonal failure.\nPath and HTTP methods PUT /_cluster/routing/awareness/&lt;attribute&gt;/weights\nGET /_cluster/routing/awareness/&lt;attribute&gt;/weights?local\nGET /_cluster/routing/awareness/&lt;attribute&gt;/weights Path parameters Parameter Type Description attribute\nString\nThe name of the awareness attribute, usually zone. The attribute name must match the values listed in the request body when assigning weights to zones. Request body parameters Parameter Type Description weights\nJSON object\nAssigns weights to attributes within the request body of the PUT request. Weights can be set in any ratio, for example, 2:3:5. In a 2:3:5 ratio with 3 zones, for every 100 requests sent to the cluster, each zone would receive either 20, 30, or 50 search requests in a random order. When assigned a weight of 0, the zone does not receive any search traffic.\n_version\nString\nImplements optimistic concurrency control (OCC) through versioning. The parameter uses simple versioning, such as 1, and increments upward based on each subsequent modification. This allows any servers from which a request originates to validate whether or not a zone has been modified. In the following example request body, zone_1 and zone_2 receive 50 requests each, whereas zone_3 is prevented from receiving requests: {\n\"weights\":\n{\n\"zone_1\": \"5\",\n\"zone_2\": \"5\",\n\"zone_3\": \"0\"\n}\n\"_version\": 1\n} Example: Weighted round robin search\nThe following example request creates a round robin shard allocation for search traffic by using an undefined ratio:\nRequest PUT /_cluster/routing/awareness/zone/weights { \"weights\": { \"zone_1\": \"1\", \"zone_2\": \"1\", \"zone_3\": \"0\" } \"_version\": 1 } copy Response {\n\"acknowledged\": true\n} Example: Getting weights for all zones\nThe following example request gets weights for all zones.\nRequest GET /_cluster/routing/awareness/zone/weights copy Response\nOpenSearch responds with the weight of each zone: { \"weights\": { \"zone_1\": \"1.0\", \"zone_2\": \"1.0\", \"zone_3\": \"0.0\" }, \"_version\": 1 } Example: Deleting weights\nYou can remove your weight ratio for each zone using the DELETE method.\nRequest DELETE /_cluster/routing/awareness/zone/weights copy Response { \"_version\": 1 } Next steps\nFor more information about zone commissioning, see Cluster decommission.\nFor more information about allocation awareness, see Cluster formation.",
    "ancestors": [
      "API reference",
      "Cluster APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cluster-api/cluster-decommission/",
    "title": "Cluster decommission",
    "content": "The cluster decommission operation adds support decommissioning based on awareness. It greatly benefits multi-zone deployments, where awareness attributes, such as zones, can aid in applying new upgrades to a cluster in a controlled fashion. This is especially useful during outages, in which case, you can decommission the unhealthy zone to prevent replication requests from stalling and prevent your request backlog from becoming too large.\nFor more information about allocation awareness, see Shard allocation awareness.\nHTTP and Path methods PUT /_cluster/decommission/awareness/{awareness_attribute_name}/{awareness_attribute_value}\nGET /_cluster/decommission/awareness/{awareness_attribute_name}/_status\nDELETE /_cluster/decommission/awareness URL parameters Parameter Type Description awareness_attribute_name\nString\nThe name of awareness attribute, usually zone.\nawareness_attribute_value\nString\nThe value of the awareness attribute. For example, if you have shards allocated in two different zones, you can give each zone a value of zone-a or zoneb. The cluster decommission operation decommissions the zone listed in the method. Example: Decommissioning and recommissioning a zone\nYou can use the following example requests to decommission and recommission a zone:\nRequest\nThe following example request decommissions zone-a: PUT /_cluster/decommission/awareness/&lt;zone&gt;/&lt;zone-a&gt; copy If you want to recommission a decommissioned zone, you can use the DELETE method: DELETE /_cluster/decommission/awareness copy Response { \"acknowledged\": true } Example: Getting zone decommission status\nThe following example requests returns the decommission status of all zones.\nRequest GET /_cluster/decommission/awareness/zone/_status copy Response { \"zone-1\": \"INIT | DRAINING | IN_PROGRESS | SUCCESSFUL | FAILED\" } Next steps\nFor more information about zone awareness and weight, see Cluster awareness.\nFor more information about allocation awareness, see Cluster formation.",
    "ancestors": [
      "API reference",
      "Cluster APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cluster-api/cluster-health/",
    "title": "Cluster health",
    "content": "Introduced 1.0\nThe most basic cluster health request returns a simple status of the health of your cluster. OpenSearch expresses cluster health in three colors: green, yellow, and red. A green status means all primary shards and their replicas are allocated to nodes. A yellow status means all primary shards are allocated to nodes, but some replicas aren’t. A red status means at least one primary shard is not allocated to any node.\nTo get the status of a specific index, provide the index name.\nExample\nThis request waits 50 seconds for the cluster to reach the yellow status or better: GET _cluster/health?wait_for_status=yellow&amp;timeout=50s copy If the cluster health becomes yellow or green before 50 seconds elapse, it returns a response immediately. Otherwise it returns a response as soon as it exceeds the timeout.\nPath and HTTP methods GET _cluster/health\nGET _cluster/health/&lt;index&gt; Query parameters\nThe following table lists the available query parameters. All query parameters are optional. Parameter Type Description expand_wildcards\nEnum\nExpands wildcard expressions to concrete indexes. Combine multiple values with commas. Supported values are all, open, closed, hidden, and none. Default is open.\nlevel\nEnum\nThe level of detail for returned health information. Supported values are cluster, indices, shards, and awareness_attributes. Default is cluster.\nawareness_attribute\nString\nThe name of the awareness attribute, for which to return cluster health (for example, zone). Applicable only if level is set to awareness_attributes.\nlocal\nBoolean\nWhether to return information from the local node only instead of from the cluster manager node. Default is false.\ncluster_manager_timeout\nTime\nThe amount of time to wait for a connection to the cluster manager node. Default is 30 seconds.\ntimeout\nTime\nThe amount of time to wait for a response. If the timeout expires, the request fails. Default is 30 seconds.\nwait_for_active_shards\nString\nWait until the specified number of shards is active before returning a response. all for all shards. Default is 0.\nwait_for_nodes\nString\nWait for N number of nodes. Use 12 for exact match, &gt;12 and &lt;12 for range.\nwait_for_events\nEnum\nWait until all currently queued events with the given priority are processed. Supported values are immediate, urgent, high, normal, low, and languid.\nwait_for_no_relocating_shards\nBoolean\nWhether to wait until there are no relocating shards in the cluster. Default is false.\nwait_for_no_initializing_shards\nBoolean\nWhether to wait until there are no initializing shards in the cluster. Default is false.\nwait_for_status\nEnum\nWait until the cluster health reaches the specified status or better. Supported values are green, yellow, and red.\nweights\nJSON object\nAssigns weights to attributes within the request body of the PUT request. Weights can be set in any ration, for example, 2:3:5. In a 2:3:5 ratio with three zones, for every 100 requests sent to the cluster, each zone would receive either 20, 30, or 50 search requests in a random order. When assigned a weight of 0, the zone does not receive any search traffic. Example request\nThe following example request retrieves cluster health for all indexes in the cluster: GET _cluster/health copy Example response\nThe response contains cluster health information: { \"cluster_name\": \"opensearch-cluster\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 2, \"number_of_data_nodes\": 2, \"discovered_master\": true, \"active_primary_shards\": 6, \"active_shards\": 12, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0, \"delayed_unassigned_shards\": 0, \"number_of_pending_tasks\": 0, \"number_of_in_flight_fetch\": 0, \"task_max_waiting_in_queue_millis\": 0, \"active_shards_percent_as_number\": 100.0 } Response fields\nThe following table lists all response fields. Field Data type Description cluster_name\nString\nThe name of the cluster.\nstatus\nString\nThe cluster health status, which represents the state of shard allocation in the cluster. May be green, yellow, or red.\nnumber_of_nodes\nInteger\nThe number of nodes in the cluster.\nnumber_of_data_nodes\nInteger\nThe number of data nodes in the cluster.\ndiscovered_cluster_manager\nBoolean\nSpecifies whether the cluster manager is discovered.\nactive_primary_shards\nInteger\nThe number of active primary shards.\nactive_shards\nInteger\nThe total number of active shards, including primary and replica shards.\nrelocating_shards\nInteger\nThe number of relocating shards.\ninitializing_shards\nInteger\nThe number of intializing shards.\nunassigned_shards\nInteger\nThe number of unassigned shards.\ndelayed_unassigned_shards\nInteger\nThe number of delayed unassigned shards.\nnumber_of_pending_tasks\nInteger\nThe number of pending tasks in the cluster.\nnumber_of_in_flight_fetch\nInteger\nThe number of unfinished fetches.\ntask_max_waiting_in_queue_millis\nInteger\nThe maximum wait time for all tasks waiting to be performed, in milliseconds.\nactive_shards_percent_as_number\nDouble\nThe percentage of active shards in the cluster.\nawareness_attributes\nObject\nContains cluster health information for each awareness attribute. Returning cluster health by awareness attribute\nTo check cluster health by awareness attribute (for example, zone or rack), specify awareness_attributes in the level query parameter: GET _cluster/health?level=awareness_attributes copy The response contains cluster health metrics partitioned by awareness attribute: { \"cluster_name\": \"runTask\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 3, \"number_of_data_nodes\": 3, \"discovered_master\": true, \"discovered_cluster_manager\": true, \"active_primary_shards\": 0, \"active_shards\": 0, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0, \"delayed_unassigned_shards\": 0, \"number_of_pending_tasks\": 0, \"number_of_in_flight_fetch\": 0, \"task_max_waiting_in_queue_millis\": 0, \"active_shards_percent_as_number\": 100, \"awareness_attributes\": { \"zone\": { \"zone-3\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 }, \"zone-1\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 }, \"zone-2\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 } }, \"rack\": { \"rack-3\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 }, \"rack-1\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 }, \"rack-2\": { \"active_shards\": 0, \"initializing_shards\": 0, \"relocating_shards\": 0, \"unassigned_shards\": 0, \"data_nodes\": 1, \"weight\": 1 } } } } If you’re interested in a particular awareness attribute, you can include the name of the awareness attribute as a query parameter: GET _cluster/health?level=awareness_attributes&amp;awareness_attribute=zone copy In response to the preceding request, OpenSearch returns cluster health information only for the zone awareness attribute.\nThe unassigned shard information will be accurate only if you enable replica count enforcement and configure forced awareness for the awareness attribute either before cluster start or after cluster start but before any indexing requests. If you enable replica enforcement after the cluster receives indexing requests, the unassigned shard information may be inaccurate. If you don’t configure replica count enforcement and forced awareness, the unassigned_shards field will contain -1.\nRequired permissions\nIf you use the Security plugin, make sure you have the appropriate permissions: cluster:monitor/health.",
    "ancestors": [
      "API reference",
      "Cluster APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cluster-api/cluster-settings/",
    "title": "Cluster settings",
    "content": "Introduced 1.0\nThe cluster settings operation lets you check the current settings for your cluster, review default settings, and change settings. When you update a setting using the API, OpenSearch applies it to all nodes in the cluster.\nPath and HTTP methods GET _cluster/settings\nPUT _cluster/settings Path parameters\nAll cluster setting parameters are optional. Parameter Data type Description flat_settings\nBoolean\nWhether to return settings in the flat form, which can improve readability, especially for heavily nested settings. For example, the flat form of \"cluster\": { \"max_shards_per_node\": 500 } is \"cluster.max_shards_per_node\": \"500\".\ninclude_defaults (GET only)\nBoolean\nWhether to include default settings as part of the response. This parameter is useful for identifying the names and current values of settings you want to update.\ncluster_manager_timeout\nTime unit\nThe amount of time to wait for a response from the cluster manager node. Default is 30 seconds.\ntimeout (PUT only)\nTime unit\nThe amount of time to wait for a response from the cluster. Default is 30 seconds. Example request GET _cluster/settings?include_defaults= true copy Example response PUT _cluster/settings { \"persistent\":{ \"action.auto_create_index\": false } } Request fields\nThe GET operation has no request body options. All cluster setting field parameters are optional.\nNot all cluster settings can be updated using the cluster settings API. You will receive the error message \"setting [cluster.some.setting], not dynamically updateable\" when trying to configure these settings via the API.\nThe following request field parameters are compatible with the cluster API. Field Data type Description action.auto_create_index\nBoolean\nAutomatically creates an index if the index doesn’t already exist. Also applies any index templates that are configured. Default is true.\naction.destructive_requires_name\nBoolean\nWhen set to true, you must specify the index name to delete an index. You cannot delete all indexes or use wildcards. Default is true.\ncluster.indices.close.enable\nBoolean\nEnables closing of open indexes in OpenSearch. Default is true.\nindices.recovery.max_bytes_per_sec\nString\nLimits the total inbound and outbound recovery traffic for each node. This applies to peer recoveries and snapshot recoveries. Default is 40mb. If you set the recovery traffic value to less than or equal to 0mb, rate limiting will be disabled, which causes recovery data to be transferred at the highest possible rate.\nindices.recovery.max_concurrent_file_chunks\nInteger\nThe number of file chunks sent in parallel for each recovery operation. Default is 2.\nindices.recovery.max_concurrent_operations\nInteger\nThe number of operations sent in parallel for each recovery. Default is 1.\nlogger.org.opensearch.discovery\nString\nLoggers accept Log4j2’s built-in log levels: OFF, FATAL, ERROR, WARN, INFO, DEBUG, and TRACE. Default is INFO.\nbreaker.model_inference.limit\nString\nThe limit for the trained model circuit breaker. Default is 50% of the JVM heap.\nbreaker.model_inference.overhead\nInteger\nThe constant that all trained model estimations are multiplied by to determine a final estimation. Default is 1.\nsearch.max_buckets\nInteger\nThe maximum number of aggregation buckets allowed in a single response. Default is 65536.\nsnapshot.max_concurrent_operations\nInteger\nThe maximum number of concurrent snapshot operations. Default is 1000.\nslm.health.failed_snapshot_warn_threshold\nString\nThe number of failed invocations since the last successful snapshot that will indicate a problem as per the health API profile. Default is five repeated failures: 5L.\nindices.breaker.total.limit\nString\nThe starting limit for the overall parent breaker. Default is 70% of the JVM heap if indices.breaker.total.use_real_memory is set to false. Default is 95% of the JVM heap if indices.breaker.total.use_real_memory is set to true.\nindices.breaker.fielddata.limit\nString\nThe limit for the fielddata breaker. Default is 40% of the JVM heap.\nindices.breaker.fielddata.overhead\nFloating point\nThe constant that all fielddata estimations are multiplied by to determine a final estimation. Default is 1.03.\nindices.breaker.request.limit\nString\nThe limit for the request breaker. Default is 60% of the JVM heap.\nindices.breaker.request.overhead\nInteger\nThe constant that all request estimations are multiplied by to determine a final estimation. Default is 1.\nnetwork.breaker.inflight_requests.limit\nString\nThe limit for the in-flight requests breaker. Default is 100% of the JVM heap.\nnetwork.breaker.inflight_requests.overhead\nInteger/Time unit\nThe constant that all in-flight request estimations are multiplied by to determine a final estimation. Default is 2.\nscript.max_compilations_rate\nString\nThe limit for the number of unique dynamic scripts within a defined interval that are allowed to be compiled. Default is 150 every 5 minutes: 150/5m.\ncluster.routing.allocation.enable\nString\nEnables or disables allocation for specific kinds of shards: all – Allows shard allocation for all types of shards. primaries – Allows shard allocation for primary shards only. new_primaries – Allows shard allocation for primary shards for new indexes only. none – No shard allocations are allowed for any indexes. Default is all.\ncluster.routing.allocation.node_concurrent_incoming_recoveries\nInteger\nConfigures how many concurrent incoming shard recoveries are allowed to happen on a node. Default is 2.\ncluster.routing.allocation.node_concurrent_outgoing_recoveries\nInteger\nConfigures how many concurrent outgoing shard recoveries are allowed to happen on a node. Default is 2.\ncluster.routing.allocation.node_concurrent_recoveries\nString\nUsed to set cluster.routing.allocation.node_concurrent_incoming_recoveries and cluster.routing.allocation.node_concurrent_outgoing_recoveries to the same value.\ncluster.routing.allocation.node_initial_primaries_recoveries\nInteger\nSets the number of recoveries for unassigned primaries after a node restart. Default is 4.\ncluster.routing.allocation.same_shard.host\nBoolean\nWhen set to true, multiple copies of a shard are prevented from being allocated to distinct nodes on the same host. Default is false.\ncluster.routing.rebalance.enable\nString\nEnables or disables rebalancing for specific kinds of shards: all – Allows shard balancing for all types of shards. primaries – Allows shard balancing for primary shards only. replicas – Allows shard balancing for replica shards only. none – No shard balancing is allowed for any indexes. Default is all.\ncluster.routing.allocation.allow_rebalance\nString\nSpecifies when shard rebalancing is allowed: always – Always allow rebalancing. indices_primaries_active – Only allow rebalancing when all primaries in the cluster are allocated. indices_all_active – Only allow rebalancing when all shards in the cluster are allocated. Default is indices_all_active.\ncluster.routing.allocation.cluster_concurrent_rebalance\nInteger\nAllows you to control how many concurrent shard rebalances are allowed across a cluster. Default is 2.\ncluster.routing.allocation.balance.shard\nFloating point\nDefines the weight factor for the total number of shards allocated per node. Default is 0.45.\ncluster.routing.allocation.balance.index\nFloating point\nDefines the weight factor for the number of shards per index allocated on a node. Default is 0.55.\ncluster.routing.allocation.balance.threshold\nFloating point\nThe minimum optimization value of operations that should be performed. Default is 1.0.\ncluster.routing.allocation.balance.prefer_primary\nBoolean\nWhen set to true, OpenSearch attempts to evenly distribute the primary shards between the cluster nodes. Enabling this setting does not always guarantee an equal number of primary shards on each node, especially in the event of failover. Changing this setting to false after it was set to true does not invoke redistribution of primary shards. Default is false.\ncluster.routing.allocation.disk.threshold_enabled\nBoolean\nWhen set to false, disables the disk allocation decider. This will also remove any existing index.blocks.read_only_allow_delete index blocks when disabled. Default is true.\ncluster.routing.allocation.disk.watermark.low\nString\nControls the low watermark for disk usage. When set to a percentage, OpenSearch will not allocate shards to nodes with that percentage of disk used. This can also be entered as ratio value, like 0.85. Finally, this can also be set to a byte value, like 400mb. This setting does not affect the primary shards of newly-created indexes, but will prevent their replicas from being allocated. Default is 85%.\ncluster.routing.allocation.disk.watermark.high\nString\nControls the high watermark. OpenSearch will attempt to relocate shards away from a node whose disk usage is above the percentage defined. This can also be entered as a ratio value, like 0.85. Finally, this can also be set to a byte value, like 400mb. This setting affects the allocation of all shards. Default is 90%.\ncluster.routing.allocation.disk.watermark.flood_stage\nString\nControls the flood stage watermark. This is a last resort to prevent nodes from running out of disk space. OpenSearch enforces a read-only index block ( index.blocks.read_only_allow_delete) on every index that has one or more shards allocated on the node, and that has at least one disk exceeding the flood stage. The index block is released once the disk utilization falls below the high watermark. This can also be entered as a ratio value, like 0.85. Finally, this can also be set to a byte value, like 400mb. Default is 95%.\ncluster.info.update.interval\nTime unit\nSets how often OpenSearch should check disk usage for each node in the cluster. Default is 30s.\ncluster.routing.allocation.include. Enum\nAllocates shards to a node whose attribute has at least one of the included comma-separated values.\ncluster.routing.allocation.require. Enum\nOnly allocates shards to a node whose attribute has all of the included comma-separated values.\ncluster.routing.allocation.exclude. Enum\nDoes not allocate shards to a node whose attribute has any of the included comma-separated values. The cluster allocation settings support the following built-in attributes: _name – Match nodes by node name. _host_ip – Match nodes by host IP address. _publish_ip – Match nodes by publish IP address. _ip – Match either _host_ip or _publish_ip. _host – Match nodes by hostname. _id – Match nodes by node ID. _tier – Match nodes by data tier role.\ncluster.blocks.read_only\nBoolean\nSets the entire cluster to read-only. Default is false.\ncluster.blocks.read_only_allow_delete\nBoolean\nSimilar to cluster.blocks.read_only but allows you to delete indexes.\ncluster.max_shards_per_node\nInteger\nLimits the total number of primary and replica shards for the cluster. The limit is calculated as follows: cluster.max_shards_per_node multiplied by the number of non-frozen data nodes. Shards for closed indexes do not count toward this limit. Default is 1000.\ncluster.persistent_tasks.allocation.enable\nString\nEnables or disables allocation for persistent tasks: all – Allows persistent tasks to be assigned to nodes. none – No allocations are allowed for persistent tasks. This does not affect persistent tasks already running. Default is all.\ncluster.persistent_tasks.allocation.recheck_interval\nTime unit\nThe cluster manager automatically checks whether or not persistent tasks need to be assigned when the cluster state changes in a significant way. There are other factors, such as memory usage, that will affect whether or not persistent tasks are assigned to nodes but do not otherwise cause the cluster state to change. This setting defines how often assignment checks are performed in response to these factors. Default is 30 seconds, with a minimum of 10 seconds being required. Example request\nFor a PUT operation, the request body must contain transient or persistent, along with the setting you want to update: PUT _cluster/settings { \"persistent\":{ \"cluster.max_shards_per_node\": 500 } } copy For more information about transient settings, persistent settings, and precedence, see OpenSearch configuration.\nExample response { \"acknowledged\": true, \"persistent\":{ \"cluster\":{ \"max_shards_per_node\": \"500\" } }, \"transient\":{} }",
    "ancestors": [
      "API reference",
      "Cluster APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cluster-api/cluster-stats/",
    "title": "Cluster stats",
    "content": "Introduced 1.0\nThe cluster stats API operation returns statistics about your cluster.\nExamples GET _cluster/stats/nodes/_cluster_manager copy Path and HTTP methods GET _cluster/stats GET _cluster/stats/nodes/&lt;node-filters&gt; URL parameters\nAll cluster stats parameters are optional. Parameter Type Description &lt;node-filters&gt;\nList\nA comma-separated list of node filters that OpenSearch uses to filter results. Although the master node is now called cluster_manager for version 2.0, we retained the master field for backwards compatibility. If you have a node that has either a master role or a cluster_manager role, the count increases for both fields by 1. To see an example node count increase, see the Response sample.\nResponse { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"opensearch-cluster\", \"cluster_uuid\": \"QravFieJS_SlZJyBMcDMqQ\", \"timestamp\": 1644607845054, \"status\": \"yellow\", \"indices\": { \"count\": 114, \"shards\": { \"total\": 121, \"primaries\": 60, \"replication\": 1.0166666666666666, \"index\": { \"shards\": { \"min\": 1, \"max\": 2, \"avg\": 1.0614035087719298 }, \"primaries\": { \"min\": 0, \"max\": 2, \"avg\": 0.5263157894736842 }, \"replication\": { \"min\": 0.0, \"max\": 1.0, \"avg\": 0.008771929824561403 } } }, \"docs\": { \"count\": 134263, \"deleted\": 115 }, \"store\": { \"size_in_bytes\": 70466547, \"reserved_in_bytes\": 0 }, \"fielddata\": { \"memory_size_in_bytes\": 664, \"evictions\": 0 }, \"query_cache\": { \"memory_size_in_bytes\": 0, \"total_count\": 1, \"hit_count\": 0, \"miss_count\": 1, \"cache_size\": 0, \"cache_count\": 0, \"evictions\": 0 }, \"completion\": { \"size_in_bytes\": 0 }, \"segments\": { \"count\": 341, \"memory_in_bytes\": 3137244, \"terms_memory_in_bytes\": 2488992, \"stored_fields_memory_in_bytes\": 167672, \"term_vectors_memory_in_bytes\": 0, \"norms_memory_in_bytes\": 346816, \"points_memory_in_bytes\": 0, \"doc_values_memory_in_bytes\": 133764, \"index_writer_memory_in_bytes\": 0, \"version_map_memory_in_bytes\": 0, \"fixed_bit_set_memory_in_bytes\": 1112, \"max_unsafe_auto_id_timestamp\": 1644269449096, \"file_sizes\": {} }, \"mappings\": { \"field_types\": [ { \"name\": \"alias\", \"count\": 1, \"index_count\": 1 }, { \"name\": \"binary\", \"count\": 1, \"index_count\": 1 }, { \"name\": \"boolean\", \"count\": 87, \"index_count\": 22 }, { \"name\": \"date\", \"count\": 185, \"index_count\": 91 }, { \"name\": \"double\", \"count\": 5, \"index_count\": 2 }, { \"name\": \"float\", \"count\": 4, \"index_count\": 1 }, { \"name\": \"geo_point\", \"count\": 4, \"index_count\": 3 }, { \"name\": \"half_float\", \"count\": 12, \"index_count\": 1 }, { \"name\": \"integer\", \"count\": 144, \"index_count\": 29 }, { \"name\": \"ip\", \"count\": 2, \"index_count\": 1 }, { \"name\": \"keyword\", \"count\": 1939, \"index_count\": 109 }, { \"name\": \"knn_vector\", \"count\": 1, \"index_count\": 1 }, { \"name\": \"long\", \"count\": 158, \"index_count\": 92 }, { \"name\": \"nested\", \"count\": 25, \"index_count\": 10 }, { \"name\": \"object\", \"count\": 420, \"index_count\": 91 }, { \"name\": \"text\", \"count\": 1768, \"index_count\": 102 }] }, \"analysis\": { \"char_filter_types\": [], \"tokenizer_types\": [], \"filter_types\": [], \"analyzer_types\": [], \"built_in_char_filters\": [], \"built_in_tokenizers\": [], \"built_in_filters\": [], \"built_in_analyzers\": [ { \"name\": \"english\", \"count\": 1, \"index_count\": 1 }] } }, \"nodes\": { \"count\": { \"total\": 1, \"coordinating_only\": 0, \"data\": 1, \"ingest\": 1, \"master\": 1, \"cluster_manager\": 1, \"remote_cluster_client\": 1 }, \"versions\": [ \"1.2.4\"], \"os\": { \"available_processors\": 6, \"allocated_processors\": 6, \"names\": [ { \"name\": \"Linux\", \"count\": 1 }], \"pretty_names\": [ { \"pretty_name\": \"Amazon Linux 2\", \"count\": 1 }], \"mem\": { \"total_in_bytes\": 6232674304, \"free_in_bytes\": 1452658688, \"used_in_bytes\": 4780015616, \"free_percent\": 23, \"used_percent\": 77 } }, \"process\": { \"cpu\": { \"percent\": 0 }, \"open_file_descriptors\": { \"min\": 970, \"max\": 970, \"avg\": 970 } }, \"jvm\": { \"max_uptime_in_millis\": 108800629, \"versions\": [ { \"version\": \"15.0.1\", \"vm_name\": \"OpenJDK 64-Bit Server VM\", \"vm_version\": \"15.0.1+9\", \"vm_vendor\": \"AdoptOpenJDK\", \"bundled_jdk\": true, \"using_bundled_jdk\": true, \"count\": 1 }], \"mem\": { \"heap_used_in_bytes\": 178956256, \"heap_max_in_bytes\": 536870912 }, \"threads\": 112 }, \"fs\": { \"total_in_bytes\": 62725623808, \"free_in_bytes\": 28442726400, \"available_in_bytes\": 25226010624 }, \"plugins\": [ { \"name\": \"opensearch-index-management\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch Index Management Plugin\", \"classname\": \"org.opensearch.indexmanagement.IndexManagementPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [ \"opensearch-job-scheduler\"], \"has_native_controller\": false }, { \"name\": \"opensearch-security\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"Provide access control related features for OpenSearch 1.0.0\", \"classname\": \"org.opensearch.security.OpenSearchSecurityPlugin\", \"custom_foldername\": \"opensearch-security\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-cross-cluster-replication\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch Cross Cluster Replication Plugin\", \"classname\": \"org.opensearch.replication.ReplicationPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-job-scheduler\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch Job Scheduler plugin\", \"classname\": \"org.opensearch.jobscheduler.JobSchedulerPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-anomaly-detection\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch anomaly detector plugin\", \"classname\": \"org.opensearch.ad.AnomalyDetectorPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [ \"lang-painless\", \"opensearch-job-scheduler\"], \"has_native_controller\": false }, { \"name\": \"opensearch-performance-analyzer\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch Performance Analyzer Plugin\", \"classname\": \"org.opensearch.performanceanalyzer.PerformanceAnalyzerPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-reports-scheduler\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"Scheduler for Dashboards Reports Plugin\", \"classname\": \"org.opensearch.reportsscheduler.ReportsSchedulerPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [ \"opensearch-job-scheduler\"], \"has_native_controller\": false }, { \"name\": \"opensearch-asynchronous-search\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"Provides support for asynchronous search\", \"classname\": \"org.opensearch.search.asynchronous.plugin.AsynchronousSearchPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-knn\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch k-NN plugin\", \"classname\": \"org.opensearch.knn.plugin.KNNPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [ \"lang-painless\"], \"has_native_controller\": false }, { \"name\": \"opensearch-alerting\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"Amazon OpenSearch alerting plugin\", \"classname\": \"org.opensearch.alerting.AlertingPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [ \"lang-painless\"], \"has_native_controller\": false }, { \"name\": \"opensearch-observability\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch Plugin for OpenSearch Dashboards Observability\", \"classname\": \"org.opensearch.observability.ObservabilityPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }, { \"name\": \"opensearch-sql\", \"version\": \"1.2.4.0\", \"opensearch_version\": \"1.2.4\", \"java_version\": \"1.8\", \"description\": \"OpenSearch SQL\", \"classname\": \"org.opensearch.sql.plugin.SQLPlugin\", \"custom_foldername\": \"\", \"extended_plugins\": [], \"has_native_controller\": false }], \"network_types\": { \"transport_types\": { \"org.opensearch.security.ssl.http.netty.SecuritySSLNettyTransport\": 1 }, \"http_types\": { \"org.opensearch.security.http.SecurityHttpServerTransport\": 1 } }, \"discovery_types\": { \"zen\": 1 }, \"packaging_types\": [ { \"type\": \"tar\", \"count\": 1 }], \"ingest\": { \"number_of_pipelines\": 0, \"processor_stats\": {} } } } Response body fields Field Description nodes\nHow many nodes returned in the response.\ncluster_name\nThe cluster’s name.\ncluster_uuid\nThe cluster’s uuid.\ntimestamp\nThe Unix epoch time of when the cluster was last refreshed.\nstatus\nThe cluster’s health status.\nindices\nStatistics about the indexes in the cluster.\nindices.count\nHow many indexes are in the cluster.\nindices.shards\nInformation about the cluster’s shards.\nindices.docs\nHow many documents are still in the cluster and how many documents are deleted.\nindices.store\nInformation about the cluster’s storage.\nindices.fielddata\nInformation about the cluster’s field data\nindices.query_cache\nData about the cluster’s query cache.\nindices.completion\nHow many bytes in memory are used to complete operations.\nindices.segments\nInformation about the cluster’s segments, which are small Lucene indexes.\nindices.mappings\nMappings within the cluster.\nindices.analysis\nInformation about analyzers used in the cluster.\nnodes\nStatistics about the nodes in the cluster.\nnodes.count\nHow many nodes were returned from the request.\nnodes.versions\nOpenSearch’s version number.\nnodes.os\nInformation abotu the operating systems used in the nodes.\nnodes.process\nThe processes the returned nodes use.\nnodes.jvm\nStatistics about the Java Virtual Machines in use.\nnodes.fs\nThe nodes’ file storage.\nnodes.plugins\nThe OpenSearch plugins integrated within the nodes.\nnodes.network_types\nThe transport and HTTP networks within the nodes.\nnodes.discovery_type\nThe method the nodes use to find other nodes within the cluster.\nnodes.packaging_types\nInformation about the nodes’ OpenSearch distribution.\nnodes.ingest\nInformation about the nodes’ ingest pipelines/nodes, if there are any.",
    "ancestors": [
      "API reference",
      "Cluster APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/cluster-api/index/",
    "title": "Cluster APIs",
    "content": "The cluster APIs allow you to manage your cluster. You can use them to check cluster health, modify settings, retrieve statistics, and more.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/common-parameters/",
    "title": "Common REST Parameters",
    "content": "OpenSearch supports the following parameters for all REST operations:\nHuman-readable output\nTo convert output units to human-readable values (for example, 1h for 1 hour and 1kb for 1,024 bytes), add?human=true to the request URL.\nExample request\nThe following request requires response values to be in human-readable format: GET &lt;index_name&gt;/_search?human= true Pretty result\nTo get back JSON responses in a readable format, add?pretty=true to the request URL.\nExample request\nThe following request requires the response to be displayed in pretty JSON format: GET &lt;index_name&gt;/_search?pretty= true Content type\nTo specify the type of content in the request body, use the Content-Type key name in the request header. Most operations support JSON, YAML, and CBOR formats.\nExample request\nThe following request specifies JSON format for the request body: curl -H \"Content-type: application/json\" -XGET localhost: 9200 /_scripts/&lt;template_name&gt; Request body in query string\nIf the client library does not accept a request body for non-POST requests, use the source query string parameter to pass the request body. Also, specify the source_content_type parameter with a supported media type such as application/json.\nExample request\nThe following request searches the documents in the shakespeare index for a specific field and value: GET shakespeare/search?source= { \"query\":{ \"exists\":{ \"field\": \"speaker\" }}} &amp;source_content_type=application/json Stack traces\nTo include the error stack trace in the response when an exception is raised, add error_trace=true to the request URL.\nExample request\nThe following request sets error_trace to true so that the response returns exception-triggered errors: GET &lt;index_name&gt;/_search?error_trace= true Filtered responses\nTo reduce the response size use the filter_path parameter to filter the fields that are returned. This parameter takes a comma-separated list of filters. It supports using wildcards to match any field or part of a field’s name. You can also exclude fields with -.\nExample request\nThe following request specifies filters to limit the fields returned in the response: GET _search?filter_path=&lt;field_name&gt;.*,-&lt;field_name&gt;",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/count/",
    "title": "Count",
    "content": "Introduced 1.0\nThe count API gives you quick access to the number of documents that match a query.\nYou can also use it to check the document count of an index, data stream, or cluster.\nExample\nTo see the number of documents that match a query: GET opensearch_dashboards_sample_data_logs/_count { \"query\": { \"term\": { \"response\": \"200\" } } } copy The following call to the search API produces equivalent results: GET opensearch_dashboards_sample_data_logs/_search { \"query\": { \"term\": { \"response\": \"200\" } }, \"size\": 0, \"track_total_hits\": true } copy To see the number of documents in an index: GET opensearch_dashboards_sample_data_logs/_count copy To check for the number of documents in a data stream, replace the index name with the data stream name.\nTo see the number of documents in your cluster: GET _count copy Alternatively, you could use the cat indices and cat count APIs to see the number of documents per index or data stream.\nPath and HTTP methods GET &lt;target&gt;/_count/&lt;id&gt;\nPOST &lt;target&gt;/_count/&lt;id&gt; URL parameters\nAll count parameters are optional. Parameter Type Description allow_no_indices Boolean\nIf false, the request returns an error if any wildcard expression or index alias targets any closed or missing indices. Default is false. analyzer String\nThe analyzer to use in the query string. analyze_wildcard Boolean\nSpecifies whether to analyze wildcard and prefix queries. Default is false. default_operator String\nIndicates whether the default operator for a string query should be AND or OR. Default is OR. df String\nThe default field in case a field prefix is not provided in the query string. expand_wildcards String\nSpecifies the type of index that wildcard expressions can match. Supports comma-separated values. Valid values are all (match any index), open (match open, non-hidden indices), closed (match closed, non-hidden indices), hidden (match hidden indices), and none (deny wildcard expressions). Default is open. ignore_unavailable Boolean\nSpecifies whether to include missing or closed indices in the response. Default is false. lenient Boolean\nSpecifies whether OpenSearch should accept requests if queries have format errors (for example, querying a text field for an integer). Default is false. min_score Float\nInclude only documents with a minimum _score value in the result. routing String\nValue used to route the operation to a specific shard. preference String\nSpecifies which shard or node OpenSearch should perform the count operation on. terminate_after Integer\nThe maximum number of documents OpenSearch should process before terminating the request. Response { \"count\": 14074, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 } }",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/document-apis/bulk/",
    "title": "Bulk",
    "content": "Introduced 1.0\nThe bulk operation lets you add, update, or delete multiple documents in a single request. Compared to individual OpenSearch indexing requests, the bulk operation has significant performance benefits. Whenever practical, we recommend batching indexing operations into bulk requests.\nExample POST _bulk { \"delete\": { \"_index\": \"movies\", \"_id\": \"tt2229499\" } } { \"index\": { \"_index\": \"movies\", \"_id\": \"tt1979320\" } } { \"title\": \"Rush\", \"year\": 2013 } { \"create\": { \"_index\": \"movies\", \"_id\": \"tt1392214\" } } { \"title\": \"Prisoners\", \"year\": 2013 } { \"update\": { \"_index\": \"movies\", \"_id\": \"tt0816711\" } } { \"doc\": { \"title\": \"World War Z\" } } copy Path and HTTP methods POST _bulk\nPOST &lt;index&gt;/_bulk Specifying the index in the path means you don’t need to include it in the request body.\nOpenSearch also accepts PUT requests to the _bulk path, but we highly recommend using POST. The accepted usage of PUT—adding or replacing a single resource at a given path—doesn’t make sense for bulk requests.\nURL parameters\nAll bulk URL parameters are optional. Parameter Type Description pipeline\nString\nThe pipeline ID for preprocessing documents.\nrefresh\nEnum\nWhether to refresh the affected shards after performing the indexing operations. Default is false. true makes the changes show up in search results immediately, but hurts cluster performance. wait_for waits for a refresh. Requests take longer to return, but cluster performance doesn’t suffer.\nrequire_alias\nBoolean\nSet to true to require that all actions target an index alias rather than an index. Default is false.\nrouting\nString\nRoutes the request to the specified shard.\ntimeout\nTime\nHow long to wait for the request to return. Default 1m.\ntype\nString\n(Deprecated) The default document type for documents that don’t specify a type. Default is _doc. We highly recommend ignoring this parameter and using a type of _doc for all indices.\nwait_for_active_shards\nString\nSpecifies the number of active shards that must be available before OpenSearch processes the bulk request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the request to succeed. Request body\nThe bulk request body follows this pattern: Action and metadata\\n\nOptional document\\n\nAction and metadata\\n\nOptional document\\n The optional JSON document doesn’t need to be minified—spaces are fine—but it does need to be on a single line. OpenSearch uses newline characters to parse bulk requests and requires that the request body end with a newline character.\nAll actions support the same metadata: _index, _id, and _require_alias. If you don’t provide an ID, OpenSearch generates one automatically, which can make it challenging to update the document at a later time.\nCreate\nCreates a document if it doesn’t already exist and returns an error otherwise. The next line must include a JSON document. { \"create\": { \"_index\": \"movies\", \"_id\": \"tt1392214\" } } { \"title\": \"Prisoners\", \"year\": 2013 } Delete\nThis action deletes a document if it exists. If the document doesn’t exist, OpenSearch doesn’t return an error, but instead returns not_found under result. Delete actions don’t require documents on the next line. { \"delete\": { \"_index\": \"movies\", \"_id\": \"tt2229499\" } } Index\nIndex actions create a document if it doesn’t yet exist and replace the document if it already exists. The next line must include a JSON document. { \"index\": { \"_index\": \"movies\", \"_id\": \"tt1979320\" } } { \"title\": \"Rush\", \"year\": 2013 } Update\nThis action updates existing documents and returns an error if the document doesn’t exist. The next line must include a full or partial JSON document, depending on how much of the document you want to update. { \"update\": { \"_index\": \"movies\", \"_id\": \"tt0816711\" } } { \"doc\": { \"title\": \"World War Z\" } } It can also include a script or upsert for more complex document updates.\nScript { \"update\": { \"_index\": \"movies\", \"_id\": \"tt0816711\" } } { \"script\": { \"source\": \"ctx._source.title = \\\" World War Z \\\" \" } } Upsert { \"update\": { \"_index\": \"movies\", \"_id\": \"tt0816711\" } } { \"doc\": { \"title\": \"World War Z\" }, \"doc_as_upsert\": true } Response\nIn the response, pay particular attention to the top-level errors boolean. If true, you can iterate over the individual actions for more detailed information. { \"took\": 11, \"errors\": true, \"items\": [ { \"index\": { \"_index\": \"movies\", \"_id\": \"tt1979320\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 1, \"status\": 201 } }, { \"create\": { \"_index\": \"movies\", \"_id\": \"tt1392214\", \"status\": 409, \"error\": { \"type\": \"version_conflict_engine_exception\", \"reason\": \"[tt1392214]: version conflict, document already exists (current version [1])\", \"index\": \"movies\", \"shard\": \"0\", \"index_uuid\": \"yhizhusbSWmP0G7OJnmcLg\" } } }, { \"update\": { \"_index\": \"movies\", \"_id\": \"tt0816711\", \"status\": 404, \"error\": { \"type\": \"document_missing_exception\", \"reason\": \"[_doc][tt0816711]: document missing\", \"index\": \"movies\", \"shard\": \"0\", \"index_uuid\": \"yhizhusbSWmP0G7OJnmcLg\" } } }] }",
    "ancestors": [
      "API reference",
      "Document APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/document-apis/delete-by-query/",
    "title": "Delete by query",
    "content": "Introduced 1.0\nYou can include a query as part of your delete request so OpenSearch deletes all documents that match that query.\nExample POST sample-index 1 /_delete_by_query { \"query\": { \"match\": { \"movie-length\": \"124\" } } } copy Path and HTTP methods POST &lt;index&gt;/_delete_by_query URL parameters\nAll URL parameters are optional. Parameter Type Description &lt;index&gt;\nString\nName or list of the data streams, indices, or aliases to delete from. Supports wildcards. If left blank, OpenSearch searches all indices.\nallow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indices. Default is true.\nanalyzer\nString\nThe analyzer to use in the query string.\nanalyze_wildcard\nBoolean\nSpecifies whether to analyze wildcard and prefix queries. Default is false.\nconflicts\nString\nIndicates to OpenSearch what should happen if the delete by query operation runs into a version conflict. Valid options are abort and proceed. Default is abort.\ndefault_operator\nString\nIndicates whether the default operator for a string query should be AND or OR. Default is OR.\ndf\nString\nThe default field in case a field prefix is not provided in the query string.\nexpand_wildcards\nString\nSpecifies the type of index that wildcard expressions can match. Supports comma-separated values. Valid values are all (match any index), open (match open, non-hidden indices), closed (match closed, non-hidden indices), hidden (match hidden indices), and none (deny wildcard expressions). Default is open.\nfrom\nInteger\nThe starting index to search from. Default is 0.\nignore_unavailable\nBoolean\nSpecifies whether to include missing or closed indices in the response. Default is false.\nlenient\nBoolean\nSpecifies whether OpenSearch should accept requests if queries have format errors (for example, querying a text field for an integer). Default is false.\nmax_docs\nInteger\nHow many documents the delete by query operation should process at most. Default is all documents.\npreference\nString\nSpecifies which shard or node OpenSearch should perform the delete by query operation on.\nq\nString\nLucene query string’s query.\nrequest_cache\nBoolean\nSpecifies whether OpenSearch should use the request cache. Default is whether it’s enabled in the index’s settings.\nrefresh\nBoolean\nIf true, OpenSearch refreshes shards to make the delete by query operation available to search results. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false.\nrequests_per_second\nInteger\nSpecifies the request’s throttling in sub-requests per second. Default is -1, which means no throttling.\nrouting\nString\nValue used to route the operation to a specific shard.\nscroll\nTime\nAmount of time the search context should be open.\nscroll_size\nInteger\nSize of the operation’s scroll requests. Default is 1000.\nsearch_type\nString\nWhether OpenSearch should use global term and document frequencies calculating revelance scores. Valid choices are query_then_fetch and dfs_query_then_fetch. query_then_fetch scores documents using local term and document frequencies for the shard. It’s usually faster but less accurate. dfs_query_then_fetch scores documents using global term and document frequencies across all shards. It’s usually slower but more accurate. Default is query_then_fetch.\nsearch_timeout\nTime\nHow long to wait until OpenSearch deems the request timed out. Default is no timeout.\nslices\nString or Integer\nHow many slices to cut the operation into for faster processing. Specify an integer to set how many slices to divide the operation into, or use auto, which tells OpenSearch it should decide how many slices to divide into. If you have a lot of shards in your index, set a lower number for better efficiency. Default is 1, which means the task should not be divided.\nsort\nString\nA comma-separated list of &lt;field&gt;: &lt;direction&gt; pairs to sort by.\n_source\nString\nSpecifies whether to include the _source field in the response.\n_source_excludes\nString\nA comma-separated list of source fields to exclude from the response.\n_source_includes\nString\nA comma-separated list of source fields to include in the response.\nstats\nString\nValue to associate with the request for additional logging.\nterminate_after\nInteger\nThe maximum number of documents OpenSearch should process before terminating the request.\ntimeout\nTime\nHow long the operation should wait from a response from active shards. Default is 1m.\nversion\nBoolean\nWhether to include the document version as a match.\nwait_for_active_shards\nString\nThe number of shards that must be active before OpenSearch executes the operation. Valid values are all or any integer up to the total number of shards in the index. Default is 1, which is the primary shard.\nwait_for_completion\nBoolean\nSetting this parameter to false indicates to OpenSearch it should not wait for completion and perform this request asynchronously. Asynchronous requests run in the background, and you can use the Tasks API to monitor progress. Request body\nTo search your index for specific documents, you must include a query in the request body that OpenSearch uses to match documents. If you don’t use a query, OpenSearch treats your delete request as a simple delete document operation. { \"query\": { \"match\": { \"movie-length\": \"124\" } } } Response { \"took\": 143, \"timed_out\": false, \"total\": 1, \"deleted\": 1, \"batches\": 1, \"version_conflicts\": 0, \"noops\": 0, \"retries\": { \"bulk\": 0, \"search\": 0 }, \"throttled_millis\": 0, \"requests_per_second\": -1.0, \"throttled_until_millis\": 0, \"failures\": [] } Response body fields Field Description took\nThe amount of time in milliseconds OpenSearch needed to complete the operation.\ntimed_out\nWhether any delete requests during the operation timed out.\ntotal\nTotal number of documents processed.\ndeleted\nTotal number of documents deleted.\nbatches\nNumber of scroll responses the request processed.\nversion_conflicts\nNumber of conflicts the request ran into.\nnoops\nHow many delete requests OpenSearch ignored during the operation. This field always returns 0.\nretries\nThe number of bulk and search retry requests.\nthrottled_millis\nNumber of throttled milliseconds during the request.\nrequests_per_second\nNumber of requests executed per second during the operation.\nthrottled_until_millis\nThe amount of time until OpenSearch executes the next throttled request. Always equal to 0 in a delete by query request.\nfailures\nAny failures that occur during the request.",
    "ancestors": [
      "API reference",
      "Document APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/document-apis/delete-document/",
    "title": "Delete document",
    "content": "Introduced 1.0\nIf you no longer need a document in your index, you can use the delete document API operation to delete it.\nExample DELETE /sample-index1/_doc/1 copy Path and HTTP methods DELETE /&lt;index&gt;/_doc/&lt;_id&gt; URL parameters Parameter Type Description Required &lt;index&gt;\nString\nThe index to delete from.\nYes\n&lt;_id&gt;\nString\nThe ID of the document to delete.\nYes\nif_seq_no\nInteger\nOnly perform the delete operation if the document’s version number matches the specified number.\nNo\nif_primary_term\nInteger\nOnly perform the delete operation if the document has the specified primary term.\nNo\nrefresh\nEnum\nIf true, OpenSearch refreshes shards to make the delete operation available to search results. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false.\nNo\nrouting\nString\nValue used to route the operation to a specific shard.\nNo\ntimeout\nTime\nHow long to wait for a response from the cluster.\tDefault is 1m.\nNo\nversion\nInteger\nThe version of the document to delete, which must match the last updated version of the document.\nNo\nversion_type\nEnum\nRetrieves a specifically typed document. Available options are external (retrieve the document if the specified version number is greater than the document’s current version) and external_gte (retrieve the document if the specified version number is greater than or equal to the document’s current version). For example, to delete version 3 of a document, use /_doc/1?version=3&amp;version_type=external.\nNo\nwait_for_active_shards\nString\nThe number of active shards that must be available before OpenSearch processes the delete request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed.\nNo Response { \"_index\": \"sample-index1\", \"_id\": \"1\", \"_version\": 2, \"result\": \"deleted\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 1, \"_primary_term\": 15 } Response body fields Field Description _index\nThe name of the index.\n_id\nThe document’s ID.\n_version\nThe document’s version.\n_result\nThe result of the delete operation.\n_shards\nDetailed information about the cluster’s shards.\ntotal\nThe total number of shards.\nsuccessful\nThe number of shards OpenSearch successfully deleted the document from.\nfailed\nThe number of shards OpenSearch failed to delete the document from.\n_seq_no\nThe sequence number assigned when the document was indexed.\n_primary_term\nThe primary term assigned when the document was indexed.",
    "ancestors": [
      "API reference",
      "Document APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/document-apis/get-documents/",
    "title": "Get document",
    "content": "Introduced 1.0\nAfter adding a JSON document to your index, you can use the get document API operation to retrieve the document’s information and data.\nExample GET sample-index 1 /_doc/ 1 copy Path and HTTP methods GET &lt;index&gt;/_doc/&lt;_id&gt;\nHEAD &lt;index&gt;/_doc/&lt;_id&gt; GET &lt;index&gt;/_source/&lt;_id&gt;\nHEAD &lt;index&gt;/_source/&lt;_id&gt; URL parameters\nAll get document URL parameters are optional. Parameter Type Description preference\nString\nSpecifies a preference of which shard to retrieve results from. Available options are _local, which tells the operation to retrieve results from a locally allocated shard replica, and a custom string value assigned to a specific shard replica. By default, OpenSearch executes get document operations on random shards.\nrealtime\nBoolean\nSpecifies whether the operation should run in realtime. If false, the operation waits for the index to refresh to analyze the source to retrieve data, which makes the operation near-realtime. Default is true.\nrefresh\nBoolean\nIf true, OpenSearch refreshes shards to make the get operation available to search results. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false.\nrouting\nString\nA value used to route the operation to a specific shard.\nstored_fields\nBoolean\nWhether the get operation should retrieve fields stored in the index. Default is false.\n_source\nString\nWhether to include the _source field in the response body. Default is true.\n_source_excludes\nString\nA comma-separated list of source fields to exclude in the query response.\n_source_includes\nString\nA comma-separated list of source fields to include in the query response.\nversion\nInteger\nThe version of the document to return, which must match the current version of the document.\nversion_type\nEnum\nRetrieves a specifically typed document. Available options are external (retrieve the document if the specified version number is greater than the document’s current version) and external_gte (retrieve the document if the specified version number is greater than or equal to the document’s current version). For example, to retrieve version 3 of a document, use /_doc/1?version=3&amp;version_type=external. Response { \"_index\": \"sample-index1\", \"_id\": \"1\", \"_version\": 1, \"_seq_no\": 0, \"_primary_term\": 9, \"found\": true, \"_source\": { \"text\": \"This is just some sample text.\" } } Response body fields Field Description _index\nThe name of the index.\n_id\nThe document’s ID.\n_version\nThe document’s version number. Updated whenever the document changes.\n_seq_no\nThe sequence number assigned when the document is indexed.\nprimary_term\nThe primary term assigned when the document is indexed.\nfound\nWhether the document exists.\n_routing\nThe shard that the document is routed to. If the document is not routed to a particular shard, this field is omitted.\n_source\nContains the document’s data if found is true. If _source is set to false or stored_fields is set to true in the URL parameters, this field is omitted.\n_fields\nContains the document’s data that’s stored in the index. Only returned if both stored_fields and found are true.",
    "ancestors": [
      "API reference",
      "Document APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/document-apis/index-document/",
    "title": null,
    "content": "Introduced 1.0\nBefore you can search for data, you must first add documents. This operation adds a single document to your index.\nExample PUT sample-index/_doc/ 1 { \"Description\": \"To be or not to be, that is the question.\" } copy Path and HTTP methods PUT &lt;index&gt;/_doc/&lt;_id&gt;\nPOST &lt;index&gt;/_doc\nPUT &lt;index&gt;/_create/&lt;_id&gt;\nPOST &lt;index&gt;/_create/&lt;_id&gt; URL parameters\nIn your request, you must specify the index you want to add your document to. If the index doesn’t already exist, OpenSearch automatically creates the index and adds in your document. All other URL parameters are optional. Parameter Type Description Required &lt;index&gt;\nString\nName of the index.\nYes\n&lt;_id&gt;\nString\nA unique identifier to attach to the document. To automatically generate an ID, use POST &lt;target&gt;/doc in your request instead of PUT.\nNo\nif_seq_no\nInteger\nOnly perform the index operation if the document has the specified sequence number.\nNo\nif_primary_term\nInteger\nOnly perform the index operation if the document has the specified primary term.\nNo\nop_type\nEnum\nSpecifies the type of operation to complete with the document. Valid values are create (create the index if it doesn’t exist) and index. If a document ID is included in the request, then the default is index. Otherwise, the default is create.\nNo\npipeline\nString\nRoute the index operation to a certain pipeline.\nNo\nrouting\nString\nvalue used to assign the index operation to a specific shard.\nNo\nrefresh\nEnum\nIf true, OpenSearch refreshes shards to make the operation visible to searching. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false.\nNo\ntimeout\nTime\nHow long to wait for a response from the cluster. Default is 1m.\nNo\nversion\nInteger\nThe document’s version number.\nNo\nversion_type\nEnum\nAssigns a specific type to the document. Valid options are external (retrieve the document if the specified version number is greater than the document’s current version) and external_gte (retrieve the document if the specified version number is greater than or equal to the document’s current version). For example, to index version 3 of a document, use /_doc/1?version=3&amp;version_type=external.\nNo\nwait_for_active_shards\nString\nThe number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed.\nNo\nrequire_alias\nBoolean\nSpecifies whether the target index must be an index alias. Default is false.\nNo Request body\nYour request body must contain the information you want to index. { \"Description\": \"This is just a sample document\" } Response { \"_index\": \"sample-index\", \"_id\": \"1\", \"_version\": 1, \"result\": \"created\", \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 0, \"_primary_term\": 1 } Response body fields Field Description _index\nThe name of the index.\n_id\nThe document’s ID.\n_version\nThe document’s version.\nresult\nThe result of the index operation.\n_shards\nDetailed information about the cluster’s shards.\ntotal\nThe total number of shards.\nsuccessful\nThe number of shards OpenSearch successfully added the document to.\nfailed\nThe number of shards OpenSearch failed to added the document to.\n_seq_no\nThe sequence number assigned when the document was indexed.\n_primary_term\nThe primary term assigned when the document was indexed.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/document-apis/index/",
    "title": "Document APIs",
    "content": "The document APIs allow you to handle documents relative to your index, such as adding, updating, and deleting documents.\nDocument APIs are separated into two categories: single document operations and multi-document operations. Multi-document operations offer performance advantages over submitting many individual requests, so whenever practical, we recommend that you use multi-document operations.\nSingle document operations\nIndex\nGet\nDelete\nUpdate\nMulti-document operations\nBulk\nMulti get\nDelete by query\nUpdate by query\nReindex",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/document-apis/multi-get/",
    "title": "Multi-get document",
    "content": "Introduced 1.0\nThe multi-get operation allows you to execute multiple GET operations in one request, so you can get back all documents that match your criteria.\nExample without specifying index in URL GET _mget { \"docs\": [ { \"_index\": \"sample-index1\", \"_id\": \"1\" }, { \"_index\": \"sample-index2\", \"_id\": \"1\", \"_source\": { \"include\": [ \"Length\"] } }] } copy Example of specifying index in URL GET sample-index 1 /_mget { \"docs\": [ { \"_id\": \"1\", \"_source\": false }, { \"_id\": \"2\", \"_source\": [ \"Director\", \"Title\"] }] } copy Path and HTTP methods GET _mget\nGET &lt;index&gt;/_mget URL parameters\nAll multi-get URL parameters are optional. Parameter Type Description &lt;index&gt;\nString\nName of the index to retrieve documents from.\npreference\nString\nSpecifies the nodes or shards OpenSearch should execute the multi-get operation on. Default is random.\nrealtime\nBoolean\nSpecifies whether the operation should run in realtime. If false, the operation waits for the index to refresh to analyze the source to retrieve data, which makes the operation near-realtime. Default is true.\nrefresh\nBoolean\nIf true, OpenSearch refreshes shards to make the multi-get operation available to search results. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false.\nrouting\nString\nValue used to route the multi-get operation to a specific shard.\nstored_fields\nBoolean\nSpecifies whether OpenSearch should retrieve documents fields from the index instead of the document’s _source. Default is false.\n_source\nString\nWhether to include the _source field in the query response. Default is true.\n_source_excludes\nString\nA comma-separated list of source fields to exclude in the query response.\n_source_includes\nString\nA comma-separated list of source fields to include in the query response. Request body\nIf you don’t specify an index in your request’s URL, you must specify your target indexes and the relevant document IDs in the request body. Other fields are optional. Field Type Description Required docs\nArray\nThe documents you want to retrieve data from. Can contain the attributes: _id, _index, _routing, _source, and _stored_fields. If you specify an index in the URL, you can omit this field and add IDs of the documents to retrieve.\nYes if an index is not specified in the URL\n_id\nString\nThe ID of the document.\nYes if docs is specified in the request body\n_index\nString\nName of the index.\nYes if an index is not specified in the URL\n_routing\nString\nThe value of the shard that has the document.\nYes if a routing value was used when indexing the document\n_source\nObject\nSpecifies whether to return the _source field from an index (boolean), whether to return specific fields (array), or whether to include or exclude certain fields.\nNo\n_source.includes\nArray\nSpecifies which fields to include in the query response. For example, \"_source\": { \"include\": [\"Title\"] } retrieves Title from the index.\nNo\n_source.excludes\nArray\nSpecifies which fields to exclude in the query response. For example, \"_source\": { \"exclude\": [\"Director\"] } excludes Director from the query response.\nNo\nids\nArray\nIDs of the documents to retrieve. Only allowed when an index is specified in the URL.\nNo Response { \"docs\": [ { \"_index\": \"sample-index1\", \"_id\": \"1\", \"_version\": 4, \"_seq_no\": 5, \"_primary_term\": 19, \"found\": true, \"_source\": { \"Title\": \"Batman Begins\", \"Director\": \"Christopher Nolan\" } }, { \"_index\": \"sample-index2\", \"_id\": \"1\", \"_version\": 1, \"_seq_no\": 6, \"_primary_term\": 19, \"found\": true, \"_source\": { \"Title\": \"The Dark Knight\", \"Director\": \"Christopher Nolan\" } }] } Response body fields Field Description _index\nThe name of the index.\n_id\nThe document’s ID.\n_version\nThe document’s version number. Updated whenever the document changes.\n_seq_no\nThe sequence number assigned when the document is indexed.\nprimary_term\nThe primary term assigned when the document is indexed.\nfound\nWhether the document exists.\n_routing\nThe shard that the document is routed to. If the document is not routed to a particular shard, this field is omitted.\n_source\nContains the document’s data if found is true. If _source is set to false or stored_fields is set to true in the URL parameters, this field is omitted.\n_fields\nContains the document’s data that’s stored in the index. Only returned if both stored_fields and found are true.",
    "ancestors": [
      "API reference",
      "Document APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/document-apis/reindex/",
    "title": "Reindex document",
    "content": "Introduced 1.0\nThe reindex document API operation lets you copy all or a subset of your data from a source index into a destination index.\nExample POST /_reindex { \"source\":{ \"index\": \"my-source-index\" }, \"dest\":{ \"index\": \"my-destination-index\" } } copy Path and HTTP methods POST /_reindex URL parameters\nAll URL parameters are optional. Parameter Type Description refresh\nBoolean\nIf true, OpenSearch refreshes shards to make the reindex operation available to search results. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false.\ntimeout\nTime\nHow long to wait for a response from the cluster. Default is 30s.\nwait_for_active_shards\nString\nThe number of active shards that must be available before OpenSearch processes the reindex request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed.\nwait_for_completion\nBoolean\nWaits for the matching tasks to complete. Default is false.\nrequests_per_second\nInteger\nSpecifies the request’s throttling in sub-requests per second. Default is -1, which means no throttling.\nrequire_alias\nBoolean\nWhether the destination index must be an index alias. Default is false.\nscroll\nTime\nHow long to keep the search context open. Default is 5m.\nslices\nInteger\nNumber of sub-tasks OpenSearch should divide this task into. Default is 1, which means OpenSearch should not divide this task. Setting this parameter to auto indicates to OpenSearch that it should automatically decide how many slices to split the task into.\nmax_docs\nInteger\nHow many documents the update by query operation should process at most. Default is all documents. Request body\nYour request body must contain the names of the source index and destination index. All other fields are optional. Field Description conflicts\nIndicates to OpenSearch what should happen if the delete by query operation runs into a version conflict. Valid options are abort and proceed. Default is abort.\nsource\nInformation about the source index to include. Valid fields are index, max_docs, query, remote, size, slice, and _source.\nindex\nThe name of the source index to copy data from.\nmax_docs\nThe maximum number of documents to reindex.\nquery\nThe search query to use for the reindex operation.\nremote\nInformation about a remote OpenSearch cluster to copy data from. Valid fields are host, username, password, socket_timeout, and connect_timeout.\nhost\nHost URL of the OpenSearch cluster to copy data from.\nusername\nUsername to authenticate with the remote cluster.\npassword\nPassword to authenticate with the remote cluster.\nsocket_timeout\nThe wait time for socket reads. Default is 30s.\nconnect_timeout\nThe wait time for remote connection timeouts. Default is 30s.\nsize\nThe number of documents to reindex.\nslice\nWhether to manually or automatically slice the reindex operation so it executes in parallel. Setting this field to auto allows OpenSearch to control the number of slices to use, which is one slice per shard, up to a maximum of 20. If there are multiple sources, the number of slices used are based on the index or backing index with the smallest number of shards.\n_source\nWhether to reindex source fields. Specify a list of fields to reindex or true to reindex all fields. Default is true.\nid\nThe ID to associate with manual slicing.\nmax\nMaximum number of slices.\ndest\nInformation about the destination index. Valid values are index, version_type, and op_type.\nindex\nName of the destination index.\nversion_type\nThe indexing operation’s version type. Valid values are internal, external, external_gt (retrieve the document if the specified version number is greater than the document’s current version), and external_gte (retrieve the document if the specified version number is greater or equal to than the document’s current version).\nop_type\nWhether to copy over documents that are missing in the destination index. Valid values are create (ignore documents with the same ID from the source index) and index (copy everything from the source index).\nscript\nA script that OpenSearch uses to apply transformations to the data during the reindex operation.\nsource\nThe actual script that OpenSearch runs.\nlang\nThe scripting language. Valid options are painless, expression, mustache, and java. Response { \"took\": 28829, \"timed_out\": false, \"total\": 111396, \"updated\": 0, \"created\": 111396, \"deleted\": 0, \"batches\": 112, \"version_conflicts\": 0, \"noops\": 0, \"retries\": { \"bulk\": 0, \"search\": 0 }, \"throttled_millis\": 0, \"requests_per_second\": -1.0, \"throttled_until_millis\": 0, \"failures\": [] } Response body fields Field Description took\nHow long the operation took in milliseconds.\ntimed_out\nWhether the operation timed out.\ntotal\nThe total number of documents processed.\nupdated\nThe number of documents updated in the destination index.\ncreated\nThe number of documents created in the destination index.\ndeleted\nThe number of documents deleted.\nbatches\nNumber of scroll responses.\nversion_conflicts\nNumber of version conflicts.\nnoops\nHow many documents OpenSearch ignored during the operation.\nretries\nNumber of bulk and search retry requests.\nthrottled_millis\nNumber of throttled milliseconds during the request.\nrequests_per_second\nNumber of requests executed per second during the operation.\nthrottled_until_millis\nThe amount of time until OpenSearch executes the next throttled request.\nfailures\nAny failures that occurred during the operation.",
    "ancestors": [
      "API reference",
      "Document APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/document-apis/update-by-query/",
    "title": "Update by query",
    "content": "Introduced 1.0\nYou can include a query and a script as part of your update request so OpenSearch can run the script to update all of the documents that match the query.\nExample POST test-index 1 /_update_by_query { \"query\": { \"term\": { \"oldValue\": 10 } }, \"script\": { \"source\": \"ctx._source.oldValue += params.newValue\", \"lang\": \"painless\", \"params\": { \"newValue\": 20 } } } copy Path and HTTP methods POST &lt;target-index1&gt;, &lt;target-index2&gt;/_update_by_query URL parameters\nAll URL parameters are optional. Parameter Type Description &lt;index&gt;\nString\nComma-separated list of indexes to update. To update all indexes, use * or omit this parameter.\nallow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indexes. Default is true.\nanalyzer\nString\nAnalyzer to use in the query string.\nanalyze_wildcard\nBoolean\nWhether the update operation should include wildcard and prefix queries in the analysis. Default is false.\nconflicts\nString\nIndicates to OpenSearch what should happen if the update by query operation runs into a version conflict. Valid options are abort and proceed. Default is abort.\ndefault_operator\nString\nIndicates whether the default operator for a string query should be AND or OR. Default is OR.\ndf\nString\nThe default field if a field prefix is not provided in the query string.\nexpand_wildcards\nString\nSpecifies the type of index that wildcard expressions can match. Supports comma-separated values. Valid values are all (match any index), open (match open, non-hidden indexes), closed (match closed, non-hidden indexes), hidden (match hidden indexes), and none (deny wildcard expressions). Default is open.\nfrom\nInteger\nThe starting index to search from. Default is 0.\nignore_unavailable\nBoolean\nWhether to exclude missing or closed indexes in the response. Default is false.\nlenient\nBoolean\nSpecifies whether OpenSearch should accept requests if queries have format errors (for example, querying a text field for an integer). Default is false.\nmax_docs\nInteger\nHow many documents the update by query operation should process at most. Default is all documents.\npipeline\nString\nID of the pipeline to use to process documents.\npreference\nString\nSpecifies which shard or node OpenSearch should perform the update by query operation on.\nq\nString\nLucene query string’s query.\nrequest_cache\nBoolean\nSpecifies whether OpenSearch should use the request cache. Default is whether it’s enabled in the index’s settings.\nrefresh\nBoolean\nIf true, OpenSearch refreshes shards to make the update by query operation available to search results. Valid options are true and false. Default is false.\nrequests_per_second\nInteger\nSpecifies the request’s throttling in sub-requests per second. Default is -1, which means no throttling.\nrouting\nString\nValue used to route the update by query operation to a specific shard.\nscroll\nTime\nHow long to keep the search context open.\nscroll_size\nInteger\nSize of the operation’s scroll request. Default is 1000.\nsearch_type\nString\nWhether OpenSearch should use global term and document frequencies calculating relevance scores. Valid choices are query_then_fetch and dfs_query_then_fetch. query_then_fetch scores documents using local term and document frequencies for the shard. It’s usually faster but less accurate. dfs_query_then_fetch scores documents using global term and document frequencies across all shards. It’s usually slower but more accurate. Default is query_then_fetch.\nsearch_timeout\nTime\nHow long to wait until OpenSearch deems the request timed out. Default is no timeout.\nslices\nInteger\nNumber of sub-tasks OpenSearch should divide this task into. Default is 1, which means OpenSearch should not divide this task.\nsort\nList\nA comma-separated list of &lt;field&gt;: &lt;direction&gt; pairs to sort by.\n_source\nString\nWhether to include the _source field in the response.\n_source_excludes\nString\nA comma-separated list of source fields to exclude from the response.\n_source_includes\nString\nA comma-separated list of source fields to include in the response.\nstats\nString\nValue to associate with the request for additional logging.\nterminate_after\nInteger\nThe maximum number of documents OpenSearch should process before terminating the request.\ntimeout\nTime\nHow long the operation should wait from a response from active shards. Default is 1m.\nversion\nBoolean\nWhether to include the document version as a match.\nwait_for_active_shards\nString\nThe number of shards that must be active before OpenSearch executes the operation. Valid values are all or any integer up to the total number of shards in the index. Default is 1, which is the primary shard.\nwait_for_completion\nboolean\nWhen set to false, the response body includes a task ID and OpenSearch executes the operation asynchronously. The task ID can be used to check the status of the task or to cancel the task. Default is set to true. Request body\nTo update your indexes and documents by query, you must include a query and a script in the request body that OpenSearch can run to update your documents. If you don’t specify a query, then every document in the index gets updated. { \"query\": { \"term\": { \"oldValue\": 20 } }, \"script\": { \"source\": \"ctx._source.oldValue += params.newValue\", \"lang\": \"painless\", \"params\": { \"newValue\": 10 } } } Response { \"took\": 21, \"timed_out\": false, \"total\": 1, \"updated\": 1, \"deleted\": 0, \"batches\": 1, \"version_conflicts\": 0, \"noops\": 0, \"retries\": { \"bulk\": 0, \"search\": 0 }, \"throttled_millis\": 0, \"requests_per_second\": -1.0, \"throttled_until_millis\": 0, \"failures\": [] } Response body fields Field Description took\nThe amount of time in milliseconds OpenSearch needed to complete the operation.\ntimed_out\nWhether any update requests during the operation timed out.\ntotal\nTotal number of documents processed.\nupdated\nTotal number of documents updated.\nbatches\nNumber of scroll responses the request processed.\nversion_conflicts\nNumber of conflicts the request ran into.\nnoops\nHow many update requests OpenSearch ignored during the operation. This field always returns 0.\nretries\nThe number of bulk and search retry requests.\nthrottled_millis\nNumber of throttled milliseconds during the request.\nrequests_per_second\nNumber of requests executed per second during the operation.\nthrottled_until_millis\nThe amount of time until OpenSearch executes the next throttled request. Always equal to 0 in an update by query request.\nfailures\nAny failures that occur during the request.",
    "ancestors": [
      "API reference",
      "Document APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/document-apis/update-document/",
    "title": "Update document",
    "content": "Introduced 1.0\nIf you need to update a document’s fields in your index, you can use the update document API operation. You can do so by specifying the new data you want in your index or by including a script in your request body, which OpenSearch runs to update the document.\nExample POST /sample-index 1 /_update/ 1 { \"doc\": { \"first_name\": \"Bruce\", \"last_name\": \"Wayne\" } } copy Script example POST /test-index 1 /_update/ 1 { \"script\": { \"source\": \"ctx._source.secret_identity = \\\" Batman \\\" \" } } copy Path and HTTP methods POST /&lt;index&gt;/_update/&lt;_id&gt; URL parameters Parameter Type Description Required &lt;index&gt;\nString\nName of the index.\nYes\n&lt;_id&gt;\nString\nThe ID of the document to update.\nYes\nif_seq_no\nInteger\nOnly perform the delete operation if the document’s version number matches the specified number.\nNo\nif_primary_term\nInteger\nPerform the update operation if the document has the specified primary term.\nNo\nlang\nString\nLanguage of the script. Default is painless.\nNo\nrequire_alias\nBoolean\nSpecifies whether the destination must be an index alias. Default is false.\nNo\nrefresh\nEnum\nIf true, OpenSearch refreshes shards to make the operation visible to searching. Valid options are true, false, and wait_for, which tells OpenSearch to wait for a refresh before executing the operation. Default is false.\nNo\nretry_on_conflict\nInteger\nThe amount of times OpenSearch should retry the operation if there’s a document conflict. Default is 0.\nNo\nrouting\nString\nValue to route the update operation to a specific shard.\nNo\n_source\nBoolean or List\nWhether or not to include the _source field in the response body. Default is false. This parameter also supports a comma-separated list of source fields for including multiple source fields in the query response.\nNo\n_source_excludes\nList\nA comma-separated list of source fields to exclude in the query response.\nNo\n_source_includes\nList\nA comma-separated list of source fields to include in the query response.\nNo\ntimeout\nTime\nHow long to wait for a response from the cluster.\nNo\nwait_for_active_shards\nString\nThe number of active shards that must be available before OpenSearch processes the update request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed.\nNo Request body\nYour request body must contain the information you want to update your document with. If you just want to replace certain fields in your document, your request body must include a doc object, which has the fields you want to update. { \"doc\": { \"first_name\": \"Thomas\", \"last_name\": \"Wayne\" } } You can also use a script to tell OpenSearch how to update your document. { \"script\": { \"source\": \"ctx._source.oldValue += params.newValue\", \"lang\": \"painless\", \"params\": { \"newValue\": 10 } } } Upsert\nUpsert is an operation that conditionally either updates an existing document or inserts a new one based on information in the object. In the sample below, the upsert object updates the last name and adds the age field if a document already exists. If a document does not exist, a new one is indexed using content in the upsert object. { \"doc\": { \"first_name\": \"Martha\", \"last_name\": \"Rivera\" }, \"upsert\": { \"last_name\": \"Oliveira\", \"age\": \"31\" } } You can also add doc_as_upsert to the request and set it to true to use the information in doc for performing the upsert operation. { \"doc\": { \"first_name\": \"Martha\", \"last_name\": \"Oliveira\", \"age\": \"31\" }, \"doc_as_upsert\": true } Response { \"_index\": \"sample-index1\", \"_id\": \"1\", \"_version\": 3, \"result\": \"updated\", \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 4, \"_primary_term\": 17 } Response body fields Field Description _index\nThe name of the index.\n_id\nThe document’s ID.\n_version\nThe document’s version.\n_result\nThe result of the delete operation.\n_shards\nDetailed information about the cluster’s shards.\ntotal\nThe total number of shards.\nsuccessful\nThe number of shards OpenSearch successfully deleted the document from.\nfailed\nThe number of shards OpenSearch failed to delete the document from.\n_seq_no\nThe sequence number assigned when the document was indexed.\n_primary_term\nThe primary term assigned when the document was indexed.",
    "ancestors": [
      "API reference",
      "Document APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/explain/",
    "title": "Explain",
    "content": "Introduced 1.0\nWondering why a specific document ranks higher (or lower) for a query? You can use the explain API for an explanation of how the relevance score ( _score) is calculated for every result.\nOpenSearch uses a probabilistic ranking framework called Okapi BM25 to calculate relevance scores. Okapi BM25 is based on the original TF/IDF framework used by Apache Lucene.\nThe explain API is an expensive operation in terms of both resources and time. On production clusters, we recommend using it sparingly for the purpose of troubleshooting.\nExample\nTo see the explain output for all results, set the explain flag to true either in the URL or in the body of the request: POST opensearch_dashboards_sample_data_ecommerce/_search?explain= true { \"query\": { \"match\": { \"customer_first_name\": \"Mary\" } } } copy More often, you want the output for a single document. In that case, specify the document ID in the URL: POST opensearch_dashboards_sample_data_ecommerce/_explain/EVz 1 Q 3 sBgg 5 eWQP 6 RSte { \"query\": { \"match\": { \"customer_first_name\": \"Mary\" } } } copy Path and HTTP methods GET &lt;target&gt;/_explain/&lt;id&gt;\nPOST &lt;target&gt;/_explain/&lt;id&gt; URL parameters\nYou must specify the index and document ID. All other URL parameters are optional. Parameter Type Description Required &lt;index&gt; String\nName of the index. You can only specify a single index.\nYes &lt;_id&gt; String\nA unique identifier to attach to the document.\nYes analyzer String\nThe analyzer to use in the query string.\nNo analyze_wildcard Boolean\nSpecifies whether to analyze wildcard and prefix queries. Default is false.\nNo default_operator String\nIndicates whether the default operator for a string query should be AND or OR. Default is OR.\nNo df String\nThe default field in case a field prefix is not provided in the query string.\nNo lenient Boolean\nSpecifies whether OpenSearch should ignore format-based query failures (for example, querying a text field for an integer). Default is false.\nNo preference String\nSpecifies a preference of which shard to retrieve results from. Available options are _local, which tells the operation to retrieve results from a locally allocated shard replica, and a custom string value assigned to a specific shard replica. By default, OpenSearch executes the explain operation on random shards.\nNo q String\nQuery in the Lucene query string syntax.\nNo stored_fields Boolean\nIf true, the operation retrieves document fields stored in the index rather than the document’s _source. Default is false.\nNo routing String\nValue used to route the operation to a specific shard.\nNo _source String\nWhether to include the _source field in the response body. Default is true.\nNo _source_excludes String\nA comma-separated list of source fields to exclude in the query response.\nNo _source_includes String\nA comma-separated list of source fields to include in the query response.\nNo Response { \"_index\": \"kibana_sample_data_ecommerce\", \"_id\": \"EVz1Q3sBgg5eWQP6RSte\", \"matched\": true, \"explanation\": { \"value\": 3.5671005, \"description\": \"weight(customer_first_name:mary in 1) [PerFieldSimilarity], result of:\", \"details\": [ { \"value\": 3.5671005, \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\", \"details\": [ { \"value\": 2.2, \"description\": \"boost\", \"details\": [] }, { \"value\": 3.4100041, \"description\": \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\", \"details\": [ { \"value\": 154, \"description\": \"n, number of documents containing term\", \"details\": [] }, { \"value\": 4675, \"description\": \"N, total number of documents with field\", \"details\": [] }] }, { \"value\": 0.47548598, \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\", \"details\": [ { \"value\": 1.0, \"description\": \"freq, occurrences of term within document\", \"details\": [] }, { \"value\": 1.2, \"description\": \"k1, term saturation parameter\", \"details\": [] }, { \"value\": 0.75, \"description\": \"b, length normalization parameter\", \"details\": [] }, { \"value\": 1.0, \"description\": \"dl, length of field\", \"details\": [] }, { \"value\": 1.1206417, \"description\": \"avgdl, average length of field\", \"details\": [] }] }] }] } } Response body fields Field Description matched Indicates if the document is a match for the query. explanation The explanation object has three properties: value, description, and details. The value shows the result of the calculation, the description explains what type of calculation is performed, and the details shows any subcalculations performed.\nTerm frequency ( tf)\nHow many times the term appears in a field for a given document. The more times the term occurs the higher is the relevance score.\nInverse document frequency ( idf)\nHow often the term appears within the index (across all the documents). The more often the term appears the lower is the relevance score.\nField normalization factor ( fieldNorm)\nThe length of the field. OpenSearch assigns a higher relevance score to a term appearing in a relatively short field. The tf, idf, and fieldNorm values are calculated and stored at index time when a document is added or updated. The values might have some (typically small) inaccuracies as it’s based on summing the samples returned from each shard.\nIndividual queries include other factors for calculating the relevance score, such as term proximity, fuzziness, and so on.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/clear-index-cache/",
    "title": "Clear Index or Data Stream Cache",
    "content": "Clear index or data stream cache\nThe clear cache API operation clears the caches of one or more indexes. For data streams, the API clears the caches of the stream’s backing indexes.\nIf you use the Security plugin, you must have the manage index privileges.\nPath parameters Parameter Data type Description target\nString\nComma-delimited list of data streams, indexes, and index aliases to which cache clearing will be applied. Wildcard expressions ( *) are supported. To target all data streams and indexes in a cluster, omit this parameter or use _all or *. Optional. Query parameters\nAll query parameters are optional. Parameter Data type Description allow_no_indices\nBoolean\nWhether to ignore wildcards, index aliases, or _all target ( target path parameter) values that don’t match any indexes. If false, the request returns an error if any wildcard expression, index alias, or _all target value doesn’t match any indexes. This behavior also applies if the request targets include other open indexes. For example, a request where the target is fig*,app* returns an error if an index starts with fig but no index starts with app. Defaults to true.\nexpand_wildcards\nString\nDetermines the index types that wildcard expressions can expand to. Accepts multiple values separated by a comma, such as open,hidden. Valid values are: all – Expand to open, closed, and hidden indexes. open – Expand only to open indexes. closed – Expand only to closed indexes hidden – Expand to include hidden indexes. Must be combined with open, closed, or both. none – Expansions are not accepted. Defaults to open.\nfielddata\nBoolean\nIf true, clears the fields cache. Use the fields parameter to clear specific fields’ caches. Defaults to true.\nfields\nString\nUsed in conjunction with the fielddata parameter. Comma-delimited list of field names that will be cleared out of the cache. Does not support objects or field aliases. Defaults to all fields.\nindex\nString\nComma-delimited list of index names that will be cleared out of the cache.\nignore_unavailable\nBoolean\nIf true, OpenSearch ignores missing or closed indexes. Defaults to false.\nquery\nBoolean\nIf true, clears the query cache. Defaults to true.\nrequest\nBoolean\nIf true, clears the request cache. Defaults to true. Example requests\nThe following example requests show multiple clear cache API uses.\nClear a specific cache\nThe following request clears the fields cache only: POST /my-index/_cache/clear?fielddata= true copy The following request clears the query cache only: POST /my-index/_cache/clear?query= true copy The following request clears the request cache only: POST /my-index/_cache/clear?request= true copy Clear the cache for specific fields\nThe following request clears the fields caches of fielda and fieldb: POST /my-index/_cache/clear?fields=fielda,fieldb copy Clear caches for specific data streams and indexes\nThe following request clears the cache for two specific indexes: POST /my-index,my-index 2 /_cache/clear copy Clear caches for all data streams and indexes\nThe following request clears the cache for all data streams and indexes: POST /_cache/clear copy Example response\nThe POST /books,hockey/_cache/clear request returns the following fields: { \"_shards\": { \"total\": 4, \"successful\": 2, \"failed\": 0 } } Response fields\nThe POST /books,hockey/_cache/clear request returns the following response fields: Field Data type Description _shards\nObject\nShard information.\ntotal\nInteger\nTotal number of shards.\nsuccessful\nInteger\nNumber of index shards with caches successfully cleared.\nfailed\nInteger\nNumber of index shards with caches that failed to clear.",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/clone/",
    "title": "Clone index",
    "content": "The clone index API operation clones all data in an existing read-only index into a new index. The new index cannot already exist.\nExample PUT /sample-index 1 /_clone/cloned-index 1 { \"settings\": { \"index\": { \"number_of_shards\": 2, \"number_of_replicas\": 1 } }, \"aliases\": { \"sample-alias1\": {} } } copy Path and HTTP methods POST /&lt;source-index&gt;/_clone/&lt;target-index&gt;\nPUT /&lt;source-index&gt;/_clone/&lt;target-index&gt; Index naming restrictions\nOpenSearch indexes have the following naming restrictions:\nAll letters must be lowercase.\nIndex names can’t begin with underscores ( _) or hyphens ( -).\nIndex names can’t contain spaces, commas, or the following characters::, \", *, +, /, \\, |,?, #, &gt;, or &lt; URL parameters\nYour request must include the source and target indexes. All other clone index parameters are optional. Parameter Type Description &lt;source-index&gt;\nString\nThe source index to clone.\n&lt;target-index&gt;\nString\nThe index to create and add cloned data to.\nwait_for_active_shards\nString\nThe number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed.\nmaster_timeout\nTime\nHow long to wait for a connection to the master node. Default is 30s.\ntimeout\nTime\nHow long to wait for the request to return. Default is 30s.\nwait_for_completion\nBoolean\nWhen set to false, the request returns immediately instead of after the operation is finished. To monitor the operation status, use the Tasks API with the task ID returned by the request. Default is true.\ntask_execution_timeout\nTime\nThe explicit task execution timeout. Only useful when wait_for_completion is set to false. Default is 1h. Request body\nThe clone index API operation creates a new target index, so you can specify any index settings and aliases to apply to the target index.\nResponse { \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"cloned-index1\" }",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/close-index/",
    "title": "Close index",
    "content": "Introduced 1.0\nThe close index API operation closes an index. Once an index is closed, you cannot add data to it or search for any data within the index.\nExample POST /sample-index/_close copy Path and HTTP methods POST /&lt;index-name&gt;/_close URL parameters\nAll parameters are optional. Parameter Type Description &lt;index-name&gt;\nString\nThe index to close. Can be a comma-separated list of multiple index names. Use _all or * to close all indices.\nallow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indices. Default is true.\nexpand_wildcards\nString\nExpands wildcard expressions to different indices. Combine multiple values with commas. Available values are all (match all indices), open (match open indices), closed (match closed indices), hidden (match hidden indices), and none (do not accept wildcard expressions). Default is open.\nignore_unavailable\nBoolean\nIf true, OpenSearch does not search for missing or closed indices. Default is false.\nwait_for_active_shards\nString\nSpecifies the number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the request to succeed.\nmaster_timeout\nTime\nHow long to wait for a connection to the master node. Default is 30s.\ntimeout\nTime\nHow long to wait for a response from the cluster. Default is 30s. Response { \"acknowledged\": true, \"shards_acknowledged\": true, \"indices\": { \"sample-index1\": { \"closed\": true } } }",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/create-index/",
    "title": "Create index",
    "content": "Introduced 1.0\nWhile you can create an index by using a document as a base, you can also create an empty index for use later.\nExample\nThe following example demonstrates how to create an index with a non-default number of primary and replica shards, specifies that age is of type integer, and assigns a sample-alias1 alias to the index. PUT /sample-index 1 { \"settings\": { \"index\": { \"number_of_shards\": 2, \"number_of_replicas\": 1 } }, \"mappings\": { \"properties\": { \"age\": { \"type\": \"integer\" } } }, \"aliases\": { \"sample-alias1\": {} } } Path and HTTP methods PUT &lt;index-name&gt; Index naming restrictions\nOpenSearch indexes have the following naming restrictions:\nAll letters must be lowercase.\nIndex names can’t begin with underscores ( _) or hyphens ( -).\nIndex names can’t contain spaces, commas, or the following characters::, \", *, +, /, \\, |,?, #, &gt;, or &lt; URL parameters\nYou can include the following URL parameters in your request. All parameters are optional. Parameter Type Description wait_for_active_shards\nString\nSpecifies the number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the request to succeed.\nmaster_timeout\nTime\nHow long to wait for a connection to the master node. Default is 30s.\ntimeout\nTime\nHow long to wait for the request to return. Default is 30s. Request body\nAs part of your request, you can supply parameters in your request’s body that specify index settings, mappings, and aliases for your newly created index. The following sections provide more information about index settings and mappings.\nIndex settings\nIndex settings are separated into two varieties: static index settings and dynamic index settings. Static index settings are settings that you specify at index creation and can’t change later. You can change dynamic settings at any time, including at index creation.\nStatic index settings Setting Description index.number_of_shards\nThe number of primary shards in the index. Default is 1.\nindex.number_of_routing_shards\nThe number of routing shards used to split an index.\nindex.shard.check_on_startup\nWhether the index’s shards should be checked for corruption. Available options are false (do not check for corruption), checksum (check for physical corruption), and true (check for both physical and logical corruption). Default is false.\nindex.codec\nThe compression type to use to compress stored data. Available values are default (optimizes for retrieval speed) and best_compression (optimizes for better compression at the expense of speed, leading to smaller data sizes on disk).\nindex.routing_partition_size\nThe number of shards a custom routing value can go to. Routing helps an imbalanced cluster by relocating values to a subset of shards rather than just a single shard. To enable, set this value to greater than 1 but less than index.number_of_shards. Default is 1.\nindex.soft_deletes.retention_lease.period\nThe maximum amount of time to retain a shard’s history of operations. Default is 12h.\nindex.load_fixed_bitset_filters_eagerly\nWhether OpenSearch should pre-load cached filters. Available options are true and false. Default is true.\nindex.hidden\nWhether the index should be hidden. Hidden indices are not returned as part of queries that have wildcards. Available options are true and false. Default is false. Dynamic index Settings Setting Description index.number_of_replicas\nThe number of replica shards each primary shard should have. For example, if you have 4 primary shards and set index.number_of_replicas to 3, the index has 12 replica shards. Default is 1.\nindex.auto_expand_replicas\nWhether the cluster should automatically add replica shards based on the number of data nodes. Specify a lower bound and upper limit (for example, 0-9), or all for the upper limit. For example, if you have 5 data nodes and set index.auto_expand_replicas to 0-3, then the cluster does not automatically add another replica shard. However, if you set this value to 0-all and add 2 more nodes for a total of 7, the cluster will expand to now have 6 replica shards. Default is disabled.\nindex.search.idle.after\nAmount of time a shard should wait for a search or get request until it goes idle. Default is 30s.\nindex.refresh_interval\nHow often the index should refresh, which publishes its most recent changes and makes them available for searching. Can be set to -1 to disable refreshing. Default is 1s.\nindex.max_result_window\nThe maximum value of from + size for searches to the index. from is the starting index to search from, and size is the amount of results to return. Default: 10000.\nindex.max_inner_result_window\nMaximum value of from + size to return nested search hits and most relevant document aggregated during the query. from is the starting index to search from, and size is the amount of top hits to return. Default is 100.\nindex.max_rescore_window\nThe maximum value of window_size for rescore requests to the index. Rescore requests reorder the index’s documents and return a new score, which can be more precise. Default is the same as index.max_inner_result_window or 10000 by default.\nindex.max_docvalue_fields_search\nMaximum amount of docvalue_fields allowed in a query. Default is 100.\nindex.max_script_fields\nMaximum amount of script_fields allowed in a query. Default is 32.\nindex.max_ngram_diff\nMaximum difference between min_gram and max_gram values for NGramTokenizer and NGramTokenFilter fields. Default is 1.\nindex.max_shingle_diff\nMaximum difference between max_shingle_size and min_shingle_size to feed into the shingle token filter. Default is 3.\nindex.max_refresh_listeners\nMaximum amount of refresh listeners each shard is allowed to have.\nindex.analyze.max_token_count\nMaximum amount of tokens that can return from the _analyze API operation. Default is 10000.\nindex.highlight.max_analyzed_offset\nThe amount of characters a highlight request can analyze. Default is 1000000.\nindex.max_terms_count\nThe maximum amount of terms a terms query can accept. Default is 65536.\nindex.max_regex_length\nThe maximum character length of regex that can be in a regexp query. Default is 1000.\nindex.query.default_field\nA field or list of fields that OpenSearch uses in queries in case a field isn’t specified in the parameters.\nindex.routing.allocation.enable\nSpecifies options for the index’s shard allocation. Available options are all (allow allocation for all shards), primaries (allow allocation only for primary shards), new_primaries (allow allocation only for new primary shards), and none (do not allow allocation). Default is all.\nindex.routing.rebalance.enable\nEnables shard rebalancing for the index. Available options are all (allow rebalancing for all shards), primaries (allow rebalancing only for primary shards), replicas (allow rebalancing only for replicas), and none (do not allow rebalancing). Default is all.\nindex.gc_deletes\nAmount of time to retain a deleted document’s version number. Default is 60s.\nindex.default_pipeline\nThe default ingest node pipeline for the index. If the default pipeline is set and the pipeline does not exist, then index requests fail. The pipeline name _none specifies that the index does not have an ingest pipeline.\nindex.final_pipeline\nThe final ingest node pipeline for the index. If the final pipeline is set and the pipeline does not exist, then index requests fail. The pipeline name _none specifies that the index does not have an ingest pipeline.",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/dangling-index/",
    "title": "Dangling indexes",
    "content": "After a node joins a cluster, dangling indexes occur if any shards exist in the node’s local directory that do not already exist in the cluster. Dangling indexes can be listed, deleted, or imported.\nPath and HTTP methods\nList dangling indexes: GET /_dangling Import a dangling index: POST /_dangling/&lt;index-uuid&gt; Delete a dangling index: DELETE /_dangling/&lt;index-uuid&gt; Path parameters\nPath parameters are required. Path parameter Description index-uuid\nUUID of index. Query parameters\nQuery parameters are optional. Query parameter Data type Description accept_data_loss\nBoolean\nMust be set to true for an import or delete because OpenSearch is unaware of where the dangling index data came from.\ntimeout\nTime units\nThe amount of time to wait for a response. If no response is received in the defined time period, an error is returned. Default is 30 seconds.\nmaster_timeout\nTime units\nThe amount of time to wait for the connection to the cluster manager. If no response is received in the defined time period, an error is returned. Default is 30 seconds. Examples\nThe following are example requests and a example response.\nSample list GET /_dangling copy Sample import POST /_dangling/msdjernajxAT23RT-BupMB?accept_data_loss = true copy Sample delete DELETE /_dangling/msdjernajxAT23RT-BupMB?accept_data_loss = true Example response body { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"opensearch-cluster\", \"dangling_indices\": [ msdjernajxAT 23 RT-BupMB] }",
    "ancestors": [
      "API reference",
      "index-apis"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/delete-index/",
    "title": "Delete index",
    "content": "Introduced 1.0\nIf you no longer need an index, you can use the delete index API operation to delete it.\nExample DELETE /sample-index copy Path and HTTP methods DELETE /&lt;index-name&gt; URL parameters\nAll parameters are optional. Parameter Type Description allow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indices. Default is true.\nexpand_wildcards\nString\nExpands wildcard expressions to different indices. Combine multiple values with commas. Available values are all (match all indices), open (match open indices), closed (match closed indices), hidden (match hidden indices), and none (do not accept wildcard expressions), which must be used with open, closed, or both. Default is open.\nignore_unavailable\nBoolean\nIf true, OpenSearch does not include missing or closed indices in the response.\nmaster_timeout\nTime\nHow long to wait for a connection to the master node. Default is 30s.\ntimeout\nTime\nHow long to wait for the response to return. Default is 30s. Response { \"acknowledged\": true }",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/exists/",
    "title": "Index exists",
    "content": "Introduced 1.0\nThe index exists API operation returns whether or not an index already exists.\nExample HEAD /sample-index copy Path and HTTP methods HEAD /&lt;index-name&gt; URL parameters\nAll parameters are optional. Parameter Type Description allow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indexes. Default is true.\nexpand_wildcards\nString\nExpands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions). Default is open.\nflat_settings\nBoolean\nWhether to return settings in the flat form, which can improve readability, especially for heavily nested settings. For example, the flat form of “index”: { “creation_date”: “123456789” } is “index.creation_date”: “123456789”.\ninclude_defaults\nBoolean\nWhether to include default settings as part of the response. This parameter is useful for identifying the names and current values of settings you want to update.\nignore_unavailable\nBoolean\nIf true, OpenSearch does not search for missing or closed indexes. Default is false.\nlocal\nBoolean\nWhether to return information from only the local node instead of from the master node. Default is false. Response\nThe index exists API operation returns only one of two possible response codes: 200 – the index exists, and 404 – the index does not exist.",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/get-index/",
    "title": "Get index",
    "content": "Introduced 1.0\nYou can use the get index API operation to return information about an index.\nExample GET /sample-index copy Path and HTTP methods GET /&lt;index-name&gt; URL parameters\nAll parameters are optional. Parameter Type Description allow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indexes. Default is true.\nexpand_wildcards\nString\nExpands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions), which must be used with open, closed, or both. Default is open.\nflat_settings\nBoolean\nWhether to return settings in the flat form, which can improve readability, especially for heavily nested settings. For example, the flat form of “index”: { “creation_date”: “123456789” } is “index.creation_date”: “123456789”.\ninclude_defaults\nBoolean\nWhether to include default settings as part of the response. This parameter is useful for identifying the names and current values of settings you want to update.\nignore_unavailable\nBoolean\nIf true, OpenSearch does not include missing or closed indexes in the response.\nlocal\nBoolean\nWhether to return information from only the local node instead of from the master node. Default is false.\nmaster_timeout\nTime\nHow long to wait for a connection to the master node. Default is 30s. Response { \"sample-index1\": { \"aliases\": {}, \"mappings\": {}, \"settings\": { \"index\": { \"creation_date\": \"1633044652108\", \"number_of_shards\": \"2\", \"number_of_replicas\": \"1\", \"uuid\": \"XcXA0aZ5S0aiqx3i1Ce95w\", \"version\": { \"created\": \"135217827\" }, \"provided_name\": \"sample-index1\" } } } } Response body fields Field Description aliases\nAny aliases associated with the index.\nmappings\nAny mappings in the index.\nsettings\nThe index’s settings\ncreation_date\nThe Unix epoch time of when the index was created.\nnumber_of_shards\nHow many shards the index has.\nnumber_of_replicas\nHow many replicas the index has.\nuuid\nThe index’s uuid.\ncreated\nThe version of OpenSearch when the index was created.\nprovided_name\nName of the index.",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/get-settings/",
    "title": "Get settings",
    "content": "Introduced 1.0\nThe get settings API operation returns all the settings in your index.\nExample GET /sample-index 1 /_settings copy Path and HTTP methods GET /_settings\nGET /&lt;target-index&gt;/_settings\nGET /&lt;target-index&gt;/_settings/&lt;setting&gt; URL parameters\nAll update settings parameters are optional. Parameter Data type Description &lt;target-index&gt;\nString\nThe index to get settings from. Can be a comma-separated list to get settings from multiple indexes, or use _all to return settings from all indexes within the cluster.\n&lt;setting&gt;\nString\nFilter to return specific settings.\nallow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indexes. Default is true.\nexpand_wildcards\nString\nExpands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions), which must be used with open, closed, or both. Default is open.\nflat_settings\nBoolean\nWhether to return settings in the flat form, which can improve readability, especially for heavily nested settings. For example, the flat form of “index”: { “creation_date”: “123456789” } is “index.creation_date”: “123456789”.\ninclude_defaults\nString\nWhether to include default settings, including settings used within OpenSearch plugins, in the response. Default is false.\nignore_unavailable\nBoolean\nIf true, OpenSearch does not include missing or closed indexes in the response.\nlocal\nBoolean\nWhether to return information from the local node only instead of the master node. Default is false.\nmaster_timeout\nTime\nHow long to wait for a connection to the master node. Default is 30s. Response { \"sample-index1\": { \"settings\": { \"index\": { \"creation_date\": \"1622672553417\", \"number_of_shards\": \"1\", \"number_of_replicas\": \"1\", \"uuid\": \"GMEA0_TkSaamrnJSzNLzwg\", \"version\": { \"created\": \"135217827\", \"upgraded\": \"135238227\" }, \"provided_name\": \"sample-index1\" } } } }",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/index/",
    "title": "Index APIs",
    "content": "The index API operations let you interact with indexes in your cluster. Using these operations, you can create, delete, close, and complete other index-related operations.\nIf you use the Security plugin, make sure you have the appropriate permissions.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/open-index/",
    "title": "Open index",
    "content": "Introduced 1.0\nThe open index API operation opens a closed index, letting you add or search for data within the index.\nExample POST /sample-index/_open copy Path and HTTP methods POST /&lt;index-name&gt;/_open URL parameters\nAll parameters are optional. Parameter Type Description &lt;index-name&gt;\nString\nThe index to open. Can be a comma-separated list of multiple index names. Use _all or * to open all indexes.\nallow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indexes. Default is true.\nexpand_wildcards\nString\nExpands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions). Default is open.\nignore_unavailable\nBoolean\nIf true, OpenSearch does not search for missing or closed indexes. Default is false.\nwait_for_active_shards\nString\nSpecifies the number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the request to succeed.\nmaster_timeout\nTime\nHow long to wait for a connection to the master node. Default is 30s.\ntimeout\nTime\nHow long to wait for a response from the cluster. Default is 30s.\nwait_for_completion\nBoolean\nWhen set to false, the request returns immediately instead of after the operation is finished. To monitor the operation status, use the Tasks API with the task ID returned by the request. Default is true.\ntask_execution_timeout\nTime\nThe explicit task execution timeout. Only useful when wait_for_completion is set to false. Default is 1h. Response { \"acknowledged\": true, \"shards_acknowledged\": true }",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/put-mapping/",
    "title": "Create or update mappings",
    "content": "Introduced 1.0\nIf you want to create or add mappings and fields to an index, you can use the put mapping API operation. For an existing mapping, this operation updates the mapping.\nYou can’t use this operation to update mappings that already map to existing data in the index. You must first create a new index with your desired mappings, and then use the reindex API operation to map all the documents from your old index to the new index. If you don’t want any downtime while you re-index your indexes, you can use aliases.\nRequired path parameter\nThe only required path parameter is the index with which to associate the mapping. If you don’t specify an index, you will get an error. You can specify a single index, or multiple indexes separated by a comma as follows: PUT /&lt;target-index&gt;/_mapping\nPUT /&lt;target-index1&gt;,&lt;target-index2&gt;/_mapping Required request body field\nThe request body must contain properties, which has all of the mappings that you want to create or update. { \"properties\":{ \"color\":{ \"type\": \"text\" }, \"year\":{ \"type\": \"integer\" } } } Optional request body fields\ndynamic\nYou can make the document structure match the structure of the index mapping by setting the dynamic request body field to strict, as seen in the following example: { \"properties\":{ \"dynamic\": \"strict\", \"color\":{ \"type\": \"text\" } } } Optional query parameters\nOptionally, you can add query parameters to make a more specific request. For example, to skip any missing or closed indexes in the response, you can add the ignore_unavailable query parameter to your request as follows: PUT /sample-index/_mapping?ignore_unavailable The following table defines the put mapping query parameters: Parameter Data type Description allow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indexes. Default is true.\nexpand_wildcards\nString\nExpands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions), which must be used with open, closed, or both. Default is open.\nignore_unavailable\nBoolean\nIf true, OpenSearch does not include missing or closed indexes in the response.\nignore_malformed\nBoolean\nUse this parameter with the ip_range data type to specify that OpenSearch should ignore malformed fields. If true, OpenSearch does not include entries that do not match the IP range specified in the index in the response. The default is false.\ncluster_manager_timeout\nTime\nHow long to wait for a connection to the cluster manager node. Default is 30s.\ntimeout\nTime\nHow long to wait for the response to return. Default is 30s.\nwrite_index_only\nBoolean\nWhether OpenSearch should apply mapping updates only to the write index. Sample Request\nThe following request creates a new mapping for the sample-index index: PUT /sample-index/_mapping { \"properties\": { \"age\": { \"type\": \"integer\" }, \"occupation\":{ \"type\": \"text\" } } } copy Sample Response\nUpon success, the response returns \"acknowledged\": true. { \"acknowledged\": true }",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/shrink-index/",
    "title": "Shrink index",
    "content": "The shrink index API operation moves all of your data in an existing index into a new index with fewer primary shards.\nExample POST /my-old-index/_shrink/my-new-index { \"settings\": { \"index.number_of_replicas\": 4, \"index.number_of_shards\": 3 }, \"aliases\":{ \"new-index-alias\": {} } } copy Path and HTTP methods POST /&lt;index-name&gt;/_shrink/&lt;target-index&gt;\nPUT /&lt;index-name&gt;/_shrink/&lt;target-index&gt; When creating new indexes with this operation, remember that OpenSearch indexes have the following naming restrictions:\nAll letters must be lowercase.\nIndex names can’t begin with underscores ( _) or hyphens ( -).\nIndex names can’t contain spaces, commas, or the following characters::, \", *, +, /, \\, |,?, #, &gt;, or &lt; URL parameters\nThe shrink index API operation requires you to specify both the source index and the target index. All other parameters are optional. Parameter Type description &lt;index-name&gt;\nString\nThe index to shrink.\n&lt;target-index&gt;\nString\nThe target index to shrink the source index into.\nwait_for_active_shards\nString\nSpecifies the number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the request to succeed.\nmaster_timeout\nTime\nHow long to wait for a connection to the master node. Default is 30s.\ntimeout\nTime\nHow long to wait for the request to return a response. Default is 30s.\nwait_for_completion\nBoolean\nWhen set to false, the request returns immediately instead of after the operation is finished. To monitor the operation status, use the Tasks API with the task ID returned by the request. Default is true.\ntask_execution_timeout\nTime\nThe explicit task execution timeout. Only useful when wait_for_completion is set to false. Default is 1h. Request body\nYou can use the request body to configure some index settings for the target index. All fields are optional. Field Type Description alias\nObject\nSets an alias for the target index. Can have the fields filter, index_routing, is_hidden, is_write_index, routing, or search_routing. See Index Aliases.\nsettings\nObject\nIndex settings you can apply to your target index. See Index Settings. max_shard_size Bytes\nSpecifies the maximum size of a primary shard in the target index. Because max_shard_size conflicts with the index.number_of_shards setting, you cannot set both of them at the same time. The max_shard_size parameter\nThe max_shard_size parameter specifies the maximum size of a primary shard in the target index. OpenSearch uses max_shard_size and the total storage for all primary shards in the source index to calculate the number of primary shards and their size for the target index.\nThe primary shard count of the target index is the smallest factor of the source index’s primary shard count for which the shard size does not exceed max_shard_size. For example, if the source index has 8 primary shards, they occupy a total of 400 GB of storage, and the max_shard_size is equal to 150 GB, OpenSearch calculates the number of primary shards in the target index using the following algorithm:\nCalculate the minimum number of primary shards as 400/150, rounded to the nearest whole integer. The minimum number of primary shards is 3.\nCalculate the number of primary shards as the smallest factor of 8 that is greater than 3. The number of primary shards is 4.\nThe maximum number of primary shards for the target index is equal to the number of primary shards in the source index because the shrink operation is used to reduce the primary shard count. As an example, consider a source index with 5 primary shards that occupy a total of 600 GB of storage. If max_shard_size is 100 GB, the minimum number of primary shards is 600/100, which is 6. However, because the number of primary shards in the source index is smaller than 6, the number of primary shards in the target index is set to 5.\nThe minimum number of primary shards for the target index is 1.",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/split/",
    "title": "Split index",
    "content": "The split index API operation splits an existing read-only index into a new index, cutting each primary shard into some amount of primary shards in the new index.\nExample PUT /sample-index 1 /_split/split-index 1 { \"settings\": { \"index\": { \"number_of_shards\": 4, \"number_of_replicas\": 2 } }, \"aliases\": { \"sample-alias1\": {} } } copy Path and HTTP methods POST /&lt;source-index&gt;/_split/&lt;target-index&gt;\nPUT /&lt;source-index&gt;/_split/&lt;target-index&gt; Index naming restrictions\nOpenSearch indexes have the following naming restrictions:\nAll letters must be lowercase.\nIndex names can’t begin with underscores ( _) or hyphens ( -).\nIndex names can’t contain spaces, commas, or the following characters::, \", *, +, /, \\, |,?, #, &gt;, or &lt; URL parameters\nYour request must include the source and target indexes. All split index parameters are optional. Parameter Type Description &lt;source-index&gt;\nString\nThe source index to split.\n&lt;target-index&gt;\nString\nThe index to create.\nwait_for_active_shards\nString\nThe number of active shards that must be available before OpenSearch processes the request. Default is 1 (only the primary shard). Set to all or a positive integer. Values greater than 1 require replicas. For example, if you specify a value of 3, the index must have two replicas distributed across two additional nodes for the operation to succeed.\nmaster_timeout\nTime\nHow long to wait for a connection to the master node. Default is 30s.\ntimeout\nTime\nHow long to wait for the request to return. Default is 30s.\nwait_for_completion\nBoolean\nWhen set to false, the request returns immediately instead of after the operation is finished. To monitor the operation status, use the Tasks API with the task ID returned by the request. Default is true.\ntask_execution_timeout\nTime\nThe explicit task execution timeout. Only useful when wait_for_completion is set to false. Default is 1h. Request body\nThe split index API operation creates a new target index, so you can specify any index settings and aliases to apply to the target index.\nResponse { \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"split-index1\" }",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index-apis/update-settings/",
    "title": "Update settings",
    "content": "Introduced 1.0\nYou can use the update settings API operation to update index-level settings. You can change dynamic index settings at any time, but static settings cannot be changed after index creation. For more information about static and dynamic index settings, see Create index.\nAside from the static and dynamic index settings, you can also update individual plugins’ settings. To get the full list of updatable settings, run GET &lt;target-index&gt;/_settings?include_defaults=true.\nExample PUT /sample-index 1 /_settings { \"index.plugins.index_state_management.rollover_skip\": true, \"index\": { \"number_of_replicas\": 4 } } copy Path and HTTP methods PUT /&lt;target-index&gt;/_settings URL parameters\nAll update settings parameters are optional. Parameter Data type Description allow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indexes. Default is true.\nexpand_wildcards\nString\nExpands wildcard expressions to different indexes. Combine multiple values with commas. Available values are all (match all indexes), open (match open indexes), closed (match closed indexes), hidden (match hidden indexes), and none (do not accept wildcard expressions), which must be used with open, closed, or both. Default is open.\nflat_settings\nBoolean\nWhether to return settings in the flat form, which can improve readability, especially for heavily nested settings. For example, the flat form of “index”: { “creation_date”: “123456789” } is “index.creation_date”: “123456789”.\nignore_unavailable\nBoolean\nIf true, OpenSearch does not include missing or closed indexes in the response.\npreserve_existing\nBoolean\nWhether to preserve existing index settings. Default is false.\nmaster_timeout\nTime\nHow long to wait for a connection to the master node. Default is 30s.\ntimeout\nTime\nHow long to wait for a connection to return. Default is 30s. Request body\nThe request body must all of the index settings that you want to update. { \"index.plugins.index_state_management.rollover_skip\": true, \"index\": { \"number_of_replicas\": 4 } } Response { \"acknowledged\": true }",
    "ancestors": [
      "API reference",
      "Index APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/index/",
    "title": "REST API reference",
    "content": "OpenSearch uses its REST API for most operations. This incomplete section includes REST API paths, HTTP verbs, supported parameters, request body details, and example responses.\nIn general, the OpenSearch REST API is no different from the Elasticsearch OSS REST API; most client code that worked with Elasticsearch OSS should also work with OpenSearch.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/ingest-apis/create-update-ingest/",
    "title": "Create or update ingest pipeline",
    "content": "The create ingest pipeline API operation creates or updates an ingest pipeline. Each pipeline requires an ingest definition defining how each processor transforms your documents.\nExample PUT _ingest/pipeline/12345\n{\n\"description\": \"A description for your pipeline\",\n\"processors\": [\n{\n\"set\": {\n\"field\": \"field-name\",\n\"value\": \"value\"\n}\n}]\n} copy Path and HTTP methods PUT _ingest/pipeline/{id} Request body fields Field Required Type Description description\nOptional\nstring\nDescription of your ingest pipeline.\nprocessors\nRequired\nArray of processor objects\nA processor that transforms documents. Runs in the order specified. Appears in index once ran. { \"description\": \"A description for your pipeline\", \"processors\": [ { \"set\": { \"field\": \"field-name\", \"value\": \"value\" } }] } URL parameters\nAll URL parameters are optional. Parameter Type Description master_timeout\ntime\nHow long to wait for a connection to the master node.\ntimeout\ntime\nHow long to wait for the request to return. Response { \"acknowledged\": true }",
    "ancestors": [
      "API reference",
      "Ingest APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/ingest-apis/delete-ingest/",
    "title": "Delete a pipeline",
    "content": "If you no longer want to use an ingest pipeline, use the delete ingest pipeline API operation.\nExample DELETE _ingest/pipeline/12345 copy Path and HTTP methods\nDelete an ingest pipeline based on that pipeline’s ID. DELETE _ingest/pipeline/ URL parameters\nAll URL parameters are optional. Parameter Type Description master_timeout\ntime\nHow long to wait for a connection to the master node.\ntimeout\ntime\nHow long to wait for the request to return. Response { \"acknowledged\": true }",
    "ancestors": [
      "API reference",
      "Ingest APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/ingest-apis/get-ingest/",
    "title": "Get ingest pipeline",
    "content": "Get ingest pipeline\nAfter you create a pipeline, use the get ingest pipeline API operation to return all the information about a specific ingest pipeline.\nExample GET _ingest/pipeline/12345 copy Path and HTTP methods\nReturn all ingest pipelines. GET _ingest/pipeline Returns a single ingest pipeline based on the pipeline’s ID. GET _ingest/pipeline/{id} URL parameters\nAll parameters are optional. Parameter Type Description master_timeout\ntime\nHow long to wait for a connection to the master node. Response { \"pipeline-id\": { \"description\": \"A description for your pipeline\", \"processors\": [ { \"set\": { \"field\": \"field-name\", \"value\": \"value\" } }] } }",
    "ancestors": [
      "API reference",
      "Ingest APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/ingest-apis/index/",
    "title": "Ingest APIs",
    "content": "Before you index your data, OpenSearch’s ingest APIs help transform your data by creating and managing ingest pipelines. Pipelines consist of processors, customizable tasks that run in the order they appear in the request body. The transformed data appears in your index after each of the processor completes.\nIngest pipelines in OpenSearch can only be managed using ingest API operations. When using ingest in production environments, your cluster should contain at least one node with the node roles permission set to ingest. For more information on setting up node roles within a cluster, see Cluster Formation.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/ingest-apis/simulate-ingest/",
    "title": "Simulate an ingest pipeline",
    "content": "Simulates an ingest pipeline with any example documents you specify.\nExample POST /_ingest/pipeline/35678/_simulate\n{\n\"docs\": [\n{\n\"_index\": \"index\",\n\"_id\": \"id\",\n\"_source\": {\n\"location\": \"document-name\"\n}\n},\n{\n\"_index\": \"index\",\n\"_id\": \"id\",\n\"_source\": {\n\"location\": \"document-name\"\n}\n}]\n} copy Path and HTTP methods\nSimulate the last ingest pipeline created. GET _ingest/pipeline/_simulate\nPOST _ingest/pipeline/_simulate Simulate a single pipeline based on the pipeline’s ID. GET _ingest/pipeline/{id}/_simulate\nPOST _ingest/pipeline/{id}/_simulate URL parameters\nAll URL parameters are optional. Parameter Type Description verbose\nboolean\nVerbose mode. Display data output for each processor in executed pipeline. Request body fields Field Required Type Description pipeline Optional\nobject\nThe pipeline you want to simulate. When included without the pipeline {id} inside the request path, the response simulates the last pipeline created. docs Required\narray of objects\nThe documents you want to use to test the pipeline. The docs field can include the following subfields: Field Required Type Description id Optional\nstring\nAn optional identifier for the document. The identifier cannot be used elsewhere in the index. index Optional\nstring\nThe index where the document’s transformed data appears. source Required\nobject\nThe document’s JSON body. Response\nResponses vary based on which path and HTTP method you choose.\nSpecify pipeline in request body { \"docs\": [ { \"doc\": { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"location\": \"new-new\", \"field2\": \"_value\" }, \"_ingest\": { \"timestamp\": \"2022-02-07T18:47:57.479230835Z\" } } }, { \"doc\": { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"location\": \"new-new\", \"field2\": \"_value\" }, \"_ingest\": { \"timestamp\": \"2022-02-07T18:47:57.47933496Z\" } } }] } Specify pipeline ID inside HTTP path { \"docs\": [ { \"doc\": { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"field-name\": \"value\", \"location\": \"document-name\" }, \"_ingest\": { \"timestamp\": \"2022-02-03T21:47:05.382744877Z\" } } }, { \"doc\": { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"field-name\": \"value\", \"location\": \"document-name\" }, \"_ingest\": { \"timestamp\": \"2022-02-03T21:47:05.382803544Z\" } } }] } Receive verbose response\nWith the verbose parameter set to true, the response shows how each processor transforms the specified document. { \"docs\": [ { \"processor_results\": [ { \"processor_type\": \"set\", \"status\": \"success\", \"doc\": { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"field-name\": \"value\", \"location\": \"document-name\" }, \"_ingest\": { \"pipeline\": \"35678\", \"timestamp\": \"2022-02-03T21:45:09.414049004Z\" } } }] }, { \"processor_results\": [ { \"processor_type\": \"set\", \"status\": \"success\", \"doc\": { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"field-name\": \"value\", \"location\": \"document-name\" }, \"_ingest\": { \"pipeline\": \"35678\", \"timestamp\": \"2022-02-03T21:45:09.414093212Z\" } } }] }] }",
    "ancestors": [
      "API reference",
      "Ingest APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/multi-search/",
    "title": "Multi-search",
    "content": "Introduced 1.0\nAs the name suggests, the multi-search operation lets you bundle multiple search requests into a single request. OpenSearch then executes the searches in parallel, so you get back the response more quickly compared to sending one request per search. OpenSearch executes each search independently, so the failure of one doesn’t affect the others.\nExample GET _msearch { \"index\": \"opensearch_dashboards_sample_data_logs\" } { \"query\": { \"match_all\": {} }, \"from\": 0, \"size\": 10 } { \"index\": \"opensearch_dashboards_sample_data_ecommerce\", \"search_type\": \"dfs_query_then_fetch\" } { \"query\": { \"match_all\": {} } } copy Path and HTTP methods GET _msearch\nGET &lt;indices&gt;/_msearch\nPOST _msearch\nPOST &lt;indices&gt;/_msearch Request body\nThe multi-search request body follows this pattern: Metadata\\n\nQuery\\n\nMetadata\\n\nQuery\\n Metadata lines include options, such as which indices to search and the type of search.\nQuery lines use the query DSL.\nJust like the bulk operation, the JSON doesn’t need to be minified—spaces are fine—but it does need to be on a single line. OpenSearch uses newline characters to parse multi-search requests and requires that the request body end with a newline character.\nURL parameters and metadata options\nAll multi-search URL parameters are optional. Some can also be applied per-search as part of each metadata line. Parameter Type Description Supported in metadata line allow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indices. Default is true.\nYes\ncancel_after_time_interval\nTime\nThe time after which the search request will be canceled. Supported at both parent and child request levels. The order of precedence is: 1. Child-level parameter 2. Parent-level parameter 3. Cluster setting. Default is -1.\nYes\ncss_minimize_roundtrips\nBoolean\nWhether OpenSearch should try to minimize the number of network round trips between the coordinating node and remote clusters (only applicable to cross-cluster search requests). Default is true.\nNo\nexpand_wildcards\nEnum\nExpands wildcard expressions to concrete indices. Combine multiple values with commas. Supported values are all, open, closed, hidden, and none. Default is open.\nYes\nignore_unavailable\nBoolean\nIf an index from the indices list doesn’t exist, whether to ignore it rather than fail the query. Default is false.\nYes\nmax_concurrent_searches\nInteger\nThe maximum number of concurrent searches. The default depends on your node count and search thread pool size. Higher values can improve performance, but risk overloading the cluster.\nNo\nmax_concurrent_shard_requests\nInteger\nMaximum number of concurrent shard requests that each search executes per node. Default is 5. Higher values can improve performance, but risk overloading the cluster.\nNo\npre_filter_shard_size\nInteger\nDefault is 128.\nNo\nrest_total_hits_as_int\nString\nWhether the hits.total property is returned as an integer ( true) or an object ( false). Default is false.\nNo\nsearch_type\nString\nAffects relevance score. Valid options are query_then_fetch and dfs_query_then_fetch. query_then_fetch scores documents using term and document frequencies for the shard (faster, less accurate), whereas dfs_query_then_fetch uses term and document frequencies across all shards (slower, more accurate). Default is query_then_fetch.\nYes\ntyped_keys\nBoolean\nWhether to prefix aggregation names with their internal types in the response. Default is false.\nNo Metadata-only options\nSome options can’t be applied as URL parameters to the entire request. Instead, you can apply them per-search as part of each metadata line. All are optional. Option Type Description index\nString, string array\nIf you don’t specify an index or multiple indices as part of the URL (or want to override the URL value for an individual search), you can include it here. Examples include \"logs-*\" and [\"my-store\", \"sample_data_ecommerce\"].\npreference\nString\nThe nodes or shards that you’d like to perform the search. This setting can be useful for testing, but in most situations, the default behavior provides the best search latencies. Options include _local, _only_local, _prefer_nodes, _only_nodes, and _shards. These last three options accept a list of nodes or shards. Examples include \"_only_nodes:data-node1,data-node2\" and \"_shards:0,1.\nrequest_cache\nBoolean\nWhether to cache results, which can improve latency for repeat searches. Default is to use the index.requests.cache.enable setting for the index (which defaults to true for new indices).\nrouting\nString\nComma-separated custom routing values (e.g. \"routing\": \"value1,value2,value3\". Response\nOpenSearch returns an array with the results of each search in the same order as the multi-search request. { \"took\": 2150, \"responses\": [ { \"took\": 2149, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 10000, \"relation\": \"gte\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"opensearch_dashboards_sample_data_logs\", \"_id\": \"_fnhBXsBgv2Zxgu9dZ8Y\", \"_score\": 1.0, \"_source\": { \"agent\": \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1;.NET CLR 1.1.4322)\", \"bytes\": 4657, \"clientip\": \"213.116.129.196\", \"extension\": \"zip\", \"geo\": { \"srcdest\": \"CN:US\", \"src\": \"CN\", \"dest\": \"US\", \"coordinates\": { \"lat\": 42.35083333, \"lon\": -86.25613889 } }, \"host\": \"artifacts.opensearch.org\", \"index\": \"opensearch_dashboards_sample_data_logs\", \"ip\": \"213.116.129.196\", \"machine\": { \"ram\": 16106127360, \"os\": \"ios\" }, \"memory\": null, \"message\": \"213.116.129.196 - - [2018-07-30T14:12:11.387Z] \\\" GET /opensearch_dashboards/opensearch_dashboards-1.0.0-windows-x86_64.zip HTTP/1.1 \\\" 200 4657 \\\" - \\\" \\\" Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1;.NET CLR 1.1.4322) \\\" \", \"phpmemory\": null, \"referer\": \"http://twitter.com/success/ellison-onizuka\", \"request\": \"/opensearch_dashboards/opensearch_dashboards-1.0.0-windows-x86_64.zip\", \"response\": 200, \"tags\": [ \"success\", \"info\"], \"timestamp\": \"2021-08-02T14:12:11.387Z\", \"url\": \"https://artifacts.opensearch.org/downloads/opensearch_dashboards/opensearch_dashboards-1.0.0-windows-x86_64.zip\", \"utc_time\": \"2021-08-02T14:12:11.387Z\", \"event\": { \"dataset\": \"sample_web_logs\" } } },...] }, \"status\": 200 }, { \"took\": 1473, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4675, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"_id\": \"efnhBXsBgv2Zxgu9ap7e\", \"_score\": 1.0, \"_source\": { \"category\": [ \"Women's Clothing\"], \"currency\": \"EUR\", \"customer_first_name\": \"Gwen\", \"customer_full_name\": \"Gwen Dennis\", \"customer_gender\": \"FEMALE\", \"customer_id\": 26, \"customer_last_name\": \"Dennis\", \"customer_phone\": \"\", \"day_of_week\": \"Tuesday\", \"day_of_week_i\": 1, \"email\": \"gwen@dennis-family.zzz\", \"manufacturer\": [ \"Tigress Enterprises\", \"Gnomehouse mom\"], \"order_date\": \"2021-08-10T16:24:58+00:00\", \"order_id\": 576942, \"products\": [ { \"base_price\": 32.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 22182, \"category\": \"Women's Clothing\", \"sku\": \"ZO0036600366\", \"taxless_price\": 32.99, \"unit_discount_amount\": 0, \"min_price\": 14.85, \"_id\": \"sold_product_576942_22182\", \"discount_amount\": 0, \"created_on\": \"2016-12-20T16:24:58+00:00\", \"product_name\": \"Jersey dress - black/red\", \"price\": 32.99, \"taxful_price\": 32.99, \"base_unit_price\": 32.99 }, { \"base_price\": 28.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Gnomehouse mom\", \"tax_amount\": 0, \"product_id\": 14230, \"category\": \"Women's Clothing\", \"sku\": \"ZO0234902349\", \"taxless_price\": 28.99, \"unit_discount_amount\": 0, \"min_price\": 13.05, \"_id\": \"sold_product_576942_14230\", \"discount_amount\": 0, \"created_on\": \"2016-12-20T16:24:58+00:00\", \"product_name\": \"Blouse - june bug\", \"price\": 28.99, \"taxful_price\": 28.99, \"base_unit_price\": 28.99 }], \"sku\": [ \"ZO0036600366\", \"ZO0234902349\"], \"taxful_total_price\": 61.98, \"taxless_total_price\": 61.98, \"total_quantity\": 2, \"total_unique_products\": 2, \"type\": \"order\", \"user\": \"gwen\", \"geoip\": { \"country_iso_code\": \"US\", \"location\": { \"lon\": -118.2, \"lat\": 34.1 }, \"region_name\": \"California\", \"continent_name\": \"North America\", \"city_name\": \"Los Angeles\" }, \"event\": { \"dataset\": \"sample_ecommerce\" } } },...] }, \"status\": 200 }] }",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/nodes-apis/index/",
    "title": "Nodes APIs",
    "content": "The nodes API makes it possible to retrieve information about individual nodes within your cluster.\nNode filters\nUse the &lt;node-filters&gt; parameter to filter the target set of nodes in the API response. Parameter Type Description &lt;node-filters&gt; String\nA comma-separated list of resolution mechanisms that OpenSearch uses to identify cluster nodes. Node filters support several node resolution mechanisms:\nPredefined constants: _local, _cluster_manager, or _all.\nAn exact match for nodeID A simple case-sensitive wildcard pattern matching for node-name, host-name, or host-IP-address.\nNode roles where the &lt;bool&gt; value is set either to true or false: cluster_manager:&lt;bool&gt; refers to all cluster manager-eligible nodes. data:&lt;bool&gt; refers to all data nodes. ingest:&lt;bool&gt; refers to all ingest nodes. voting_only:&lt;bool&gt; refers to all voting-only nodes. ml:&lt;bool&gt; refers to all machine learning (ML) nodes. coordinating_only:&lt;bool&gt; refers to all coordinating-only nodes.\nA simple case-sensitive wildcard pattern matching for node attributes: &lt;node attribute*&gt;:&lt;attribute value*&gt;. The wildcard matching pattern can be used in both the key and value at the same time.\nResolution mechanisms are applied sequentially in the order specified by the client. Each mechanism specification can either add or remove nodes.\nTo get statistics from the elected cluster manager node only, use the following query: GET /_nodes/_cluster_manager/stats copy To get statistics from nodes that are data-only nodes, use the following query: GET /_nodes/data: true /stats copy Order of resolution mechanisms\nThe order of resolution mechanisms is applied sequentially, and each can add or remove nodes. The following examples yield different results.\nTo get statistics from all the nodes except the cluster manager node, use the following query: GET /_nodes/_all,cluster_manager: false /stats copy However, if you switch the resolution mechanisms, the result will include all the cluster nodes, including the cluster manager node: GET /_nodes/cluster_manager: false,_all/stats copy",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/nodes-apis/nodes-hot-threads/",
    "title": "Nodes hot threads",
    "content": "The nodes hot threads endpoint provides information about busy JVM threads for selected cluster nodes. It provides a unique view of the of activity each node.\nExample GET /_nodes/hot_threads copy Path and HTTP methods GET /_nodes/hot_threads GET /_nodes/&lt;nodeId&gt;/hot_threads Path parameters\nYou can include the following optional path parameter in your request. Parameter Type Description nodeId\nString\nA comma-separated list of node IDs used to filter results. Supports node filters. Defaults to _all. Query parameters\nYou can include the following query parameters in your request. All query parameters are optional. Parameter Type Description snapshots\nInteger\nThe number of samples of thread stacktraces. Defaults to 10.\ninterval\nTime\nThe interval between consecutive samples. Defaults to 500ms.\nthreads\nInteger\nThe number of the busiest threads to return information about. Defaults to 3.\nignore_idle_threads\nBoolean\nDon’t show threads that are in known idle states, such as waiting on a socket select or pulling from an empty task queue. Defaults to true.\ntype\nString\nSupported thread types are cpu, wait, or block. Defaults to cpu.\ntimeout\nTime\nSets the time limit for node response. Default value is 30s. Example request GET /_nodes/hot_threads copy Example response::: { opensearch }{ F-ByTQzVQ3GQeYzQJArJGQ }{ GxbcLdCATPWggOuQHJAoCw }{ 127.0.0.1 }{ 127.0.0.1:9300 }{ dimr }{ shard_indexing_pressure_enabled = true } Hot threads at 2022-09-29T19:46:44.533Z, interval = 500ms, busiestThreads = 3, ignoreIdleThreads = true:\n0.1% ( 455.5micros out of 500ms) cpu usage by thread 'ScheduledMetricCollectorsExecutor' 10/10 snapshots sharing following 2 elements\njava.base@17.0.4/java.lang.Thread.sleep ( Native Method) org.opensearch.performanceanalyzer.collectors.ScheduledMetricCollectorsExecutor.run ( ScheduledMetricCollectorsExecutor.java:100) Response\nUnlike the majority of OpenSearch API responses, this response is in a text format.\nIt consists of one section per each cluster node included in the response.\nEach section starts with a single line containing the following segments: Line segment Description:::&nbsp; Line start (a distinct visual symbol). {global-eu-35} Node name. {uFPbKLDOTlOmdnwUlKW8sw} NodeId. {OAM8OT5CQAyasWuIDeVyUA} EphemeralId. {global-eu-35.local} Host name. {[gdv2:a284:2acv:5fa6:0:3a2:7260:74cf]:9300} Host address. {dimr} Node roles (d=data, i=ingest, m=cluster manager, r=remote cluster client). {zone=west-a2, shard_indexing_pressure_enabled=true} Node attributes. Then information about threads of the selected type is provided.::: { global-eu-35 }{ uFPbKLDOTlOmdnwUlKW8sw }{ OAM8OT5CQAyasWuIDeVyUA }{ global-eu-35.local }{[ gdv2:a284:2acv:5fa6:0:3a2:7260:74cf]:9300 }{ dimr }{ zone = west-a2, shard_indexing_pressure_enabled = true } Hot threads at 2022-04-01T15:15:27.658Z, interval = 500ms, busiestThreads = 3, ignoreIdleThreads = true:\n0.1% ( 645micros out of 500ms) cpu usage by thread 'opensearch[global-eu-35][transport_worker][T#7]' 4/10 snapshots sharing following 3 elements\nio.netty.util.concurrent.SingleThreadEventExecutor $4.run ( SingleThreadEventExecutor.java:986) io.netty.util.internal.ThreadExecutorMap $2.run ( ThreadExecutorMap.java:74) java.base@11.0.14.1/java.lang.Thread.run ( Thread.java:829)::: { global-eu-62 }{ 4knOxAdERlOB19zLQIT1bQ }{ HJuZs2HiQ_-8Elj0Fvi_1g }{ global-eu-62.local }{[ gdv2:a284:2acv:5fa6:0:3a2:bba6:fe3f]:9300 }{ dimr }{ zone = west-a2, shard_indexing_pressure_enabled = true } Hot threads at 2022-04-01T15:15:27.659Z, interval = 500ms, busiestThreads = 3, ignoreIdleThreads = true:\n18.7% ( 93.4ms out of 500ms) cpu usage by thread 'opensearch[global-eu-62][transport_worker][T#3]' 6/10 snapshots sharing following 3 elements\nio.netty.util.concurrent.SingleThreadEventExecutor $4.run ( SingleThreadEventExecutor.java:986) io.netty.util.internal.ThreadExecutorMap $2.run ( ThreadExecutorMap.java:74) java.base@11.0.14.1/java.lang.Thread.run ( Thread.java:829)::: { global-eu-44 }{ 8WW3hrkcTwGvgah_L8D_jw }{ Sok7spHISFyol0jFV6i0kw }{ global-eu-44.local }{[ gdv2:a284:2acv:5fa6:0:3a2:9120:e79e]:9300 }{ dimr }{ zone = west-a2, shard_indexing_pressure_enabled = true } Hot threads at 2022-04-01T15:15:27.659Z, interval = 500ms, busiestThreads = 3, ignoreIdleThreads = true:\n42.6% ( 212.7ms out of 500ms) cpu usage by thread 'opensearch[global-eu-44][write][T#5]' 2/10 snapshots sharing following 43 elements\njava.base@11.0.14.1/sun.nio.ch.IOUtil.write1 ( Native Method) java.base@11.0.14.1/sun.nio.ch.EPollSelectorImpl.wakeup ( EPollSelectorImpl.java:254) io.netty.channel.nio.NioEventLoop.wakeup ( NioEventLoop.java:787) io.netty.util.concurrent.SingleThreadEventExecutor.execute ( SingleThreadEventExecutor.java:846) io.netty.util.concurrent.SingleThreadEventExecutor.execute ( SingleThreadEventExecutor.java:815) io.netty.channel.AbstractChannelHandlerContext.safeExecute ( AbstractChannelHandlerContext.java:989) io.netty.channel.AbstractChannelHandlerContext.write ( AbstractChannelHandlerContext.java:796) io.netty.channel.AbstractChannelHandlerContext.writeAndFlush ( AbstractChannelHandlerContext.java:758) io.netty.channel.DefaultChannelPipeline.writeAndFlush ( DefaultChannelPipeline.java:1020) io.netty.channel.AbstractChannel.writeAndFlush ( AbstractChannel.java:311) org.opensearch.transport.netty4.Netty4TcpChannel.sendMessage ( Netty4TcpChannel.java:159) app//org.opensearch.transport.OutboundHan... Required permissions\nIf you use the Security plugin, make sure you set the following permissions: cluster:monitor/nodes/hot_threads.",
    "ancestors": [
      "API reference",
      "Nodes APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/nodes-apis/nodes-info/",
    "title": "Nodes info",
    "content": "The nodes info API represents mostly static information about your cluster’s nodes, including but not limited to:\nHost system information\nJVM\nProcessor Type\nNode settings\nThread pools settings\nInstalled plugins\nExample\nTo get information about all nodes in a cluster, use the following query: GET /_nodes copy To get thread pool information about the cluster manager node only, use the following query: GET /_nodes/master: true /thread_pool copy Path and HTTP methods GET /_nodes\nGET /_nodes/&lt;nodeId&gt;\nGET /_nodes/&lt;metrics&gt;\nGET /_nodes/&lt;nodeId&gt;/&lt;metrics&gt; # or full path equivalent GET /_nodes/&lt;nodeId&gt;/info/&lt;metrics&gt; Path parameters\nThe following table lists the available path parameters. All path parameters are optional. Parameter Type Description nodeId\nString\nA comma-separated list of nodeIds used to filter results. Supports node filters. Defaults to _all.\nmetrics\nString\nA comma-separated list of metric groups that will be included in the response. For example, jvm,thread_pool. Defaults to all metrics. The following table lists all available metric groups. Metric Description settings\nA node’s settings. This is a combination of the default settings, custom settings from the configuration file, and dynamically updated settings.\nos\nStatic information about the host OS, including version, processor architecture, and available/allocated processors.\nprocess\nContains the process ID.\njvm\nDetailed static information about the running JVM, including arguments.\nthread_pool\nConfigured options for all individual thread pools.\ntransport\nMostly static information about the transport layer.\nhttp\nMostly static information about the HTTP layer.\nplugins\nInformation about installed plugins and modules.\ningest\nInformation about ingest pipelines and available ingest processors.\naggregations\nInformation about available aggregations.\nindices\nStatic index settings configured at the node level. Query parameters\nYou can include the following query parameters in your request. All query parameters are optional. Parameter Type Description flat_settings\nBoolean\nSpecifies whether to return the settings object of the response in flat format. Default is false.\ntimeout\nTime\nSets the time limit for node response. Default value is 30s. Example request\nThe following query requests the process and transport metrics from the cluster manager node: GET /_nodes/cluster_manager: true /process,transport copy Example response\nThe response contains the metric groups specified in the &lt;metrics&gt; request parameter (in this case, process and transport): { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"opensearch\", \"nodes\": { \"VC0d4RgbTM6kLDwuud2XZQ\": { \"name\": \"node-m1-23\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1\", \"version\": \"1.3.1\", \"build_type\": \"tar\", \"build_hash\": \"c4c0672877bf0f787ca857c7c37b775967f93d81\", \"roles\": [ \"data\", \"ingest\", \"master\", \"remote_cluster_client\"], \"attributes\": { \"shard_indexing_pressure_enabled\": \"true\" }, \"process\": { \"refresh_interval_in_millis\": 1000, \"id\": 44584, \"mlockall\": false }, \"transport\": { \"bound_address\": [ \"[::1]:9300\", \"127.0.0.1:9300\"], \"publish_address\": \"127.0.0.1:9300\", \"profiles\": { } } } } } Response fields\nThe response contains the basic node identification and build info for every node matching the &lt;nodeId&gt; request parameter. The following table lists the response fields. Field Description name\nThe node’s name.\ntransport_address\nThe node’s transport address.\nhost\nThe node’s host address.\nip\nThe node’s host IP address.\nversion\nThe node’s OpenSearch version.\nbuild_type\nThe node’s build type, like rpm, docker, tar, etc.\nbuild_hash\nThe git commit hash of the build.\ntotal_indexing_buffer\nThe maximum heap size in bytes used to hold newly indexed documents. Once this heap size is exceeded, the documents are written to disk.\nroles\nThe list of the node’s roles.\nattributes\nThe node’s attributes.\nos\nInformation about the OS, including name, version, architecture, refresh interval, and the number of available and allocated processors.\nprocess\nInformation about the currently running process, including PID, refresh interval, and mlockall, which specifies whether the process address space has been successfully locked in memory.\njvm\nInformation about the JVM, including PID, version, memory information, garbage collector information, and arguments.\nthread_pool\nInformation about the thread pool.\ntransport\nInformation about the transport address, including bound address, publish address, and profiles.\nhttp\nInformation about the HTTP address, including bound address, publish address, and maximum content length, in bytes.\nplugins\nInformation about the installed plugins, including name, version, OpenSearch version, Java version, description, class name, custom folder name, a list of extended plugins, and has_native_controller, which specifies whether the plugin has a native controller process.\nmodules\nInformation about the modules, including name, version, OpenSearch version, Java version, description, class name, custom folder name, a list of extended plugins, and has_native_controller, which specifies whether the plugin has a native controller process. Modules are different from plugins because modules are loaded into OpenSearch automatically, while plugins have to be installed manually.\ningest\nInformation about ingest pipelines and processors.\naggregations\nInformation about the available aggregation types. Required permissions\nIf you use the Security plugin, make sure you have the appropriate permissions: cluster:monitor/nodes/info.",
    "ancestors": [
      "API reference",
      "Nodes APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/nodes-apis/nodes-reload-secure/",
    "title": "Nodes reload secure settings",
    "content": "The nodes reload secure settings endpoint allows you to change secure settings on a node and reload the secure settings without restarting the node.\nPath and HTTP methods POST _nodes/reload_secure_settings\nPOST _nodes/&lt;nodeId&gt;/reload_secure_settings Path parameter\nYou can include the following optional path parameter in your request. Parameter Type Description nodeId\nString\nA comma-separated list of nodeIds used to filter results. Supports node filters. Defaults to _all. Request fields\nThe request may include an optional object containing the password for the OpenSearch keystore. { \"secure_settings_password\": \"keystore_password\" } Example request\nThe following is an example API request: POST _nodes/reload_secure_settings copy Example response\nThe following is an example response: { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"opensearch-cluster\", \"nodes\": { \"t7uqHu4SSuWObK3ElkCRfw\": { \"name\": \"opensearch-node1\" } } } Required permissions\nIf you use the Security plugin, make sure you set the following permissions: cluster:manage/nodes.",
    "ancestors": [
      "API reference",
      "Nodes APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/nodes-apis/nodes-stats/",
    "title": "Nodes stats",
    "content": "The nodes stats API returns statistics about your cluster.\nPath and HTTP methods GET /_nodes/stats GET /_nodes/&lt;node_id&gt;/stats GET /_nodes/stats/&lt;metric&gt; GET /_nodes/&lt;node_id&gt;/stats/&lt;metric&gt; GET /_nodes/stats/&lt;metric&gt;/&lt;index_metric&gt; GET /_nodes/&lt;node_id&gt;/stats/&lt;metric&gt;/&lt;index_metric&gt; Path parameters\nThe following table lists the available path parameters. All path parameters are optional. Parameter Type Description nodeId\nString\nA comma-separated list of nodeIds used to filter results. Supports node filters. Defaults to _all.\nmetric\nString\nA comma-separated list of metric groups that will be included in the response. For example, jvm,fs. See the list of all metrics below. Defaults to all metrics.\nindex_metric\nString\nA comma-separated list of index metric groups that will be included in the response. For example, docs,store. See the list of all index metrics below. Defaults to all index metrics. The following table lists all available metric groups. Metric Description indices\nIndex statistics, such as size, document count, and search, index, and delete times for documents.\nos\nStatistics about the host OS, including load, memory, and swapping.\nprocess\nStatistics about processes, including their memory consumption, open file descriptors, and CPU usage.\njvm\nStatistics about the JVM, including memory pool, buffer pool, and garbage collection, and the number of loaded classes.\nfs\nFile system statistics, such as read/write statistics, data path, and free disk space.\ntransport\nTransport layer statistics about send/receive in cluster communication.\nhttp\nStatistics about the HTTP layer.\nbreaker\nStatistics about the field data circuit breakers.\nscript\nStatistics about scripts, such as compilations and cache evictions.\ndiscovery\nStatistics about cluster states.\ningest\nStatistics about ingest pipelines.\nadaptive_selection\nStatistics about adaptive replica selection, which selects an eligible node using shard allocation awareness.\nscript_cache\nStatistics about script cache.\nindexing_pressure\nStatistics about the node’s indexing pressure.\nshard_indexing_pressure\nStatistics about shard indexing pressure. To filter the information returned for the indices metric, you can use specific index_metric values. You can use these only when you use the following query types: GET _nodes/stats/ GET _nodes/stats/_all GET _nodes/stats/indices The following index metrics are supported:\ndocs\nstore\nindexing\nget\nsearch\nmerge\nrefresh\nflush\nwarmer\nquery_cache\nfielddata\ncompletion\nsegments\ntranslog\nrequest_cache\nFor example, the following query requests statistics for docs and search: GET _nodes/stats/indices/docs,search copy Query parameters\nThe following table lists the available query parameters. All query parameters are optional. Parameter Type Description completion_fields\nString\nThe fields to include in completion statistics. Supports comma-separated lists and wildcard expressions.\nfielddata_fields\nString\nThe fields to include in fielddata statistics. Supports comma-separated lists and wildcard expressions.\nfields\nString\nThe fields to include. Supports comma-separated lists and wildcard expressions.\ngroups\nString\nA comma-separated list of search groups to include in the search statistics.\nlevel\nString\nSpecifies whether statistics are aggregated at the cluster, index, or shard level. Valid values are indices, node, and shard.\ntimeout\nTime\nSets the time limit for node response. Default is 30s.\ninclude_segment_file_sizes\nBoolean\nIf segment statistics are requested, this field specifies to return the aggregated disk usage of every Lucene index file. Default is false. Example request GET _nodes/stats/ copy Example response { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"docker-cluster\", \"nodes\": { \"F-ByTQzVQ3GQeYzQJArJGQ\": { \"timestamp\": 1664484195257, \"name\": \"opensearch\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"cluster_manager\", \"data\", \"ingest\", \"remote_cluster_client\"], \"attributes\": { \"shard_indexing_pressure_enabled\": \"true\" }, \"indices\": { \"docs\": { \"count\": 13160, \"deleted\": 12 }, \"store\": { \"size_in_bytes\": 6263461, \"reserved_in_bytes\": 0 }, \"indexing\": { \"index_total\": 0, \"index_time_in_millis\": 0, \"index_current\": 0, \"index_failed\": 0, \"delete_total\": 204, \"delete_time_in_millis\": 427, \"delete_current\": 0, \"noop_update_total\": 0, \"is_throttled\": false, \"throttle_time_in_millis\": 0 }, \"get\": { \"total\": 4, \"time_in_millis\": 18, \"exists_total\": 4, \"exists_time_in_millis\": 18, \"missing_total\": 0, \"missing_time_in_millis\": 0, \"current\": 0 }, \"search\": { \"open_contexts\": 4, \"query_total\": 194, \"query_time_in_millis\": 467, \"query_current\": 0, \"fetch_total\": 194, \"fetch_time_in_millis\": 143, \"fetch_current\": 0, \"scroll_total\": 0, \"scroll_time_in_millis\": 0, \"scroll_current\": 0, \"point_in_time_total\": 0, \"point_in_time_time_in_millis\": 0, \"point_in_time_current\": 0, \"suggest_total\": 0, \"suggest_time_in_millis\": 0, \"suggest_current\": 0 }, \"merges\": { \"current\": 0, \"current_docs\": 0, \"current_size_in_bytes\": 0, \"total\": 1, \"total_time_in_millis\": 5, \"total_docs\": 12, \"total_size_in_bytes\": 3967, \"total_stopped_time_in_millis\": 0, \"total_throttled_time_in_millis\": 0, \"total_auto_throttle_in_bytes\": 251658240 }, \"refresh\": { \"total\": 74, \"total_time_in_millis\": 201, \"external_total\": 57, \"external_total_time_in_millis\": 314, \"listeners\": 0 }, \"flush\": { \"total\": 28, \"periodic\": 28, \"total_time_in_millis\": 1261 }, \"warmer\": { \"current\": 0, \"total\": 45, \"total_time_in_millis\": 99 }, \"query_cache\": { \"memory_size_in_bytes\": 0, \"total_count\": 0, \"hit_count\": 0, \"miss_count\": 0, \"cache_size\": 0, \"cache_count\": 0, \"evictions\": 0 }, \"fielddata\": { \"memory_size_in_bytes\": 356, \"evictions\": 0 }, \"completion\": { \"size_in_bytes\": 0, \"fields\": { } }, \"segments\": { \"count\": 17, \"memory_in_bytes\": 0, \"terms_memory_in_bytes\": 0, \"stored_fields_memory_in_bytes\": 0, \"term_vectors_memory_in_bytes\": 0, \"norms_memory_in_bytes\": 0, \"points_memory_in_bytes\": 0, \"doc_values_memory_in_bytes\": 0, \"index_writer_memory_in_bytes\": 0, \"version_map_memory_in_bytes\": 0, \"fixed_bit_set_memory_in_bytes\": 288, \"max_unsafe_auto_id_timestamp\": -1, \"file_sizes\": { } }, \"translog\": { \"operations\": 12, \"size_in_bytes\": 1452, \"uncommitted_operations\": 12, \"uncommitted_size_in_bytes\": 1452, \"earliest_last_modified_age\": 164160 }, \"request_cache\": { \"memory_size_in_bytes\": 1649, \"evictions\": 0, \"hit_count\": 0, \"miss_count\": 18 }, \"recovery\": { \"current_as_source\": 0, \"current_as_target\": 0, \"throttle_time_in_millis\": 0 } }, \"os\": { \"timestamp\": 1664484195263, \"cpu\": { \"percent\": 0, \"load_average\": { \"1m\": 0.0, \"5m\": 0.0, \"15m\": 0.0 } }, \"mem\": { \"total_in_bytes\": 13137076224, \"free_in_bytes\": 9265442816, \"used_in_bytes\": 3871633408, \"free_percent\": 71, \"used_percent\": 29 }, \"swap\": { \"total_in_bytes\": 4294967296, \"free_in_bytes\": 4294967296, \"used_in_bytes\": 0 }, \"cgroup\": { \"cpuacct\": { \"control_group\": \"/\", \"usage_nanos\": 338710071600 }, \"cpu\": { \"control_group\": \"/\", \"cfs_period_micros\": 100000, \"cfs_quota_micros\": -1, \"stat\": { \"number_of_elapsed_periods\": 0, \"number_of_times_throttled\": 0, \"time_throttled_nanos\": 0 } }, \"memory\": { \"control_group\": \"/\", \"limit_in_bytes\": \"9223372036854771712\", \"usage_in_bytes\": \"1432346624\" } } }, \"process\": { \"timestamp\": 1664484195263, \"open_file_descriptors\": 556, \"max_file_descriptors\": 65536, \"cpu\": { \"percent\": 0, \"total_in_millis\": 170870 }, \"mem\": { \"total_virtual_in_bytes\": 6563344384 } }, \"jvm\": { \"timestamp\": 1664484195264, \"uptime_in_millis\": 21232111, \"mem\": { \"heap_used_in_bytes\": 308650480, \"heap_used_percent\": 57, \"heap_committed_in_bytes\": 536870912, \"heap_max_in_bytes\": 536870912, \"non_heap_used_in_bytes\": 147657128, \"non_heap_committed_in_bytes\": 152502272, \"pools\": { \"young\": { \"used_in_bytes\": 223346688, \"max_in_bytes\": 0, \"peak_used_in_bytes\": 318767104, \"peak_max_in_bytes\": 0, \"last_gc_stats\": { \"used_in_bytes\": 0, \"max_in_bytes\": 0, \"usage_percent\": -1 } }, \"old\": { \"used_in_bytes\": 67068928, \"max_in_bytes\": 536870912, \"peak_used_in_bytes\": 67068928, \"peak_max_in_bytes\": 536870912, \"last_gc_stats\": { \"used_in_bytes\": 34655744, \"max_in_bytes\": 536870912, \"usage_percent\": 6 } }, \"survivor\": { \"used_in_bytes\": 18234864, \"max_in_bytes\": 0, \"peak_used_in_bytes\": 32721280, \"peak_max_in_bytes\": 0, \"last_gc_stats\": { \"used_in_bytes\": 18234864, \"max_in_bytes\": 0, \"usage_percent\": -1 } } } }, \"threads\": { \"count\": 80, \"peak_count\": 80 }, \"gc\": { \"collectors\": { \"young\": { \"collection_count\": 18, \"collection_time_in_millis\": 199 }, \"old\": { \"collection_count\": 0, \"collection_time_in_millis\": 0 } } }, \"buffer_pools\": { \"mapped\": { \"count\": 23, \"used_in_bytes\": 6232113, \"total_capacity_in_bytes\": 6232113 }, \"direct\": { \"count\": 63, \"used_in_bytes\": 9050069, \"total_capacity_in_bytes\": 9050068 }, \"mapped - 'non-volatile memory'\": { \"count\": 0, \"used_in_bytes\": 0, \"total_capacity_in_bytes\": 0 } }, \"classes\": { \"current_loaded_count\": 20693, \"total_loaded_count\": 20693, \"total_unloaded_count\": 0 } }, \"thread_pool\": { \"OPENSEARCH_ML_TASK_THREAD_POOL\": { \"threads\": 0, \"queue\": 0, \"active\": 0, \"rejected\": 0, \"largest\": 0, \"completed\": 0 }, \"ad-batch-task-threadpool\": { \"threads\": 0, \"queue\": 0, \"active\": 0, \"rejected\": 0, \"largest\": 0, \"completed\": 0 },... }, \"fs\": { \"timestamp\": 1664484195264, \"total\": { \"total_in_bytes\": 269490393088, \"free_in_bytes\": 261251477504, \"available_in_bytes\": 247490805760 }, \"data\": [ { \"path\": \"/usr/share/opensearch/data/nodes/0\", \"mount\": \"/ (overlay)\", \"type\": \"overlay\", \"total_in_bytes\": 269490393088, \"free_in_bytes\": 261251477504, \"available_in_bytes\": 247490805760 }], \"io_stats\": { } }, \"transport\": { \"server_open\": 0, \"total_outbound_connections\": 0, \"rx_count\": 0, \"rx_size_in_bytes\": 0, \"tx_count\": 0, \"tx_size_in_bytes\": 0 }, \"http\": { \"current_open\": 5, \"total_opened\": 1108 }, \"breakers\": { \"request\": { \"limit_size_in_bytes\": 322122547, \"limit_size\": \"307.1mb\", \"estimated_size_in_bytes\": 0, \"estimated_size\": \"0b\", \"overhead\": 1.0, \"tripped\": 0 }, \"fielddata\": { \"limit_size_in_bytes\": 214748364, \"limit_size\": \"204.7mb\", \"estimated_size_in_bytes\": 356, \"estimated_size\": \"356b\", \"overhead\": 1.03, \"tripped\": 0 }, \"in_flight_requests\": { \"limit_size_in_bytes\": 536870912, \"limit_size\": \"512mb\", \"estimated_size_in_bytes\": 0, \"estimated_size\": \"0b\", \"overhead\": 2.0, \"tripped\": 0 }, \"parent\": { \"limit_size_in_bytes\": 510027366, \"limit_size\": \"486.3mb\", \"estimated_size_in_bytes\": 308650480, \"estimated_size\": \"294.3mb\", \"overhead\": 1.0, \"tripped\": 0 } }, \"script\": { \"compilations\": 0, \"cache_evictions\": 0, \"compilation_limit_triggered\": 0 }, \"discovery\": { \"cluster_state_queue\": { \"total\": 0, \"pending\": 0, \"committed\": 0 }, \"published_cluster_states\": { \"full_states\": 2, \"incompatible_diffs\": 0, \"compatible_diffs\": 10 } }, \"ingest\": { \"total\": { \"count\": 0, \"time_in_millis\": 0, \"current\": 0, \"failed\": 0 }, \"pipelines\": { } }, \"adaptive_selection\": { \"F-ByTQzVQ3GQeYzQJArJGQ\": { \"outgoing_searches\": 0, \"avg_queue_size\": 0, \"avg_service_time_ns\": 501024, \"avg_response_time_ns\": 794105, \"rank\": \"0.8\" } }, \"script_cache\": { \"sum\": { \"compilations\": 0, \"cache_evictions\": 0, \"compilation_limit_triggered\": 0 }, \"contexts\": [ { \"context\": \"aggregation_selector\", \"compilations\": 0, \"cache_evictions\": 0, \"compilation_limit_triggered\": 0 }, { \"context\": \"aggs\", \"compilations\": 0, \"cache_evictions\": 0, \"compilation_limit_triggered\": 0 },...] }, \"indexing_pressure\": { \"memory\": { \"current\": { \"combined_coordinating_and_primary_in_bytes\": 0, \"coordinating_in_bytes\": 0, \"primary_in_bytes\": 0, \"replica_in_bytes\": 0, \"all_in_bytes\": 0 }, \"total\": { \"combined_coordinating_and_primary_in_bytes\": 40256, \"coordinating_in_bytes\": 40256, \"primary_in_bytes\": 45016, \"replica_in_bytes\": 0, \"all_in_bytes\": 40256, \"coordinating_rejections\": 0, \"primary_rejections\": 0, \"replica_rejections\": 0 }, \"limit_in_bytes\": 53687091 } }, \"shard_indexing_pressure\": { \"stats\": { }, \"total_rejections_breakup_shadow_mode\": { \"node_limits\": 0, \"no_successful_request_limits\": 0, \"throughput_degradation_limits\": 0 }, \"enabled\": false, \"enforced\": false } } } } Response fields\nThe following table lists all response fields. Field Data type Description _nodes\nObject\nStatistics about the nodes that are returned.\n_nodes.total\nInteger\nThe total number of nodes for this request.\n_nodes.successful\nInteger\nThe number of nodes for which the request was successful.\n_nodes.failed\nInteger\nThe number of nodes for which the request failed. If there are nodes for which the request failed, the failure message is included.\ncluster_name\nString\nThe name of the cluster. nodes Object\nStatistics for the nodes included in this request. nodes The nodes object contains all nodes that are returned by the request, along with their IDs. Each node has the following properties. Field Data type Description timestamp\nInteger\nThe time the nodes statistics were collected, in milliseconds since the epoch.\nname\nString\nThe name of the node.\ntransport_address\nIP address\nThe host and port of the transport layer that is used by nodes in a cluster to communicate internally.\nhost\nIP address\nThe network host of the node.\nip\nIP address\nThe IP address and port of the node.\nroles\nArray\nThe roles of the node (for example, cluster_manager, data, or ingest).\nattributes\nObject\nThe attributes of the node (for example, shard_indexing_pressure_enabled). indices Object\nIndex statistics for each index that has shards on the node. os Object\nStatistics about the OS for the node. process Object\nProcess statistics for the node. jvm Object\nStatistics about the JVM for the node. thread_pool Object\nStatistics about each thread pool for the node. fs Object\nStatistics about the file stores for the node. transport Object\nTransport statistics for the node.\nhttp\nObject\nHTTP statistics for the node.\nhttp.current_open\nInteger\nThe number of currently open HTTP connections for the node.\nhttp.total_opened\nInteger\nThe total number of HTTP connections the node has opened since it started. breakers Object\nStatistics about the circuit breakers for the node. script Object\nScript statistics for the node. script_cache Object\nScript cache statistics for the node. discovery Object\nNode discovery statistics for the node. ingest Object\nIngest statistics for the node. adaptive_selection Object\nStatistics about adaptive selections for the node. indexing_pressure Object\nStatistics related to the node’s indexing pressure. shard_indexing_pressure Object\nStatistics related to indexing pressure at the shard level. search_backpressure Object\nStatistics related to search backpressure. indices The indices object contains the index statistics for each index with shards on this node. Each index has the following properties. Field Field type Description docs\nObject\nDocument statistics for all primary shards that exist on the node.\ndocs.count\nInteger\nThe number of documents reported by Lucene. Excludes deleted documents and recently indexed documents that are not yet assigned to a segment. Nested documents are counted separately.\ndocs.deleted\nInteger\nThe number of deleted documents reported by Lucene. Excludes recent deletion operations that have not yet affect the segment.\nstore\nObject\nStatistics about the shard sizes of the shards on the node.\nstore.size_in_bytes\nInteger\nTotal size of all shards on the node.\nstore.reserved_in_bytes\nInteger\nThe predicted number of bytes the shard store will grow to be because of activities such as restoring snapshots and peer recoveries.\nindexing\nObject\nStatistics about indexing operations for the node.\nindexing.index_total\nInteger\nThe total number of indexing operations on the node.\nindexing.index_time_in_millis\nInteger\nThe total time for all indexing operations, in milliseconds.\nindexing.index_current\nInteger\nThe number of indexing operations that are currently running.\nindexing.index_failed\nInteger\nThe number of indexing operations that have failed.\nindexing.delete_total\nInteger\nThe total number of deletions.\nindexing.delete_time_in_millis\nInteger\nThe total time for all deletion operations, in milliseconds.\nindexing.delete_current\nInteger\nThe number of deletion operations that are currently running.\nindexing.noop_update_total\nInteger\nThe total number of noop operations.\nindexing.is_throttled\nBoolean\nSpecifies whether any operations were throttled.\nindexing.throttle_time_in_millis\nInteger\nThe total time for throttling operations, in milliseconds.\nget\nObject\nStatistics about the get operations for the node.\nget.total\nInteger\nThe total number of get operations.\nget.time_in_millis\nInteger\nThe total time for all get operations, in milliseconds.\nget.exists_total\nInteger\nThe total number of successful get operations.\nget.exists_time_in_millis\nInteger\nThe total time for all successful get operations, in milliseconds.\nget.missing_total\nInteger\nThe number of failed get operations.\nget.missing_time_in_millis\nInteger\nThe total time for all failed get operations, in milliseconds.\nget.current\nInteger\nThe number of get operations that are currently running.\nsearch\nObject\nStatistics about the search operations for the node.\nsearch.point_in_time_total\nInteger\nThe total number of Point in Time contexts that have been created (completed and active) since the node last restarted.\nsearch.point_in_time_time_in_millis\nInteger\nThe amount of time that Point in Time contexts have been held open since the node last restarted, in milliseconds.\nsearch.point_in_time_current\nInteger\nThe number of Point in Time contexts currently open.\nsearch.open_contexts\nInteger\nThe number of open search contexts.\nsearch.query_total\nInteger\nThe total number of query operations.\nsearch.query_time_in_millis\nInteger\nThe total time for all query operations, in milliseconds.\nsearch.query_current\nInteger\nThe number of query operations that are currently running.\nsearch.fetch_total\nInteger\nThe total number of fetch operations.\nsearch.fetch_time_in_millis\nInteger\nThe total time for all fetch operations, in milliseconds.\nsearch.fetch_current\nInteger\nThe number of fetch operations that are currently running.\nsearch.scroll_total\nInteger\nThe total number of scroll operations.\nsearch.scroll_time_in_millis\nInteger\nThe total time for all scroll operations, in milliseconds.\nsearch.scroll_current\nInteger\nThe number of scroll operations that are currently running.\nsearch.suggest_total\nInteger\nThe total number of suggest operations.\nsearch.suggest_time_in_millis\nInteger\nThe total time for all suggest operations, in milliseconds.\nsearch.suggest_current\nInteger\nThe number of suggest operations that are currently running.\nmerges\nObject\nStatistics about merge operations for the node.\nmerges.current\nInteger\nThe number of merge operations that are currently running.\nmerges.current_docs\nInteger\nThe number of document merges that are currently running.\nmerges.current_size_in_bytes\nInteger\nThe memory size, in bytes, that is used to perform current merge operations.\nmerges.total\nInteger\nThe total number of merge operations.\nmerges.total_time_in_millis\nInteger\nThe total time for merges, in milliseconds.\nmerges.total_docs\nInteger\nThe total number of documents that have been merged.\nmerges.total_size_in_bytes\nInteger\nThe total size of all merged documents, in bytes.\nmerges.total_stopped_time_in_millis\nInteger\nThe total time spent on stopping merge operations, in milliseconds.\nmerges.total_throttled_time_in_millis\nInteger\nThe total time spent on throttling merge operations, in milliseconds.\nmerges.total_auto_throttle_in_bytes\nInteger\nThe total size of automatically throttled merge operations, in bytes.\nrefresh\nObject\nStatistics about refresh operations for the node.\nrefresh.total\nInteger\nThe total number of refresh operations.\nrefresh.total_time_in_millis\nInteger\nThe total time for all refresh operations, in milliseconds.\nrefresh.external_total\nInteger\nThe total number of external refresh operations.\nrefresh.external_total_time_in_millis\nInteger\nThe total time for all external refresh operations, in milliseconds.\nrefresh.listeners\nInteger\nThe number of refresh listeners.\nflush\nObject\nStatistics about flush operations for the node.\nflush.total\nInteger\nThe total number of flush operations.\nflush.periodic\nInteger\nThe total number of periodic flush operations.\nflush.total_time_in_millis\nInteger\nThe total time for all flush operations, in milliseconds.\nwarmer\nObject\nStatistics about the index warming operations for the node.\nwarmer.current\nInteger\nThe number of current index warming operations.\nwarmer.total\nInteger\nThe total number of index warming operations.\nwarmer.total_time_in_millis\nInteger\nThe total time for all index warming operations, in milliseconds.\nquery_cache\nStatistics about query cache operations for the node.\n \nquery_cache.memory_size_in_bytes\nInteger\nThe amount of memory used for the query cache for all shards in the node.\nquery_cache.total_count\nInteger\nThe total number of hits, misses, and cached queries in the query cache.\nquery_cache.hit_count\nInteger\nThe total number of hits in the query cache.\nquery_cache.miss_count\nInteger\nThe total number of misses in the query cache.\nquery_cache.cache_size\nInteger\nThe size of the query cache, in bytes.\nquery_cache.cache_count\nInteger\nThe number of queries in the query cache.\nquery_cache.evictions\nInteger\nThe number of evictions in the query cache.\nfielddata\nObject\nStatistics about the field data cache for all shards in the node.\nfielddata.memory_size_in_bytes\nInteger\nThe total amount of memory used for the field data cache for all shards in the node.\nfielddata.evictions\nInteger\nThe number of evictions in the field data cache.\nfielddata.fields\nObject\nContains all field data fields.\ncompletion\nObject\nStatistics about completions for all shards in the node.\ncompletion.size_in_bytes\nInteger\nThe total amount of memory used for completion for all shards in the node, in bytes.\ncompletion.fields\nObject\nContains completion fields.\nsegments\nObject\nStatistics about segments for all shards in the node.\nsegments.count\nInteger\nThe total number of segments.\nsegments.memory_in_bytes\nInteger\nThe total amount of memory, in bytes.\nsegments.terms_memory_in_bytes\nInteger\nThe total amount of memory used for terms, in bytes.\nsegments.stored_fields_memory_in_bytes\nInteger\nThe total amount of memory used for stored fields, in bytes.\nsegments.term_vectors_memory_in_bytes\nInteger\nThe total amount of memory used for term vectors, in bytes.\nsegments.norms_memory_in_bytes\nInteger\nThe total amount of memory used for normalization factors, in bytes.\nsegments.points_memory_in_bytes\nInteger\nThe total amount of memory used for points, in bytes.\nsegments.doc_values_memory_in_bytes\nInteger\nThe total amount of memory used for doc values, in bytes.\nsegments.index_writer_memory_in_bytes\nInteger\nThe total amount of memory used by all index writers, in bytes.\nsegments.version_map_memory_in_bytes\nInteger\nThe total amount of memory used by all version maps, in bytes.\nsegments.fixed_bit_set_memory_in_bytes\nInteger\nThe total amount of memory used by fixed bit sets, in bytes. Fixed bit sets are used for nested objects and join fields.\nsegments.max_unsafe_auto_id_timestamp\nInteger\nThe timestamp for the most recently retired indexing request, in milliseconds since the epoch.\nsegments.file_sizes\nInteger\nStatistics about the size of the segment files.\ntranslog\nObject\nStatistics about transaction log operations for the node.\ntranslog.operations\nInteger\nThe number of translog operations.\ntranslog.size_in_bytes\nInteger\nThe size of the translog, in bytes.\ntranslog.uncommitted_operations\nInteger\nThe number of uncommitted translog operations.\ntranslog.uncommitted_size_in_bytes\nInteger\nThe size of uncommitted translog operations, in bytes.\ntranslog.earliest_last_modified_age\nInteger\nThe earliest last modified age for the translog.\nrequest_cache\nObject\nStatistics about the request cache for the node.\nrequest_cache.memory_size_in_bytes\nInteger\nThe memory size used by the request cache, in bytes.\nrequest_cache.evictions\nInteger\nThe number of request cache evictions.\nrequest_cache.hit_count\nInteger\nThe number of request cache hits.\nrequest_cache.miss_count\nInteger\nThe number of request cache misses.\nrecovery\nObject\nStatistics about recovery operations for the node.\nrecovery.current_as_source\nInteger\nThe number of recovery operations that have used an index shard as a source.\nrecovery.current_as_target\nInteger\nThe number of recovery operations that have used an index shard as a target.\nrecovery.throttle_time_in_millis\nInteger\nThe delay of recovery operations due to throttling, in milliseconds. os The os object has the OS statistics for the node and has the following properties. Field Field type Description timestamp\nInteger\nThe last refresh time for the OS statistics, in milliseconds since the epoch.\ncpu\nObject\nStatistics about the node’s CPU usage.\ncpu.percent\nInteger\nRecent CPU usage for the system.\ncpu.load_average\nObject\nStatistics about load averages for the system.\ncpu.load_average.1m\nFloat\nThe load average for the system for the time period of one minute.\ncpu.load_average.5m\nFloat\nThe load average for the system for the time period of five minutes.\ncpu.load_average.15m\nFloat\nThe load average for the system for the time period of 15 minutes.\ncpu.mem\nObject\nStatistics about memory usage for the node.\ncpu.mem.total_in_bytes\nInteger\nThe total amount of physical memory, in bytes.\ncpu.mem.free_in_bytes\nInteger\nThe total amount of free physical memory, in bytes.\ncpu.mem.used_in_bytes\nInteger\nThe total amount of used physical memory, in bytes.\ncpu.mem.free_percent\nInteger\nThe percentage of memory that is free.\ncpu.mem.used_percent\nInteger\nThe percentage of memory that is used.\ncpu.swap\nObject\nStatistics about swap space for the node.\ncpu.swap.total_in_bytes\nInteger\nThe total amount of swap space, in bytes.\ncpu.swap.free_in_bytes\nInteger\nThe total amount of free swap space, in bytes.\ncpu.swap.used_in_bytes\nInteger\nThe total amount of used swap space, in bytes.\ncpu.cgroup\nObject\nContains cgroup statistics for the node. Returned for Linux only.\ncpu.cgroup.cpuacct\nObject\nStatistics about the cpuacct control group for the node.\ncpu.cgroup.cpu\nObject\nStatistics about the CPU control group for the node.\ncpu.cgroup.memory\nObject\nStatistics about the memory control group for the node. process The process object contains process statistics for the node and has the following properties. Field Field type Description timestamp\nInteger\nThe last refresh time for the process statistics, in milliseconds since the epoch.\nopen_file_descriptors\nInteger\nThe number of opened file descriptors associated with the current process.\nmax_file_descriptors\nInteger\nThe maximum number of file descriptors for the system.\ncpu\nObject\nStatistics about the CPU for the node.\ncpu.percent\nInteger\nThe percentage of CPU usage for the process.\ncpu.total_in_millis\nInteger\nThe total CPU time used by the process on which the JVM is running, in milliseconds.\nmem\nObject\nStatistics about the memory for the node.\nmem.total_virtual_in_bytes\nInteger\nThe total amount of virtual memory that is guaranteed to be available to the process that is currently running, in bytes. jvm The jvm object contains statistics about the JVM for the node and has the following properties. Field Field type Description timestamp\nInteger\nThe last refresh time for the JVM statistics, in milliseconds since the epoch.\nuptime_in_millis\nInteger\nThe JVM uptime, in milliseconds.\nmem\nObject\nStatistics for the JVM memory usage on the node.\nmem.heap_used_in_bytes\nInteger\nThe amount of memory that is currently being used, in bytes.\nmem.heap_used_percent\nInteger\nThe percentage of memory that is currently used by the heap.\nmem.heap_committed_in_bytes\nInteger\nThe amount of memory available for use by the heap, in bytes.\nmem.heap_max_in_bytes\nInteger\nThe maximum amount of memory available for use by the heap, in bytes.\nmem.non_heap_used_in_bytes\nInteger\nThe amount of non-heap memory that is currently used, in bytes.\nmem.non_heap_committed_in_bytes\nInteger\nThe maximum amount of non-heap memory available for use, in bytes.\nmem.pools\nObject\nStatistics about heap memory usage for the node.\nmem.pools.young\nObject\nStatistics about the young generation heap memory usage for the node. Contains the amount of memory used, the maximum amount of memory available, and the peak amount of memory used.\nmem.pools.old\nObject\nStatistics about the old generation heap memory usage for the node. Contains the amount of memory used, the maximum amount of memory available, and the peak amount of memory used.\nmem.pools.survivor\nObject\nStatistics about the survivor space memory usage for the node. Contains the amount of memory used, the maximum amount of memory available, and the peak amount of memory used.\nthreads\nObject\nStatistics about the JVM thread usage for the node.\nthreads.count\nInteger\nThe number of threads that are currently active in the JVM.\nthreads.peak_count\nInteger\nThe maximum number of threads in the JVM.\ngc.collectors\nObject\nStatistics about the JVM garbage collectors for the node.\ngc.collectors.young\nInteger\nStatistics about JVM garbage collectors that collect young generation objects.\ngc.collectors.young.collection_count\nInteger\nThe number of garbage collectors that collect young generation objects.\ngc.collectors.young.collection_time_in_millis\nInteger\nThe total time spent on garbage collection of young generation objects, in milliseconds.\ngc.collectors.old\nInteger\nStatistics about JVM garbage collectors that collect old generation objects.\ngc.collectors.old.collection_count\nInteger\nThe number of garbage collectors that collect old generation objects.\ngc.collectors.old.collection_time_in_millis\nInteger\nThe total time spent on garbage collection of old generation objects, in milliseconds.\nbuffer_pools\nObject\nStatistics about the JVM buffer pools for the node.\nbuffer_pools.mapped\nObject\nStatistics about the mapped JVM buffer pools for the node.\nbuffer_pools.mapped.count\nInteger\nThe number of mapped buffer pools.\nbuffer_pools.mapped.used_in_bytes\nInteger\nThe amount of memory used by mapped buffer pools, in bytes.\nbuffer_pools.mapped.total_capacity_in_bytes\nInteger\nThe total capacity of the mapped buffer pools, in bytes.\nbuffer_pools.direct\nObject\nStatistics about the direct JVM buffer pools for the node.\nbuffer_pools.direct.count\nInteger\nThe number of direct buffer pools.\nbuffer_pools.direct.used_in_bytes\nInteger\nThe amount of memory used by direct buffer pools, in bytes.\nbuffer_pools.direct.total_capacity_in_bytes\nInteger\nThe total capacity of the direct buffer pools, in bytes.\nclasses\nObject\nStatistics about the classes loaded by the JVM for the node.\nclasses.current_loaded_count\nInteger\nThe number of classes currently loaded by the JVM.\nclasses.total_loaded_count\nInteger\nThe total number of classes loaded by the JVM since it started.\nclasses.total_unloaded_count\nInteger\nThe total number of classes unloaded by the JVM since it started. thread_pool The thread_pool object contains a list of all thread pools. Each thread pool is a nested object specified by its ID with the properties listed below. Field Field type Description threads\nInteger\nThe number of threads in the pool.\nqueue\nInteger\nThe number of threads in queue.\nactive\nInteger\nThe number of active threads in the pool.\nrejected\nInteger\nThe number of tasks that have been rejected.\nlargest\nInteger\nThe peak number of threads in the pool.\ncompleted\nInteger\nThe number of tasks completed. fs The fs object represents statistics about the file stores for the node. It has the following properties. Field Field type Description timestamp\nInteger\nThe last refresh time for the file store statistics, in milliseconds since the epoch.\ntotal\nObject\nStatistics for all file stores of the node.\ntotal.total_in_bytes\nInteger\nThe total memory size of all file stores, in bytes.\ntotal.free_in_bytes\nInteger\nThe total unallocated disk space in all file stores, in bytes.\ntotal.available_in_bytes\nInteger\nThe total disk space available to the JVM on all file stores. Represents the actual amount of memory, in bytes, that OpenSearch can use.\ndata\nArray\nThe list of all file stores. Each file store has the properties listed below.\ndata.path\nString\nThe path to the file store.\ndata.mount\nString\nThe mount point of the file store.\ndata.type\nString\nThe type of the file store (for example, overlay).\ndata.total_in_bytes\nInteger\nThe total size of the file store, in bytes.\ndata.free_in_bytes\nInteger\nThe total unallocated disk space in the file store, in bytes.\ndata.available_in_bytes\nInteger\nThe total amount of disk space available to the JVM for the file store, in bytes.\nio_stats\nObject\nI/O statistics for the node (Linux only). Includes devices, read and write operations, and the I/O operation time. transport The transport object has the following properties. Field Field type Description server_open\nInteger\nThe number of open inbound TCP connections that OpenSearch nodes use for internal communication.\ntotal_outbound_connections\nInteger\nThe total number of outbound transport connections that the node has opened since it started.\nrx_count\nInteger\nThe total number of RX (receive) packets the node received during internal communication.\nrx_size_in_bytes\nInteger\nThe total size of RX packets the node received during internal communication, in bytes.\ntx_count\nInteger\nThe total number of TX (transmit) packets the node sent during internal communication.\ntx_size_in_bytes\nInteger\nThe total size of TX (transmit) packets the node sent during internal communication, in bytes. breakers The breakers object contains statistics about the circuit breakers for the node. Each circuit breaker is a nested object listed by name and contains the following properties. Field Field type Description limit_size_in_bytes\nInteger\nThe memory limit for the circuit breaker, in bytes.\nlimit_size\nByte value\nThe memory limit for the circuit breaker in human-readable format (for example, 307.1mb).\nestimated_size_in_bytes\nInteger\nThe estimated memory used for the operation, in bytes.\nestimated_size\nByte value\nThe estimated memory used for the operation in human-readable format (for example, 356b).\noverhead\nFloat\nA factor that all estimates are multiplied by to calculate the final estimate.\ntripped\nInteger\nThe total number of times the circuit breaker has been activated to prevent an out-of-memory error. script and script_cache The script and script_cache objects have the following properties. Field Field type Description script\nObject\nScript statistics for the node.\nscript.compilations\nInteger\nThe total number of script compilations for the node.\nscript.cache_evictions\nInteger\nThe total number of times the script cache has purged old data.\nscript.compilation_limit_triggered\nInteger\nThe total number of times script compilation was limited by a circuit breaker.\nscript_cache\nObject\nScript cache statistics for the node.\nscript_cache.sum.compilations\nInteger\nThe total number of script compilations in the cache for the node.\nscript_cache.sum.cache_evictions\nInteger\nThe total number of times the script cache has purged old data.\nscript_cache.sum.compilation_limit_triggered\nInteger\nThe total number of times script compilation in the cache was limited by a circuit breaker.\nscript_cache.contexts\nArray of objects\nThe list of contexts for the script cache. Each context contains its name, the number of compilations, the number of cache evictions, and the number of times the script was limited by a circuit breaker. discovery The discovery object contains the node discovery statistics and has the following properties. Field Field type Description cluster_state_queue\nObject\nCluster state queue statistics for the node.\ncluster_state_queue.total\nInteger\nThe total number of cluster states in the queue.\ncluster_state_queue.pending\nInteger\nThe number of pending cluster states in the queue.\ncluster_state_queue.committed\nInteger\nThe number of committed cluster states in the queue.\npublished_cluster_states\nObject\nStatistics for the published cluster states for the node.\npublished_cluster_states.full_states\nInteger\nThe number of published cluster states.\npublished_cluster_states.incompatible_diffs\nInteger\nThe number of incompatible differences between published cluster states.\npublished_cluster_states.compatible_diffs\nInteger\nThe number of compatible differences between published cluster states. ingest The ingest object contains the ingest statistics and has the following properties. Field Field type Description total\nInteger\nIngest statistics for the node’s lifetime.\ntotal.count\nInteger\nThe total number of documents ingested by the node.\ntotal.time_in_millis\nInteger\nThe total amount of time for preprocessing ingest documents, in milliseconds.\ntotal.current\nInteger\nThe total number of documents that are currently being ingested by the node.\ntotal.failed\nInteger\nThe total number of failed ingestions for the node.\npipelines\nObject\nIngest pipeline statistics for the node. Each pipeline is a nested object specified by its ID with the properties listed below.\npipelines. id.count\nInteger\nThe number of documents preprocessed by the ingest pipeline.\npipelines. id.time_in_millis\nInteger\nThe total amount of time for preprocessing documents in the ingest pipeline, in milliseconds.\npipelines. id.failed\nInteger\nThe total number of failed ingestions for the ingest pipeline.\npipelines. id.processors\nArray of objects\nStatistics for the ingest processors. Includes the number of documents that are currently transformed, the total number of transformed documents, the number of failed transformations, and the time spent transforming documents. adaptive_selection The adaptive_selection object contains the adaptive selection statistics. Each entry is specified by the node ID and has the properties listed below. Field Field type Description outgoing_searches\nInteger\nThe number of outgoing search requests for the node.\navg_queue_size\nInteger\nThe rolling average queue size of search requests for the node (exponentially weighted).\navg_service_time_ns\nInteger\nThe rolling average service time for search requests, in nanoseconds (exponentially weighted).\navg_response_time_ns\nInteger\nThe rolling average response time for search requests, in nanoseconds (exponentially weighted).\nrank\nString\nThe node’s rank that is used to select shards when routing requests. indexing_pressure The indexing_pressure object contains the indexing pressure statistics and has the following properties. Field Field type Description memory\nObject\nStatistics related to memory consumption for the indexing load.\nmemory.current\nObject\nStatistics related to memory consumption for the current indexing load.\nmemory.current.combined_coordinating_and_primary_in_bytes\nInteger\nThe total memory used by indexing requests in the coordinating or primary stages, in bytes. A node can reuse the coordinating memory if the primary stage is run locally, so the total memory does not necessarily equal the sum of the coordinating and primary stage memory usage.\nmemory.current.coordinating_in_bytes\nThe total memory consumed by indexing requests in the coordinating stage, in bytes.\n \nmemory.current.primary_in_bytes\nInteger\nThe total memory consumed by indexing requests in the primary stage, in bytes.\nmemory.current.replica_in_bytes\nInteger\nThe total memory consumed by indexing requests in the replica stage, in bytes.\nmemory.current.all_in_bytes\nInteger\nThe total memory consumed by indexing requests in the coordinating, primary, or replica stages. shard_indexing_pressure The shard_indexing_pressure object contains the shard indexing pressure statistics and has the following properties. Field Field type Description stats Object\nStatistics about shard indexing pressure.\ntotal_rejections_breakup_shadow_mode\nObject\nIf running in shadow mode, the total_rejections_breakup_shadow_mode object contains statistics about the request rejection criteria of all shards in the node.\ntotal_rejections_breakup_shadow_mode.node_limits\nInteger\nThe total number of rejections due to the node memory limit. When all shards reach the memory limit assigned to the node (for example, 10% of heap size), the shard is unable to take in more traffic on the node, and the indexing request is rejected.\ntotal_rejections_breakup_shadow_mode.no_successful_request_limits\nInteger\nThe total number of rejections when the node occupancy level is breaching its soft limit and the shard has multiple outstanding requests that are waiting to be executed. In this case, additional indexing requests are rejected until the system recovers.\ntotal_rejections_breakup_shadow_mode.throughput_degradation_limits\nInteger\nThe total number of rejections when the node occupancy level is breaching its soft limit and there is a constant deterioration in the request turnaround at the shard level. In this case, additional indexing requests are rejected until the system recovers.\nenabled\nBoolean\nSpecifies whether the shard indexing pressure feature is turned on for the node.\nenforced\nBoolean\nIf true, the shard indexing pressure runs in enforced mode (there are rejections). If false, the shard indexing pressure runs in shadow mode (there are no rejections, but statistics are recorded and can be retrieved in the total_rejections_breakup_shadow_mode object). Only applicable if shard indexing pressure is enabled. Required permissions\nIf you use the Security plugin, make sure you have the appropriate permissions: cluster:monitor/nodes/stats.",
    "ancestors": [
      "API reference",
      "Nodes APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/nodes-apis/nodes-usage/",
    "title": "Nodes usage",
    "content": "The nodes usage endpoint returns low-level information about REST action usage on nodes.\nPath and HTTP methods GET _nodes/usage\nGET _nodes/&lt;nodeId&gt;/usage\nGET _nodes/usage/&lt;metric&gt;\nGET _nodes/&lt;nodeId&gt;/usage/&lt;metric&gt; Path parameters\nYou can include the following optional path parameters in your request. Parameter Type Description nodeId\nString\nA comma-separated list of nodeIds used to filter results. Supports node filters. Defaults to _all.\nmetric\nString\nThe metrics that will be included in the response. You can set the string to either _all or rest_actions. rest_actions returns the total number of times an action has been called on the node. _all returns all stats from the node. Defaults to _all. Query parameters\nYou can include the following optional query parameters in your request. Parameter Type Description timeout\nTime\nSets the time limit for a response from the node. Default is 30s.\ncluster_manager_timeout\nTime\nSets the time limit for a response from the cluster manager. Default is 30s. Example request\nThe following request returns usage details for all nodes: GET _nodes/usage copy Example response\nThe following is an example response: { \"_nodes\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"cluster_name\": \"opensearch-cluster\", \"nodes\": { \"t7uqHu4SSuWObK3ElkCRfw\": { \"timestamp\": 1665695174312, \"since\": 1663994849643, \"rest_actions\": { \"opendistro_get_rollup_action\": 3, \"nodes_usage_action\": 1, \"list_dangling_indices\": 1, \"get_index_template_action\": 258, \"nodes_info_action\": 152665, \"get_mapping_action\": 259, \"get_data_streams_action\": 12, \"cat_indices_action\": 6, \"get_indices_action\": 3, \"ism_explain_action\": 7, \"nodes_reload_action\": 1, \"get_policy_action\": 3, \"PerformanceAnalyzerClusterConfigAction\": 2, \"index_policy_action\": 1, \"rank_eval_action\": 3, \"search_action\": 592, \"get_aliases_action\": 258, \"document_mget_action\": 2, \"document_get_action\": 30, \"count_action\": 1, \"main_action\": 1 }, \"aggregations\": { } } } } Required permissions\nIf you use the Security plugin, make sure you set the following permissions: cluster:manage/nodes or cluster:monitor/nodes.",
    "ancestors": [
      "API reference",
      "Nodes APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/popular-api/",
    "title": "Popular APIs",
    "content": "This page contains example requests for popular OpenSearch operations.\nTable of contents Create index with non-default settings Index a document with a random ID Index a document with a specific ID Index several documents at once List all indices Open or close all indices that match a pattern Delete all indices that match a pattern Create an index alias List all aliases Search an index or all indices that match a pattern Get cluster settings, including defaults Change disk watermarks (or other cluster settings) Get cluster health List nodes in the cluster Get node statistics Get snapshots in a repository Take a snapshot Restore a snapshot Create index with non-default settings PUT my-logs { \"settings\": { \"number_of_shards\": 4, \"number_of_replicas\": 2 }, \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\" }, \"year\": { \"type\": \"integer\" } } } } Index a document with a random ID POST my-logs/_doc { \"title\": \"Your Name\", \"year\": \"2016\" } Index a document with a specific ID PUT my-logs/_doc/ 1 { \"title\": \"Weathering with You\", \"year\": \"2019\" } Index several documents at once\nThe blank line at the end of the request body is required. If you omit the _id field, OpenSearch generates a random ID. POST _bulk { \"index\": { \"_index\": \"my-logs\", \"_id\": \"2\" } } { \"title\": \"The Garden of Words\", \"year\": 2013 } { \"index\": { \"_index\": \"my-logs\", \"_id\": \"3\" } } { \"title\": \"5 Centimeters Per Second\", \"year\": 2007 } List all indices GET _cat/indices?v&amp;expand_wildcards=all Open or close all indices that match a pattern POST my-logs*/_open\nPOST my-logs*/_close Delete all indices that match a pattern DELETE my-logs* Create an index alias\nThis request creates the alias my-logs-today for the index my-logs-2019-11-13. PUT my-logs-2019-11-13/_alias/my-logs-today List all aliases GET _cat/aliases?v Search an index or all indices that match a pattern GET my-logs/_search?q=test\nGET my-logs*/_search?q=test Get cluster settings, including defaults GET _cluster/settings?include_defaults=true Change disk watermarks (or other cluster settings) PUT _cluster/settings { \"transient\": { \"cluster.routing.allocation.disk.watermark.low\": \"80%\", \"cluster.routing.allocation.disk.watermark.high\": \"85%\" } } Get cluster health GET _cluster/health List nodes in the cluster GET _cat/nodes?v Get node statistics GET _nodes/stats Get snapshots in a repository GET _snapshot/my-repository/_all Take a snapshot PUT _snapshot/my-repository/my-snapshot Restore a snapshot POST _snapshot/my-repository/my-snapshot/_restore { \"indices\": \"-.opendistro_security\", \"include_global_state\": false }",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/rank-eval/",
    "title": "Ranking evaluation",
    "content": "The rank eval endpoint allows you to evaluate the quality of ranked search results.\nPath and HTTP methods GET &lt;index_name&gt;/_rank_eval\nPOST &lt;index_name&gt;/_rank_eval Query parameters\nQuery parameters are optional. Parameter Data type Description ignore_unavailable\nBoolean\nDefaults to false. When set to false the response body will return an error if an index is closed or missing.\nallow_no_indices\nBoolean\nDefaults to true. When set to false the response body will return an error if a wildcard expression points to indexes that are closed or missing.\nexpand_wildcards\nString\nExpand wildcard expressions for indexes that are open, closed, hidden, none, or all.\nsearch_type\nString\nSet search type to either query_then_fetch or dfs_query_then_fetch. Request fields\nThe request body must contain at least one parameter. Field Type Description id\nDocument or template ID.\nrequests\nSet multiple search requests within the request field section.\nratings\nDocument relevance score.\nk\nThe number of documents returned per query. Default is set to 10.\nrelevant_rating_threshold\nThe threshold at which documents are considered relevant. Default is set to 1.\nnormalize\nDiscounted cumulative gain will be calculated when set to true.\nmaximum_relevance\nSets the maximum relevance score when using the expected reciprocal rank metric.\nignore_unlabeled\nDefaults to false. Unlabeled documents are ignored when set to true.\ntemplate_id\nTemplate ID.\nparams\nParameters used in the template. Example request GET shakespeare/_rank_eval { \"requests\": [ { \"id\": \"books_query\", \"request\": { \"query\": { \"match\": { \"text\": \"thou\" } } }, \"ratings\": [ { \"_index\": \"shakespeare\", \"_id\": \"80\", \"rating\": 0 }, { \"_index\": \"shakespeare\", \"_id\": \"115\", \"rating\": 1 }, { \"_index\": \"shakespeare\", \"_id\": \"117\", \"rating\": 2 }] }, { \"id\": \"words_query\", \"request\": { \"query\": { \"match\": { \"text\": \"art\" } } }, \"ratings\": [ { \"_index\": \"shakespeare\", \"_id\": \"115\", \"rating\": 2 }] }] } copy Example response { \"rank_eval\": { \"metric_score\": 0.7, \"details\": { \"query_1\": { \"metric_score\": 0.9, \"unrated_docs\": [ { \"_index\": \"shakespeare\", \"_id\": \"1234567\" },...], \"hits\": [ { \"hit\": { \"_index\": \"shakespeare\", \"_type\": \"page\", \"_id\": \"1234567\", \"_score\": 5.123456789 }, \"rating\": 1 },...], \"metric_details\": { \"precision\": { \"relevant_docs_retrieved\": 3, \"docs_retrieved\": 6 } } }, \"query_2\": { [...] } }, \"failures\": { [...] } } }",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/reload-search-analyzer/",
    "title": "Reload search analyzer",
    "content": "The reload search analyzer API operation detects any changes to synonym files for any configured search analyzers. The reload search analyzer request needs to be run on all nodes. Additionally, the synonym token filter must be set to true.\nPath and HTTP methods POST /&lt;index&gt;/_reload_search_analyzers\nGET /&lt;index&gt;/_reload_search_analyzers Request body fields\nRequest body parameters are optional. Field Type Data type Description allow_no_indices\nBoolean\nWhen set to false, an error is returned for indexes that are closed or missing and match any wildcard expression. Default is set to true.\nexpand_wildcards\nString\nAllows you to set the wildcards that can be matched to a type of index. Available options are open, closed, all, none, and hidden. Default is set to open.\nignore_unavailable\nBoolean\nIf an index is closed or missing, an error is returned when ignore_unavailable is set to false. Default is set to false. Examples\nThe following are an example request and response.\nExample request POST /shakespeare/_reload_search_analyzers copy Example response { \"_shards\": { \"total\": 1, \"successful\": 1, \"failed\": 0 }, \"reload_details\": [ { \"index\": \"shakespeare\", \"reloaded_analyzers\": [ \"analyzers-synonyms-test\"], \"reloaded_node_ids\": [ \"opensearch-node1\"] }] }",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/remote-info/",
    "title": "Remote cluster information",
    "content": "Introduced 1.0\nThis operation provides connection information for any remote OpenSearch clusters that you’ve configured for the local cluster, such as the remote cluster alias, connection mode ( sniff or proxy), IP addresses for seed nodes, and timeout settings.\nThe response is more comprehensive and useful than a call to _cluster/settings, which only includes the cluster alias and seed nodes.\nPath and HTTP methods GET _remote/info copy Response { \"opensearch-cluster2\": { \"connected\": true, \"mode\": \"sniff\", \"seeds\": [ \"172.28.0.2:9300\"], \"num_nodes_connected\": 1, \"max_connections_per_cluster\": 3, \"initial_connect_timeout\": \"30s\", \"skip_unavailable\": false } }",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/script-apis/create-stored-script/",
    "title": "Create or Update Stored Script",
    "content": "Create or update stored script\nCreates or updates a stored script or search template.\nFor additional information about Painless scripting, see: k-NN Painless Scripting extensions. k-NN.\nPath parameters Parameter Data type Description script-id\nString\nStored script or search template ID. Must be unique across the cluster. Required. Query parameters\nAll parameters are optional. Parameter Data type Description context\nString\nContext in which the script or search template is to run. To prevent errors, the API immediately compiles the script or template in this context.\ncluster_manager_timeout\nTime\nAmount of time to wait for a connection to the cluster manager. Defaults to 30 seconds.\ntimeout\nTime\nThe period of time to wait for a response. If a response is not received before the timeout value, the request fails and returns an error. Defaults to 30 seconds. Request fields Field Data type Description script\nObject\nDefines the script or search template, its parameters, and its language. See Script object below. Script object Field Data type Description lang\nString\nScripting language. Required.\nsource\nString or Object\nRequired. For scripts, a string with the contents of the script. For search templates, an object that defines the search template. Supports the same parameters as the Search API request body. Search templates also support Mustache variables. Example request\nThe sample uses an index called books with the following documents: { \"index\":{ \"_id\": 1 }} { \"name\": \"book1\", \"author\": \"Faustine\", \"ratings\":[ 4, 3, 5]} { \"index\":{ \"_id\": 2 }} { \"name\": \"book2\", \"author\": \"Amit\", \"ratings\":[ 5, 5, 5]} { \"index\":{ \"_id\": 3 }} { \"name\": \"book3\", \"author\": \"Gilroy\", \"ratings\":[ 2, 1, 5]} The following request creates the Painless script my-first-script. It sums the ratings for each book and displays the sum in the output. PUT _scripts/my-first-script { \"script\": { \"lang\": \"painless\", \"source\": \"\"\"\nint total = 0;\nfor (int i = 0; i &lt; doc['ratings'].length; ++i) {\ntotal += doc['ratings'][i];\n}\nreturn total;\n\"\"\" } } copy The example above uses the syntax of the Dev Tools console in OpenSearch Dashboards. You can also use a curl request.\nThe following curl request is equivalent to the previous Dashboards console example: curl -XPUT \"http://opensearch:9200/_scripts/my-first-script\" -H 'Content-Type: application/json' -d' { \"script\": { \"lang\": \"painless\", \"source\": \" \\n int total = 0; \\n for (int i = 0; i &lt; doc[' \\' 'ratings' \\' '].length; ++i) { \\n total += doc[' \\' 'ratings' \\' '][i]; \\n } \\n return total; \\n \" } } ' copy The following request creates the Painless script my-first-script, which sums the ratings for each book and displays the sum in the output: PUT _scripts/my-first-script { \"script\": { \"lang\": \"painless\", \"source\": \"\"\"\nint total = 0;\nfor (int i = 0; i &lt; doc['ratings'].length; ++i) {\ntotal += doc['ratings'][i];\n}\nreturn total;\n\"\"\" } } copy See Execute Painless stored script for information about running the script.\nExample response\nThe PUT _scripts/my-first-script request returns the following field: { \"acknowledged\": true } To determine whether the script was successfully created, use the Get stored script API, passing the script name as the script path parameter.\nResponse fields Field Data type Description acknowledged\nBoolean\nWhether the request was received. Creating or updating a stored script with parameters\nThe Painless script supports params to pass variables to the script.\nExample\nThe following request creates the Painless script multiplier-script. The request sums the ratings for each book, multiplies the summed value by the multiplier parameter, and displays the result in the output: PUT _scripts/multiplier-script { \"script\": { \"lang\": \"painless\", \"source\": \"\"\"\nint total = 0;\nfor (int i = 0; i &lt; doc['ratings'].length; ++i) {\ntotal += doc['ratings'][i];\n}\nreturn total * params['multiplier'];\n\"\"\" } } copy Example response\nThe PUT _scripts/multiplier-script request returns the following field: { \"acknowledged\": true }",
    "ancestors": [
      "API reference",
      "Script APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/script-apis/delete-script/",
    "title": "Delete Script",
    "content": "Delete script\nDeletes a stored script\nPath parameters\nPath parameters are optional. Parameter Data type Description script-id\nString\nID of script to delete. Query parameters Parameter Data type Description cluster_manager_timeout\nTime\nAmount of time to wait for a connection to the cluster manager. Optional, defaults to 30s.\ntimeout\nTime\nThe period of time to wait for a response. If a response is not received before the timeout value, the request will be dropped. Example request\nThe following request deletes the my-first-script script: DELETE _scripts/my-script copy Example response\nThe DELETE _scripts/my-first-script request returns the following field: { \"acknowledged\": true } To determine whether the stored script was successfully deleted, use the Get stored script API, passing the script name as the script path parameter.\nResponse fields\nThe request returns the following response fields: Field Data type Description acknowledged\nBoolean\nWhether the delete script request was received.",
    "ancestors": [
      "API reference",
      "Script APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/script-apis/exec-script/",
    "title": "Execute Painless script",
    "content": "Execute Painless script\nThe Execute Painless script API allows you to run a script that is not stored.\nPath and HTTP methods GET /_scripts/painless/_execute POST /_scripts/painless/_execute Request fields Field Description script\nThe script to run. Required\ncontext\nA context for the script. Optional. Default is painless_test.\ncontext_setup\nSpecifies additional parameters for the context. Optional. Example request\nThe following request uses the default painless_context for the script: GET /_scripts/painless/_execute { \"script\": { \"source\": \"(params.x + params.y)/ 2\", \"params\": { \"x\": 80, \"y\": 100 } } } copy Example response\nThe response contains the average of two script parameters: { \"result\": \"90\" } Response fields Field Description result\nThe script result. Script contexts\nChoose different contexts to control the variables that are available to the script and the result’s return type. The default context is painless_test.\nPainless test context\nThe painless_test context is the default script context that provides only the params variable to the script. The returned result is always converted to a string. See the preceding example request for a usage example.\nFilter context\nThe filter context runs the script as if the script were inside a script query. You must provide a test document in the context. The _source, stored fields, and _doc variables will be available to the script.\nYou can specify the following parameters for the filter context in the context_setup. Parameter Description document\nThe document that is indexed in memory temporarily and available to the script.\nindex\nThe name of the index that contains a mapping for the document. For example, first create an index with a mapping for a test document: PUT /testindex 1 { \"mappings\": { \"properties\": { \"grad\": { \"type\": \"boolean\" }, \"gpa\": { \"type\": \"float\" } } } } copy Run a script to determine if a student is eligible to graduate with honors: POST /_scripts/painless/_execute { \"script\": { \"source\": \"doc['grad'].value == true &amp;&amp; doc['gpa'].value &gt;= params.min_honors_gpa\", \"params\": { \"min_honors_gpa\": 3.5 } }, \"context\": \"filter\", \"context_setup\": { \"index\": \"testindex1\", \"document\": { \"grad\": true, \"gpa\": 3.79 } } } copy The response contains the result: { \"result\": true } Score context\nThe score context runs a script as if the script were in a script_score function in a function_score query.\nYou can specify the following parameters for the score context in the context_setup. Parameter Description document\nThe document that is indexed in memory temporarily and available to the script.\nindex\nThe name of the index that contains a mapping for the document.\nquery\nIf the script uses the _score parameter, the query can specify to use the _score field to compute the score. For example, first create an index with a mapping for a test document: PUT /testindex 1 { \"mappings\": { \"properties\": { \"gpa_4_0\": { \"type\": \"float\" } } } } copy Run a script that converts a GPA on a 4.0 scale into a different scale that is provided as a parameter: POST /_scripts/painless/_execute { \"script\": { \"source\": \"doc['gpa_4_0'].value * params.max_gpa / 4.0\", \"params\": { \"max_gpa\": 5.0 } }, \"context\": \"score\", \"context_setup\": { \"index\": \"testindex1\", \"document\": { \"gpa_4_0\": 3.5 } } } copy The response contains the result: { \"result\": 4.375 }",
    "ancestors": [
      "API reference",
      "Script APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/script-apis/exec-stored-script/",
    "title": "Execute Painless stored script",
    "content": "Execute Painless stored script\nRuns a stored script written in the Painless language.\nOpenSearch provides several ways to run a script; the following sections show how to run a script by passing script information in the request body of a GET &lt;index&gt;/_search request.\nRequest fields Field Data type Description query\nObject\nA filter that specifies documents to process.\nscript_fields\nObject\nFields to include in output.\nscript\nObject\nID of the script that produces a value for a field. Example request\nThe following request runs the stored script that was created in Create or update stored script. The script sums the ratings for each book and displays the sum in the total_ratings field in the output.\nThe script’s target is the books index.\nThe \"match_all\": {} property value is an empty object indicating to process each document in the index.\nThe total_ratings field value is the result of the my-first-script execution. See Create or update stored script. GET books/_search { \"query\": { \"match_all\": {} }, \"script_fields\": { \"total_ratings\": { \"script\": { \"id\": \"my-first-script\" } } } } copy Example response\nThe GET books/_search request returns the following fields: { \"took\": 2, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 3, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"books\", \"_id\": \"1\", \"_score\": 1.0, \"fields\": { \"total_ratings\": [ 12] } }, { \"_index\": \"books\", \"_id\": \"2\", \"_score\": 1.0, \"fields\": { \"total_ratings\": [ 15] } }, { \"_index\": \"books\", \"_id\": \"3\", \"_score\": 1.0, \"fields\": { \"total_ratings\": [ 8] } }] } } Response fields Field Data type Description took\nInteger\nHow long the operation took in milliseconds.\ntimed_out\nBoolean\nWhether the operation timed out.\n_shards\nObject\nTotal number of shards processed and also the total number of successful, skipped, and not processed.\nhits\nObject\nContains high-level information about the documents processed and an array of hits objects. See Hits object. Hits object Field Data type Description total\nObject\nTotal number of documents processed and their relationship to the match request field.\nmax_score\nDouble\nHighest relevance score returned from all the hits.\nhits\nArray\nInformation about each document that was processed. See Document object. Document object Field Data type Description _index\nString\nIndex that contains the document.\n_id\nString\nDocument ID.\n_score\nFloat\nDocument’s relevance score.\nfields\nObject\nFields and their value returned from the script. Running a Painless stored script with parameters\nTo pass different parameters to the script each time when running a query, define params in script_fields.\nExample\nThe following request runs the stored script that was created in Create or update stored script. The script sums the ratings for each book, multiplies the summed value by the multiplier parameter, and displays the result in the output.\nThe script’s target is the books index.\nThe \"match_all\": {} property value is an empty object, indicating that it processes each document in the index.\nThe total_ratings field value is the result of the multiplier-script execution. See Creating or updating a stored script with parameters. \"multiplier\": 2 in the params field is a variable passed to the stored script multiplier-script: GET books/_search { \"query\": { \"match_all\": {} }, \"script_fields\": { \"total_ratings\": { \"script\": { \"id\": \"multiplier-script\", \"params\": { \"multiplier\": 2 } } } } } copy Example response {\n\"took\": 12,\n\"timed_out\": false,\n\"_shards\": {\n\"total\": 1,\n\"successful\": 1,\n\"skipped\": 0,\n\"failed\": 0\n},\n\"hits\": {\n\"total\": {\n\"value\": 3,\n\"relation\": \"eq\"\n},\n\"max_score\": 1.0,\n\"hits\": [\n{\n\"_index\": \"books\",\n\"_type\": \"_doc\",\n\"_id\": \"3\",\n\"_score\": 1.0,\n\"fields\": {\n\"total_ratings\": [\n16]\n}\n},\n{\n\"_index\": \"books\",\n\"_type\": \"_doc\",\n\"_id\": \"2\",\n\"_score\": 1.0,\n\"fields\": {\n\"total_ratings\": [\n30]\n}\n},\n{\n\"_index\": \"books\",\n\"_type\": \"_doc\",\n\"_id\": \"1\",\n\"_score\": 1.0,\n\"fields\": {\n\"total_ratings\": [\n24]\n}\n}]\n}\n} Sort results using painless stored script\nYou can use painless stored script to sort results.\nSample request GET books/_search\n{\n\"query\": {\n\"match_all\": {}\n},\n\"script_fields\": {\n\"total_ratings\": {\n\"script\": {\n\"id\": \"multiplier-script\",\n\"params\": {\n\"multiplier\": 2\n}\n}\n}\n},\n\"sort\": {\n\"_script\": {\n\"type\": \"number\",\n\"script\": {\n\"id\": \"multiplier-script\",\n\"params\": {\n\"multiplier\": 2\n}\n},\n\"order\": \"desc\"\n}\n}\n} Sample response {\n\"took\": 90,\n\"timed_out\": false,\n\"_shards\": {\n\"total\": 5,\n\"successful\": 5,\n\"skipped\": 0,\n\"failed\": 0\n},\n\"hits\": {\n\"total\": {\n\"value\": 3,\n\"relation\": \"eq\"\n},\n\"max_score\": null,\n\"hits\": [\n{\n\"_index\": \"books\",\n\"_type\": \"_doc\",\n\"_id\": \"2\",\n\"_score\": null,\n\"fields\": {\n\"total_ratings\": [\n30]\n},\n\"sort\": [\n30.0]\n},\n{\n\"_index\": \"books\",\n\"_type\": \"_doc\",\n\"_id\": \"1\",\n\"_score\": null,\n\"fields\": {\n\"total_ratings\": [\n24]\n},\n\"sort\": [\n24.0]\n},\n{\n\"_index\": \"books\",\n\"_type\": \"_doc\",\n\"_id\": \"3\",\n\"_score\": null,\n\"fields\": {\n\"total_ratings\": [\n16]\n},\n\"sort\": [\n16.0]\n}]\n}\n}",
    "ancestors": [
      "API reference",
      "Script APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/script-apis/get-script-contexts/",
    "title": "Get Stored Script Contexts",
    "content": "Get stored script contexts\nRetrieves all contexts for stored scripts.\nExample request GET _script_context copy Example response\nThe GET _script_context request returns the following fields: { \"contexts\": [ { \"name\": \"aggregation_selector\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"boolean\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"aggs\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"java.lang.Object\", \"params\": [] }, { \"name\": \"getDoc\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"get_score\", \"return_type\": \"java.lang.Number\", \"params\": [] }, { \"name\": \"get_value\", \"return_type\": \"java.lang.Object\", \"params\": [] }] }, { \"name\": \"aggs_combine\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"java.lang.Object\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getState\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"aggs_init\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"void\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getState\", \"return_type\": \"java.lang.Object\", \"params\": [] }] }, { \"name\": \"aggs_map\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"void\", \"params\": [] }, { \"name\": \"getDoc\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getState\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"get_score\", \"return_type\": \"double\", \"params\": [] }] }, { \"name\": \"aggs_reduce\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"java.lang.Object\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getStates\", \"return_type\": \"java.util.List\", \"params\": [] }] }, { \"name\": \"analysis\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"boolean\", \"params\": [ { \"type\": \"org.opensearch.analysis.common.AnalysisPredicateScript$Token\", \"name\": \"token\" }] }] }, { \"name\": \"bucket_aggregation\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"java.lang.Number\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"field\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"java.lang.Object\", \"params\": [] }, { \"name\": \"getDoc\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"filter\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"boolean\", \"params\": [] }, { \"name\": \"getDoc\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"ingest\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"void\", \"params\": [ { \"type\": \"java.util.Map\", \"name\": \"ctx\" }] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"interval\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"boolean\", \"params\": [ { \"type\": \"org.opensearch.index.query.IntervalFilterScript$Interval\", \"name\": \"interval\" }] }] }, { \"name\": \"moving-function\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"double\", \"params\": [ { \"type\": \"java.util.Map\", \"name\": \"params\" }, { \"type\": \"double[]\", \"name\": \"values\" }] }] }, { \"name\": \"number_sort\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"double\", \"params\": [] }, { \"name\": \"getDoc\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"get_score\", \"return_type\": \"double\", \"params\": [] }] }, { \"name\": \"painless_test\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"java.lang.Object\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"processor_conditional\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"boolean\", \"params\": [ { \"type\": \"java.util.Map\", \"name\": \"ctx\" }] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"score\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"double\", \"params\": [ { \"type\": \"org.opensearch.script.ScoreScript$ExplanationHolder\", \"name\": \"explanation\" }] }, { \"name\": \"getDoc\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"get_score\", \"return_type\": \"double\", \"params\": [] }] }, { \"name\": \"script_heuristic\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"double\", \"params\": [ { \"type\": \"java.util.Map\", \"name\": \"params\" }] }] }, { \"name\": \"similarity\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"double\", \"params\": [ { \"type\": \"double\", \"name\": \"weight\" }, { \"type\": \"org.opensearch.index.similarity.ScriptedSimilarity$Query\", \"name\": \"query\" }, { \"type\": \"org.opensearch.index.similarity.ScriptedSimilarity$Field\", \"name\": \"field\" }, { \"type\": \"org.opensearch.index.similarity.ScriptedSimilarity$Term\", \"name\": \"term\" }, { \"type\": \"org.opensearch.index.similarity.ScriptedSimilarity$Doc\", \"name\": \"doc\" }] }] }, { \"name\": \"similarity_weight\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"double\", \"params\": [ { \"type\": \"org.opensearch.index.similarity.ScriptedSimilarity$Query\", \"name\": \"query\" }, { \"type\": \"org.opensearch.index.similarity.ScriptedSimilarity$Field\", \"name\": \"field\" }, { \"type\": \"org.opensearch.index.similarity.ScriptedSimilarity$Term\", \"name\": \"term\" }] }] }, { \"name\": \"string_sort\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"java.lang.String\", \"params\": [] }, { \"name\": \"getDoc\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"get_score\", \"return_type\": \"double\", \"params\": [] }] }, { \"name\": \"template\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"java.lang.String\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"terms_set\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"java.lang.Number\", \"params\": [] }, { \"name\": \"getDoc\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"trigger\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"boolean\", \"params\": [ { \"type\": \"org.opensearch.alerting.script.QueryLevelTriggerExecutionContext\", \"name\": \"ctx\" }] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }, { \"name\": \"update\", \"methods\": [ { \"name\": \"execute\", \"return_type\": \"void\", \"params\": [] }, { \"name\": \"getCtx\", \"return_type\": \"java.util.Map\", \"params\": [] }, { \"name\": \"getParams\", \"return_type\": \"java.util.Map\", \"params\": [] }] }] } Response fields\nThe GET _script_context request returns the following response fields: Field Data type Description contexts\nList\nA list of all contexts. See Script object. Script context Field Data type Description name\nString\nThe context name.\nmethods\nList\nList of the context’s allowable methods. See Script object. Context methods Field Data type Description name\nString\nMethod name.\nname\nString\nType that the method returns ( boolean, object, number, and so on).\nparams\nList\nList of the parameters accepted by the method. See Script object. Method parameters Field Data type Description type\nString\nParameter data type.\nname\nString\nParameter name.",
    "ancestors": [
      "API reference",
      "Script APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/script-apis/get-script-language/",
    "title": "Get Script Language",
    "content": "Get script language\nThe get script language API operation retrieves all supported script languages and their contexts.\nExample request GET _script_language copy Example response\nThe GET _script_language request returns the available contexts for each language: { \"types_allowed\": [ \"inline\", \"stored\"], \"language_contexts\": [ { \"language\": \"expression\", \"contexts\": [ \"aggregation_selector\", \"aggs\", \"bucket_aggregation\", \"field\", \"filter\", \"number_sort\", \"score\", \"terms_set\"] }, { \"language\": \"mustache\", \"contexts\": [ \"template\"] }, { \"language\": \"opensearch_query_expression\", \"contexts\": [ \"aggs\", \"filter\"] }, { \"language\": \"painless\", \"contexts\": [ \"aggregation_selector\", \"aggs\", \"aggs_combine\", \"aggs_init\", \"aggs_map\", \"aggs_reduce\", \"analysis\", \"bucket_aggregation\", \"field\", \"filter\", \"ingest\", \"interval\", \"moving-function\", \"number_sort\", \"painless_test\", \"processor_conditional\", \"score\", \"script_heuristic\", \"similarity\", \"similarity_weight\", \"string_sort\", \"template\", \"terms_set\", \"trigger\", \"update\"] }] } Response fields\nThe request contains the following response fields. Field Data type Description types_allowed\nList of strings\nThe types of scripts that are enabled, determined by the script.allowed_types setting. May contain inline and/or stored.\nlanguage_contexts\nList of objects\nA list of objects, each of which maps a supported language to its available contexts.\nlanguage_contexts.language\nString\nThe name of the registered scripting language.\nlanguage_contexts.contexts\nList of strings\nA list of all contexts for the language, determined by the script.allowed_contexts setting.",
    "ancestors": [
      "API reference",
      "Script APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/script-apis/get-stored-script/",
    "title": "Get Stored Script",
    "content": "Get stored script\nRetrieves a stored script.\nPath parameters Parameter Data type Description script\nString\nStored script or search template name. Required. Query parameters Parameter Data type Description cluster_manager_timeout\nTime\nAmount of time to wait for a connection to the cluster manager. Optional, defaults to 30s. Example request\nThe following retrieves the my-first-script stored script. GET _scripts/my-first-script copy Example response\nThe GET _scripts/my-first-script request returns the following fields: { \"_id\": \"my-first-script\", \"found\": true, \"script\": { \"lang\": \"painless\", \"source\": \"\"\"\nint total = 0;\nfor (int i = 0; i &lt; doc['ratings'].length; ++i) {\ntotal += doc['ratings'][i];\n}\nreturn total;\n\"\"\" } } Response fields\nThe GET _scripts/my-first-script request returns the following response fields: Field Data type Description _id\nString\nThe script’s name.\nfound\nBoolean\nThe requested script exists and was retrieved.\nscript\nObject\nThe script definition. See Script object. Script object Field Data type Description lang\nString\nThe script’s language.\nsource\nString\nThe script’s body.",
    "ancestors": [
      "API reference",
      "Script APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/script-apis/index/",
    "title": "Script APIs",
    "content": "The script APIs allow you to work with stored scripts. Stored scripts are part of the cluster state and reduce compilation time and enhance search speed. The default scripting language is Painless.\nYou can perform the following operations on stored scripts: Create or update stored script Execute Painless stored script Get stored script Delete script Get stored script contexts. Get script language",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/scroll/",
    "title": "Scroll",
    "content": "Introduced 1.0\nYou can use the scroll operation to retrieve a large number of results. For example, for machine learning jobs, you can request an unlimited number of results in batches.\nTo use the scroll operation, add a scroll parameter to the request header with a search context to tell OpenSearch how long you need to keep scrolling. This search context needs to be long enough to process a single batch of results.\nBecause search contexts consume a lot of memory, we suggest you don’t use the scroll operation for frequent user queries. Instead, use the sort parameter with the search_after parameter to scroll responses for user queries.\nExample\nTo set the number of results that you want returned for each batch, use the size parameter: GET shakespeare/_search?scroll= 10 m { \"size\": 10000 } copy OpenSearch caches the results and returns a scroll ID to access them in batches: \"_scroll_id\": \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAUWdmpUZDhnRFBUcWFtV21nMmFwUGJEQQ==\" Pass this scroll ID to the scroll operation to get back the next batch of results: GET _search/scroll { \"scroll\": \"10m\", \"scroll_id\": \"DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAAUWdmpUZDhnRFBUcWFtV21nMmFwUGJEQQ==\" } copy Using this scroll ID, you get results in batches of 10,000 as long as the search context is still open. Typically, the scroll ID does not change between requests, but it can change, so make sure to always use the latest scroll ID. If you don’t send the next scroll request within the set search context, the scroll operation does not return any results.\nIf you expect billions of results, use a sliced scroll. Slicing allows you to perform multiple scroll operations for the same request, but in parallel.\nSet the ID and the maximum number of slices for the scroll: GET shakespeare/_search?scroll= 10 m { \"slice\": { \"id\": 0, \"max\": 10 }, \"query\": { \"match_all\": {} } } copy With a single scroll ID, you get back 10 results.\nYou can have up to 10 IDs.\nClose the search context when you’re done scrolling, because the scroll operation continues to consume computing resources until the timeout: DELETE _search/scroll/DXF 1 ZXJ 5 QW 5 kRmV 0 Y 2 gBAAAAAAAAAAcWdmpUZDhnRFBUcWFtV 21 nMmFwUGJEQQ== copy To close all open scroll contexts: DELETE _search/scroll/_all copy The scroll operation corresponds to a specific timestamp. It doesn’t consider documents added after that timestamp as potential results.\nPath and HTTP methods GET _search/scroll\nPOST _search/scroll GET _search/scroll/&lt;scroll-id&gt;\nPOST _search/scroll/&lt;scroll-id&gt; URL parameters\nAll scroll parameters are optional. Parameter Type Description scroll\nTime\nSpecifies the amount of time the search context is maintained.\nscroll_id\nString\nThe scroll ID for the search.\nrest_total_hits_as_int\nBoolean\nWhether the hits.total property is returned as an integer ( true) or an object ( false). Default is false. Response { \"succeeded\": true, \"num_freed\": 1 }",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/search/",
    "title": "Search",
    "content": "Introduced 1.0\nThe Search API operation lets you execute a search request to search your cluster for data.\nExample GET /movies/_search { \"query\": { \"match\": { \"text_entry\": \"I am the night\" } } } copy Path and HTTP Methods GET /&lt;target-index&gt;/_search\nGET /_search\nPOST /&lt;target-index&gt;/_search\nPOST /_search URL Parameters\nAll URL parameters are optional. Parameter Type Description allow_no_indices\nBoolean\nWhether to ignore wildcards that don’t match any indexes. Default is true.\nallow_partial_search_results\nBoolean\nWhether to return partial results if the request runs into an error or times out. Default is true.\nanalyzer\nString\nAnalyzer to use in the query string.\nanalyze_wildcard\nBoolean\nWhether the update operation should include wildcard and prefix queries in the analysis. Default is false.\nbatched_reduce_size\nInteger\nHow many shard results to reduce on a node. Default is 512.\ncancel_after_time_interval\nTime\nThe time after which the search request will be canceled. Request-level parameter takes precedence over cancel_after_time_interval cluster setting. Default is -1.\nccs_minimize_roundtrips\nBoolean\nWhether to minimize roundtrips between a node and remote clusters. Default is true.\ndefault_operator\nString\nIndicates whether the default operator for a string query should be AND or OR. Default is OR.\ndf\nString\nThe default field in case a field prefix is not provided in the query string.\ndocvalue_fields\nString\nThe fields that OpenSearch should return using their docvalue forms.\nexpand_wildcards\nString\nSpecifies the type of index that wildcard expressions can match. Supports comma-separated values. Valid values are all (match any index), open (match open, non-hidden indexes), closed (match closed, non-hidden indexes), hidden (match hidden indexes), and none (deny wildcard expressions). Default is open.\nexplain\nBoolean\nWhether to return details about how OpenSearch computed the document’s score. Default is false.\nfrom\nInteger\nThe starting index to search from. Default is 0.\nignore_throttled\nBoolean\nWhether to ignore concrete, expanded, or indexes with aliases if indexes are frozen. Default is true.\nignore_unavailable\nBoolean\nSpecifies whether to include missing or closed indexes in the response. Default is false.\nlenient\nBoolean\nSpecifies whether OpenSearch should accept requests if queries have format errors (for example, querying a text field for an integer). Default is false.\nmax_concurrent_shard_requests\nInteger\nHow many concurrent shard requests this request should execute on each node. Default is 5.\npre_filter_shard_size\nInteger\nA prefilter size threshold that triggers a prefilter operation if the request exceeds the threshold. Default is 128 shards.\npreference\nString\nSpecifies which shard or node OpenSearch should perform the count operation on.\nq\nString\nLucene query string’s query.\nrequest_cache\nBoolean\nSpecifies whether OpenSearch should use the request cache. Default is whether it’s enabled in the index’s settings.\nrest_total_hits_as_int\nBoolean\nWhether to return hits.total as an integer. Returns an object otherwise. Default is false.\nrouting\nString\nValue used to route the update by query operation to a specific shard.\nscroll\nTime\nHow long to keep the search context open.\nsearch_type\nString\nWhether OpenSearch should use global term and document frequencies when calculating relevance scores. Valid choices are query_then_fetch and dfs_query_then_fetch. query_then_fetch scores documents using local term and document frequencies for the shard. It’s usually faster but less accurate. dfs_query_then_fetch scores documents using global term and document frequencies across all shards. It’s usually slower but more accurate. Default is query_then_fetch.\nseq_no_primary_term\nBoolean\nWhether to return sequence number and primary term of the last operation of each document hit.\nsize\nInteger\nHow many results to include in the response.\nsort\nList\nA comma-separated list of &lt;field&gt;: &lt;direction&gt; pairs to sort by.\n_source\nString\nWhether to include the _source field in the response.\n_source_excludes\nList\nA comma-separated list of source fields to exclude from the response.\n_source_includes\nList\nA comma-separated list of source fields to include in the response.\nstats\nString\nValue to associate with the request for additional logging.\nstored_fields\nBoolean\nWhether the get operation should retrieve fields stored in the index. Default is false.\nsuggest_field\nString\nFields OpenSearch can use to look for similar terms.\nsuggest_mode\nString\nThe mode to use when searching. Available options are always (use suggestions based on the provided terms), popular (use suggestions that have more occurrences), and missing (use suggestions for terms not in the index).\nsuggest_size\nInteger\nHow many suggestions to return.\nsuggest_text\nString\nThe source that suggestions should be based off of.\nterminate_after\nInteger\nThe maximum number of documents OpenSearch should process before terminating the request. Default is 0.\ntimeout\nTime\nHow long the operation should wait for a response from active shards. Default is 1m.\ntrack_scores\nBoolean\nWhether to return document scores. Default is false.\ntrack_total_hits\nBoolean or Integer\nWhether to return how many documents matched the query.\ntyped_keys\nBoolean\nWhether returned aggregations and suggested terms should include their types in the response. Default is true.\nversion\nBoolean\nWhether to include the document version as a match. Request body\nAll fields are optional. Field Type Description docvalue_fields\nArray of objects\nThe fields that OpenSearch should return using their docvalue forms. Specify a format to return results in a certain format, such as date and time.\nfields\nArray\nThe fields to search for in the request. Specify a format to return results in a certain format, such as date and time.\nexplain\nString\nWhether to return details about how OpenSearch computed the document’s score. Default is false.\nfrom\nInteger\nThe starting index to search from. Default is 0.\nindices_boost\nArray of objects\nValues used to boost the score of specified indexes. Specify in the format of &lt;index&gt;: &lt;boost-multiplier&gt;\nmin_score\nInteger\nSpecify a score threshold to return only documents above the threshold.\nquery\nObject\nThe DSL query to use in the request.\nseq_no_primary_term\nBoolean\nWhether to return sequence number and primary term of the last operation of each document hit.\nsize\nInteger\nHow many results to return. Default is 10.\n_source\n \nWhether to include the _source field in the response.\nstats\nString\nValue to associate with the request for additional logging.\nterminate_after\nInteger\nThe maximum number of documents OpenSearch should process before terminating the request. Default is 0.\ntimeout\nTime\nHow long to wait for a response. Default is no timeout.\nversion\nBoolean\nWhether to include the document version in the response. Response body { \"took\": 3, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 1, \"relation\": \"eq\" }, \"max_score\": 1.0, \"hits\": [ { \"_index\": \"superheroes\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": { \"superheroes\": [ { \"Hero name\": \"Superman\", \"Real identity\": \"Clark Kent\", \"Age\": 28 }, { \"Hero name\": \"Batman\", \"Real identity\": \"Bruce Wayne\", \"Age\": 26 }, { \"Hero name\": \"Flash\", \"Real identity\": \"Barry Allen\", \"Age\": 28 }, { \"Hero name\": \"Robin\", \"Real identity\": \"Dick Grayson\", \"Age\": 15 }] } }] } }",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/snapshots/create-repository/",
    "title": "Register Snapshot Repository",
    "content": "You can register a new repository in which to store snapshots or update information for an existing repository by using the snapshots API.\nThere are two types of snapshot repositories:\nFile system ( fs): For instructions on creating an fs repository, see Register repository shared file system.\nAmazon Simple Storage Service (Amazon S3) bucket ( s3): For instructions on creating an s3 repository, see Register repository Amazon S3.\nFor instructions on creating a repository, see Register repository.\nPath and HTTP methods POST /_snapshot/my-first-repo/\nPUT /_snapshot/my-first-repo/ Path parameters Parameter Data type Description repository\nString\nRepository name Request parameters\nRequest parameters depend on the type of repository: fs or s3.\nfs repository Request field Description location The file system directory for snapshots, such as a mounted directory from a file server or a Samba share. Must be accessible by all nodes. Required. chunk_size Breaks large files into chunks during snapshot operations (e.g. 64mb, 1gb), which is important for cloud storage providers and far less important for shared file systems. Default is null (unlimited). Optional. compress Whether to compress metadata files. This setting does not affect data files, which might already be compressed, depending on your index settings. Default is false. Optional. max_restore_bytes_per_sec The maximum rate at which snapshots restore. Default is 40 MB per second ( 40m). Optional. max_snapshot_bytes_per_sec The maximum rate at which snapshots take. Default is 40 MB per second ( 40m). Optional. readonly Whether the repository is read-only. Useful when migrating from one cluster ( \"readonly\": false when registering) to another cluster ( \"readonly\": true when registering). Optional. Example request\nThe following example registers an fs repository using the local directory /mnt/snapshots as location. PUT /_snapshot/my-fs-repository { \"type\": \"fs\", \"settings\": { \"location\": \"/mnt/snapshots\" } } copy s3 repository Request field Description base_path The path within the bucket in which you want to store snapshots (for example, my/snapshot/directory). Optional. If not specified, snapshots are stored in the S3 bucket root. bucket Name of the S3 bucket. Required. buffer_size The threshold beyond which chunks (of chunk_size) should be broken into pieces (of buffer_size) and sent to S3 using a different API. Default is the smaller of two values: 100 MB or 5% of the Java heap. Valid values are between 5mb and 5gb. We don’t recommend changing this option. canned_acl S3 has several canned ACLs that the repository-s3 plugin can add to objects as it creates them in S3. Default is private. Optional. chunk_size Breaks files into chunks during snapshot operations (e.g. 64mb, 1gb), which is important for cloud storage providers and far less important for shared file systems. Default is 1gb. Optional. client When specifying client settings (e.g. s3.client.default.access_key), you can use a string other than default (e.g. s3.client.backup-role.access_key). If you used an alternate name, change this value to match. Default and recommended value is default. Optional. compress Whether to compress metadata files. This setting does not affect data files, which might already be compressed, depending on your index settings. Default is false. Optional. max_restore_bytes_per_sec The maximum rate at which snapshots restore. Default is 40 MB per second ( 40m). Optional. max_snapshot_bytes_per_sec The maximum rate at which snapshots take. Default is 40 MB per second ( 40m). Optional. readonly Whether the repository is read-only. Useful when migrating from one cluster ( \"readonly\": false when registering) to another cluster ( \"readonly\": true when registering). Optional. server_side_encryption Whether to encrypt snapshot files in the S3 bucket. This setting uses AES-256 with S3-managed keys. See Protecting data using server-side encryption. Default is false. Optional. storage_class Specifies the S3 storage class for the snapshots files. Default is standard. Do not use the glacier and deep_archive storage classes. Optional. For the base_path parameter, do not enter the s3:// prefix when entering your S3 bucket details. Only the name of the bucket is required.\nExample request\nThe following request registers a new S3 repository called my-opensearch-repo in an existing bucket called my-open-search-bucket. By default, all snapshots are stored in the my/snapshot/directory. PUT /_snapshot/my-opensearch-repo { \"type\": \"s3\", \"settings\": { \"bucket\": \"my-open-search-bucket\", \"base_path\": \"my/snapshot/directory\" } } copy Example response\nUpon success, the following JSON object is returned: { \"acknowledged\": true } To verify that the repository was registered, use the Get snapshot repository API, passing the repository name as the repository path parameter.",
    "ancestors": [
      "API reference",
      "Snapshot APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/snapshots/create-snapshot/",
    "title": "Create Snapshot",
    "content": "Create snapshot\nCreates a snapshot within an existing repository.\nTo learn more about snapshots, see Snapshots.\nTo view a list of your repositories, see Get snapshot repository.\nPath and HTTP methods PUT /_snapshot/&lt;repository&gt;/&lt;snapshot&gt; POST /_snapshot/&lt;repository&gt;/&lt;snapshot&gt; Path parameters Parameter Data type Description repository\nString\nRepostory name to contain the snapshot.\nsnapshot\nString\nName of Snapshot to create. Query parameters Parameter Data type Description wait_for_completion\nBoolean\nWhether to wait for snapshot creation to complete before continuing. If you include this parameter, the snapshot definition is returned after completion. Request fields\nThe request body is optional. Field Data type Description indices String\nThe indices you want to include in the snapshot. You can use, to create a list of indices, * to specify an index pattern, and - to exclude certain indices. Don’t put spaces between items. Default is all indices. ignore_unavailable Boolean\nIf an index from the indices list doesn’t exist, whether to ignore it rather than fail the snapshot. Default is false. include_global_state Boolean\nWhether to include cluster state in the snapshot. Default is true. partial Boolean\nWhether to allow partial snapshots. Default is false, which fails the entire snapshot if one or more shards fails to stor Example requests\nRequest without a body\nThe following request creates a snapshot called my-first-snapshot in an S3 repository called my-s3-repository. A request body is not included because it is optional. POST _snapshot/my-s 3 -repository/my-first-snapshot copy Request with a body\nYou can also add a request body to include or exclude certain indices or specify other settings: PUT _snapshot/my-s 3 -repository/ 2 { \"indices\": \"opensearch-dashboards*,my-index*,-my-index-2016\", \"ignore_unavailable\": true, \"include_global_state\": false, \"partial\": false } copy Example responses\nUpon success, the response content depends on whether you include the wait_for_completion query parameter. wait_for_completion not included { \"accepted\": true } To verify that the snapshot was created, use the Get snapshot API, passing the snapshot name as the snapshot path parameter. wait_for_completion included\nThe snapshot definition is returned. { \"snapshot\": { \"snapshot\": \"5\", \"uuid\": \"ZRH4Zv7cSnuYev2JpLMJGw\", \"version_id\": 136217927, \"version\": \"2.0.1\", \"indices\": [ \".opendistro-reports-instances\", \".opensearch-observability\", \".kibana_1\", \"opensearch_dashboards_sample_data_flights\", \".opensearch-notifications-config\", \".opendistro-reports-definitions\", \"shakespeare\"], \"data_streams\": [], \"include_global_state\": true, \"state\": \"SUCCESS\", \"start_time\": \"2022-08-10T16:52:15.277Z\", \"start_time_in_millis\": 1660150335277, \"end_time\": \"2022-08-10T16:52:18.699Z\", \"end_time_in_millis\": 1660150338699, \"duration_in_millis\": 3422, \"failures\": [], \"shards\": { \"total\": 7, \"failed\": 0, \"successful\": 7 } } } Response fields Field Data type Description snapshot\nstring\nSnapshot name.\nuuid\nstring\nSnapshot’s universally unique identifier (UUID).\nversion_id\nint\nBuild ID of the Open Search version that created the snapshot.\nversion\nfloat\nOpen Search version that created the snapshot.\nindices\narray\nIndices in the snapshot.\ndata_streams\narray\nData streams in the snapshot.\ninclude_global_state\nboolean\nWhether the current cluster state is included in the snapshot.\nstart_time\nstring\nDate/time when the snapshot creation process began.\nstart_time_in_millis\nlong\nTime (in milliseconds) when the snapshot creation process began.\nend_time\nstring\nDate/time when the snapshot creation process ended.\nend_time_in_millis\nlong\nTime (in milliseconds) when the snapshot creation process ended.\nduration_in_millis\nlong\nTotal time (in milliseconds) that the snapshot creation process lasted.\nfailures\narray\nFailures, if any, that occured during snapshot creation.\nshards\nobject\nTotal number of shards created along with number of successful and failed shards.\nstate\nstring\nSnapshot status. Possible values: IN_PROGRESS, SUCCESS, FAILED, PARTIAL.",
    "ancestors": [
      "API reference",
      "Snapshot APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/snapshots/delete-snapshot-repository/",
    "title": "Delete Snapshot Repository",
    "content": "Delete snapshot repository configuration\nDeletes a snapshot repository configuration.\nA repository in OpenSearch is simply a configuration that maps a repository name to a type (file system or s3 repository) along with other information depending on the type. The configuration is backed by a file system location or an s3 bucket. When you invoke the API, the physical file system or s3 bucket itself is not deleted. Only the configuration is deleted.\nTo learn more about repositories, see Register or update snapshot repository.\nPath parameters Parameter Data type Description repository\nString\nRepository to delete. Example request\nThe following request deletes the my-opensearch-repo repository: DELETE _snapshot/my-opensearch-repo copy Example response\nUpon success, the response returns the following JSON object: { \"acknowledged\": true } To verify that the repository was deleted, use the Get snapshot repository API, passing the repository name as the repository path parameter.",
    "ancestors": [
      "API reference",
      "Snapshot APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/snapshots/delete-snapshot/",
    "title": "Delete Snapshot",
    "content": "Delete snapshot\nDeletes a snapshot from a repository.\nTo learn more about snapshots, see Snapshots.\nTo view a list of your repositories, see cat repositories.\nTo view a list of your snapshots, see cat snapshots.\nPath parameters Parameter Data type Description repository\nString\nRepostory that contains the snapshot.\nsnapshot\nString\nSnapshot to delete. Example request\nThe following request deletes a snapshot called my-first-snapshot from the my-opensearch-repo repository: DELETE _snapshot/my-opensearch-repo/my-first-snapshot copy Example response\nUpon success, the response returns the following JSON object: { \"acknowledged\": true } To verify that the snapshot was deleted, use the Get snapshot API, passing the snapshot name as the snapshot path parameter.",
    "ancestors": [
      "API reference",
      "Snapshot APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/snapshots/get-snapshot-repository/",
    "title": "Get Snapshot Repository",
    "content": "Get snapshot repository.\nRetrieves information about a snapshot repository.\nTo learn more about repositories, see Register repository.\nYou can also get details about a snapshot during and after snapshot creation. See Get snapshot status.\nPath parameters Parameter Data type Description repository\nString\nA comma-separated list of snapshot repository names to retrieve. Wildcard ( *) expressions are supported including combining wildcards with exclude patterns starting with -. Query parameters Parameter Data type Description local\nBoolean\nWhether to get information from the local node. Optional, defaults to false.\ncluster_manager_timeout\nTime\nAmount of time to wait for a connection to the master node. Optional, defaults to 30 seconds. Example request\nThe following request retrieves information for the my-opensearch-repo repository: GET /_snapshot/my-opensearch-repo copy Example response\nUpon success, the response returns repositry information. This sample is for an s3 repository type. { \"my-opensearch-repo\": { \"type\": \"s3\", \"settings\": { \"bucket\": \"my-open-search-bucket\", \"base_path\": \"snapshots\" } } } Response fields Field Data type Description type\nstring\nBucket type: fs (file system) or s3 (s3 bucket)\nbucket\nstring\nS3 bucket name.\nbase_path\nstring\nFolder within the bucket where snapshots are stored.",
    "ancestors": [
      "API reference",
      "Snapshot APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/snapshots/get-snapshot-status/",
    "title": "Get Snapshot Status",
    "content": "Get snapshot status\nReturns details about a snapshot’s state during and after snapshot creation.\nTo learn about snapshot creation, see Create snapshot.\nIf you use the Security plugin, you must have the monitor_snapshot, create_snapshot, or manage cluster privileges.\nPath parameters\nPath parameters are optional. Parameter Data type Description repository\nString\nRepository containing the snapshot.\nsnapshot\nString\nSnapshot to return. Three request variants provide flexibility: GET _snapshot/_status returns the status of all currently running snapshots in all repositories. GET _snapshot/&lt;repository&gt;/_status returns the status of only currently running snapshots in the specified repository. This is the preferred variant. GET _snapshot/&lt;repository&gt;/&lt;snapshot&gt;/_status returns the status of all snapshots in the specified repository whether they are running or not.\nUsing the API to return state for other than currently running snapshots can be very costly for (1) machine machine resources and (2) processing time if running in the cloud. For each snapshot, each request causes file reads from all a snapshot’s shards.\nRequest fields Field Data type Description ignore_unavailable\nBoolean\nHow to handles requests for unavailable snapshots. If false, the request returns an error for unavailable snapshots. If true, the request ignores unavailable snapshots, such as those that are corrupted or temporarily cannot be returned. Defaults to false. Example request\nThe following request returns the status of my-first-snapshot in the my-opensearch-repo repository. Unavailable snapshots are ignored. GET _snapshot/my-opensearch-repo/my-first-snapshot/_status { \"ignore_unavailable\": true } copy Example response\nThe example that follows corresponds to the request above in the Example request section.\nThe GET _snapshot/my-opensearch-repo/my-first-snapshot/_status request returns the following fields: { \"snapshots\": [ { \"snapshot\": \"my-first-snapshot\", \"repository\": \"my-opensearch-repo\", \"uuid\": \"dCK4Qth-TymRQ7Tu7Iga0g\", \"state\": \"SUCCESS\", \"include_global_state\": true, \"shards_stats\": { \"initializing\": 0, \"started\": 0, \"finalizing\": 0, \"done\": 7, \"failed\": 0, \"total\": 7 }, \"stats\": { \"incremental\": { \"file_count\": 31, \"size_in_bytes\": 24488927 }, \"total\": { \"file_count\": 31, \"size_in_bytes\": 24488927 }, \"start_time_in_millis\": 1660666841667, \"time_in_millis\": 14054 }, \"indices\": { \".opensearch-observability\": { \"shards_stats\": { \"initializing\": 0, \"started\": 0, \"finalizing\": 0, \"done\": 1, \"failed\": 0, \"total\": 1 }, \"stats\": { \"incremental\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"total\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"start_time_in_millis\": 1660666841868, \"time_in_millis\": 201 }, \"shards\": { \"0\": { \"stage\": \"DONE\", \"stats\": { \"incremental\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"total\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"start_time_in_millis\": 1660666841868, \"time_in_millis\": 201 } } } }, \"shakespeare\": { \"shards_stats\": { \"initializing\": 0, \"started\": 0, \"finalizing\": 0, \"done\": 1, \"failed\": 0, \"total\": 1 }, \"stats\": { \"incremental\": { \"file_count\": 4, \"size_in_bytes\": 18310117 }, \"total\": { \"file_count\": 4, \"size_in_bytes\": 18310117 }, \"start_time_in_millis\": 1660666842470, \"time_in_millis\": 13050 }, \"shards\": { \"0\": { \"stage\": \"DONE\", \"stats\": { \"incremental\": { \"file_count\": 4, \"size_in_bytes\": 18310117 }, \"total\": { \"file_count\": 4, \"size_in_bytes\": 18310117 }, \"start_time_in_millis\": 1660666842470, \"time_in_millis\": 13050 } } } }, \"opensearch_dashboards_sample_data_flights\": { \"shards_stats\": { \"initializing\": 0, \"started\": 0, \"finalizing\": 0, \"done\": 1, \"failed\": 0, \"total\": 1 }, \"stats\": { \"incremental\": { \"file_count\": 10, \"size_in_bytes\": 6132245 }, \"total\": { \"file_count\": 10, \"size_in_bytes\": 6132245 }, \"start_time_in_millis\": 1660666843476, \"time_in_millis\": 6221 }, \"shards\": { \"0\": { \"stage\": \"DONE\", \"stats\": { \"incremental\": { \"file_count\": 10, \"size_in_bytes\": 6132245 }, \"total\": { \"file_count\": 10, \"size_in_bytes\": 6132245 }, \"start_time_in_millis\": 1660666843476, \"time_in_millis\": 6221 } } } }, \".opendistro-reports-definitions\": { \"shards_stats\": { \"initializing\": 0, \"started\": 0, \"finalizing\": 0, \"done\": 1, \"failed\": 0, \"total\": 1 }, \"stats\": { \"incremental\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"total\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"start_time_in_millis\": 1660666843076, \"time_in_millis\": 200 }, \"shards\": { \"0\": { \"stage\": \"DONE\", \"stats\": { \"incremental\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"total\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"start_time_in_millis\": 1660666843076, \"time_in_millis\": 200 } } } }, \".opendistro-reports-instances\": { \"shards_stats\": { \"initializing\": 0, \"started\": 0, \"finalizing\": 0, \"done\": 1, \"failed\": 0, \"total\": 1 }, \"stats\": { \"incremental\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"total\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"start_time_in_millis\": 1660666841667, \"time_in_millis\": 201 }, \"shards\": { \"0\": { \"stage\": \"DONE\", \"stats\": { \"incremental\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"total\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"start_time_in_millis\": 1660666841667, \"time_in_millis\": 201 } } } }, \".kibana_1\": { \"shards_stats\": { \"initializing\": 0, \"started\": 0, \"finalizing\": 0, \"done\": 1, \"failed\": 0, \"total\": 1 }, \"stats\": { \"incremental\": { \"file_count\": 13, \"size_in_bytes\": 45733 }, \"total\": { \"file_count\": 13, \"size_in_bytes\": 45733 }, \"start_time_in_millis\": 1660666842673, \"time_in_millis\": 2007 }, \"shards\": { \"0\": { \"stage\": \"DONE\", \"stats\": { \"incremental\": { \"file_count\": 13, \"size_in_bytes\": 45733 }, \"total\": { \"file_count\": 13, \"size_in_bytes\": 45733 }, \"start_time_in_millis\": 1660666842673, \"time_in_millis\": 2007 } } } }, \".opensearch-notifications-config\": { \"shards_stats\": { \"initializing\": 0, \"started\": 0, \"finalizing\": 0, \"done\": 1, \"failed\": 0, \"total\": 1 }, \"stats\": { \"incremental\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"total\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"start_time_in_millis\": 1660666842270, \"time_in_millis\": 200 }, \"shards\": { \"0\": { \"stage\": \"DONE\", \"stats\": { \"incremental\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"total\": { \"file_count\": 1, \"size_in_bytes\": 208 }, \"start_time_in_millis\": 1660666842270, \"time_in_millis\": 200 } } } } } }] } Response fields Field Data type Description repository\nString\nName of repository that contains the snapshot.\nsnapshot\nString\nSnapshot name.\nuuid\nString\nSnapshot Universally unique identifier (UUID).\nstate\nString\nSnapshot’s current status. See Snapshot states.\ninclude_global_state\nBoolean\nWhether the current cluster state is included in the snapshot.\nshards_stats\nObject\nSnapshot’s shard counts. See Shard stats.\nstats\nObject\nDetails of files included in the snapshot. file_count: number of files. size_in_bytes: total of all fie sizes. See Snapshot file stats.\nindex\nlist of Objects\nList of objects that contain information about the indices in the snapshot. See Index objects. Snapshot states State Description FAILED\nThe snapshot terminated in an error and no data was stored.\nIN_PROGRESS\nThe snapshot is currently running.\nPARTIAL\nThe global cluster state was stored, but data from at least one shard was not stored. The failures property of the Create snapshot response contains additional details.\nSUCCESS\nThe snapshot finished and all shards were stored successfully. Shard stats\nAll property values are Integers. Property Description initializing\nNumber of shards that are still initializing.\nstarted\nNumber of shards that have started but not are not finalized.\nfinalizing\nNumber of shards that are finalizing but are not done.\ndone\nNumber of shards that initialized, started, and finalized successfully.\nfailed\nNumber of shards that failed to be included in the snapshot.\ntotal\nTotal number of shards included in the snapshot. Snapshot file stats Property Type Description incremental\nObject\nNumber and size of files that still need to be copied during snapshot creation. For completed snapshots, incremental provides the number and size of files that were not already in the repository and were copied as part of the incremental snapshot.\nprocessed\nObject\nNumber and size of files already uploaded to the snapshot. The processed file_count and size_in_bytes are incremented in stats after a file is uploaded.\ntotal\nObject\nTotal number and size of files that are referenced by the snapshot.\nstart_time_in_millis\nLong\nTime (in milliseconds) when snapshot creation began.\ntime_in_millis\nLong\nTotal time (in milliseconds) that the snapshot took to complete. Index objects Property Type Description shards_stats\nObject\nSee Shard stats.\nstats\nObject\nSee Snapshot file stats.\nshards\nlist of Objects\nList of objects containing information about the shards that include the snapshot. Properies of the shards are listed below in bold text. stage: Current state of shards in the snapshot. Shard states are: * DONE: Number of shards in the snapshot that were successfully stored in the repository. * FAILURE: Number of shards in the snapshot that were not successfully stored in the repository. * FINALIZE: Number of shards in the snapshot that are in the finalizing stage of being stored in the repository. * INIT: Number of shards in the snapshot that are in the initializing stage of being stored in the repository. * STARTED: Number of shards in the snapshot that are in the started stage of being stored in the repository. stats: See Snapshot file stats. total: Total number and size of files referenced by the snapshot. start_time_in_millis: Time (in milliseconds) when snapshot creation began. time_in_millis: Total time (in milliseconds) that the snapshot took to complete.",
    "ancestors": [
      "API reference",
      "Snapshot APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/snapshots/get-snapshot/",
    "title": "Get Snapshot",
    "content": "Get snapshot.\nRetrieves information about a snapshot.\nPath parameters Parameter Data type Description repository\nString\nThe repository that contains the snapshot to retrieve.\nsnapshot\nString\nSnapshot to retrieve. Query parameters Parameter Data type Description verbose\nBoolean\nWhether to show all, or just basic snapshot information. If true, returns all information. If false, omits information like start/end times, failures, and shards. Optional, defaults to true.\nignore_unavailable\nBoolean\nHow to handle snapshots that are unavailable (corrupted or otherwise temporarily can’t be returned). If true and the snapshot is unavailable, the request does not return the snapshot. If false and the snapshot is unavailable, the request returns an error. Optional, defaults to false. Example request\nThe following request retrieves information for the my-first-snapshot located in the my-opensearch-repo repository: GET _snapshot/my-opensearch-repo/my-first-snapshot copy Example response\nUpon success, the response returns snapshot information: { \"snapshots\": [ { \"snapshot\": \"my-first-snapshot\", \"uuid\": \"3P7Qa-M8RU6l16Od5n7Lxg\", \"version_id\": 136217927, \"version\": \"2.0.1\", \"indices\": [ \".opensearch-observability\", \".opendistro-reports-instances\", \".opensearch-notifications-config\", \"shakespeare\", \".opendistro-reports-definitions\", \"opensearch_dashboards_sample_data_flights\", \".kibana_1\"], \"data_streams\": [], \"include_global_state\": true, \"state\": \"SUCCESS\", \"start_time\": \"2022-08-11T20:30:00.399Z\", \"start_time_in_millis\": 1660249800399, \"end_time\": \"2022-08-11T20:30:14.851Z\", \"end_time_in_millis\": 1660249814851, \"duration_in_millis\": 14452, \"failures\": [], \"shards\": { \"total\": 7, \"failed\": 0, \"successful\": 7 } }] } Response fields Field Data type Description snapshot\nstring\nSnapshot name.\nuuid\nstring\nSnapshot’s universally unique identifier (UUID).\nversion_id\nint\nBuild ID of the Open Search version that created the snapshot.\nversion\nfloat\nOpen Search version that created the snapshot.\nindices\narray\nIndices in the snapshot.\ndata_streams\narray\nData streams in the snapshot.\ninclude_global_state\nboolean\nWhether the current cluster state is included in the snapshot.\nstart_time\nstring\nDate/time when the snapshot creation process began.\nstart_time_in_millis\nlong\nTime (in milliseconds) when the snapshot creation process began.\nend_time\nstring\nDate/time when the snapshot creation process ended.\nend_time_in_millis\nlong\nTime (in milliseconds) when the snapshot creation process ended.\nduration_in_millis\nlong\nTotal time (in milliseconds) that the snapshot creation process lasted.\nfailures\narray\nFailures, if any, that occured during snapshot creation.\nshards\nobject\nTotal number of shards created along with number of successful and failed shards.\nstate\nstring\nSnapshot status. Possible values: IN_PROGRESS, SUCCESS, FAILED, PARTIAL.",
    "ancestors": [
      "API reference",
      "Snapshot APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/snapshots/index/",
    "title": "Snapshot APIs",
    "content": "The snapshot APIs allow you to manage snapshots and snapshot repositories.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/snapshots/restore-snapshot/",
    "title": "Restore Snapshot",
    "content": "Restore Snapshot\nRestores a snapshot of a cluster or specified data streams and indices.\nFor information about indices and clusters, see Introduction to OpenSearch.\nFor information about data streams, see Data streams.\nIf open indexes with the same name that you want to restore already exist in the cluster, you must close, delete, or rename the indexes. See Example request for information about renaming an index. See Close index for information about closing an index.\nPath parameters Parameter Data type Description repository\nString\nRepository containing the snapshot to restore.\nsnapshot\nString\nSnapshot to restore. Query parameters Parameter Data type Description wait_for_completion\nBoolean\nWhether to wait for snapshot restoration to complete before continuing. Request fields\nAll request body parameters are optional. Parameter Data type Description ignore_unavailable\nBoolean\nHow to handle data streams or indices that are missing or closed. If false, the request returns an error for any data stream or index that is missing or closed. If true, the request ignores data streams and indices in indices that are missing or closed. Defaults to false.\nignore_index_settings\nBoolean\nA comma-delimited list of index settings that you don’t want to restore from a snapshot.\ninclude_aliases\nBoolean\nHow to handle index aliases from the original snapshot. If true, index aliases from the original snapshot are restored. If false, aliases along with associated indices are not restored. Defaults to true.\ninclude_global_state\nBoolean\nWhether to restore the current cluster state 1. If false, the cluster state is not restored. If true, the current cluster state is restored. Defaults to false.\nindex_settings\nString\nA comma-delimited list of settings to add or change in all restored indices. Use this parameter to override index settings during snapshot restoration. For data streams, these index settings are applied to the restored backing indices.\nindices\nString\nA comma-delimited list of data streams and indices to restore from the snapshot. Multi-index syntax is supported. By default, a restore operation includes all data streams and indices in the snapshot. If this argument is provided, the restore operation only includes the data streams and indices that you specify.\npartial\nBoolean\nHow the restore operation will behave if indices in the snapshot do not have all primary shards available. If false, the entire restore operation fails if any indices in the snapshot do not have all primary shards available. If true, allows the restoration of a partial snapshot of indices with unavailable shards. Only shards that were successfully included in the snapshot are restored. All missing shards are recreated as empty. By default, the entire restore operation fails if one or more indices included in the snapshot do not have all primary shards available. To change this behavior, set partial to true. Defaults to false.\nrename_pattern\nString\nThe pattern to apply to restored data streams and indices. Data streams and indices matching the rename pattern will be renamed according to rename_replacement. The rename pattern is applied as defined by the regular expression that supports referencing the original text. The request fails if two or more data streams or indices are renamed into the same name. If you rename a restored data stream, its backing indices are also renamed. For example, if you rename the logs data stream to recovered-logs, the backing index.ds-logs-1 is renamed to.ds-recovered-logs-1. If you rename a restored stream, ensure an index template matches the new stream name. If there are no matching index template names, the stream cannot roll over and new backing indices are not created.\nrename_replacement\nString\nThe rename replacement string. See rename_pattern for more information.\nwait_for_completion\nBoolean\nWhether to return a response after the restore operation has completed. If false, the request returns a response when the restore operation initializes. If true, the request returns a response when the restore operation completes. Defaults to false. 1 The cluster state includes:\nPersistent cluster settings\nIndex templates\nLegacy index templates\nIngest pipelines\nIndex lifecycle policies\nExample request\nThe following request restores the opendistro-reports-definitions index from my-first-snapshot. The rename_pattern and rename_replacement combination causes the index to be renamed to opendistro-reports-definitions_restored because duplicate open index names in a cluster are not allowed. POST /_snapshot/my-opensearch-repo/my-first-snapshot/_restore { \"indices\": \"opendistro-reports-definitions\", \"ignore_unavailable\": true, \"include_global_state\": false, \"rename_pattern\": \"(.+)\", \"rename_replacement\": \"$1_restored\", \"include_aliases\": false } Example response\nUpon success, the response returns the following JSON object: { \"snapshot\": { \"snapshot\": \"my-first-snapshot\", \"indices\": [], \"shards\": { \"total\": 0, \"failed\": 0, \"successful\": 0 } } } Except for the snapshot name, all properties are empty or 0. This is because any changes made to the volume after the snapshot was generated are lost. However, if you invoke the Get snapshot API to examine the snapshot, a fully populated snapshot object is returned.\nResponse fields Field Data type Description snapshot\nstring\nSnapshot name.\nindices\narray\nIndices in the snapshot.\nshards\nobject\nTotal number of shards created along with number of successful and failed shards. If open indices in a snapshot already exist in a cluster, and you don’t delete, close, or rename them, the API returns an error like the following: { \"error\": { \"root_cause\": [ { \"type\": \"snapshot_restore_exception\", \"reason\": \"[my-opensearch-repo:my-first-snapshot/dCK4Qth-TymRQ7Tu7Iga0g] cannot restore index [.opendistro-reports-definitions] because an open index with same name already exists in the cluster. Either close or delete the existing index or restore the index under a different name by providing a rename pattern and replacement name\" }], \"type\": \"snapshot_restore_exception\", \"reason\": \"[my-opensearch-repo:my-first-snapshot/dCK4Qth-TymRQ7Tu7Iga0g] cannot restore index [.opendistro-reports-definitions] because an open index with same name already exists in the cluster. Either close or delete the existing index or restore the index under a different name by providing a rename pattern and replacement name\" }, \"status\": 500 }",
    "ancestors": [
      "API reference",
      "Snapshot APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/snapshots/verify-snapshot-repository/",
    "title": "Verify Snaphot Repository",
    "content": "Verify snapshot repository\nVerifies that a snapshot repository is functional. Verifies the repository on each node in a cluster.\nIf verification is successful, the verify snapshot repository API returns a list of nodes connected to the snapshot repository. If verification failed, the API returns an error.\nIf you use the Security plugin, you must have the manage cluster privilege.\nPath parameters\nPath parameters are optional. Parameter Data type Description repository\nString\nName of repository to verify. Query parameters Parameter Data type Description cluster_manager_timeout\nTime\nAmount of time to wait for a connection to the master node. Optional, defaults to 30s.\ntimeout\nTime\nThe period of time to wait for a response. If a response is not received before the timeout value, the request fails and returns an error. Defaults to 30s. Example request\nThe following request verifies that the my-opensearch-repo is functional: POST /_snapshot/my-opensearch-repo/_verify?timeout= 0 s&amp;cluster_manager_timeout= 50 s Example response\nThe example that follows corresponds to the request above in the Example request section.\nThe POST /_snapshot/my-opensearch-repo/_verify?timeout=0s&amp;cluster_manager_timeout=50s request returns the following fields: { \"nodes\": { \"by1kztwTRoeCyg4iGU5Y8A\": { \"name\": \"opensearch-node1\" } } } In the preceding sample, one node is connected to the snapshot repository. If more were connected, you would see them in the response. Example: { \"nodes\": { \"lcfL6jv2jo6sMEtp4idMvg\": { \"name\": \"node-1\" }, \"rEPtFT/B+cuuOHnQn0jy4s\": { \"name\": \"node-2\" } } Response fields Field Data type Description nodes\nObject\nA list (not an array) of nodes connected to the snapshot repository. Each node itself is a property where the node ID is the key and the name has an ID (Object) and a name (String).",
    "ancestors": [
      "API reference",
      "Snapshot APIs"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/tasks/",
    "title": "Tasks",
    "content": "Introduced 1.0\nA task is any operation you run in a cluster. For example, searching your data collection of books for a title or author name is a task. When you run OpenSearch, a task is automatically created to monitor your cluster’s health and performance. For more information about all of the tasks currently executing in your cluster, you can use the tasks API operation.\nThe following request returns information about all of your tasks: GET _tasks copy By including a task ID, you can get information specific to a particular task. Note that a task ID consists of a node’s identifying string and the task’s numerical ID. For example, if your node’s identifying string is nodestring and the task’s numerical ID is 1234, then your task ID is nodestring:1234. You can find this information by running the tasks operation: GET _tasks/&lt;task_id&gt; copy Note that if a task finishes running, it won’t be returned as part of your request. For an example of a task that takes a little longer to finish, you can run the _reindex API operation on a larger document, and then run tasks. Sample Response { \"nodes\": { \"Mgqdm0r9SEGClWxp_RbnaQ\": { \"name\": \"opensearch-node1\", \"transport_address\": \"172.18.0.3:9300\", \"host\": \"172.18.0.3\", \"ip\": \"172.18.0.3:9300\", \"roles\": [ \"data\", \"ingest\", \"master\", \"remote_cluster_client\"], \"tasks\": { \"Mgqdm0r9SEGClWxp_RbnaQ:17416\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 17416, \"type\": \"transport\", \"action\": \"cluster:monitor/tasks/lists\", \"start_time_in_millis\": 1613599752458, \"running_time_in_nanos\": 994000, \"cancellable\": false, \"headers\": {} } }, \"Mgqdm0r9SEGClWxp_RbnaQ:17413\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 17413, \"type\": \"transport\", \"action\": \"indices:data/write/bulk\", \"start_time_in_millis\": 1613599752286, \"running_time_in_nanos\": 172846500, \"cancellable\": false, \"parent_task_id\": \"Mgqdm0r9SEGClWxp_RbnaQ:17366\", \"headers\": {} }, \"Mgqdm0r9SEGClWxp_RbnaQ:17366\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 17366, \"type\": \"transport\", \"action\": \"indices:data/write/reindex\", \"start_time_in_millis\": 1613599750929, \"running_time_in_nanos\": 1529733100, \"cancellable\": true, \"headers\": {} } } } } } You can also use the following parameters with your query. Parameter Data type Description nodes List\nA comma-separated list of node IDs or names to limit the returned information. Use _local to return information from the node you’re connecting to, specify the node name to get information from specific nodes, or keep the parameter empty to get information from all nodes. actions List\nA comma-separated list of actions that should be returned. Keep empty to return all. detailed Boolean\nReturns detailed task information. (Default: false) parent_task_id String\nReturns tasks with a specified parent task ID (node_id:task_number). Keep empty or set to -1 to return all. wait_for_completion Boolean\nWaits for the matching tasks to complete. (Default: false) group_by Enum\nGroups tasks by parent/child relationships or nodes. (Default: nodes) timeout Time\nAn explicit operation timeout. (Default: 30 seconds) master_timeout Time\nThe time to wait for a connection to the primary node. (Default: 30 seconds) For example, this request returns tasks currently running on a node named opensearch-node1: Sample Request GET /_tasks?nodes=opensearch-node1 copy Sample Response { \"nodes\": { \"Mgqdm0r9SEGClWxp_RbnaQ\": { \"name\": \"opensearch-node1\", \"transport_address\": \"sample_address\", \"host\": \"sample_host\", \"ip\": \"sample_ip\", \"roles\": [ \"data\", \"ingest\", \"master\", \"remote_cluster_client\"], \"tasks\": { \"Mgqdm0r9SEGClWxp_RbnaQ:24578\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 24578, \"type\": \"transport\", \"action\": \"cluster:monitor/tasks/lists\", \"start_time_in_millis\": 1611612517044, \"running_time_in_nanos\": 638700, \"cancellable\": false, \"headers\": {} }, \"Mgqdm0r9SEGClWxp_RbnaQ:24579\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 24579, \"type\": \"direct\", \"action\": \"cluster:monitor/tasks/lists[n]\", \"start_time_in_millis\": 1611612517044, \"running_time_in_nanos\": 222200, \"cancellable\": false, \"parent_task_id\": \"Mgqdm0r9SEGClWxp_RbnaQ:24578\", \"headers\": {} } } } } } The following request returns detailed information about active search tasks: Sample Request curl -XGET \"localhost:9200/_tasks?actions=*search&amp;detailed copy Sample Response { \"nodes\": { \"CRqNwnEeRXOjeTSYYktw-A\": { \"name\": \"runTask-0\", \"transport_address\": \"127.0.0.1:9300\", \"host\": \"127.0.0.1\", \"ip\": \"127.0.0.1:9300\", \"roles\": [ \"cluster_manager\", \"data\", \"ingest\", \"remote_cluster_client\"], \"attributes\": { \"testattr\": \"test\", \"shard_indexing_pressure_enabled\": \"true\" }, \"tasks\": { \"CRqNwnEeRXOjeTSYYktw-A:677\": { \"node\": \"CRqNwnEeRXOjeTSYYktw-A\", \"id\": 677, \"type\": \"transport\", \"action\": \"indices:data/read/search\", \"description\": \"indices[], search_type[QUERY_THEN_FETCH], source[{ \\\" query \\\":{ \\\" query_string \\\":&lt;QUERY_STRING&gt;}}]\", \"start_time_in_millis\": 1660106254525, \"running_time_in_nanos\": 1354236, \"cancellable\": true, \"cancelled\": false, \"headers\": { }, \"resource_stats\": { \"total\": { \"cpu_time_in_nanos\": 0, \"memory_in_bytes\": 0 } } } } } } } Task canceling\nAfter getting a list of tasks, you can cancel all cancelable tasks with the following request: POST _tasks/_cancel copy Note that not all tasks are cancelable. To see if a task is cancelable, refer to the cancellable field in the response to your tasks API request.\nYou can also cancel a task by including a specific task ID. POST _tasks/&lt;task_id&gt;/_cancel copy The cancel operation supports the same parameters as the tasks operation. The following example shows how to cancel all cancelable tasks on multiple nodes. POST _tasks/_cancel?nodes=opensearch-node1,opensearch-node2 copy Attaching headers to tasks\nTo associate requests with tasks for better tracking, you can provide a X-Opaque-Id:&lt;ID_number&gt; header as part of the HTTPS request reader of your curl command. The API will attach the specified header in the returned result.\nUsage: curl -i -H \"X-Opaque-Id: 111111\" \"https://localhost:9200/_tasks\" -u 'admin:admin' --insecure copy The _tasks operation returns the following result. HTTP/ 1.1 200 OK X-Opaque-Id: 111111 content-type: application/json; charset=UTF -8 content-length: 768 { \"nodes\": { \"Mgqdm0r9SEGClWxp_RbnaQ\": { \"name\": \"opensearch-node1\", \"transport_address\": \"172.18.0.4:9300\", \"host\": \"172.18.0.4\", \"ip\": \"172.18.0.4:9300\", \"roles\": [ \"data\", \"ingest\", \"master\", \"remote_cluster_client\"], \"tasks\": { \"Mgqdm0r9SEGClWxp_RbnaQ:30072\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 30072, \"type\": \"direct\", \"action\": \"cluster:monitor/tasks/lists[n]\", \"start_time_in_millis\": 1613166701725, \"running_time_in_nanos\": 245400, \"cancellable\": false, \"parent_task_id\": \"Mgqdm0r9SEGClWxp_RbnaQ:30071\", \"headers\": { \"X-Opaque-Id\": \"111111\" } }, \"Mgqdm0r9SEGClWxp_RbnaQ:30071\": { \"node\": \"Mgqdm0r9SEGClWxp_RbnaQ\", \"id\": 30071, \"type\": \"transport\", \"action\": \"cluster:monitor/tasks/lists\", \"start_time_in_millis\": 1613166701725, \"running_time_in_nanos\": 658200, \"cancellable\": false, \"headers\": { \"X-Opaque-Id\": \"111111\" } } } } } } This operation supports the same parameters as the tasks operation. The following example shows how you can associate X-Opaque-Id with specific tasks: curl -i -H \"X-Opaque-Id: 123456\" \"https://localhost:9200/_tasks?nodes=opensearch-node1\" -u 'admin:admin' --insecure copy",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/api-reference/units/",
    "title": "Supported units",
    "content": "OpenSearch supports the following units for all REST operations: Unit Description Example Times\nThe supported units for time are d for days, h for hours, m for minutes, s for seconds, ms for milliseconds, micros for microseconds, and nanos for nanoseconds. 5d or 7h Bytes\nThe supported units for byte size are b for bytes, kb for kibibytes, mb for mebibytes, gb for gibibytes, tb for tebibytes, and pb for pebibytes. Despite the base-10 abbreviations, these units are base-2; 1kb is 1,024 bytes, 1mb is 1,048,576 bytes, etc. 7kb or 6gb Distances\nThe supported units for distance are mi for miles, yd for yards, ft for feet, in for inches, km for kilometers, m for meters, cm for centimeters, mm for millimeters, and nmi or NM for nautical miles. 5mi or 4ft Quantities without units\nFor large values that don’t have a unit, use k for kilo, m for mega, g for giga, t for tera, and p for peta. 5k for 5,000 To convert output units to human-readable values, see Common REST parameters.",
    "ancestors": [
      "API reference"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/troubleshoot/index/",
    "title": "Common issues",
    "content": "This page contains a list of common issues and workarounds.\nOpenSearch Dashboards fails to start\nIf you encounter the error FATAL Error: Request Timeout after 30000ms during startup, try running OpenSearch Dashboards on a more powerful machine. We recommend four CPU cores and 8 GB of RAM.\nRequests to OpenSearch Dashboards fail with “Request must contain a osd-xsrf header”\nIf you run legacy Kibana OSS scripts against OpenSearch Dashboards—for example, curl commands that import saved objects from a file—they might fail with the following error: { \"status\": 400, \"body\": \"Request must contain a osd-xsrf header.\" } In this case, your scripts likely include the \"kbn-xsrf: true\" header. Switch it to the osd-xsrf: true header: curl -XPOST -u 'admin:admin' 'https://DASHBOARDS_ENDPOINT/api/saved_objects/_import' -H 'osd-xsrf:true' --form file=@export.ndjson Multi-tenancy issues in OpenSearch Dashboards\nIf you’re testing multiple users in OpenSearch Dashboards and encounter unexpected changes in tenant, use Google Chrome in an Incognito window or Firefox in a Private window.\nExpired certificates\nIf your certificates have expired, you might receive the following error or something similar: ERROR org.opensearch.security.ssl.transport.SecuritySSLNettyTransport - Exception during establishing a SSL connection: javax.net.ssl.SSLHandshakeException: PKIX path validation failed: java.security.cert.CertPathValidatorException: validity check failed\nCaused by: java.security.cert.CertificateExpiredException: NotAfter: Thu Sep 16 11:27:55 PDT 2021 To check the expiration date for a certificate, run this command: openssl x509 -enddate -noout -in &lt;certificate&gt; Encryption at rest\nThe operating system for each OpenSearch node handles encryption of data at rest. To enable encryption at rest in most Linux distributions, use the cryptsetup command: cryptsetup luksFormat --key-file &lt;key&gt; &lt;partition&gt; For full documentation about the command, see cryptsetup(8) — Linux manual page.\nCan’t update by script when FLS, DLS, or field masking is active\nThe Security plugin blocks the update by script operation ( POST &lt;index&gt;/_update/&lt;id&gt;) when field-level security, document-level security, or field masking are active. You can still update documents using the standard index operation ( PUT &lt;index&gt;/_doc/&lt;id&gt;).\nIllegal reflective access operation in logs\nThis is a known issue with Performance Analyzer that shouldn’t affect functionality.",
    "ancestors": [
      "Troubleshooting"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/troubleshoot/openid-connect/",
    "title": "Troubleshoot OpenID Connect",
    "content": "This page includes troubleshooting steps for using OpenID Connect with the Security plugin.\nTable of contents Set log level to debug “Failed when trying to obtain the endpoints from your IdP” “ValidationError: child ‘opensearch_security’ fails” “Authentication failed. Please provide a new token.” Leftover cookies or cached credentials Wrong client secret “Failed to get subject from JWT claims” “Failed to get roles from JWT claims with roles_key” Set log level to debug\nTo help troubleshoot OpenID Connect, set the log level to debug on OpenSearch. Add the following lines in config/log4j2.properties and restart the node: logger.securityjwt.name = com.amazon.dlic.auth.http.jwt\nlogger.securityjwt.level = trace This setting prints a lot of helpful information to your log file. If this information isn’t sufficient, you can also set the log level to trace.\n“Failed when trying to obtain the endpoints from your IdP”\nThis error indicates that the Security plugin can’t reach the metadata endpoint of your IdP. In opensearch_dashboards.yml, check the following setting: plugins.security.openid.connect_url: \"http://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration\" If this error occurs on OpenSearch, check the following setting in config.yml: openid_auth_domain: enabled: true order: 1 http_authenticator: type: \" openid\"... config: openid_connect_url: http://keycloak.examplesss.com:8080/auth/realms/master/.well-known/openid-configuration... “ValidationError: child ‘opensearch_security’ fails”\nThis indicates that one or more of the OpenSearch Dashboards configuration settings are missing.\nCheck opensearch_dashboards.yml and make sure you have set the following minimal configuration: plugins.security.openid.connect_url: \"...\" plugins.security.openid.client_id: \"...\" plugins.security.openid.client_secret: \"...\" “Authentication failed. Please provide a new token.”\nThis error has several potential root causes.\nLeftover cookies or cached credentials\nPlease delete all cached browser data, or try again in a private browser window.\nWrong client secret\nTo trade the access token for an identity token, most IdPs require you to provide a client secret. Check if the client secret in opensearch_dashboards.yml matches the client secret of your IdP configuration: plugins.security.openid.client_secret: \"...\" “Failed to get subject from JWT claims”\nThis error is logged on OpenSearch and means that the username could not be extracted from the ID token. Make sure the following setting matches the claims in the JWT your IdP issues: openid_auth_domain:\nenabled: true\norder: 1\nhttp_authenticator:\ntype: \"openid\"...\nconfig:\nsubject_key: &lt;subject key&gt;... “Failed to get roles from JWT claims with roles_key”\nThis error indicates that the roles key you configured in config.yml does not exist in the JWT issued by your IdP. Make sure the following setting matches the claims in the JWT your IdP issues: openid_auth_domain:\nenabled: true\norder: 1\nhttp_authenticator:\ntype: \"openid\"...\nconfig:\nroles_key: &lt;roles key&gt;...",
    "ancestors": [
      "Troubleshooting"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/troubleshoot/saml/",
    "title": "Troubleshoot SAML",
    "content": "This page includes troubleshooting steps for using SAML for OpenSearch Dashboards authentication.\nTable of contents Check sp.entity_id Check the SAML assertion consumer service URL Sign all documents Role settings Inspect the SAML response Check role mapping Inspect the JWT token Check sp.entity_id\nMost identity providers (IdPs) allow you to configure multiple authentication methods for different applications. For example, in Okta, these clients are called “Applications.” In Keycloak, they are called “Clients.” Each one has its own entity ID. Make sure to configure sp.entity_id to match those settings: saml:... http_authenticator: type: ' saml' challenge: true config:... sp: entity_id: opensearch-dashboards-saml Check the SAML assertion consumer service URL\nAfter a successful login, your IdP sends a SAML response using HTTP POST to OpenSearch Dashboards’s “assertion consumer service URL” (ACS).\nThe endpoint the OpenSearch Dashboards Security plugin provides is: /_opendistro/_security/saml/acs Make sure that you have configured this endpoint correctly in your IdP. Some IdPs also require you to add all endpoints to the allow list that they send requests to. Ensure that the ACS endpoint is listed.\nOpenSearch Dashboards also requires you to add this endpoint to the allow list. Make sure you have the following entry in opensearch_dashboards.yml: server.xsrf.allowlist: [/_opendistro/_security/saml/acs] Sign all documents\nSome IdPs do not sign the SAML documents by default. Make sure the IdP signs all documents.\nKeycloak Role settings\nIncluding user roles in the SAML response is dependent on your IdP. For example, in Keycloak, this setting is in the Mappers section of your client. In Okta, you have to set group attribute statements. Make sure this is configured correctly and that the roles_key in the SAML configuration matches the role name in the SAML response: saml:... http_authenticator: type: ' saml' challenge: true config:... roles_key: Role Inspect the SAML response\nIf you are not sure what the SAML response of your IdP contains and where it places the username and roles, you can enable debug mode in the log4j2.properties: logger.token.name = com.amazon.dlic.auth.http.saml.Token\nlogger.token.level = debug This setting prints the SAML response to the OpenSearch log file so that you can inspect and debug it. Setting this logger to debug generates many statements, so we don’t recommend using it in production.\nAnother way of inspecting the SAML response is to monitor network traffic while logging in to OpenSearch Dashboards. The IdP uses HTTP POST requests to send Base64-encoded SAML responses to: /_opendistro/_security/saml/acs Inspect the payload of this POST request, and use a tool like base64decode.org to decode it.\nCheck role mapping\nThe Security plugin uses a standard role mapping to map a user or backend role to one or more Security roles.\nFor username, the Security plugin uses the NameID attribute of the SAML response by default. For some IdPs, this attribute does not contain the expected username, but some internal user ID. Check the content of the SAML response to locate the element you want to use as username, and configure it by setting the subject_key: saml:... http_authenticator: type: ' saml' challenge: true config:... subject_key: preferred_username For checking that the correct backend roles are contained in the SAML response, inspect the contents, and set the correct attribute name: saml:... http_authenticator: type: ' saml' challenge: true config:... roles_key: Role Inspect the JWT token\nThe Security plugin trades the SAML response for a more lightweight JSON web token. The username and backend roles in the JWT are ultimately mapped to roles in the Security plugin. If there is a problem with the mapping, you can enable the token debug mode using the same setting as Inspect the SAML response.\nThis setting prints the JWT to the OpenSearch log file so that you can inspect and debug it using a tool like JWT.io.",
    "ancestors": [
      "Troubleshooting"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/troubleshoot/security-admin/",
    "title": "Troubleshoot securityadmin.sh",
    "content": "This page includes troubleshooting steps for securityadmin.sh. The script can be found at /plugins/opensearch-security/tools/securityadmin.sh. For more information about using this tool, see Applying changes to configuration files.\nTable of contents Cluster not reachable Check hostname Check the port None of the configured nodes are available Check cluster name Check hostname verification Check cluster state Check the security index name “ERR: DN is not an admin user” Use the diagnose option Cluster not reachable\nIf securityadmin.sh can’t reach the cluster, it outputs: OpenSearch Security Admin v6\nWill connect to localhost:9300\nERR: Seems there is no opensearch running on localhost:9300 - Will exit Check hostname\nBy default, securityadmin.sh uses localhost. If your cluster runs on any other host, specify the hostname using the -h option.\nCheck the port\nCheck that you are running securityadmin.sh against the transport port, not the HTTP port.\nBy default, securityadmin.sh uses 9300. If your cluster runs on a different port, use the -p option to specify the port number.\nNone of the configured nodes are available\nIf securityadmin.sh can reach the cluster, but can’t update the configuration, it outputs this error: Contacting opensearch cluster 'opensearch' and wait for YELLOW clusterstate...\nCannot retrieve cluster state due to: None of the configured nodes are available: [{#transport#-1}{mr2NlX3XQ3WvtVG0Dv5eHw}{localhost}{127.0.0.1:9300}]. This is not an error, will keep on trying... Try running securityadmin.sh with -icl and -nhnv.\nIf this works, check your cluster name as well as the hostnames in your SSL certificates. If this does not work, try running securityadmin.sh with --diagnose and see diagnose trace log file.\nAdd --accept-red-cluster to allow securityadmin.sh to operate on a red cluster.\nCheck cluster name\nBy default, securityadmin.sh uses opensearch as the cluster name.\nIf your cluster has a different name, you can either ignore the name completely using the -icl option or specify the name using the -cn option.\nCheck hostname verification\nBy default, securityadmin.sh verifies that the hostname in your node’s certificate matches the node’s actual hostname.\nIf this is not the case (e.g. if you’re using the demo certificates), you can disable hostname verification by adding the -nhnv option.\nCheck cluster state\nBy default, securityadmin.sh only executes if the cluster state is at least yellow.\nIf your cluster state is red, you can still execute securityadmin.sh, but you need to add the -arc option.\nCheck the security index name\nBy default, the Security plugin uses.opendistro_security as the name of the configuration index. If you configured a different index name in opensearch.yml, specify it using the -i option.\n“ERR: DN is not an admin user”\nIf the TLS certificate used to start securityadmin.sh isn’t an admin certificate, the script outputs: Connected as CN=node-0.example.com,OU=SSL,O=Test,L=Test,C=DE\nERR: CN=node-0.example.com,OU=SSL,O=Test,L=Test,C=DE is not an admin user You must use an admin certificate when executing the script. To learn more, see Configuring admin certificates.\nUse the diagnose option\nFor more information on why securityadmin.sh is not executing, add the --diagnose option:./securityadmin.sh -diagnose -cd../../../config/opensearch-security/ -cacert... -cert... -key... -keypass... The script prints the location of the generated diagnostic file.",
    "ancestors": [
      "Troubleshooting"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/troubleshoot/tls/",
    "title": "Troubleshoot TLS",
    "content": "This page includes troubleshooting steps for configuring TLS certificates with the Security plugin.\nTable of contents Validate YAML View contents of PEM certificates Check for special characters and whitespace in DNs Check certificate IP addresses Validate certificate chain Check the configured alias View contents of your keystore and truststore Check SAN hostnames and IP addresses Check OID for node certificates Check EKU field for node certificates TLS versions Supported ciphers Validate YAML opensearch.yml and the files in config/opensearch-security/ are in the YAML format. A linter like YAML Validator can help verify that you don’t have any formatting errors.\nView contents of PEM certificates\nYou can use OpenSSL to display the content of each PEM certificate: openssl x509 -subject -nameopt RFC2253 -noout -in node1.pem Then ensure that the value matches the one in opensearch.yml.\nFor more complete information on a certificate: openssl x509 -in node1.pem -text -noout Check for special characters and whitespace in DNs\nThe Security plugin uses the string representation of Distinguished Names (RFC1779) when validating node certificates.\nIf parts of your DN contain special characters (e.g. a comma), make sure you escape it in your configuration: plugins.security.nodes_dn: - ' CN=node-0.example.com,OU=SSL,O=My\\, Test,L=Test,C=DE' You can have whitespace within a field, but not between fields.\nBad configuration plugins.security.nodes_dn: - ' CN=node-0.example.com, OU=SSL,O=My\\, Test, L=Test, C=DE' Good configuration plugins.security.nodes_dn: - ' CN=node-0.example.com,OU=SSL,O=My\\, Test,L=Test,C=DE' Check certificate IP addresses\nSometimes the IP address in your certificate is not the one communicating with the cluster. This problem can occur if your node has multiple interfaces or is running on a dual stack network (IPv6 and IPv4).\nIf this problem occurs, you might see the following in the node’s OpenSearch log: SSL Problem Received fatal alert: certificate_unknown javax.net.ssl.SSLException: Received fatal alert: certificate_unknown You might also see the following message in your cluster’s master log when the new node tries to join the cluster: Caused by: java.security.cert.CertificateException: No subject alternative names matching IP address 10.0.0.42 found Check the IP address in the certificate: IPAddress: 2001:db8:0:1:1.2.3.4 In this example, the node tries to join the cluster with the IPv4 address of 10.0.0.42, but the certificate contains the IPv6 address of 2001:db8:0:1:1.2.3.4.\nValidate certificate chain\nTLS certificates are organized in a certificate chain. You can check with keytool that the certificate chain is correct by inspecting the owner and the issuer of each certificate. If you used the demo installation script that ships with the Security plugin, the chain looks like:\nNode certificate Owner: CN=node-0.example.com, OU=SSL, O=Test, L=Test, C=DE\nIssuer: CN=Example Com Inc. Signing CA, OU=Example Com Inc. Signing CA, O=Example Com Inc., DC=example, DC=com Signing certificate Owner: CN=Example Com Inc. Signing CA, OU=Example Com Inc. Signing CA, O=Example Com Inc., DC=example, DC=com\nIssuer: CN=Example Com Inc. Root CA, OU=Example Com Inc. Root CA, O=Example Com Inc., DC=example, DC=com Root certificate Owner: CN=Example Com Inc. Root CA, OU=Example Com Inc. Root CA, O=Example Com Inc., DC=example, DC=com\nIssuer: CN=Example Com Inc. Root CA, OU=Example Com Inc. Root CA, O=Example Com Inc., DC=example, DC=com From the entries, you can see that the root certificate signed the intermediate certificate, which signed the node certificate. The root certificate signed itself, hence the name “self-signed certificate.” If you’re using separate keystore and truststore files, your root CA can most likely in the truststore.\nGenerally, the keystore contains client or node certificate and all intermediate certificates, and the truststore contains the root certificate.\nCheck the configured alias\nIf you have multiple entries in the keystore and you are using aliases to refer to them, make sure that the configured alias in opensearch.yml matches the one in the keystore. If there is only one entry in the keystore, you do not need to configure an alias.\nView contents of your keystore and truststore\nIn order to view information about the certificates stored in your keystore or truststore, use the keytool command like: keytool -list -v -keystore keystore.jks keytool prompts for the password of the keystore and lists all entries. For example, you can use this output to check for the correctness of the SAN and EKU settings.\nCheck SAN hostnames and IP addresses\nThe valid hostnames and IP addresses of a TLS certificates are stored as SAN entries. Check that the hostname and IP entries in the SAN section are correct, especially when you use hostname verification: Certificate[1]:\nOwner: CN=node-0.example.com, OU=SSL, O=Test, L=Test, C=DE...\nExtensions:...\n#5: ObjectId: 2.5.29.17 Criticality=false\nSubjectAlternativeName [\nDNSName: node-0.example.com\nDNSName: localhost\nIPAddress: 127.0.0.1...] Check OID for node certificates\nIf you are using OIDs to denote valid node certificates, check that the SAN extension for your node certificate contains the correct OIDName: Certificate[1]:\nOwner: CN=node-0.example.com, OU=SSL, O=Test, L=Test, C=DE...\nExtensions:...\n#5: ObjectId: 2.5.29.17 Criticality=false\nSubjectAlternativeName [...\nOIDName: 1.2.3.4.5.5] Check EKU field for node certificates\nNode certificates need to have both serverAuth and clientAuth set in the extended key usage field: #3: ObjectId: 2.5.29.37 Criticality=false\nExtendedKeyUsages [\nserverAuth\nclientAuth] TLS versions\nThe Security plugin disables TLS version 1.0 by default; it is outdated, insecure, and vulnerable. If you need to use TLSv1 and accept the risks, you can enable it in opensearch.yml: plugins.security.ssl.http.enabled_protocols: - \" TLSv1\" - \" TLSv1.1\" - \" TLSv1.2\" Supported ciphers\nTLS relies on the server and client negotiating a common cipher suite. Depending on your system, the available ciphers will vary. They depend on the JDK or OpenSSL version you’re using, and whether or not the JCE Unlimited Strength Jurisdiction Policy Files are installed.\nFor legal reasons, the JDK does not include strong ciphers like AES256. In order to use strong ciphers you need to download and install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files. If you don’t have them installed, you might see an error message on startup: [INFO] AES-256 not supported, max key length for AES is 128 bit.\nThat is not an issue, it just limits possible encryption strength.\nTo enable AES 256 install 'Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files' The Security plugin still works and falls back to weaker cipher suites. The plugin also prints out all available cipher suites during startup: [INFO] sslTransportClientProvider:\nJDK with ciphers [TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,\nTLS_DHE_DSS_WITH_AES_128_CBC_SHA256,...]",
    "ancestors": [
      "Troubleshooting"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/about/",
    "title": "About OpenSearch",
    "content": "OpenSearch is a distributed search and analytics engine based on Apache Lucene. After adding your data to OpenSearch, you can perform full-text searches on it with all of the features you might expect: search by field, search multiple indices, boost fields, rank results by score, sort results by field, and aggregate results.\nUnsurprisingly, people often use search engines like OpenSearch as the backend for a search application—think Wikipedia or an online store. It offers excellent performance and can scale up and down as the needs of the application grow or shrink.\nAn equally popular, but less obvious use case is log analytics, in which you take the logs from an application, feed them into OpenSearch, and use the rich search and visualization functionality to identify issues. For example, a malfunctioning web server might throw a 500 error 0.5% of the time, which can be hard to notice unless you have a real-time graph of all HTTP status codes that the server has thrown in the past four hours. You can use OpenSearch Dashboards to build these sorts of visualizations from data in OpenSearch.\nClusters and nodes\nIts distributed design means that you interact with OpenSearch clusters. Each cluster is a collection of one or more nodes, servers that store your data and process search requests.\nYou can run OpenSearch locally on a laptop—its system requirements are minimal—but you can also scale a single cluster to hundreds of powerful machines in a data center.\nIn a single node cluster, such as a laptop, one machine has to do everything: manage the state of the cluster, index and search data, and perform any preprocessing of data prior to indexing it. As a cluster grows, however, you can subdivide responsibilities. Nodes with fast disks and plenty of RAM might be great at indexing and searching data, whereas a node with plenty of CPU power and a tiny disk could manage cluster state. For more information on setting node types, see Cluster formation.\nIndices and documents\nOpenSearch organizes data into indices. Each index is a collection of JSON documents. If you have a set of raw encyclopedia articles or log lines that you want to add to OpenSearch, you must first convert them to JSON. A simple JSON document for a movie might look like this: { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } When you add the document to an index, OpenSearch adds some metadata, such as the unique document ID: { \"_index\": \"&lt;index-name&gt;\", \"_type\": \"_doc\", \"_id\": \"&lt;document-id&gt;\", \"_version\": 1, \"_source\": { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } } Indices also contain mappings and settings:\nA mapping is the collection of fields that documents in the index have. In this case, those fields are title and release_date.\nSettings include data like the index name, creation date, and number of shards.\nPrimary and replica shards\nOpenSearch splits indices into shards for even distribution across nodes in a cluster. For example, a 400 GB index might be too large for any single node in your cluster to handle, but split into ten shards, each one 40 GB, OpenSearch can distribute the shards across ten nodes and work with each shard individually.\nBy default, OpenSearch creates a replica shard for each primary shard. If you split your index into ten shards, for example, OpenSearch also creates ten replica shards. These replica shards act as backups in the event of a node failure—OpenSearch distributes replica shards to different nodes than their corresponding primary shards—but they also improve the speed and rate at which the cluster can process search requests. You might specify more than one replica per index for a search-heavy workload.\nDespite being a piece of an OpenSearch index, each shard is actually a full Lucene index—confusing, we know. This detail is important, though, because each instance of Lucene is a running process that consumes CPU and memory. More shards is not necessarily better. Splitting a 400 GB index into 1,000 shards, for example, would place needless strain on your cluster. A good rule of thumb is to keep shard size between 10–50 GB.\nREST API\nYou interact with OpenSearch clusters using the REST API, which offers a lot of flexibility. You can use clients like curl or any programming language that can send HTTP requests. To add a JSON document to an OpenSearch index (i.e. index a document), you send an HTTP request: PUT https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_doc/&lt;document-id&gt; { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } To run a search for the document: GET https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_search?q=wind To delete the document: DELETE https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_doc/&lt;document-id&gt; You can change most OpenSearch settings using the REST API, modify indices, check the health of the cluster, get statistics—almost everything.",
    "ancestors": [
      "OpenSearch documentation"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/breaking-changes/",
    "title": "Breaking changes",
    "content": "2.0.0\nRemove mapping types parameter\nThe type parameter has been removed from all OpenSearch API endpoints. Instead, indexes can be categorized by document type. For more details, see issue #1940.\nDeprecate non-inclusive terms\nNon-inclusive terms are deprecated in version 2.x and will be permanently removed in OpenSearch 3.0. We are using the following replacements:\n“Whitelist” is now “Allow list”\n“Blacklist” is now “Deny list”\n“Master” is now “Cluster Manager”\nAdd OpenSearch Notifications plugins\nIn OpenSearch 2.0, the Alerting plugin is now integrated with new plugins for Notifications. If you want to continue to use the notification action in the Alerting plugin, install the new backend plugins notifications-core and notifications. If you want to manage notifications in OpenSearch Dashboards, use the new notificationsDashboards plugin. For more information, see Questions about destinations on the Monitors page.\nDrop support for JDK 8\nA Lucene upgrade forced OpenSearch to drop support for JDK 8. As a consequence, the Java high-level REST client no longer supports JDK 8. Restoring JDK 8 support is currently an opensearch-java proposal #156 and will require removing OpenSearch core as a dependency from the Java client (issue #262).",
    "ancestors": [
      "OpenSearch documentation"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/",
    "title": "OpenSearch documentation",
    "content": "OpenSearch Documentation Learn to use OpenSearch, the highly scalable and extensible open-source software suite for search, analytics, observability, and other data-intensive applications. Contribute About OpenSearch Quickstart Install OpenSearch Install OpenSearch Dashboards See the FAQ Why use OpenSearch?\nWith OpenSearch, you can perform the following use cases: Fast, Scalable Full-text Search\nApplication and Infrastructure Monitoring\nSecurity and Event Information Management\nOperational Health Tracking\nHelp users find the right information within your application, website, or data lake catalog.\nEasily store and analyze log data, and set automated alerts for underperformance.\nCentralize logs to enable real-time security monitoring and forensic analysis.\nUse observability logs, metrics, and traces to monitor your applications and business in real time. Additional features and plugins: OpenSearch has several features and plugins to help index, secure, monitor, and analyze your data. Most OpenSearch plugins have corresponding OpenSearch Dashboards plugins that provide a convenient, unified user interface. Anomaly detection - Identify atypical data and receive automatic notifications KNN - Find “nearest neighbors” in your vector data Performance Analyzer - Monitor and optimize your cluster SQL - Use SQL or a piped processing language to query your data Index State Management - Automate index operations ML Commons plugin - Train and execute machine-learning models Asynchronous search - Run search requests in the background Cross-cluster replication - Replicate your data across multiple OpenSearch clusters\nThe secure path forward\nOpenSearch includes a demo configuration so that you can get up and running quickly, but before using OpenSearch in a production environment, you must configure the Security plugin manually with your own certificates, authentication method, users, and passwords.\nLooking for the Javadoc?\nSee opensearch.org/javadocs/.\nGet involved OpenSearch is supported by Amazon Web Services. All components are available under the Apache License, Version 2.0 on GitHub.\nThe project welcomes GitHub issues, bug fixes, features, plugins, documentation—anything at all. To get involved, see Contributing on the OpenSearch website. OpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V.",
    "ancestors": [

    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/quickstart/",
    "title": "Quickstart",
    "content": "Get started using OpenSearch and OpenSearch Dashboards by deploying your containers with Docker. Before proceeding, you need to get Docker and Docker Compose installed on your local machine.\nThe Docker Compose commands used in this guide are written with a hyphen (for example, docker-compose). If you installed Docker Desktop on your machine, which automatically installs a bundled version of Docker Compose, then you should remove the hyphen. For example, change docker-compose to docker compose.\nStarting your cluster\nYou’ll need a special file, called a Compose file, that Docker Compose uses to define and create the containers in your cluster. The OpenSearch Project provides a sample Compose file that you can use to get started. Learn more about working with Compose files by reviewing the official Compose specification.\nBefore running OpenSearch on your machine, you should disable memory paging and swapping performance on the host to improve performance and increase the number of memory maps available to OpenSearch. See important system settings for more information. # Disable memory paging and swapping. sudo swapoff -a # Edit the sysctl config file that defines the host's max map count. sudo vi /etc/sysctl.conf # Set max map count to the recommended value of 262144. vm.max_map_count = 262144 # Reload the kernel parameters. sudo sysctl -p Download the sample Compose file to your host. You can download the file with command line utilities like curl and wget, or you can manually copy docker-compose.yml from the OpenSearch Project documentation-website repository using a web browser. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/docker-compose.yml # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/docker-compose.yml In your terminal application, navigate to the directory containing the docker-compose.yml file you just downloaded, and run the following command to create and start the cluster as a background process. docker-compose up -d Confirm that the containers are running with the command docker-compose ps. You should see an output like the following: $ docker-compose ps\nNAME COMMAND SERVICE STATUS PORTS\nopensearch-dashboards \"./opensearch-dashbo…\" opensearch-dashboards running 0.0.0.0:5601-&gt;5601/tcp\nopensearch-node1 \"./opensearch-docker…\" opensearch-node1 running 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9650/tcp\nopensearch-node2 \"./opensearch-docker…\" opensearch-node2 running 9200/tcp, 9300/tcp, 9600/tcp, 9650/tcp Query the OpenSearch REST API to verify that the service is running. You should use -k (also written as --insecure) to disable host name checking because the default security configuration uses demo certificates. Use -u to pass the default username and password ( admin:admin). curl https://localhost:9200 -ku admin:admin Sample response: { \"name\": \"opensearch-node1\", \"cluster_name\": \"opensearch-cluster\", \"cluster_uuid\": \"W0B8gPotTAajhMPbC9D4ww\", \"version\": { \"distribution\": \"opensearch\", \"number\": \"2.6.0\", \"build_type\": \"tar\", \"build_hash\": \"7203a5af21a8a009aece1474446b437a3c674db6\", \"build_date\": \"2023-02-24T18:58:37.352296474Z\", \"build_snapshot\": false, \"lucene_version\": \"9.5.0\", \"minimum_wire_compatibility_version\": \"7.10.0\", \"minimum_index_compatibility_version\": \"7.0.0\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Explore OpenSearch Dashboards by opening http://localhost:5601/ in a web browser on the same host that is running your OpenSearch cluster. The default username is admin and the default password is admin.\nCreate an index and field mappings using sample data\nCreate an index and define field mappings using a dataset provided by the OpenSearch Project. The same fictitious e-commerce data is also used for sample visualizations in OpenSearch Dashboards. To learn more, see Getting started with OpenSearch Dashboards.\nDownload ecommerce-field_mappings.json. This file defines a mapping for the sample data you will use. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/ecommerce-field_mappings.json # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/ecommerce-field_mappings.json Download ecommerce.json. This file contains the index data formatted so that it can be ingested by the bulk API. To learn more, see index data and Bulk. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/ecommerce.json # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.7/assets/examples/ecommerce.json Define the field mappings with the mapping file. curl -H \"Content-Type: application/x-ndjson\" -X PUT \"https://localhost:9200/ecommerce\" -ku admin:admin --data-binary \"@ecommerce-field_mappings.json\" Upload the index to the bulk API. curl -H \"Content-Type: application/x-ndjson\" -X PUT \"https://localhost:9200/ecommerce/_bulk\" -ku admin:admin --data-binary \"@ecommerce.json\" Query the data using the search API. The following command submits a query that will return documents where customer_first_name is Sonya. curl -H 'Content-Type: application/json' -X GET \"https://localhost:9200/ecommerce/_search?pretty=true\" -ku admin:admin -d ' {\"query\":{\"match\":{\"customer_first_name\":\"Sonya\"}}}' Queries submitted to the OpenSearch REST API will generally return a flat JSON by default. For a human readable response body, use the query parameter pretty=true. For more information about pretty and other useful query parameters, see Common REST parameters.\nAccess OpenSearch Dashboards by opening http://localhost:5601/ in a web browser on the same host that is running your OpenSearch cluster. The default username is admin and the default password is admin.\nOn the top menu bar, go to Management &gt; Dev Tools.\nIn the left pane of the console, enter the following: GET ecommerce/_search { \"query\": { \"match\": { \"customer_first_name\": \"Sonya\" } } } Choose the triangle icon at the top right of the request to submit the query. You can also submit the request by pressing Ctrl+Enter (or Cmd+Enter for Mac users). To learn more about using the OpenSearch Dashboards console for submitting queries, see Running queries in the console.\nNext steps\nYou successfully deployed your own OpenSearch cluster with OpenSearch Dashboards and added some sample data. Now you’re ready to learn about configuration and functionality in more detail. Here are a few recommendations on where to begin: About the Security plugin OpenSearch configuration OpenSearch plugin installation Getting started with OpenSearch Dashboards OpenSearch tools Index APIs Common issues\nReview these common issues and suggested solutions if your containers fail to start or exit unexpectedly.\nDocker commands require elevated permissions\nEliminate the need for running your Docker commands with sudo by adding your user to the docker user group. See Docker’s Post-installation steps for Linux for more information. sudo usermod -aG docker $USER Error message: “-bash: docker-compose: command not found”\nIf you installed Docker Desktop, then Docker Compose is already installed on your machine. Try docker compose (without the hyphen) instead of docker-compose. See Use Docker Compose.\nError message: “docker: ‘compose’ is not a docker command.”\nIf you installed Docker Engine, then you must install Docker Compose separately, and you will use the command docker-compose (with a hyphen). See Docker Compose.\nError message: “max virtual memory areas vm.max_map_count [65530] is too low”\nOpenSearch will fail to start if your host’s vm.max_map_count is too low. Review the important system settings if you see the following errors in the service log, and set vm.max_map_count appropriately. opensearch-node1 | ERROR: [ 1] bootstrap checks failed\nopensearch-node1 | [ 1]: max virtual memory areas vm.max_map_count [ 65530] is too low, increase to at least [ 262144]\nopensearch-node1 | ERROR: OpenSearch did not exit normally - check the logs at /usr/share/opensearch/logs/opensearch-cluster.log",
    "ancestors": [
      "OpenSearch documentation"
    ],
    "type": "DOCS"
  },
  {
    "url": "/docs/latest/version-history/",
    "title": "Version history",
    "content": "OpenSearch version Release highlights Release date 2.7.0 Includes searchable snapshots and segment replication, which are now generally available. Adds multiple data sources, observability features, dynamic tenant management, component templates, and shape-based map filters in OpenSearch Dashboards. Includes the flat object field type, hot shard identification, and a new automatic reloading mechanism for ML models. For a full list of release highlights, see the Release Notes.\n02 May 2023 2.6.0 Includes simple schema for observability, index management UI enhancements, Security Analytics enhancements, search backpressure at the coordinator node level, and the ability to add maps to dashboards. Experimental features include a new ML model health dashboard, new text embedding models in ML, and SigV4 authentication in Dashboards. For a full list of release highlights, see the Release Notes.\n28 February 2023 2.5.0 Includes index management UI enhancements, multi-layer maps, Jaeger support for observability, Debian distributions, returning cluster health by awareness attribute, cluster manager task throttling, weighted zonal search request routing policy, and query string support in index rollups. Experimental features include request-level durability in remote-backed storage and GPU acceleration for ML nodes. For a full list of release highlights, see the Release Notes.\n24 January 2023 2.4.1 Includes maintenance changes and bug fixes for gradle check and indexing pressure tests. Adds support for skipping changelog.\n13 December 2022 2.4.0 Includes Windows support, Point-in-time search, custom k-NN filtering, xy_point and xy_shape field types for Cartesian coordinates, GeoHex grid aggregation, and resilience enhancements, including search backpressure. In OpenSearch Dashboards, this release adds snapshot restore functionality, multiple authentication, and aggregate view of saved objects. This release includes the following experimental features: searchable snapshots, Compare Search Results, multiple data sources in OpenSearch Dashboards, a new Model Serving Framework in ML Commons, a new Neural Search plugin that supports semantic search, and a new Security Analytics plugin to analyze security logs. For a full list of release highlights, see the Release Notes.\n15 November 2022 2.3.0 This release includes the following experimental features: segment replication, remote-backed storage, and drag and drop for OpenSearch Dashboards. Experimental features allow you to test new functionality in OpenSearch. Because these features are still being developed, your testing and feedback can help shape the development of the feature before it’s official released. We do not recommend use of experimental features in production. Additionally, this release adds maketime and makedate datetime functions for the SQL plugin. Creates a new OpenSearch Playground demo site for OpenSearch Dashboards. For a full list of release highlights, see the Release Notes.\n14 September 2022 2.2.1 Includes gradle updates and bug fixes for gradle check.\n01 September 2022 2.2.0 Includes support for Logistic Regression and RCF Summarize machine learning algorithms in ML Commons, Lucene or C-based Nmslib and Faiss libraries for approximate k-NN search, search by relevance using SQL and PPL queries, custom region maps for visualizations, and rollup enhancements. For a full list of release highlights, see the Release Notes.\n11 August 2022 2.1.0 Includes support for dedicated ML node in the ML Commons plugin, relevance search and other features in SQL, multi-terms aggregation, and Snapshot Management. For a full list of release highlights, see the Release Notes.\n07 July 2022 2.0.1 Includes bug fixes and maintenance updates for Alerting and Anomaly Detection.\n16 June 2022 2.0.0 Includes document-level monitors for alerting, OpenSearch Notifications plugins, and Geo Map Tiles in OpenSearch Dashboards. Also adds support for Lucene 9 and bug fixes for all OpenSearch plugins. For a full list of release highlights, see the Release Notes.\n26 May 2022 2.0.0-rc1 The Release Candidate for 2.0.0. This version allows you to preview the upcoming 2.0.0 release before the GA release. The preview release adds document-level alerting, support for Lucene 9, and the ability to use term lookup queries in document level security.\n03 May 2022 1.3.9 Adds Debian support. Includes upgrades, enhancements, and maintenance updates for OpenSearch core, k-NN, and OpenSearch security.\n16 March 2023 1.3.8 Adds OpenSearch security enhancements. Updates tool scripts to run on Windows. Includes maintenance updates and bug fixes for Anomaly Detection and OpenSearch security.\n02 February 2023 1.3.7 Adds Windows support. Includes maintenance updates and bug fixes for error handling.\n13 December 2022 1.3.6 Includes maintenance updates and bug fixes for tenancy in the OpenSearch Security Dashboards plugin.\n06 October 2022 1.3.5 Includes maintenance updates and bug fixes for gradle check and OpenSearch security.\n01 September 2022 1.3.4 Includes maintenance updates and bug fixes for OpenSearch and OpenSearch Dashboards.\n14 July 2022 1.3.3 Adds enhancements to Anomaly Detection and ML Commons. Bug fixes for Anomaly Detection, Observability, and k-NN.\n09 June 2022 1.3.2 Bug fixes for Anomaly Detection and the Security Dashboards Plugin, adds the option to install OpenSearch using RPM, as well as enhancements to the ML Commons execute task, and the removal of the job-scheduler zip in Anomaly Detection.\n05 May 2022 1.3.1 Bug fixes when using document-level security, and adjusted ML Commons to use the latest RCF jar and protostuff to RCF model serialization.\n30 March 2022 1.3.0 Adds Model Type Validation to Validate Detector API, continuous transforms, custom actions, applied policy parameter to Explain API, default action retries, and new rollover and transition conditions to Index Management, new ML Commons plugin, parse command to SQL, Application Analytics, Live Tail, Correlation, and Events Flyout to Observability, and auto backport and support for OPENSEARCH_JAVA_HOME to Performance Analyzer. Bug fixes.\n17 March 2022 1.2.4 Updates Performance Analyzer, SQL, and Security plugins to Log4j 2.17.1, Alerting and Job Scheduler to cron-utils 9.1.6, and gson in Anomaly Detection and SQL.\n18 January 2022 1.2.3 Updates the version of Log4j used in OpenSearch to Log4j 2.17.0 as recommended by the advisory in CVE-2021-45105.\n22 December 2021 1.2.0 Adds observability, new validation API for Anomaly Detection, shard-level indexing back-pressure, new “match” query type for SQL and PPL, support for Faiss libraries in k-NN, and custom Dashboards branding.\n23 November 2021 1.1.0 Adds cross-cluster replication, security for Index Management, bucket-level alerting, a CLI to help with upgrading from Elasticsearch OSS to OpenSearch, and enhancements to high cardinality data in the anomaly detection plugin.\n05 October 2021 1.0.1 Bug fixes.\n01 September 2021 1.0.0 General availability release. Adds compatibility setting for clients that require a version check before connecting.\n12 July 2021 1.0.0-rc1 First release candidate.\n07 June 2021 1.0.0-beta1 Initial beta release. Refactors plugins to work with OpenSearch.\n13 May 2021",
    "ancestors": [
      "OpenSearch documentation"
    ],
    "type": "DOCS"
  }
]
