[
  {
    "url": "/blog/Check-out-earlier-blogposts-on-Open-Distro-for-Elasticsearch/",
    "title": "Check out earlier blog posts on Open Distro for Elasticsearch",
    "content": "Hi Open Distro blog readers,\nCheck out earlier blogs posts for Open Distro for Elasticsearch which were published on the AWSOpen blog. Going forward, bookmark the Open Distro blog as your source for in-depth, technical articles on the project. You‚Äôll find how-tos, release notes, case studies, event announcements, and if you‚Äôre inspired to contribute, your own content, too! Add Single Sign-On (SSO) to Open Distro for Elasticsearch Kibana Using SAML and Okta by Jagadeesh Pusapadi, Aug 16 2019\nOpen Distro for Elasticsearch Security implements the web browser single sign-on (SSO) profile of the SAML 2.0 protocol. This enables you to configure federated access with any SAML 2.0 compliant identity provider (IdP). In a prior post, I discussed setting up SAML-based SSO using Microsoft Active Directory Federation Services (ADFS). In this post, I‚Äôll cover the Okta-specific configuration. Read more Demystifying Elasticsearch Shard Allocation by Vigya Sharma and Jon Handler, Aug 13 2019\nAt the core of Open Distro for Elasticsearch‚Äôs ability to provide a seamless scaling experience, lies its ability distribute its workload across machines. This is achieved via sharding. When you create an index you set a primary and replica shard count for that index. Elasticsearch distributes your data and requests across those shards, and the shards across your data nodes. Read more Open Distro for Elasticsearch 1.1.0 Released by Alolita Sharma and Jon Handler, Aug 07 2019\nWe are happy to announce that Open Distro for Elasticsearch 1.1.0 is now available for download! Version 1.1.0 includes the upstream open source versions of Elasticsearch 7.1.1, Kibana 7.1.1, and the latest updates for alerting, SQL, security, performance analyzer, and Kibana plugins, as well as the SQL JDBC driver. You can find details on enhancements, bug fixes, and more in the release notes for each plugin in their respective GitHub repositories. Read more Use Elasticsearch‚Äôs rollover API For Efficient Storage Distribution by Jon Handler, Aug 06 2019\nMany Open Distro for Elasticsearch users manage data life cycle in their clusters by creating an index based on a standard time period, usually one index per day. This pattern has many advantages - ingest tools like Logstash support index rollover out of the box; defining a retention window is straightforward; and deleting old data is as simple as dropping an index. Read more Add Single Sign-On to Open Distro for Elasticsearch Kibana Using SAML and ADFS by Jagadeesh Pusapadi, Aug 02 2019\nOpen Distro for Elasticsearch Security (Open Distro Security) comes with authentication and access control out of the box. Prior posts have discussed LDAP integration with Open Distro for Elasticsearch and JSON Web Token authentication with Open Distro for Elasticsearch. Read more Open Distro for Elasticsearch version 1.0.0 is now available by Alolita Sharma, Jul 01 2019\nOpen Distro for Elasticsearch 1.0.0 is now available for you to download and run! The 1.0.0 release includes Elasticsearch 7.0.1 and Kibana 7.0.1 from upstream and the latest versions of the Open Distro for Elasticsearch plugins for alerting, performance analyzer, SQL, and security. The Kibana UI components for security and alerting are also part of this release. You can find Open Distro for Elasticsearch Docker images and Kibana Docker images on Docker Hub. Read more Manage Your Open Distro for Elasticsearch Alerting Monitors With odfe-monitor-cli by Mihir Soni, Jun 12 2019\nWhen you use Open Distro for Elasticsearch Alerting, you create monitors in Kibana. Setting up monitors with a UI is fast and convenient, making it easy to get started. If monitoring is a major workload for your cluster, though, you may have hundreds or even thousands of monitors to create, update, and tune over time. Setting so many monitors using the Kibana UI would be time-consuming and tedious. Read more Set up Multi-Tenant Kibana Access in Open Distro for Elasticsearch by Jon Handler, Jun 04 2019\nElasticsearch has become a default choice for storing and analyzing log data to deliver insights on your application‚Äôs performance, your security stance, and your users‚Äô interactions with your application. It‚Äôs so useful that many teams adopt Elasticsearch early in their development cycle to support DevOps. This grass-roots adoption often mushrooms into a confusing set of clusters and users across a large organization. Read more New! Open Distro for Elasticsearch‚Äôs Job Scheduler Plugin by Alolita Sharma and Jon Handler, May 29 2019\nOpen Distro for Elasticsearch‚Äôs JobScheduler plugin provides a framework for developers to accomplish common, scheduled tasks on their cluster. You can implement Job Scheduler‚Äôs Service Provider Interface (SPI) to take snapshots, manage your data‚Äôs lifecycle, run periodic jobs, and much more. When you use Job Scheduler, you build a plugin that implements interfaces provided in the Job Scheduler library. Read more Store Open Distro for Elasticsearch‚Äôs Performance Analyzer Output in Elasticsearch by Jon Handler, May 25 2019\nOpen Distro for Elasticsearch‚Äòs Performance Analyzer plugin exposes a REST API that returns metrics from your Elasticsearch cluster. To get the most out of these metrics, you can store them in Elasticsearch and use Kibana to visualize them. While you can use Open Distro for Elasticsearch‚Äôs PerfTop to build visualizations, PerfTop doesn‚Äôt retain data and is meant to be lightweight. Read more Use JSON Web Tokens (JWTs) to Authenticate in Open Distro for Elasticsearch and Kibana by Neeraj Prashar and Jon Handler, May 23 2019\nToken-based authentication systems are popular in the world of web services. They provide many benefits, including (but not limited to) security, scalability, statelessness, and extensibility. With Amazon‚Äôs Open Distro for Elasticsearch, users now have an opportunity to take advantage of the numerous security features included in the Security plugin. One such feature is the ability to authenticate users with JSON Web Tokens (JWT) for a single sign-on experience. Read more Build Your Own - Open Distro for Elasticsearch Build Scripts Now Available by Alolita Sharma, May 16 2019\nWant to craft your own Docker images using Open Distro for Elasticsearch build scripts? Or build your RPM or Debian packages to customize your own Open Distro for Elasticsearch stack? Our build scripts for Elasticsearch and for Kibana are now available for you to do just that. Read more Running Open Distro for Elasticsearch on Kubernetes by Saad Rana, May 15 2019\nThis post is a walk-through on deploying Open Distro for Elasticsearch on Kubernetes as a production-grade deployment. Read more Run Rally with Open Distro for Elasticsearch by Atri Sharma and Jon Handler, May 09 2019\nIt‚Äôs hard to size and scale an Elasticsearch cluster. You need to have sufficient storage for your data, but your mappings and the contents of the data are key components to your data‚Äôs size on disk. You need capacity for your queries and updates, but the amounts of CPU, JVM, disk, and network bandwidth you use are critically dependent on the queries you run and the updates you send. Read more Open Distro for Elasticsearch 0.9.0 Now Available by Alolita Sharma, May 03 2019\nThe 0.9.0 release includes Elasticsearch 6.7.1, Kibana 6.7.1 from upstream and the latest Open Distro plugins which include the alerting plugin, the performance analyzer, the SQL plugin and the security plugin. The Kibana UI components for the Open Distro plugins are also part of this release. You can find the details on enhancements, bug fixes, and more in the release notes for each plugin in their respective GitHub repository. Read more LDAP Integration for Open Distro for Elasticsearch by Jagadeesh Pusapadi, Apr 19 2019\nOpen Distro for Elasticsearch‚Äôs security plugin comes with authentication and access control out of the box. In prior posts we showed how you can change your admin password in Open Distro for Elasticsearch and how you can add your own SSL certificates to Open Distro for Elasticsearch. Read more Open Distro for Elasticsearch Debian Packages Now Available for Version 0.8.0 by Allen Yin and Alolita Sharma, Apr 17 2019\nYou can now download Open Distro for Elasticsearch version 0.8.0 for Debian and Ubuntu environments. Open Distro for Elasticsearch 0.8.0 is built on the Apache 2.0 licensed versions of Elasticsearch and Kibana 6.6.2. See the Open Distro for Elasticsearch downloads page for instructions on how to download and install the.deb packages. Read more Lightweight Debugging with Performance Analyzer and PerfTop in Open Distro for Elasticsearch by Jon Handler, Apr 12 2019\nWhen you want to monitor your Elasticsearch cluster or debug an issue, you have a number of choices. You can use the various cat and stats APIs to pull information out of the cluster. You can monitor and profile the JVM itself. These options can be cumbersome, and they lack visual displays. While you could push cat and stats data back into Elasticsearch and visualize with Kibana, sometimes you want a more lightweight method. Read more New Release! Open Distro for Elasticsearch version 0.8.0 by Alolita Sharma, Apr 10 2019\nOpen Distro for Elasticsearch 0.8.0 is now available for you to download and run. Release highlights include support for Elasticsearch 6.6.2, Kibana 6.6.2 and various minor enhancements and bug fixes for the plugins. The alerting plugin has been updated to the latest Kotlin version 1.3. The SQL plugin index pattern queries have been fixed in the JDBC driver. Read more Use Open Distro for Elasticsearch to Alert on Security Events by Shivang Doshi, Apr 05 2019\nOpen Distro for Elasticsearch‚Äôs Security plugin ships with the capability to create an audit log to track access to your cluster. You can surface various types of audit events like authentications, and failed logins. In a prior post, we covered the basics of setting an alert in Open Distro for Elasticsearch. Read more Set an Alert in Open Distro for Elasticsearch by Jon Handler, Apr 03 2019\nOne of Elasticsearch‚Äôs primary use cases is log analytics - you collect logs from your infrastructure, transform each log line into JSON documents, and send those documents to Elasticsearch‚Äôs bulk API. A transformed log line contains many fields, each containing values. Read more Add Your Own SSL Certificates to Open Distro for Elasticsearch by Jagadeesh Pusapadi and Jon Handler, Mar 29 2019\nOpen Distro for Elasticsearch‚Äôs security plugin comes with authentication and access control out of the box. To make it easy to get started, the binary distributions contain passwords and SSL certificates that let you try out the plugin. Read more Build and Run the Open Distro For Elasticsearch SQL Plugin with Elasticsearch OSS by Jon Handler, Mar 27 2019\nOpen Distro for Elasticsearch comprises four plugins - SQL, Security, Alerting and Performance. In this blog post, we start with the SQL plugin. Other plugins have different codebases and compilation methods. Stay tuned! Read more Change your Admin Passwords in Open Distro for Elasticsearch by Jon Handler, Mar 21 2019\nOpen Distro for Elasticsearch ships with an advanced security plugin. The plugin comes pre-configured with a number of different users and default passwords for them ‚Äì of course, you will want to change those defaults! Passwords for some of the preconfigured users‚Äîkibanaro, logstash, readall, and snapshotrestore‚Äîare available to change in the Security UI in Kibana. The admin and kibanaserver users are set to read-only, and must be changed in the configuration files. Read more Get Up and Running with Open Distro for Elasticsearch by Jon Handler, Mar 19 2019\nOn March 11, 2019, we released Open Distro for Elasticsearch, a value-added distribution of Elasticsearch that is 100% open source (Apache 2.0 license) and supported by AWS. Read more Enjoy!",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Welcome-to-the-Open-Distro-for-Elasticsearch-Blog/",
    "title": "Welcome to the Open Distro for Elasticsearch Blog!",
    "content": "Welcome to the new Open Distro for Elasticsearch blog. Open Distro for Elasticsearch is an Apache 2.0-licensed distribution of Elasticsearch and Kibana. This blog will be a source for in-depth, technical content on the project. You‚Äôll find how-tos, release notes, case studies, event announcements, and if you‚Äôre inspired, your own content, too! Or you can join the conversation by commenting on blog posts in the discussion forum.\nWhether you are using Open Distro for Elasticsearch out of the box or building on top of it, we invite you to write about your experience as a guest blogger. Contributing is a great way to share your expertise with the community, spark new conversations, and further the project‚Äôs mission. Send us your article idea and a brief abstract and get started on your first guest post. We have a handful of editors who can help take your post from rough draft to finished product.\nNeed inspiration? Here are a few of our most popular blog posts: Open Distro 1.1.0 released Add Your Own SSL Certificates to Open Distro for Elasticsearch Demystifying Elasticsearch Shard Allocation Add Single Sign-On to Open Distro for Elasticsearch Kibana Using SAML and ADFS Add Single Sign-On (SSO) to Open Distro for Elasticsearch Kibana Using SAML and Okta Running Open Distro for Elasticsearch on Kubernetes Setting up multi-tenant Kibana access in Open Distro for Elasticsearch We can‚Äôt wait to highlight all of the use cases and innovations that Open Distro for Elasticsearch enables. Happy blogging!\nAlolita Sharma, Jon Handler, and the entire Open Distro for Elasticsearch team",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Open-Distro-for-Elasticsearch-1.2.0-released/",
    "title": "Open Distro for Elasticsearch 1.2.0 Released",
    "content": "Open Distro for Elasticsearch 1.2.0 is now available with new Linux tarball packages. What‚Äôs Included in 1.2.0 Release 1.2.0 includes upstream open source versions of Elasticsearch 7.2.0, Kibana 7.2.0 and all Open Distro plugins including alerting, performance analyzer, security, SQL, Kibana plugins for alerting, security, SQL, and the SQL JDBC driver. It also includes PerfTop, a client for Performance Analyzer. You can find details on enhancements, bug fixes, and more in the release notes for each plugin on GitHub. See Open Distro‚Äôs version history table if you need to use a previous release of the distribution. Download the Latest Packages Docker images for this release of Open Distro for Elasticsearch and Kibana can be downloaded from Docker Hub. If you are using Docker, make sure your compose file specifies 1.2.0 or uses the latest tag. Additionally, RPMs and Debian packages are available for installation. You can download the PerfTop client here and our SQL JDBC driver here. You can also find our Security plugin artifacts on Maven Central.\nAlso check out the new Open Distro for Elasticsearch Linux tarball for version 1.2.0 as well as the Linux tarball for Open Distro Kibana version 1.2.0. Release Details ALERTING Cleanup ElasticThreadContextElement (# 95)\nDon‚Äôt allow interval to be set with 0 or negative values (# 92)\nUpdate execute API to keep thread context. Use the ElasticThreadContextElement when executing a monitor to preserve the context variables needed (# 90) ALERTING KIBANA UI Bump fstream from 1.0.11 to 1.0.12 (# 82) PERFORMANCE ANALYZER Add RCA RFC (#72)\nReorder imports, refactor unit tests\nFix unit tests on Mac. Fix Null Pointer Exception during MasterServiceEventMetrics collection\nFix NullPointerException when Performance Analyzer starts collecting metrics before master node is fully up SECURITY Make permissions for protected index equal to that of the security index. Protected Index Kibana Fix 1.1 (# 132)\nAdd ability to block indices and index patterns to certain roles, adding another level of protection for these indices. Ability to protect indices even further. (# 126)\nInitialize opendistro index if injected user enabled. (# 125)\nFix security configuration\nBump com.fasterxml.jackson.core to version 2.9.9.2 SECURITY ADVANCED MODULES Add supporting changes for protected index. Changes to support PrivilegesEvaluator in OpenDistroSecurityFlsDlsIndexSearcherWrapper. (# 37)\nFix API endpoint naming\nFix security configuration\nBump com.fasterxml.jackson.core to version 2.9.9.2 SECURITY KIBANA UI Fixed incorrect argument order when calling build.sh\nFix password validation error\nAdd ability to configure logout_url for 1.2 (# 82)\nFix API endpoint naming SQL Support vanilla LEFT JOIN on nested docs (# 167)\nAdd parent for SQLExpr in AST if missing (# 180)\nIgnore easily broken test on join limit hint (# 181)\nSupport using attributes aliases in nested query where condition (# 178)\nAdded 2 new functions LOWER and UPPER that receive field name and locale (# 177)\nChanged identifier generation strategy to id per function name instead of global id (# 128)\nAdded ability to have aliases for ORDER BY and GROUP BY expressions (# 171)\nRemoved timeouts from flaky tests, replacing them with mocked clock to check invariants (# 172)\nFix for inlines corresponding to fields and expressions in parser and AggregationQueryAction (# 162)\nInline ORDER BY expressions (# 168)\nEnhance ORDER BY to support cases (# 158)\nReturn all fields when * and fieldName are selected (# 165)\nAdding left-out import statement from resolving conflict during merge (# 164)\nSupports queries with WHERE clauses that have True/False in the condition (# 157)\nEnabled checkstyle and fixed the issues for the code to build (# 163)\nMore records in aggregation query output for script functions (# 160, # 156)\nAdded support for PERCENTILES in JDBC driver; Fix for #26 (# 146)\nFix single condition results for text+keyword field for nested query (# 135)\nAdded.vscode and build/ to.gitignore (# 139)\nSupport IN predicate subquery (# 126)\nFix bug, terminate integTestCluster even when integration test failed (# 133)\nFixed unit test failure that was identified on a Jenkins: date format needs to be in UTC for proper comparison (# 130) SQL JDBC Support customer AWS credential providers (# 22) JOB SCHEDULER, PERFTOP, SECURITY PARENT No changes.\nYou can find the latest release notes for each component at these URLs: Alerting, Alerting Kibana UI, Performance Analyzer, PerfTop, Security, Security Kibana UI, SQL, SQL JDBC driver, and Job Scheduler. Features In Development Check out and contribute to features in development! You can also download and use the development versions of these plugins to test, experiment, and provide feedback. k-NN search Index Management Index Management Kibana UI RCA Design RFC is now open for comments We‚Äôd love to get your feedback on the Performance Analyzer Root Cause Analysis (RCA) design RFC. Review it here. You can provide comments here. Questions? Please feel free to ask questions on the project community discussion forum. I also invite you to help answer questions on the forums for other community members to learn from. Report a bug or request a feature? You can file bugs, request features, or propose new ideas to enhance Open Distro on our GitHub community issues page. If you find bugs or want to propose a feature for a particular plug-in, you can go to the specific repo and file an issue on the plug-in repo. Getting Started? If you‚Äôre getting started on building your open source contribution karma, you can select an issue tagged as a ‚ÄúGood First Issue‚Äù to start contributing to Open Distro for Elasticsearch. There is extensive technical documentation on the project website to help you get started.\nGo build with Open Distro for Elasticsearch! üöÄ",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Introducing-Root-Cause-Analysis-with-Open-Distro-for-Elasticsearch/",
    "title": "Introducing Root Cause Analysis with Open Distro for Elasticsearch",
    "content": "If you‚Äôre interested in the operational behavior of your Elasticsearch cluster, then root cause analysis can help you identify fundamental issues that affect availability and performance of the cluster. Root cause analysis (RCA) is a problem solving technique used to examine symptoms of problems you‚Äôre interested in solving and to work backwards from those symptoms to the causes of the problems.\nWe are building a root cause analysis engine for Open Distro for Elasticsearch. This smart engine along with Performance Analyzer will help users improve availability and performance of their Elasticsearch clusters. While we design the root cause analysis engine, you can weigh in with your feedback on the design proposal. We‚Äôd love it if you add your comments and use cases so that Open Distro for Elasticsearch will support your needs too!\nOpen Distro for Elasticsearch comes with a Performance Analyzer plugin that helps compute and expose diagnostic metrics for Elasticsearch clusters. This useful tool enables Elasticsearch users to measure and understand bottlenecks in their clusters. Open Distro for Elasticsearch also bundles a light weight client - PerfTop. You can learn more about debugging with PerfTop which provides real time visualization of Performance Analyzer‚Äôs diagnostic metrics. The root cause analysis framework extends on the Performance Analyzer architecture by building a data flow graph that computes root causes.\nIf you have any questions, please feel free to reach out to us. You can tag us on GitHub (Alolita @alolita, Partha @aesgithub, Balaji @sendkb) with your questions.\nWe anticipate starting development on RCA framework in the next couple of weeks. We invite the community to collaborate with us on building this framework, and in making Elasticsearch manageability more seamless.\nWith your interest and feedback, the root cause analysis framework will become a valuable tool for everyone using Open Distro for Elasticsearch.\nLook forward to your feedback!\nAlolita Sharma, Partha Kanuparthy, Balaji Kannan",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Open-Distro-for-Elasticsearch-1.2.1-released/",
    "title": "Open Distro for Elasticsearch 1.2.1 Released",
    "content": "Open Distro for Elasticsearch version 1.2.1 is available for download. What‚Äôs included in version 1.2.1 Version 1.2.1 includes upstream open source versions of Elasticsearch 7.2.1, Kibana 7.2.1, and all Open Distro plugins including Alerting, Performance Analyzer, Security, SQL, and corresponding Kibana plugins too. A SQL JDBC driver and PerfTop, a client for Performance Analyzer, also comes with the distribution. You can find details on enhancements, bug fixes, and new features in the release notes for each plugin on GitHub. See Open Distro‚Äôs version history table if you need to use a previous release of the distribution. Download the latest packages Docker images for this release of Open Distro for Elasticsearch and Kibana can be downloaded from Docker Hub. If you are using Docker, make sure your compose file specifies 1.2.1 or uses the latest tag. Additionally, RPM, Debian and Linux tarball packages are available for installation. You can download the PerfTop client here and our SQL JDBC driver here. You can also find our Security plugin artifacts on Maven Central. As many of you may already have discovered, the 1.2.1 artifacts and packages have been available for download since early November. Release details ALERTING Update build.gradle and release notes for v1.2.0.1 (#105) Upgrade to ES 7.2.1 (#110) Alerting Anomaly detection integration * Update plugin version * Fix parse exception of ad result response (#132) ALERTING KIBANA UI Update package.json and release notes (#99) Alerting Anomaly detection integration * PERFORMANCE ANALYZER Update PerformanceAnalyzer to support Elasticsearch v7.2.1 * Merge pull request (#76) SECURITY Support for ES 7.2.1 (#153) Extended proxy authenticator to pass additional attributes via header (#174) SECURITY ADVANCED MODULES Support for ES 7.2.1 (#49) SECURITY PARENT Support for ES 7.2.1 (#26) SECURITY KIBANA UI Support for ES 7.2.1 (#106) Update opendistro_security version to 7.2.1 * Update Open Distro version 1.2.1.0 for security kibana plugin (#107) SQL Support for Open Distro 1.2.1 (#219) SQL JDBC Changed version to 1.2.1 and updated release notes (#27) Update version in version.java JOB SCHEDULER Support Elasticsearch 7.2.1 (#27) PERFTOP Update PerfTop to support OpenDistro 1.2.1 release * Update version 1.2.1 on package.json * Fix vulnerabilities * Merge pull request (#31) You can find the latest release notes for each component at these URLs: Alerting, Alerting Kibana UI, Performance Analyzer, PerfTop, Security, Security Kibana UI, SQL, SQL JDBC driver, and Job Scheduler. Features in development Check out and contribute to features in development! You can also download and use the development versions of these plugins to test, experiment, and provide feedback. k-NN Index Management Index Management Kibana UI Anomaly Detection Anomaly Detection Kibana UI RCA design RFC needs your feedback We‚Äôd love to get your feedback on the Performance Analyzer Root Cause Analysis (RCA) design RFC. Review it here. You can provide comments here. Questions? Please feel free to ask questions on the project community discussion forum. I also invite you to help answer questions on the forums for other community members. We also have a 30-minute online community meeting every two weeks that you can join to ask questions. Report a bug or request a feature? You can file bugs, request features, or propose new ideas to enhance Open Distro on our GitHub community issues page. If you find bugs or want to propose a feature for a particular plug-in, you can go to the specific repo and file an issue on the plug-in repo. Getting Started? If you‚Äôre getting started on building your open source contribution karma, you can select an issue tagged as a ‚ÄúGood First Issue‚Äù to start contributing to Open Distro for Elasticsearch. There is extensive technical documentation on the project website to help you get started.\nFeel free to reach out to me (alolitas) if you have any questions. Thanks for using Open Distro for Elasticsearch. Go build with Open Distro!",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/random-cut-forests/",
    "title": "Random Cut Forests",
    "content": "We plan to publish a series of blogs discussing the science and math behind Random Cut Forests (RCF). In this first post we begin with a brief overview of Random Forests as a class of Machine Learning model. Next, we introduce RCFs and highlight their relevance to streaming data. Finally, we close with a look to the future, giving ideas for how RCFs support an Online Learning approach to a variety of statistical problems.\nRandom Forests in Machine Learning\nRandom Forests (RF) [3, 4] are a well-known ML technique consisting of an ensemble of tree-based models, where each tree is typically trained on a random subset of data and features. Individual trees are built up recursively using a partitioning rule, where each non-leaf node in the tree corresponds to a rule that splits or partitions the training data into disjoint subsets. When a tree is used to score a new point, we traverse a path from the tree root to a leaf node by applying those same rules to that point. In the leaf node we assign a score value to the point based on the assumption that, because of our partitioning rule, the new point is similar to the training points that were assigned to that leaf node. Thus, the key ingredients of the RF model are the use of random subsets of training data, which makes the model robust with little tuning by the user, and the choice of the splitting or partitioning rule, which determines how new data points are matched to similar points in the training data. In a traditional RF the partitioning rule is defined by the Classification and Regression Tree (CART) model, but many other partitioning rules have been studied [13]. One thing that standard RF methods have in common is that the partition rule is optimal in some way. For example, a CART model defines its rule as identifying the feature in the data that best separates records into distinct classes of interest. Because the partitioning rule is optimal (and, hence, powerful), the subsequent scoring or inference rule tends to be simple.\nThere is a drawback, however, to the approach of combining a powerful partition rule with a simple inference rule: trees in an RF model cannot be updated dynamically and have to be rebuilt in order to accommodate changes to the training data set. This comes to the fore in the context of continually evolving data streams. While micro-batch or mini-batch forest rebuilding strategies are reasonable for large static data sets, continually evolving streams have a premium on recency that necessitates continuous learning. Continuous learning in this scenario decomposes into an update phase, in which the model data structures are updated with new data, and an inference phase.\nRandom Cut Forests\nRandom Cut Forests (RCF) are organized around this central tenet: updates are better served with simpler choices of partition. More advanced algorithms in the inference phase can compensate for simpler updates. There are two immediate benefits of following this tenet. First, it provides a general purpose framework that enables continuous learning over data streams. RCFs serve as sketches or synopses of evolving data streams and multiple different scoring functions or even different types of forests are supported with the same set of trees. Second, it frames streaming estimation algorithms as a small operation on a summarized multi-level local context. This enables one to review foundational premises in different statistical estimation algorithms and perhaps even invent new ones.\nRCFs as Sketches\nRCFs were first developed in the context of anomaly detection [11], improving upon the Isolation Forest (IF) algorithm [15], which is an example of an RF algorithm as described above. Typical to other members of the class, IF does not allow incremental inference respecting the arrow of time without expensive rebuilding. However, anomaly detection is often not the end point but rather a the starting point of an eventual root cause analysis. A single bit prediction indicating whether a given point is an anomaly may not, by itself, be actionable. In many scenarios, anomalies are a starting point of investigation and one would like to answer auxiliary questions. The questions can be seeking further information or counterfactual. As examples of the former, one is often interested in answering ‚Äúwhich dimensions had substantial impact‚Äù or ‚Äúdid the density of points drop or rise in nearby regions‚Äù? As examples of latter, one could ask ‚Äúwhat should have been the observed value at this time‚Äù or ‚Äúwhy was this point not marked an anomaly‚Äù? At an intuitive level, a data structure such an RCF that has some capacity to discern ‚Äúanomalous‚Äù in an unknown distribution should also be able to inform us about normal characteristics. Further, in making that information explicit the structure provides us answers to the kinds of auxiliary questions posed here. But that step of making the information explicit requires rethinking the inference phase. It is instructive that the framework primarily developed for anomaly detection can provide applications in forecasting, (anisotropic) density estimation, and distance estimation, which are all examples of applications that have been used to define classes of anomaly detection algorithms. (See references in the extended surveys [1, 6].) Densities, distances, and predictions provide different vignettes of a stochastic process. A primary goal of sketches or synopses is to enable a multitude of such vignettes.\nThe RCF implementation provides a few generic traversal methods such that a user only has to define computations at ‚Äúinternal nodes‚Äù and ‚Äúleaf nodes‚Äù to design and evaluate their own scoring functions. Provable maintenance under insertion and deletions allows one to conceptually separate the context of sampling and inference. Users can swap scoring functions programmatically on the fly, run an ensemble of functions, or have a console experience of deep dive with new trial functions. This potentially allows for the development of domain specific or regime specific scoring ‚Äì optimizing over a space of functions not unlike hyperparameter optimization. An interesting consequence of the setup (and randomization) is that one could also simulate different classes of forests in a dynamic manner. For example, how would an RCF built using probabilistic choices similar to IF score this given point? This simulation is Monte Carlo and has errors, but allows greater options for a user without incurring resource costs for maintaining or foreseeing every possible eventuality. Quite literally, one can bring one‚Äôs own hyper-forest to the analysis.\nRethinking Random Forests\nFrom a foundational perspective, the ability to defer difficult computation to the simulation or inference phase while keeping updates simple opens up a plethora of tantalizing possibilities. Partition strategies in random forests (over possibly categorical variables) are typically complicated because they attempt to keep subtrees ‚Äúpure‚Äù (informed by explicit or implicit classification) ‚Äî dating back to stochastic discrimination introduced in [14]. It does not escape our attention that for two well separated Gaussians, a random cut would always produce one subtree to be ‚Äúalmost pure‚Äù and the other subtree can be mixed. This property or random cuts is agnostic of the complexity of the definition of separation, and remains true as long as the separation is observable. The fact that a random cut would make one subtree ‚Äúalmost pure‚Äù is a plausible explanation of the recorded success of IF. (In addition to distance-based explanations in [11].) Simulation at inference time allows us to address a foundational issue which can be missing in naive applications of ensemble forests. The basic question is: is the evaluation criterion cognizant of the point being evaluated? Affirmative answer corresponds to transduction, where we are trying to decide ‚Äúthis point, which we are staring at, is an anomaly?‚Äù. Negative answer corresponds to modeling the entire observable space and all eventualities, which corresponds to induction. Transduction is regarded to be easier than induction and for anomaly detection, perhaps transduction is a better fit since anomalies are often hard to define but easy to verify a la Justice Stewart Potter. However inductive explanations (mismatch from a prediction) are often easier to follow. The balance of these two aspects is key ‚Äì much of inference lie in the ability of discerning between transduction and induction. For example in semi-supervised learning [17], similar assumptions about the point under consideration affect a global inference.\nOn a more prosaic level, there exists a large literature that considers generalization error in RF which cannot be summarized herein. The solution in said literature seems to necessitate more state, which is typical of induction, where all eventualities are pre-considered. These ideas often uses a combination of (i) more trees (ii) diversified adaptive sampling (iii) linearity (iv) symmetry (isotropy) (v) convexity, and similar assumptions which are difficult to even verify for a continually evolving data stream. However each of these ideas can have serious side effects, for example, adaptive sampling (unless formally proved) can break the arrow of time and not be interpretable for one-pass streaming. The ability to simulate different aspects of RF without having to worry about addressing unverifiable assumptions or distorting the arrow of time will likely lead to next generation of ideas for RF, in particular in relation to addressing time as a variable. The feature where an user simply specifies small evaluations with limited state and context, at internal nodes or leaves, can be useful in this regard because it nudges an user to view estimation in terms of recursive strategies (which are dynamic in nature) as opposed to designing static functions and retrofitting time. The analysis ideas appear to tie these forests to Information Geometry [2] and sketches of information distances [8, 10].\nWe hope that ideas herein foster new innovation from the community in all the aspects discussed above ‚Äì from computational considerations to better mathematical modeling and to better theoretical understanding of stochastic processes. We close with the remark that anomaly detection is typically the starting point of inquiry.\nReferences\n[1] C. C. Aggarwal. Outlier Analysis. Springer New York, 2013.\n[2] S. Amari and H. Nagaoka. Methods of Information Geometry. Translations of Mathematical Monographs, American Mathematical Society. 2000.\n[3] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation, 9(7):1545‚Äì1588, 1997.\n[4] L. Breiman. Random forests. Machine Learning, 45(1):532, 2001.\n[5] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and regression trees. Chapman and Hall &amp; CRC, 1984.\n[6] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Comput. Surv., 41(3):15:1‚Äì15:58, July 2009.\n[7] G. Ghare, S. Guha, L. Moos, G. Roy, J. Tokle, and A. Satish. Anisotropic local interpolation for data streams with applications. Manuscript, 2019.\n[8] S. Guha, P. Indyk, and A. McGregor. Sketching information divergences. Machine Learning, 72:5‚Äì19, 2008.\n[9] S. Guha, S. Kalki, and A. Satish. Streaming algorithms for continuous forecasting. Manuscript, 2018.\n[10] S. Guha, A. McGregor, and S. Venkatasubramanian. Sublinear estimation of entropy and information distances. ACM Trans. Algorithms, 5(4)(35):1‚Äì35, 2009.\n[11] S. Guha, N. Mishra, G. Roy, and O. Schrijver. Robust random cut based anomaly detection in dynamic data streams. Proc. of ICML (JMLR Proceedings Series), 2016.\n[12] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer New York Inc., New York, NY, USA, 2001.\n[13] H. Ishwaran. The effect of splitting on random forest. Machine learning, 99:75‚Äì118, 2014.\n[14] E. M. Kleinberg. Stochastic discrimination. Annals of Mathematics and Artificial Intelligence, 1990.\n[15] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation-based anomaly detection. ACM Trans. Knowl. Discov. Data, 6(1):13:39, 2012.\n[16] M. L. Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer, 1998.\n[17] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and semi-supervised learn- ing using gaussian fields and harmonic functions. In ICML 2003 workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, pages 58‚Äì65, 2003.",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/real-time-anomaly-detection-in-open-distro-for-elasticsearch/",
    "title": "Real Time Anomaly Detection in Open Distro for Elasticsearch",
    "content": "Today, we released Anomaly Detection (preview) on Open Distro for Elasticsearch. We are excited to continue our work on anomaly detection as a part of Open Distro for Elasticsearch in the coming months, and invite developers in the larger search community to join in and co-develop some parts. The feature includes a nice mix of machine learning algorithms, statistics methods, systems work, visualization and UI, and enterprise primitives (for working on anomalies).\nAnalytics data continues to increase exponentially with time. An exponential data surge reduces the usage of a traditional analytics user workflow, which has been a set of canned queries and dashboards. This is because actionable queries require keeping track of data changes and distribution of each field over time, which is harder to achieve when data volumes increase significantly; heterogeneity (e.g., attack behavior in security traces) complicates understanding the data itself.\nOur goal is to build a set of real time analytics features for Elasticsearch that makes it easier for Open Distro for Elasticsearch users to automatically mine real time patterns across data streams at ingestion. We want to provide users with an interactive and guided exploration of data without having them to worry about tuning the analytics ‚Äúblackbox‚Äù that includes the models, hyperparameters and labels (with an option, however, for semi-supervised approaches). We started off by building and releasing anomaly detection as an Open Distro for Elasticsearch feature. In this blog, we discuss foundational aspects upon which anomaly detection is built: Random Cut Forest (RCF) machine learning algorithms underpinning the detection, the system architecture and workflow.\nRequirements\nThe first step towards actionable data analytics is anomaly detection. In this document, we propose to build anomaly detection as a feature on Elasticsearch. Our goal is to build algorithms that are: (1) lightweight, i.e., streaming, (2) unsupervised, (3) accurate, (4) interactive, i.e., low training time to detect anomalies, and (5) embarassingly parallelizable and elastic. In addition, the algorithms should be able to account for (or not) seasonality in data, and be able to detect not only changes in first and second order magnitude, but also non-distributional effects such as changes in frequency content and phase. We want to support multi-dimensional detection methods. For power users, we may support ‚Äúbring your own detection algorithms‚Äù and ensemble methods.\nWhen building the system, our goal is to leverage statistical multiplexing of resources on the Elasticsearch cluster to minimize cost. The system should partition and distribute the work of doing anomaly detection across the cluster. Note that the overhead of anomaly detection will be negligible if we use lightweight streaming algorithms. Such algorithms give us control over bounding compute and memory resources for training and detection and are also embarassingly parallelizable. This will enable anomaly detection with zero additional cost (e.g., cost of adding anomaly detection nodes) on the existing ingestion and query workload. The detection system should be able to support using separate anomaly detection node(s) if desired. The system should be elastic to changes in cluster membership, and to available resources (which also makes it fault tolerant and highly available).\nActionability and Interfaces\nThe simplest way to do anomaly detection is to run detectors on all fields (and their combinations) in the data. While this requires zero inputs from the user, it risks generating too many false positives from the standpoint of being actionable, and sometimes, false negatives (if the fields do not describe the metric directly). It also results in wasted work (and the cost of provisioning for that work).\nTo generate actionable insights from real time data, we would need inputs from the user to understand what is considered actionable. Treating the anomaly detector as a blackbox, we have two places from where we can capture user intent on actionability: the input and the output to/from the detector. Two input intents are feature definitions and examples of anomalies (i.e., labeled data). Feature definitions are arbitrary functions of the fields in the data (and can be the fields themselves). An example feature definition is an error count computation on the HTTP return codes in log data, where each log line corresponds to an HTTP session in each time window. In this example, running an anomaly detector on the return codes directly would cause anomalies that can be false positives or ones hard for the user to explain. For most applications of Elasticsearch, we do not expect well-calibrated labeled examples ‚Äî and using such examples without calibration can cause over/under-fitting. We instead focus on algorithms with the largest customer reach, i.e., unsupervised learning.\nThe output intents we can get from the user include explicit user-defined tags on the data we can use as potential labels (e.g., upgrade windows, outages); and interactive validations, where the user marks anomalies as false positives or undetected. Semi-supervised methods can be used on such user inputs. In our first version, we consider unsupervised methods, but those which can naturally be extended to be semi-supervised methods. These intents express actionability to some extent, but can easily miss windows that the user could consider actionable (because it would be tedious for the user).\nAcross both input and output user intents, the most useful intent is the feature definitions. We consider that in our first version. We define some terminology here. The user would define an anomaly detector that includes one or more features in an index, a periodicity for the feature computation, and data filtering criteria (if any). Each feature definition could be as simple as selecting a field, to defining an aggregation function over the field to an arbitrary script on one or more fields. Note this also implies that users can specify a multi-dimensional detector (over multiple features). For multi-dimensional detectors, the user could optionally indicate an importance parameter for one or more features. We have built both API and Kibana UI interfaces to these concepts.\nThe output of an anomaly detector (i.e., anomalies) can be consumed by the user using Open Distro for Elasticsearch Alerts, or can be browsed using Kibana (including historic anomalies and feature timeseries). We want to enable an interactive experience when defining a detector using Kibana ‚Äî the user should be able to interactively add/modify feature definitions and see changes in anomalies detected (along with the feature timeseries) in real time. We divide the design discussion into algorithms and systems aspects.\nBuilding on the Random Cut Forest Algorithm\nThere are several unsupervised learning approaches to anomaly detection: from the traditional statistical timeseries methods to machine learning (e.g., nearest neighbor, clustering, subspace or spectral methods, density estimation, including ensemble methods) [ 6]. We focus on the latter, since we are also interested in non-distributional changes in data (frequency content and phase). Anomaly detections by many of the machine learning methods are hard to explain for the user and compute-heavy.\nWe restrict to a specific class of ensemble methods based on random forests. Random forests have been used successfully for lightweight density estimation. The Isolation Forest (2008) method [ 1] uses random forests to isolate anomalous data. Isolation Forest recursively partitions the hyperspace of features to construct trees (the leaf nodes are feature samples), and assigns an anomaly score to each data point based on the sample tree heights. It is a batch processing method. The Random Cut Forest (RCF, 2016) method [ 2] adapted Isolation Forest to work on data streams with bounded memory and lightweight compute. RCF incrementally updates the forest on each feature sample and interleaves training and inference. RCF also emits an anomaly score for each feature sample. The RCF estimator has been proven as it have been used in production settings, for example Amazon Kinesis Analytics [ 3], Quicksight [ 4] and SageMaker [ 5]. Using data shingling, RCFs can detect non-distributional anomalies such as frequency and phase changes. RCFs scale to high-dimensional data streams. Putting RCFs to practice for real time anomaly detection in a ‚Äúset it and forget it‚Äù environment requires additional work; we list them down here. First, RCFs emit an anomaly score that is hard to reason about for the user, its magnitude is a function of the data timeseries on which the RCF is trained. We need an additional learning primitive that continuously learns the baseline anomaly score distribution to detect large score values it is a classifier function that maps the anomaly score to a boolean outcome (anomaly or not). Note that this classifier is different from RCF itself. RCF isolates anomalies (i.e., not the baseline) and gives a score timeseries that captures and quantifies anomalous events; the classifier can also be simple, since it operates on one-dimensional positive data. The classifier needs to work with small amounts of data, so it does not block anomaly detection. The classifier emits two values to aid the user: (1) anomaly grade, quantifying severity of the anomaly, and (2) confidence, quantifying the amount of data seen and RCF size.\nSecond, RCFs require training time to learn the initial distribution (i.e., the forest). An RCF requires hundreds of samples, which takes time to arrive at the Elasticsearch cluster. This prevents both interactive exploration of anomalies on current and past data (e.g., using queries), and delays the time-to-detection. In practice, users would define and turn on a detector after ingesting some data - we can leverage this to train an initial model. Further, to support interactive ad-hoc exploration, we need to a fast RCF construction primitive on data at rest.\nSystem Design\nSince the anomaly detection system builds on top of Elasticsearch, it should be very lightweight and highly elastic to changes in cluster state and resource availability. We leverage statistical multiplexing to use available resources on the cluster and have the system adapt to cluster state changes in real time, to keep resources intact for the user workload. Note that the algorithms themselves are very lightweight and have tight memory and compute requirements. This also ensures that we build the lowest cost anomaly detectors on the Elasticsearch cluster. Feature Computation The first step to anomaly detection is feature computation. The user defines features via the anomaly detection API or the Kibana anomaly detector interface. Each feature is mapped to an Elasticsearch query string. The system issues queries for each feature at the frequency specified by the user; we upper-bound the frequency to once a minute to contain query overheads. Queries also leverage the power statistical multiplexing, since they are distributed computations across all shards of the index. We also limit the set of aggregations that feature computations support, since feature samples are currently limited to one-dimensional numbers. The system aborts queries that take longer than a threshold and throttles queries if the latencies are high (the impact of this is lower confidence of detection; not detection itself). Training and Inference The next step is to schedule training and inference for anomaly detection. Since a random forest is an independent set of trees, this is a parallel execution on the cluster. An elected node on the cluster acts as the coordinator node for an anomaly detector. The coordinator schedules queries for feature computation; and schedules and manages partitions of the RCF on different nodes and the computation for the classifier (score-to-detection function). Changes in cluster membership trigger reassignments of the compute jobs. Each compute job periodically checkpoints its state (i.e., trees and classifier parameters) in an Elasticsearch index to handle job reassignments and for fault tolerance. The checkpoints eliminate the need to re-train trees and classifier for a new job assignment. Reassignments are also throttled when necessary. Orchestrating RCF and score classifier computation on Elasticsearch cluster. Fault tolerance, Elasticity and Availability The system is designed to be highly elastic. It adapts in real time to cluster membership changes (described above), and to changes in resource availability on the cluster nodes. Each node of the cluster adapts to heap availability on the JVM to size the number of trees on the RCF partition (there is also an upper bound on number of trees in a partition). A node-level circuit breaker also shuts down all RCF partitions on a node in extreme cases. Note that these actions only impact the confidence of the anomaly detections not the availability of anomaly detection service itself. When a detector temporarily aborts or skips query samples to adapt to resource availability drops, we use linear or nearest neighbor interpolation to fill sparse ‚Äúholes‚Äù in feature samples (this has minimal impact on detection accuracy). The only case where a detector is unavailable is when all partitions are shut down or the threshold model is shut down; this is a case where the Elasticsearch cluster is under stress, and may be low on availability as well.\nIt is equally important to build monitoring primitives to allow the user to understand the detector output and availability. As mentioned above, the system exposes confidence and anomaly grade for each detection. In addition, the system should notify or record when there are missing data samples for periods of time, changes in compute job sizes, and the impact (if any) of anomaly detectors on Elasticsearch ingestion and query workloads.\nSummary\nIn this blog post, we introduced the real time anomaly detection feature in Open Distro in Elasticsearch. It runs asynchronously during ingestion time, and has very low overhead, which makes it suitable to run within the cluster without impacting cluster performance. We covered the Random Cut Forest algorithm and the model partitioning and elasticity in the system, which makes anomaly detection highly available. We briefly touched upon support that we are introducing to enable interactive exploration of anomalies on live and offline data in Elasticsearch. We are excited for the future of real time anomaly detection for Elasticsearch and welcome you to come join in and contribute with us in building anomaly detection and machine learning capabilities in Open Distro for Elasticsearch.\nReferences\n[1] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008. Isolation Forest. In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining (ICDM ‚Äò08) [2] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers. 2016. Robust random cut forest based anomaly detection on streams. In Proceedings of the 33rd International Conference on International Conference on Machine Learning (ICML‚Äô16) [3] RANDOM_CUT_FOREST, Amazon Kinesis Data Analytics documentation [4] What RCF Is and What It Does, Amazon Quicksight documentation [5] Random Cut Forest (RCF) Algorithm, Amazon SageMaker Developer Guide [6] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection: A survey. ACM Computing Surveys,* 41, 3, Article 15 (July 2009), 58 pages",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/announcing-open-distro-for-elasticsearch-1-3-0-w-helm-chart-for-kubernetes-and-windows-support/",
    "title": "Announcing Open Distro for Elasticsearch 1.3.0, Helm chart for Kubernetes and Windows support",
    "content": "We are pleased to announce the release of Open Distro for Elasticsearch 1.3.0. Version 1.3.0 includes the upstream open source versions of Elasticsearch 7.3.2, Kibana 7.3.2 and the latest updates for the alerting, SQL, security, performance analyzer and Kibana plugins. We are excited to announce the general availability of Open Distro for Elasticsearch Index Management, installer for Windows deployments, Helm chart support for Kubernetes and cloud formation templates for Elasticsearch deployments. We would like to thank the community for their contributions and support that enabled us to release some of these new features. Here are the full release notes.\nDownload the latest packages\nYou can find Docker Hub images Open Distro for Elasticsearch 1.3.0 and Open Distro for Elasticsearch Kibana 1.3.0 on Docker Hub. Make sure your compose file specifies 1.3.0 or uses the latest tag. See our documentation on how to install Open Distro for Elasticsearch with RPMs and install Open Distro for Elasticsearch with Debian packages. You can find our Open Distro for Elasticsearch security plugin artifacts on Maven Central. You can download the latest versions of Open Distro for Elasticsearch‚Äôs PerfTop client, and Open Distro for Elasticsearch‚Äôs SQL JDBC driver.\nNew Features\nIndex Management\nThe Index management plugin in Open Distro for Elasticsearch provides an automated system for recurring index state management tasks, eliminating the need rely on external systems to execute them periodically. The Kibana plugin for index management provides users an administrative panel to monitor the indices and apply policies at different stages in its lifecycle based on user defined criteria like index age, size, or number of documents. Index state management lets users define unlimited custom policies to be applied to index patterns, and move indices from one state to another. Within each state users can define a list of actions to perform and the criteria to transition to a new state.\nOpen Distro for Elasticsearch Deployment for Kubernetes\nWith the new Helm chart for Open Distro for Elasticsearch, users can scale out Elasticsearch deployments in Kubernetes and Amazon EKS. This Helm chart supports installation and update of Elasticsearch and Kibana. The Helm chart enables users to set up master, client and data nodes. It also provides options for security settings like TLS, custom certificates, RBAC, multi-tenancy for the cluster.\nCloud formation templates\nUsers can now create a full Open Distro for Elasticsearch cluster, including secure networking provided through VPC, configurable data notes, master nodes and a client node. The client node provides Kibana access with a public IP address. Packages and features under development\nWe‚Äôre also excited to announce an alpha version of a Windows installer and new plugins in development. We invite you to join in to submit issues and PRs on features, bugs, and tests you need or build.\nOpen Distro for Elasticsearch deployment on Windows\nWith Windows exe supporting version 1.3.0, users can now easily install Elasticsearch and Kibana on Windows and run it as a service. This service has been tested on Windows 10 and Windows Server 2019.\nAnomaly Detection\nMachine learning based anomaly detection has been released as an alpha plugin and is under active development. The anomaly detection feature can handle large volumes of high dimensional data and detect outliers in real-time. The anomaly detection feature depends on the underlying Random Cut Forest (RCF) library, a proven algorithm for streaming use cases. RCF is also now open source and provides better visibility into the anomaly detection decision framework.\nPerformance Analyzer Root Cause Analysis (RCA) framework\nThe Performance Analyzer RCA is a framework that builds on the Performance Analyzer engine to support Root Cause Analysis (RCA) of performance and reliability problems in Elasticsearch clusters. This framework executes real time root cause analyses using Performance Analyzer metrics. You can now weigh in your feedback on the design proposal released recently.\nQuestions?\nPlease feel free to ask questions on the Open Distro for Elasticsearch community discussion forum.\nReport a bug or request a feature\nPlease feel free to file a bug or an issue, request a feature, or propose new ideas to enhance Open Distro for Elasticsearch. To file bugs, raise a PR or propose a feature for a particular plugin, navigate to the specific plugin repository and add an issue. This will help us manage the content better.\nGetting Started\nFeel free to select an issue tagged as a ‚ÄúGood First Issue‚Äù in the plugin repos to start contributing to Open Distro for Elasticsearch. Read the Open Distro technical documentation on the project website to help you get started. We look forward to sharing new ideas, seek feedback and development Open Distro collaboratively with you!",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/real-time-root-cause-analysis-in-open-distro-for-elasticsearch/",
    "title": "Real Time Root Cause Analysis in Open Distro for Elasticsearch",
    "content": "The Open Distro for Elasticsearch Performance Analyzer captures Elasticsearch and JVM activity, as well as lower-level resource usage (e.g. disk, network, CPU and memory) of these activities. Based on this instrumentation, Performance Analyzer computes and exposes diagnostic metrics, with the goal of enabling Elasticsearch users and administrators to measure and understand bottlenecks in their Elasticsearch clusters. The Open Distro for Elasticsearch PerfTop client provides real-time visualization of these diagnostic metrics to surface bottlenecks to Elasticsearch users and operators.\nToday, we are open sourcing the Root Cause Analysis framework for Open Distro for Elasticsearch. This new framework conducts real-time analysis of Performance Analyzer metrics to surface performance and reliability problems for Elasticsearch instances. The source code can be found here. We are excited to continue building out the Root Cause Analysis framework as a part of Open Distro for Elasticsearch, and invite developers in the larger search community to join in and collaborate with us on development, design, and testing. The feature includes a rich mix of distributed data flow graph processing, gRPC for networking, basic statistics for metric evaluation, systems work, and UI.\nWe believe this framework can significantly improve operations, administration, and provisioning of Elasticsearch clusters and help development teams to tune their workloads to reduce errors.\nModel\nWe define a root cause as a function of one or more symptoms. A symptom is an operation applied to one or more metrics and/or other symptoms. Root causes may also be a function of other root causes. Note that these operations may involve aggregations; for example, a symptom could consume a time average of a metric. In addition, for confidence, a root cause could be a computation over a sufficiently long window of time. This definition does not allow for cycles in the dependency graph between metrics and root causes. The following equations show an example of these relationships: Note that any of the functions above can take metadata as inputs, such as thresholds.\nRoot causes can span the entire stack, from the infrastructure layers (e.g. the OS, host, virtualization layers, and the network) to the Java Virtual Machine to the Elasticsearch engine. Root causes also include problems related to the input workload to Elasticsearch.\nSystem Design\nBased on the recursive model definition above, we build an acyclic data flow graph that takes metric streams generated by the Performance Analyzer plugin as input. Nodes of the data flow graph include computations such as metrics output (source nodes), aggregations, symptoms, and root causes (sink nodes). The data flow graph across all root causes would span all nodes of an Elasticsearch cluster (including master nodes).\nEdges of the graph transfer the output of a parent node to all child nodes.The framework treats this output as an opaque stream since the data format between nodes is a contract between each pair of nodes. The framework explicitly requires nodes to send timestamps‚Äîthis is necessary for a node to diagnose issues with a parent node and handle staleness in data (e.g. data delivered late). Message delivery is ordered and provides at most once semantics (i.e. messages could be dropped to keep up with stream rate); small message loss isn‚Äôt a significant issue for root cause analysis because such algorithms rely significantly on statistical data. The following figure shows the above equations as a data flow graph (sources and sinks are shaded): The framework is also fault tolerant for Elasticsearch, JVM, and infrastructure performance and reliability problems. It exposes an API to query the current (or recent) set of diagnoses across some nodes or the entire cluster. The output could be used by diagnostic tools (e.g. the Performance Analyzer PerfTop) or automated control plane actions.\nAll RCAs must be registered with the framework. This allows the framework to de-duplicate computations and optimize the streaming runtime. It exposes root causes and their context for applications to consume. Note that the framework resides in the agent process, so it is isolated from failures and performance problems in the Elasticsearch JVM. The architecture is shown below. The framework is designed to be fast and compute root causes in parallel. It executes each graph node in topological order as defined in the analysis graph. Nodes with no dependency are executed in parallel. If a host depends on a remote data stream for RCA computation, it subscribes to the data stream on startup. Subsequently, the output of every RCA execution on the upstream host is streamed to the downstream subscriber.\nSummary\nIn this blog post, we introduced the real-time root cause analysis feature in Open Distro for Elasticsearch. It runs asynchronously as a side-car agent and has very low overhead, which makes it suitable to run within the cluster without impacting cluster performance. We covered the basic concepts used in the framework and the system architecture, which makes root cause analysis process seamless.\nWe‚Äôre planning to build out functionality around identifying JVM bottlenecks and handling complex root causes for performance. We are excited for the future of real-time root cause analysis for Elasticsearch and welcome you to come join in and contribute with us in building the root cause analysis framework in Open Distro for Elasticsearch.",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Open-Distro-for-Elasticsearch-1.4.0-with-K-nearest-neighbor-(k-NN)-search-support-is-now-available/",
    "title": "Open Distro for Elasticsearch 1.4.0 with K-nearest neighbor (k-NN) search support is now available",
    "content": "We are happy to announce the release of Open Distro for Elasticsearch 1.4.0. Version 1.4.0 includes the upstream open source versions of Elasticsearch 7.4.2, Kibana 7.4.2 and the latest updates for the alerting, SQL, security, performance analyzer and Kibana plugins. We are also pleased to announce the general availability of Open Distro for Elasticsearch k-NN plugin, and installer for Windows deployments. We would like to thank the community for their contributions and support in testing out the new features. Here are the full release notes.\nDownload the latest packages\nYou can find Docker Hub images Open Distro for Elasticsearch 1.4.0 and Open Distro for Elasticsearch Kibana 1.4.0 on Docker Hub. Make sure your compose file specifies 1.4.0 or uses the latest tag. See our documentation on how to install Open Distro for Elasticsearch with RPMs and install Open Distro for Elasticsearch with Debian packages. With Windows exe supporting version 1.4.0, users can now easily install Elasticsearch and Kibana on Windows and run it as a service. You can find our Open Distro for Elasticsearch security plugin artifacts on Maven Central. You can download the latest versions of Open Distro for Elasticsearch‚Äôs PerfTop client, and Open Distro for Elasticsearch‚Äôs SQL JDBC driver.\nNote: k-NN plugin is only available as part of docker image in this release.\nNew Features\nk-NN plugin\nOur new k-NN search plugin enables high scale, low latency nearest neighbor search on billions of documents across thousands of dimensions with the same ease as running any regular Elasticsearch query. Built using the Non-Metric Space Library (NMSLIB), this plugin can power use cases such as product recommendations, fraud detection, and image, video, and related document search. We have extended the Apache Lucene codec to introduce a new file format to store vector data. k-NN Search uses the standard Elasticsearch mapping and query syntax ‚Äîto designate a field as a k-NN vector you can simply map it to the new k-NN field type provided by the k-NN plugin. k-NN functionality integrates seamlessly with other Elasticsearch features. This provides users the flexibility to use Elasticsearch‚Äôs extensive search features such as\naggregations and filtering with k-NN to further improve the search results. Learn more at k-NN Packages and features under development\nAnomaly Detection\nMachine learning based anomaly detection has been released as an alpha plugin and is under active development. The anomaly detection feature can handle large volumes of high dimensional data and detect outliers in real-time. The anomaly detection feature depends on the underlying Random Cut Forest (RCF) library, a proven algorithm for streaming use cases. RCF is also now open source and provides better visibility into the anomaly detection decision framework.\nPerformance Analyzer Root Cause Analysis (RCA) framework\nThe Performance Analyzer RCA is a framework that builds on the Performance Analyzer engine to support Root Cause Analysis (RCA) of performance and reliability problems in Elasticsearch clusters. This framework executes real time root cause analyses using Performance Analyzer metrics. You can now weigh in your feedback on the design proposal released recently.\nQuestions?\nPlease feel free to ask questions on the Open Distro for Elasticsearch community discussion forum.\nReport a bug or request a feature\nPlease feel free to file a bug or an issue, request a feature, or propose new ideas to enhance Open Distro for Elasticsearch. To file bugs, raise a PR or propose a feature for a particular plugin, navigate to the specific plugin repository and add an issue. This will help us manage the content better.\nGetting Started\nFeel free to select an issue tagged as a ‚ÄúGood First Issue‚Äù in the plugin repos to start contributing to Open Distro for Elasticsearch. Read the Open Distro technical documentation on the project website to help you get started. We look forward to sharing new ideas, seek feedback and development Open Distro collaboratively with you!",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Open-Distro-for-Elasticsearch-1.6.0-released/",
    "title": "Open Distro for Elasticsearch 1.6.0 Released",
    "content": "Open Distro for Elasticsearch 1.6.0 is now available for download.\nThis release consists of Apache-2 licensed Elasticsearch 7.6.1, Kibana 7.6.1 and plugins for alerting, index management, performance analyzer, security, SQL and machine learning with k-NN. Also included are a SQL JDBC driver and PerfTop, a client for Performance Analyzer.\nDownload the latest packages\nYou can find Docker Hub images Open Distro for Elasticsearch 1.6.0 and Open Distro for Elasticsearch Kibana 1.6.0 on Docker Hub. Make sure your compose file specifies 1.6.0 or uses the latest tag.\nIf you‚Äôre using RPMs or DEBs, see our documentation on how to install Open Distro for Elasticsearch with RPMs and install Open Distro for Elasticsearch with Debian packages. A tarball is also available for testing and other applications.\nWith Windows exe supporting version 1.6.0, users can now easily install Elasticsearch and Kibana on Windows.\nIf you‚Äôre using Kubernetes, check out the Helm chart to install Open Distro for Elasticsearch.\nYou can find our Open Distro for Elasticsearch security plugin artifacts on Maven Central. You can download the latest versions of Open Distro for Elasticsearch‚Äôs PerfTop client, and Open Distro for Elasticsearch‚Äôs SQL JDBC driver.\nRelease highlights\nRelease highlights include security plugin optimization for a faster version of the implied permission type, memoization of results for batch requests, implementation of lazy loading for efSearch parameter in the k-NN plugin, improved exception handling and report date handling using standard formats for the SQL plugin.\nPlease take a look at the 1.6.0 release notes for the latest features, enhancements, infra and config updates and bug fixes. Upgrade to 1.6.0 to leverage the latest features and bug fixes.\nWe invite you to get involved in ongoing development of new Open Distro for Elasticsearch plugins. Consider contributing code, documentation or tests for Performance Analyzer RCA Engine, machine learning components like Anomaly Detection and its Kibana UI as well as the SQL ODBC Driver.\nCome join our community!\nThere are many easy ways to participate. Ask questions and share your knowledge with other community members on the Open Distro discussion forums. Attend our bi-weekly online community meetup to learn more about Elasticsearch, security, performance, machine learning and more.\nFile an issue, request an enhancement you need or suggest a plugin you need at github.com/opendistro-for-elasticsearch. Contribute code, tests, documentation and even release packages at github.com/opendistro-for-elasticsearch. If you want to showcase how you‚Äôre using Open Distro, write a blog post for opensearch.org/blog. Please reach out to us on Twitter. You can find me @alolita or Jon at @jon_handler.\nStay safe. Stay healthy!",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Building-k-Nearest-Neighbor-(k-NN)-Similarity-Search-Engine-with-Elasticsearch/",
    "title": "Build K-Nearest Neighbor (k-NN) Similarity Search Engine with Elasticsearch",
    "content": "Blog refreshed for technical accuracy on 06 Jan 2023 Since the OpenSearch Project introduced the k-nearest neighbor (k-NN) plugin in 2019, it has supported both exact and approximate k-NN search. The approximate k-NN (ANN) search method is more efficient for large datasets with high dimensionality because it reduces the cardinality of searchable vectors. This approach is superior in speed at the cost of a slight reduction in accuracy. Currently, OpenSearch supports three similarity search libraries that implement ANN algorithms: Non-Metric Space Library (NMSLIB), Facebook AI Similarity Search (Faiss), and Lucene. This post describes k-NN search and its underlying Hierarchical Navigable Small World (HNSW) algorithm, and then focuses on the integration of NMSLIB with OpenSearch and the customizations made to support the feature in OpenSearch.\nWhat is k-NN?\nA k-NN algorithm is a technique for performing similarity search: given a query data point, what are the k data points in an index that are most similar to the query? k-NN is largely popular for its use in content-based recommendation systems. For example, in a music streaming service, when a user generates an on-demand playlist, the recommendation system adds the songs that match the attributes of that playlist using k-NN. In a k-NN search algorithm, the elements of a data set are represented by vectors. Each song is a vector, containing several dimensions (attributes) like artist, album, genre, year of release, etc. The search assumes you have a defined distance function between the data elements (vectors) and returns most similar items to the one provided as input, where closer distance translates to greater item similarity. Other use cases with similarity search include fraud detection, image recognition, and semantic document retrieval.\nTo choose a k-NN algorithm for ANN search, we evaluated four primary dimensions to measure the algorithm‚Äôs effectiveness: Speed - How quickly does the algorithm return the approximate k-nearest neighbors, measured in latency of a single or batch query? Recall - How accurate are the results, measured by ratio of the returned k-nearest neighbors indeed in the list of the actual k nearest neighbors to the value of k? Scalability - Can the algorithm handle data sets with millions or billions of vectors and thousands of dimensions? Updates - Does the algorithm allow addition, deletion, and updating points without having to rebuild an index, a process that can take hours or more?\nWe selected HNSW graphs because it aligned with our architectural requirements and met most of our evaluation criteria. Given a dataset, the algorithm constructs a graph on the data such that the greedy search algorithm finds the approximate nearest neighbor to a query in logarithmic time. HNSW consistently outperforms other libraries in this space based on ANN benchmark metrics. HNSW excels at speed, recall, and cost, though it is restricted in scalability and updates. While the HNSW algorithm allows incremental addition of points, it forbids deletion and modification of indexed points. We offset the scalability and updates challenges by leveraging the OpenSearch distributed architecture, which scales with large data sets and inherently supports incremental updates to the data sets that become available in the search results in near real-time.\nHierarchical Navigable Small World Algorithm\nThe Hierarchical Navigable Small World (HNSW) graph algorithm is a fast and accurate solution to the approximate k-nearest neighbors (k-NN) search problem.\nA straightforward, yet naive solution to the k-NN problem is to first compute the distances from a given query point to every data point within an index and then select the data points with the smallest k distances to the query. While this approach is effective when the index contains 10,000 or fewer data points, it does not scale to the sizes of datasets used by our customers. An approximate k-NN (ANN) algorithm may greatly reduce search latency at the cost of precision. When designing an ANN algorithm, there are two general approaches to improve latency:\nCompute fewer distances\nMake distance computations cheaper\nThe HNSW algorithm uses the first approach by building a graph data structure on the constituent points of the data set.\nWith a graph data structure on the data set, approximate nearest neighbors can be found using graph traversal methods. Given a query point, we find its nearest neighbors by starting at a random point in the graph and computing its distance to the query point. From this entry point, we explore the graph, computing the distance to the query of each newly visited data point until the traversal can find no closer data points. To compute fewer distances while still retaining high accuracy, the HNSW algorithm builds on top of previous work on Navigable Small World (NSW) graphs. The NSW algorithm builds a graph with two key properties. The ‚Äúsmall world‚Äù property means that the number of edges in the shortest path between any pair of points grows poly-logarithmically with the number of points in the graph. The ‚Äúnavigable‚Äù property asserts that the greedy algorithm is likely to stay on this shortest path. Combining these two properties results in a graph structure that allows the greedy algorithm to find the data point nearest to a query in logarithmic time. Figure 1: A depiction of an NSW graph built on blue data points. The dark blue edges represent long-range connections that help ensure the small-world property. Starting at the entry point, at each iteration the greedy algorithm will move to the neighbor closest to the query point. The chosen path from the entry point to the query‚Äôs nearest neighbor is highlighted in magenta and, by the ‚Äúnavigable‚Äù property, is likely to be the shortest path from the entry point to the query‚Äôs nearest neighbor. HNSW extends the NSW algorithm by building multiple layers of interconnected NSW-like graphs. The top layer is a coarse graph built on a small subset of the data points in the index. Each lower layer incorporates more points in its graph until reaching the bottom layer, which consists of an NSW-like graph on every data point. To find the approximate nearest neighbors to a query, the search process finds the nearest neighbors in the graph at the top layer and uses these points as the entry points to the subsequent layer. This strategy results in a nearest neighbors search algorithm which runs logarithmically with respect to the number of data points in the index.\nNon-Metric Space Library NMSLIB, an Apache 2 licensed library, is the open source implementation of HNSW. It is lightweight and works particularly well for our use cases that that requires minimal impact on the OpenSearch application workloads. To index the vectors and to query the nearest neighbors for the given query vector, our k-NN plugin makes calls to the NMSLIB implementation of HNSW. We use the Java Native Interface (JNI) as a bridge between OpenSearch, which is written in Java, and NMSLIB libraries, which are written in C++. OpenSearch now supports several distance functions, such as Euclidean, cosine similarity, and inner product, for calculating similarity scores. For a list of all supported distance functions, see Spaces.\nOpenSearch Integration with ANN\nThe OpenSearch distributed engine allows us to distribute the millions of vectors across multiple shards spread across multiple nodes within a cluster and scales horizontally as the data grows. Users can also take advantage of OpenSearch‚Äôs support for index updates to make any modifications to the dataset and reflect the changes in the results in near real-time. While OpenSearch‚Äôs plugin-based architecture makes it easy to add extensions, we had to make some customizations to support the ANN Search.\nFirst, we added a new field type, knn_vector, using the Mapper plugin, to represent the vectors as arrays of floating point numbers in a document. ANN requires support for storing high cardinality vectors. The knn_vector field supports vectors up to 16,000 dimensions. We also introduced a new Apache Lucene codec, KNNCodec, to add a new index file format for storing and retrieving the vectors and make Apache Lucene aware of the graphs built by NMSLIB. These file formats co-exist with the other Apache Lucene file formats and are immutable just like the other Apache Lucene files, making them file system cache friendly and thread safe.\nLet‚Äôs create a KNN index myindex and add data of type knn_vector to the field my_vector: PUT /myindex { \"settings\": { \"index.knn\": true }, \"mappings\": { \"properties\": { \"my_vector\": { \"type\": \"knn_vector\", \"dimension\": 4, \"method\": { \"name\": \"hnsw\", \"space_type\": \"l2\", \"engine\": \"nmslib\", \"parameters\": { \"ef_construction\": 128, \"m\": 24 } } } } } } For a description of more advanced settings, see knn_vector data type documentation.\nYou can then index your documents as you would normally do using any of OpenSearch index APIs: PUT /myindex/_doc/ 1 { \"my_vector\": [ 1.5, 2.5] } PUT/myindex/_doc/ 2 { \"my_vector\": [ 2.5, 3.5] } We also added a new knn query clause. You can use the this clause in a query and specify the point of interest as my_vector ( knn_vector) and the number of nearest neighbors to fetch as k. The following response shows two nearest documents to the input point [3, 4]. The score indicates the distance between the two vectors, which is the deciding factor for selecting the neighbors: POST /myindex/_search { \"size\": 2, \"query\": { \"knn\": { \"my_vector\": { \"vector\": [ 3, 4], \"k\": 2 } } } } The response contains the matching documents: { \"took\": 7, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 2, \"relation\": \"eq\" }, \"max_score\": 0.5857864, \"hits\": [{ \"_index\": \"myindex\", \"_type\": \"_doc\", \"_id\": \"2\", \"_score\": 0.5857864, \"_source\": { \"my_vector\": [ 2.5, 3.5] } }, { \"_index\": \"myindex\", \"_type\": \"_doc\", \"_id\": \"1\", \"_score\": 0.32037726, \"_source\": { \"my_vector\": [ 1.5, 2.5] } }] } } You can also combine the knn query clause with other query clauses as you would normally do with compound queries. In the following example, the user first runs the knn query to find the closest five neighbors ( k =5) to the vector [3,4] and then applies post filter to the results using the boolean query to focus on items that are priced less than 15 units: POST /myindex/_search { \"size\": 5, \"query\": { \"bool\": { \"must\": { \"knn\": { \"my_vector\": { \"vector\": [ 3, 4], \"k\": 5 } } }, \"filter\": { \"range\": { \"price\": { \"lt\": 15 } } } } } } Memory Monitoring\nThe NSW graphs, created by the underlying NMSLIB C++ library, are loaded outside the OpenSearch JVM, and the garbage collection does not reclaim the memory used by the graphs. We developed a monitoring solution using Guava cache to limit memory consumption by the graphs to prevent OOM (out of memory) issues and to trigger garbage collection when the indices are deleted or when the existing segments are merged to newer segments as part of segment merges. We also introduced an additional stats API to monitor the cache metrics like totalLoadTime, evictionCount, hitCount, and graphMemoryUsage to assist with efficient memory management. For the complete set of metrics, refer to k-NN settings.\nConclusion\nOur k-NN solution enables you to build a scalable, distributed, and reliable framework for similarity searches. You can further enhance the results with strong analytics and query support from OpenSearch.\nFor more details on k-NN search, check out the k-NN documentation. If you are interested in contributing, be sure to access the k-NN plugin repo.",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/The-Elasticsearch-Weight-Function/",
    "title": "The Elasticsearch Weight Function",
    "content": "Distributed systems scale by coordinating and distributing their workloads horizontally, across several machines. In Elasticsearch, this is done by partitioning indexes into shards and distributing them across data nodes in the cluster. The Elasticsearch Weight Function Image credit: Yuri Samoilov\nShards receive read and write traffic, and consume resources like disk, memory, JVM heap, and network. The overall resource consumption (workload) on a data node, depends on the shards it holds and the traffic they receive. Thus, a balanced distribution of shards corresponds to even workloads and efficient node utilization. In Elasticsearch, this responsibility belongs to the ShardsAllocator component.\nIn a previous post, we discussed the internal Elasticsearch algorithms for allocation and rebalance. Each shard is compared against eligible destination nodes, and the best fit is chosen. These comparisons require some internal yardstick to rank nodes, which is provided by the Shard Allocation Weight Function.\nIn this post, I will dive into the default weight function implementation to weigh the pros and cons of the default algorithm and look at some of the considerations in making shard allocation more responsive to transient signals. You will gain a deeper understanding of shard placement in your clusters, why Elasticsearch chose a particular node for a shard, and how future placement decisions will be evaluated. Knowing this will help you design for future workloads and scaling requirements.\nThe Weight Function\nWeight function, in Elasticsearch, is a neat abstraction to process parameters that influence a shard‚Äôs resource footprint on a node, and assign measurable weight values to each shard - node combination. The node with lowest weight value is considered as the best destination for shard in question. Similarly, a high difference in weight values implies imbalance ‚Äì shards must be moved from high to low weighted nodes.\nThe default weight function uses two parameters to balance shards 2 Total number of shards on a node across all indexes.\nNumber of shards on a node for given index. shard-weight = theta0 * (num-shards-on-node ‚Äì mean-shards-per-node)\nindex-weight = theta1 * (num-index-shards-on-node ‚Äì mean-shards-per-node-for-index)\nWeight (shard, node) = shard-weight + index-weight\n# theta0 and theta1 are user configurable constants.\n# theta0 + theta1 = 1\n# mean-shards-per-node = num-of-shards-in-cluster / num-nodes-in-cluster\n# mean-shards-per-node-for-index = num-shards-in-cluster-for-index / num-nodes-in-cluster The function ensures that all nodes hold the same number of shards, and shards for each index are spread across nodes.\nIf a node holds too many shards, its deviation from mean-shards-per-node is high, which increases the shard-weight factor. If too many shards of an index land on the same node, its deviation from mean-shards-per-node-for-index goes up, increasing the index-weight factor. Both of these increase the overall weight for shard on a node, indicating that shard be moved to a node with lesser weight.\nThe contribution of each factor can be controlled by two dynamic settings. cluster.routing.allocation.balance.shard ‚Äì Controls shard-weight (Reduced to theta0 in above equations [ code]) cluster.routing.allocation.balance.index ‚Äì Controls index-weight (Reduced to theta1 in above equations [ code])\nThere is another knob to control rebalancing ‚Äî cluster.routing.allocation.balance.threshold. Shard balancing is an optimization problem. Moving a shard from one node to another demands system resources like CPU and network. At some point, the benefit achieved by rebalancing ceases to outweigh the cost of moving shards around. The threshold setting lets us fine-tune this tradeoff. Elasticsearch will rebalance only if the weight delta between nodes, is higher than configured threshold [ code].\nAny non-negative float value is acceptable for the threshold variable. Elasticsearch will rebalance shards, if the weight difference after rebalance is more than this threshold. Deciding the right value for this threshold however, is involved. You could substitute the number of nodes, shards, and shards per index into the weight function above to get an idea. Or experiment with some values to see what works best.\nThe Beauty of Using Shard Count\nSimple solutions to complex problems have intangible engineering value. Shard count provides a simple, lightweight heuristic around how loaded a node is. And for a majority of use cases, it is a reasonable signal. Nodes with more shards get more traffic, and have more disk, CPU, and memory consumption as compared to nodes with fewer shards. Equalizing on shard count works especially well, if all your indexes handle similar workloads.\nShard allocation can be seen as a modified bin-packing problem. You want to distribute m items (shards) across n bins (nodes) so as to minimize load on the most loaded bin.\nUsing shard count as the balancing signal, simplifies this problem since shard count is a uniform, deterministic value. Assigning items (shards) to the least filled bin (node) so that all bins fill up uniformly gives even distribution. Changing this to actual resource usage signals like JVM, CPU, disk or network footprint of a shard, makes the items non-uniform, which considerably complicates the problem space\nShard count is a uniform signal. Metrics like JVM heap, CPU or memory consumption fluctuate very frequently, and require smoothing approximation mechanisms like moving averages. Using shard count eliminates this extra need for signal cleanup.\nChanges to shard count, like adding/deleting indexes, changing replica counts for existing indexes, or shrink/split APIs, all go via cluster state updates. Distributing these changes in cluster state allows for the current event driven model for shard balancer, where all allocation and rebalance scenarios are evaluated only in response to cluster state updates (or explicit reroute API calls).\nIn contrast, balancing on metrics like shard size (disk usage) requires periodic rebalance checks based on updated shard sizes. Elasticsearch does have a periodic internal monitor to track disk usage. But it is used by the balancer, only when disk watermarks are breached. At which point, the node stops receiving new shards (low watermark) or moves existing shards out (high watermark). Disk usage does not factor into shard balancing until watermarks are hit.\nRoad Ahead\nThe shard count heuristic provided a good foundational metric for early Elasticsearch versions. If you are running a small to medium sized cluster, or even a production grade cluster with homogeneous workloads, it can provide acceptable performance. But at AWS scale, we see clusters pushed to their limits. When throwing more machines ceases to help with a problem, we must go back and think from first principles.\nBeyond Homogeneity\nAt petabyte scale, non-uniform workloads are a norm rather than the exception. For example, you might be supporting an enterprise-wide analytics platform with many different business units storing their own indexes. Shards across such indexes can vary significantly in ingestion rates and query traffic. One team might index several gigabytes of data every hour, while another may take a month to ingest 1gb.\nWorkarounds today involve splitting your cluster by workload and using cross cluster search, index rollover by time (e.g. daily) or shard size, or creating an index life cycle with hot, warm and cold stages. Individually, these features solve separate important problems. Using them for load balancing however, is trying to force homogeneity onto a problem that is inherently diverse.\nThese workarounds have drawbacks.\nCross cluster is great to organize and split clusters by business use cases. You could create a cluster for finance and another one for inventory. But predicting workloads, creating uniform cluster splits and mapping each team to the right cluster, incurs significant management overhead. Not to mention the boiler plate cost of each split cluster.\nRolling over by size or time still creates skewed indexes within the rollover window. Rotating at smaller sizes reduces skew, but quickly explodes to an unstable cluster with too many shards. Index lifecycle is great for archiving old data and clearing up resources. But it is not a guarantee of uniformity. One team‚Äôs hot shard may have lower footprint than what another team considers a cold shard.\nWe need to embrace that shards have inherently diverse resource requirements, and balancing should consider their individual footprints.\nDiverse Signals, Hybrid Clusters\nShards could be balanced by shard heat ‚Äì the actual resource footprint of a shard. Signals like JVM heap, CPU, memory, network and disk consumption could be actual indicators for shard heat. Balancing would then, map shard heat to resource availability on nodes.\nThe present-day shard count is a placeholder signal that occasionally correlates with resource consumption. Future balancers should consider multiple relevant signals. For example, shard size alone is a good signal for disk usage, but not sufficient by itself. Large shards are often cold shards from index rollovers. And in most modern-day systems, JVM heap and CPU, are more precious than disk space. To work across these multiple dimensions, resources could define priority ‚Äì balance on memory before disk usage.\nMapping shard requirement to resource availability opens gates for diversity in resources as well. Clusters can comprise hybrid nodes with different capabilities to best fit the price-performance metric.\nCompute Intensive in Critical Path\nOur former post described algorithms used to check for move and rebalance operations. These operations run in the order of num-shards * num-nodes, and are performed by the master node alone during cluster state changes.\nThis incurs significant processing cost in clusters with high shard and node count. While shard movement is a cluster state change decision that has to happen at master, checking for imbalance could be made periodic and moved out of the state update path.\nIndexing Hot Spots\nThe current count-based weight function considers deviation from mean in total-node-shard-count and index-level-node-shard-count. In a sufficiently sized cluster, each node can hold a few hundred shards. In contrast, a single index would typically have only 5-10 shards.\nWhen you add an empty node to this cluster, during cluster scale out or failed node replacement, the new node joins with zero shard count. If you calculate the weight for this node, the total shard count deviation heavily outweighs the deviation created due to index level shard count. Even when all shards of an index land on the new node, its net weight is still very low due to the large negative factor added by total-node-shard-count ‚Äì mean-node-shard-count.\nThis low weight value keeps the new node as most eligible for receiving all shards of any new index. It is only when the node gets sufficiently filled up, and total-node-shard-count approaches mean-node-shard-count, that the index-weight becomes significant. At this point, balancer moves the new index shards, out of this new node.\nThis is the index-level shard allocation hot spot problem. In any cluster of reasonable size, if you add a single node, or replace a failed node, all shards of any newly created index land on the newly added node. Since new indexes are usually high traffic targets, this node then becomes an indexing bottleneck in the cluster. The node continues to remain as a hotspot until shards from other nodes fill it up, which can be considerably long, since shard movement takes time. Furthermore, there is an added overhead of moving the new shards out when the node finally gets filled up and index-weight kicks in.\nParting Thoughts\nCustomer obsession and diving deep, are guiding principles at AWS. The problems we discuss here, were realized working backwards from actual customer issues. Shard balancing is an involved multi-variable optimization problem. The default allocator implementations served as a good starting ground, it powers the distributed engine we all love today. But as we push the envelope with scale, we must innovate and re-imagine the future of these components.\nWe are working on the ideas discussed above 3, and will keep the open-source community involved in our progress. Suggestions, ideas and inputs from the OpenDistro for Elasticsearch community are welcome. You can post your suggestions here.\nFootnotes There are other functions that also consume node resources ‚Äì like cluster coordination on master node, query coordination and result aggregation on coordinator node, or ingestion related tasks on ingest nodes. But since shards are at the center of any activity in Elasticsearch, shard footprint is the dominant resource utilization signal on data nodes. As of this writing, i.e. Elasticsearch v7.6.1 Solving indexing hot spots with allocation constraints. See Issue",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Open-Distro-for-Elasticsearch-1.7.0-released/",
    "title": "Open Distro for Elasticsearch 1.7.0 Released",
    "content": "Open Distro for Elasticsearch 1.7.0 is now available for download. Upgrade to 1.7.0 to leverage the latest features and bug fixes.\nThe release consists of Apache 2.0 licensed Elasticsearch version 7.6.1, Kibana version 7.6.1, new plugins for anomaly detection and SQL workbench, a new SQL ODBC driver and SQL CLI client. Other plugins in the distribution include alerting, index management, performance analyzer, security, SQL and machine learning with k-NN. A SQL JDBC driver and PerfTop, a client for Performance Analyzer are also available for download.\nDownload the latest packages\nYou can find Docker Hub images Open Distro for Elasticsearch 1.7.0 and Open Distro for Elasticsearch Kibana 1.7.0 on Docker Hub. Make sure your compose file specifies 1.7.0 or uses the latest tag.\nIf you‚Äôre using RPMs or DEBs, see our documentation on how to install Open Distro for Elasticsearch with RPMs and install Open Distro for Elasticsearch with Debian packages. A tarball is also available for testing and other applications.\nA Windows ready package supporting version 1.7.0 enables users to easily install Elasticsearch and Kibana on Windows.\nIf you‚Äôre using Kubernetes, check out the Helm chart to install Open Distro for Elasticsearch.\nYou can find Open Distro for Elasticsearch security, alerting notification and job scheduler artifacts on Maven Central.\nYou can download the latest versions of Open Distro for Elasticsearch‚Äôs PerfTop client on NPM, Open Distro for Elasticsearch‚Äôs latest SQL CLI client on PyPi. SQL drivers supporting ODBC and JDBC are also available.\nRelease Highlights\nThe anomaly detection plugin has moved out of the preview phase and is now generally available. It comes with a easy-to-use Kibana user interface and supports real-time anomaly detection.\nThe SQL plugin has greatly expanded its supported operations, added a new Kibana user interface, and now has an interactive CLI with autocomplete.\nA new SQL ODBC driver is also now available.\nA new package, the Open Distro for Elasticsearch 1.7.0 AMI, is now available for use with Amazon EC2. Search for ‚Äúopen distro‚Äù when you start a new instance.\nPlease take a look at the detailed 1.7.0 release notes for the latest features, enhancements, infra and documentation updates and bug fixes. Upgrade to 1.7.0 to leverage the latest features and bug fixes.\nCome join our community!\nThere are many easy ways to participate in the Open Distro community!\nAsk questions and share your knowledge with other community members on the Open Distro discussion forums.\nAttend our bi-weekly online community meetup to learn more about Elasticsearch, security, performance, machine learning and more.\nFile an issue, request an enhancement or feature you need or suggest a plugin you need at github.com/opendistro-for-elasticsearch.\nContribute code, tests, documentation and even release packages at github.com/opendistro-for-elasticsearch. If you want to showcase how you‚Äôre using Open Distro, write a blog post for opensearch.org/blog. If you‚Äôre interested, please reach out to us on Twitter. You can find me @alolita or Jon at @jon_handler.\nCheck out what‚Äôs in development and get involved in components like the Performance Analyzer RCA Engine.\nWe also invite you to get involved in ongoing development of new Open Distro for Elasticsearch plugins, clients, drivers. Contribute code, documentation or tests for Performance Analyzer RCA Engine.\nYou can also track upcoming features in Open Distro for Elasticsearch by watching the code repositories or checking the project website. Thanks again for using and contributing to Open Distro for Elasticsearch and being part of the project‚Äôs growing community!",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Real-time-Anomaly-Detection-is-now-available-in-Open-Distro-for-Elasticsearch-1.7.0/",
    "title": "Real-time Anomaly Detection is now available in Open Distro for Elasticsearch 1.7.0",
    "content": "We are excited to announce the general availability of real-time anomaly detection for streaming applications in this release. We would like to thank the community for their feedback on the preview release of the feature. The anomaly detection feature is built on RCF (Random Cut Forest), an unsupervised algorithm, that detects anomalies on live data and identifies issues as they evolve in real time. RCF is a proven algorithm built on years of academic and industry research. We are glad to announce the general availability of the open source RCF libraries for the greater benefit of our data science community.\nOne of our key considerations was to design the anomaly detection to be lightweight so there is no overhead on the system resources processing application workloads. The computation of anomaly models are distributed across the nodes in Elasticsearch cluster, which makes the implementation highly scalable, and not requiring dedicated machine learning nodes. For more deep dive into anomaly detection system design and RCF algorithm, we recommend these previously released blogs: Real-time Anomaly Detection in Open Distro for Elasticsearch and Random Cut Forests. Our new Kibana user interface for anomaly detection makes it easy for users with no prior machine learning knowledge to take advantage of the feature. The rich visualizations make it easy for users to detect the data points that contributed to an anomaly. The plugin is integrated with Open Distro for Elasticsearch Alerting to notify users through various supported channels as the anomalies are detected.\nFigure 1: The new Anomaly Detection dashboard Figure 2: The Anomaly Detection results view",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Index-State-Management-in-Open-Distro-for-Elasticsearch/",
    "title": "Index State Management in Open Distro for Elasticsearch",
    "content": "Elasticsearch is an open source distributed search and analytics engine based on Apache Lucene. After adding your data to Elasticsearch, you can perform full-text searches on the data with all of the features you may expect: search by field, search multiple indices, boost fields, rank results by score, sort results by field, and aggregate results. You can also use Kibana to build visualizations for data in Elasticsearch.\nIn this blog post, I used the Index State Management (ISM) plugins in Open Distro for Elasticsearch, a fully Apache 2 licensed open source distribution of Elasticsearch, to define custom management policies to automate routine tasks and apply them to indices and index patterns. The ISM plugin provides an automated solution for repetitive index management tasks such as set up and management of external custom processes to run your index operations.\nAs I evaluated the ISM plugin in Open Distro for Elasticsearch, my goal was to find a solution for my customer‚Äôs challenge of managing time-series data. I was looking to maintain multiple indices for a diverse data workloads and set up an automated solution to periodically perform certain housekeeping index management functions. I prioritized new data over existing data. The housekeeping functions included marking the indices as read-only, reducing the segment count of the indices, closing these indices to improve performance and then deleting these indices when they are no longer needed.\nSolution implemented\nElasticsearch can contain any number of indices. The data is unique to each index. I used the ISM plugin to define a lifecycle index management policy that has four states - read-only, force_merge, close and delete. Note that as a best practice, you should be setting your index to read_only before calling force_merge.\nI installed Open Distro for Elasticsearch using a Docker image using directions from this blog post. There are several other download options supporting Open Distro as outlined here.\nOnce Open Distro for Elasticsearch is installed, the first step is to log in using the Kibana UI. See figure 1. Figure 1: Login screen for Open Distro for Elasticsearch On Kibana‚Äôs splash page, choose to add the all the sample data listed in the screen by selecting Try our sample data and clicking on Add data. See figure 2. Figure 2: Adding data using Kibana interface Once the data is added, switch to the Index Management tab which lands into the Index Policies page as shown in figure 3. Figure 3: Index policies summary page Click on Create Policy, enter a policy name and replace the default policy with the policy mentioned below. Don‚Äôt worry on the policy content yet, I will discuss about each section at a later point. See figure 4. Figure 4: Policy creation page This code block below describes the policy I created for this post. {\n\"policy\": {\n\"description\": \"Lifecycle Management Policy\",\n\"default_state\": \"force_merge\",\n\"states\": [\n{\n\"name\": \"read_only\",\n\"actions\": [\n{\n\"read_only\": {}\n}],\n\"transitions\": [\n{\n\"state_name\": \"force_merge\",\n\"conditions\": {\n\"min_index_age\": \"1d\"\n}\n}]\n},\n{\n\"name\": \"force_merge\",\n\"actions\": [\n{\n\"force_merge\": {\n\"max_num_segments\": 1\n}\n}],\n\"transitions\": [\n{\n\"state_name\": \"close\",\n\"conditions\": {\n\"min_index_age\": \"2d\"\n}\n}]\n},\n{\n\"name\": \"close\",\n\"actions\": [\n{\n\"close\": {}\n}],\n\"transitions\": [\n{\n\"state_name\": \"delete\",\n\"conditions\": {\n\"min_index_age\": \"3d\"\n}\n}]\n},\n{\n\"name\": \"delete\",\n\"actions\": [\n{\n\"notification\": {\n\"destination\": {\n\"chime\": {\n\"url\": \"&lt;CHIME_WEBHOOK_URL&gt;\"\n}\n},\n\"message_template\": {\n\"source\": \"The index is being deleted because of actioned policy \"\n}\n}\n},\n{\n\"delete\": {}\n}]\n}]\n}\n} Click Create to save the policy. I named the policy as ‚Äúism-policy-sample‚Äù as seen in figure 5. Figure 5: Index policies summary page Next, navigate to the Indices tab and search for the kibana-sample indices keyword which should list all the sample data indices which was added earlier. See figure 6. Figure 6: Indices summary page Choose all the indices and select Apply Policy.\nFrom the Policy ID dropdown, select the policy created in the previous step and click on Apply as seen in figure 7. Figure 7: Policy creation confirmation page The policy is now assigned and starts managing the indices. Navigate to the Managed Indices tabs and observe the status as Initializing as seen in figure 8. Figure 8: Managed indices summary page The pane also provides an option to set a refresh frequency to refresh the managed indices status information as seen in figure 9. Figure 9: Refresh frequency settings Demystifying the policy\nLet me explain about the index policy tenets and how they are structured.\nPolicies are JSON documents that defines - * the states that an index can be in\n* any actions that you want the plugin to take when an index enters the state\n* conditions that must be met for an index to move or transition into a new state The policy document begins with basic metadata like description, default_state that the index should enter, and finally a series of state definitions.\nLet me start defining the first state ‚Äì read_only. In this state, the managed indices will be set to read_only and the transition will move to force_merge state after one day. {\n\"name\": \"read_only\",\n\"actions\": [\n{\n\"read_only\": {}\n}],\n\"transitions\": [\n{\n\"state_name\": \"force_merge\",\n\"conditions\": {\n\"min_index_age\": \"1d\"\n}\n}]\n} Let us quickly verify from the console. The current state is read_only and the transitions to force_merge after one day. See figure 10. Figure 10: Managed indices summary at the first state The force_merge action defined will compress the indices segments. Transitions define the conditions that need to be met for a state to change, which in my case, is close state once the index crosses 24 hours. {\n\"name\": \"force_merge\",\n\"actions\": [\n{\n\"force_merge\": {\n\"max_num_segments\": 1\n}\n}],\n\"transitions\": [\n{\n\"state_name\": \"close\",\n\"conditions\": {\n\"min_index_age\": \"2d\"\n}\n}]\n} Verify the status of the managed indices at this stage again. See figure 11. Figure 11: Managed indices summary at this state Note that closed indices remain on disk, but do not consume any CPU or memory. I can‚Äôt read from, write to, or search closed indices. If I need to use a closed index again, reopening the closed index is simpler than restoring the index from a snapshot.\nThe action defines the indices to be in a closed state for a day and then transitions them to a final delete state when the index age is three days old. {\n\"name\": \"close\",\n\"actions\": [\n{\n\"close\": {}\n}],\n\"transitions\": [\n{\n\"state_name\": \"delete\",\n\"conditions\": {\n\"min_index_age\": \"3d\"\n}\n}]\n} The status of the indices in this state can be verified again. See figure 12. Figure 12: Managed indices summary at this state Notice that the delete state has two actions defined. The first action is self-explanatory, sends a notification as defined in the message_template to the destination. {\n\"name\": \"delete\",\n\"actions\": [\n{\n\"notification\": {\n\"destination\": {\n\"chime\": {\n\"url\": \"&lt;CHIME_WEBHOOK_URL&gt;\"\n}\n},\n\"message_template\": {\n\"source\": \"The index is being deleted because of actioned policy \"\n}\n}\n},\n{\n\"delete\": {}\n}]\n} I have configured the notification endpoint to be on Amazon Chime CHIME_WEBHOOK_URL. More information on using webhooks for Amazon Chime can be found here.\nAt this state, I have received the notification on the Chime webhook as seen in the figure below. Figure 13: Chime webhook notifications Below is the status from the console. See figure 14. Figure 14: Final status of the managed indices Once the notification is successfully sent, the policy executes the next action in the state which is deleting the indices. After this final state, the indices will no longer appear in the Managed Indices section.\nConclusion\nHopefully this blog post provided you an understanding of how-to use the index state management plugin in Open Distro for Elasticsearch. My walkthrough of how indices can be managed using this plugin with a sample life-cycle policy.\nCheck out the full documentation for the index state management plugin here. If you need enhancements or have other feature requests, feel free to file an issue here. Also check out the contribution guidelines here to get more involved in the project.\nA big takeaway for me as I evaluated the ISM plugin in Open Distro was that the index state management plugin is fully compatible and works on the Amazon Elasticsearch Service. This is very useful for using Open Distro for Elasticsearch as a on-premise or internal solution while leveraging a managed service for your production workloads. If you are using hybrid environments, you can learn more here about how this index state management functionality works on the managed service.\nSpecial thanks to Alolita Sharma, Sriram Kosuri and Drew Baugher from the ISM search engineering team for their help on this post.",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Open-Distro-for-Elasticsearch-1.8.0-is-released-supports-Cosine-Similarity-in-k-NN/",
    "title": "Open Distro for Elasticsearch 1.8.0 is released, supports Cosine Similarity in k-NN",
    "content": "We are pleased to announce the release for Open Distro for Elasticsearch 1.8.0. that now supports cosine similarity distance metric with k-NN. We released k-NN similarity search feature in Open Distro for Elasticsearch 1.4.0 with support for Euclidean distance to calculate similarity between the vectors. With cosine similarity, you can now measure the orientation between two vectors while Euclidean distance is used to measure the magnitude. We would also like to thank our community for contributing snapshot support in Index Management. This feature enables users to define snapshot action in Index State Management for retention and to migrate indices from one cluster to another. Open Distro for Elasticsearch 1.8.0 can be downloaded here.\nThe release consists of Apache 2.0 licensed Elasticsearch version 7.7.0, and Kibana version 7.7.0. This distribution also includes alerting, anomaly detection, index management, performance analyzer, security, SQL and k-NN plugins. Other components including SQL Workbench, SQL ODBC and JDBC drivers, SQL CLI client, and PerfTop, a client for Performance Analyzer are also available for download.\nDownload the latest packages\nYou can find Docker Hub images Open Distro for Elasticsearch 1.8.0 and Open Distro for Elasticsearch Kibana 1.8.0 on Docker Hub. Make sure your compose file specifies 1.8.0 or uses the ‚Äòlatest‚Äô tag.\nIf you‚Äôre using RPMs or DEBs, see our documentation on how to install Open Distro for Elasticsearch with RPMs and install Open Distro for Elasticsearch with Debian packages. A tarball is also available for testing and other applications.\nA Windows ready package supporting version 1.8.0 enables users to easily install Elasticsearch and Kibana on Windows.\nIf you‚Äôre using Kubernetes, check out the Helm chart to install Open Distro for Elasticsearch.\nYou can find Open Distro for Elasticsearch security, alerting notification and job scheduler artifacts on Maven Central.\nYou can download the latest versions of Open Distro for Elasticsearch‚Äôs PerfTop client on npm.org, Open Distro for Elasticsearch‚Äôs latest SQL CLI client on PyPi. SQL drivers supporting ODBC and JDBC are also available. Release Highlights\nNew feature Cosine Similarity is available for use in k-NN plugin.\nThe snapshot feature is now available in the Index Management plugin.\nAnomaly Detection plugin releases the new count aggregation feature to detect anomalies.\nSupport for connecting PerfTop CLI, a client to interact with Performance Analyzer, to clusters with basic authentication.\nPlease take a look at the 1.8.0 release notes for the latest features, enhancements, infra and documentation updates and bug fixes. Upgrade to 1.8.0 to leverage the latest features and bug fixes.\nCome join our community!\nThere are many easy ways to participate in the Open Distro community!\nAsk questions and share your knowledge with other community members on the Open Distro discussion forums.\nAttend our bi-weekly online community meetup to learn more about Elasticsearch, security, performance, machine learning and more.\nFile an issue, request an enhancement you need or suggest a plugin you need at github.com/opendistro-for-elasticsearch.\nContribute code, tests, documentation and even release packages at github.com/opendistro-for-elasticsearch. If you want to showcase how you‚Äôre using Open Distro, write a blog post for opensearch.org/blog. If you‚Äôre interested, please reach out to us on Twitter. You can find us at Viraj at @vrphanse or Alolita at @alolita or Jon at @jon_handler.\nWe also invite you to get involved in ongoing development of new Open Distro for Elasticsearch plugins, clients, drivers. Contribute code, documentation or tests for Performance Analyzer RCA Engine. You can also track upcoming features in Open Distro for Elasticsearch by watching the code repositories or checking the project website.\nThanks again for using and contributing to Open Distro for Elasticsearch and being part of the project‚Äôs growing community!\nAbout the Authors\nPavani Baddepudi is a Senior Product Manager working in Search Services at Amazon Web Services.\nAlolita Sharma is a Principal Technologist at AWS focused on all things open source including Open Distro for Elasticsearch. You can find her on GitHub or Twitter @alolita.",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/An-overview-of-the-SQL-Engine-in-Open-Distro-for-Elasticsearch/",
    "title": "An overview of the SQL Engine in Open Distro for Elasticsearch",
    "content": "Open Distro for Elasticsearch is a popular choice for use cases such as log analytics, search, real-time application monitoring and click-stream analysis and more. One commonality among the various use cases is the need to write and run queries to obtain search results at lightning speed, and that in turn, requires the user to have expertise in the JSON based Elasticsearch query domain-specific language (Query DSL). Although Query DSL is powerful, it has a steep learning curve, and was not designed as a human interface to easily create ad hoc queries and explore user data. In order to solve this problem, we provided a SQL engine with Open Distro for Elasticsearch, which we have been expanding since the initial release. As part of this continued investment, we are happy to announce new capabilities we are adding including a Kibana-based SQL Workbench and a new SQL CLI that will make it easier than ever for Open Distro for Elasticsearch users to use the SQL engine to work with their data.\nStructured Query Language (SQL) is not only the de facto standard for data and analytics, but also one of the most popular languages among engineers and DevOps experts. Introducing SQL in Open Distro for Elasticsearch, allows users to manifest search results in a tabular format with documents represented as rows, fields represented as columns, and index names represented as a table names in the WHERE clause. This acts as a straightforward and declarative way to represent complex DSL queries in a readable format. The newly added tools, the SQL Workbench and the SQL CLI, in addition to the existing ODBC and JDBC drivers, and the SQL engine‚Äôs support for joins, and mathematical and string functions, can act as a powerful yet simplified way to extract and analyze data, and support complex analytics use cases.\nBelow is an overview of the features of the SQL engine in Open Distro for Elasticsearch.\nQuery Tools SQL Workbench A comprehensive and integrated visual tool to run on-demand SQL queries, translate SQL into its REST equivalent, and view and save results as text, JSON, JDBC or CSV SQL CLI An interactive stand-alone command line tool to run on-demand SQL queries, translate SQL into its REST equivalent, and view and save results as text, JSON, JDBC or CSV Connectors and Drivers ODBC Driver Open Database Connectivity (ODBC) driver enables connecting with business intelligence (BI) applications such as Tableau, and exporting data to CSV and JSON JDBC Driver Java Database Connectivity (JDBC) driver enables connecting with business intelligence (BI) applications such as Tableau, and exporting data to CSV and JSON\nQueries, Delete Support Basic Queries Support for SELECT clause, along with FROM, WHERE, GROUP BY, HAVING, ORDER BY, and LIMIT to search and aggregate data Complex Queries Support for complex queries such as subquery, join, union, and minus operating on more than one Elasticsearch index Metadata Queries Support for querying basic metadata about Elasticsearch indices using SHOW and DESCRIBE commands Delete Allows deleting of all the documents or documents that satisfy predicates in the WHERE clause from search results and not from the actual Elasticsearch index\nJSON and Full-text search Support JSON Support for JSON by following PartiQL specification, a SQL-compatible query language that lets you query semi-structured and nested data for any data format Full-Text Search Support Support for full-text search on millions of documents using SQL commands\nFunctions and Operator Support Functions and Operators Support for string functions and operators, numeric functions and operators, and date-time functions by enabling field data in the document mapping\nSettings Settings Allows viewing, configuring and modifying setting to control the behavior of SQL without need to restart/bounce the Elasticsearch cluster\nInterfaces End points Explain shows the translated SQL query as Elasticsearch Query DSL, and cursor helps obtain a paginated response\nMonitoring Monitoring Node level statistics can be obtained using stats endpoint\nRequest and Response Protocols Request Protocol Support for HTTP POST as a request protocol Response Protocols Support for response protocols such as JDBC, CSV, Elastic DSL, Raw Format\nSQL engine in Open Distro for Elasticsearch provides a comprehensive, flexible and user friendly set of features to obtain search results from Elasticsearch in a declarative manner using SQL. To learn more, click here.\nHave feedback on the SQL engine for Open Distro for Elasticsearch or any of the new tools? Suggestions, ideas and inputs from the OpenDistro for Elasticsearch community are always welcome. You can post your suggestions here.",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Introducing-Real-Time-Root-Cause-Analysis-Engine-in-Elasticsearch/",
    "title": "Introducing real-time Root Cause Analysis Engine in Elasticsearch ",
    "content": "We are excited to release the real-time Root Cause Analysis engine for Elasticsearch in Open Distro for Elasticsearch version 1.9.0.\nThe RCA engine enables operators to diagnose and identify bottlenecks in their Elasticsearch clusters.\nThis is a value-add feature on top of the current array of low-level metrics they have access to with Performance Analyzer.\nOperators can leverage the RCA to tune the relevant Elasticsearch settings and adjust the workloads to ensure cluster\nstability and performance. In this blog, we dive into some aspects of the RCA engine and how it can be leveraged for\nElasticsearch cluster diagnosis.\nIntroduction\nElasticsearch can be difficult to manage, especially at scale. We released Performance Analyzer to provide deep visibility into Elasticsearch shards, nodes, storage and network resources to quantify resource health\nand usage (starvation, overuse and contention). However, operators are required to have deep insights into\nElasticsearch to debug and identify the root cause once things start to go sideways. The RCA engine is incredibly useful\nfor reducing the operational burden of Elasticsearch clusters. Several mission-critical systems and applications rely\non Elasticsearch and when the cluster experiences performance or stability issues, it is critical to get it back on\ntrack as quickly as possible. RCA engine helps operators transition immediately from being alerted about an issue to\nexecuting remediation actions. RCA uses the metrics emitted by the Performance Analyzer to infer and pinpoint\nperformance and reliability problems in the database and the underlying infrastructure layers.\nRCAs are designed to diagnose the problems, with accuracy (precision and recall), and in the shortest time possible for\nthat accuracy level.\nLet‚Äôs examine a use case on how RCA engine can be utilized. Imagine you are managing a 3 node Elasticsearch cluster\n(nodes A, B, and C) containing indexes without any replicas. The cluster health appears GREEN, but every few minutes,\nthe query rejection rate spikes up. While investigating this issue, you notice that all the data for the rejected\nqueries resides on node B. Using Performance Analyzer metrics, you notice that the JVM memory pressure and CPU\nutilization metrics coming from Node B follows a sawtooth pattern.\nThese metrics line up with the query rejections you noticed, great! While you‚Äôve made progress on your investigation,\nyou still don‚Äôt know the root cause of this behavior. Is it the CPU or the memory? How can you fix the issue?\nAt this point, you might be tempted to simply restart the Elasticsearch node. When you restart your node, the spiky\nbehavior stops! Everything seems to go back to normal until you get paged a few minutes later with the exact same issue.\nTime for more investigation now ‚Ä¶\nThis investigation process can be tiring and may breach your service SLAs. This is where the RCA engine comes in handy\nand helps alleviate the burden involved in debugging Elasticsearch issues. Let‚Äôs examine how we could investigate the\nexact same issue with the help of the RCA engine.\nWe notice something is wrong so we issue a general RCA query to our elected master node A curl &lt;master node&gt;:9600/_opendistro/_performanceanalyzer/rca -XGET Most of our resources look healthy, but the HighHeapUsageClusterRca is in an unhealthy state. So we drill down into that specific RCA using curl &lt;master node&gt;:9600/_opendistro/_performanceanalyzer/rca?name=HighHeapUsageClusterRca -XGET Look at the RCA response, see if anything stands out to you: { \"HighHeapUsageClusterRca\": [ { \"rca_name\": \"HighHeapUsageClusterRca\", \"state\": \"unhealthy\", \"timestamp\": 1587426650942, \"HotClusterSummary\": [ { \"number_of_nodes\": 2, \"number_of_unhealthy_nodes\": 1, \"HotNodeSummary\": [ { \"host_address\": \"192.168.0.X\", \"node_id\": \"JtlEoRowSI6iNpzpjlbp_Q\", \"HotResourceSummary\": [ { \"resource_type\": \"young gen\", \"unit_type\": \"promotion rate in mb/s\", \"threshold\": 400, \"value\": 500,... }] }] }] }] } Notice that while your acceptable young garbage collection (GC) promotion rate is 400 MB/s,\nthe actual rate on node B is 500 MB/s. You can conclude that this young generation promotion rate is the most likely\ncandidate for your cluster‚Äôs spiky, degraded performance. When objects are quickly promoted from the young generation,\nthey start to fill up the old generation (or tenured space) of the JVM heap. Once the tenured space becomes full,\nan expensive event called Major GC takes place. When this occurs, the JVM stops executing instructions until the\ngarbage collection completes. This garbage collection operation can be CPU intensive as well. The solution to this\nproblem can be a simple change to the young generation size by altering -XX:NewSize, -XX:MaxNewSize and -XX:SurvivorRatio parameters of the Elasticsearch process.\nRCA particularly shines at scale. Imagine the same problem discussed above in a larger, 100 node multi-tenant production\ncluster. Manual diagnosis of the issue is like finding a needle in a haystack, but isolating the problem using RCA\nfollows the same basic steps: issue a query to the master, read the output, and take appropriate actions. RCA framework is\npowerful, lightweight and highly extensible that enables you to write your own RCAs for the resources or combination of resources that are\nimportant for your particular cluster. We plan to release a series of blogs with the RCAs that you can leverage and reduce\nthe mean time to repair (MTTR) your cluster.\nRCA Framework System Design\nWe define a root cause as a function of one or more symptoms and/or one or more RCAs and metric(s).\nA symptom is an operation applied to one or more metrics and/or other symptoms. Root causes may also be a function of\nother root causes. The following equations show an example of these relationships // A resource state is a function of one or many metrics and theresholds. The\n// function over the metrics and threshold is the evaluation function.\nresource_state = f(metric1, threshold1)\n// An rca is a function of one or many resource states and other metrics and some\n// other RCAs. Not all of them are required.\nrca = g(resource_state, metric2, threshold2, rca1) The RCA framework is modeled as a distributed data-flow graph (see Fig 1) where data flows downstream from the leaf\nnodes to the root. Leaf nodes of the graph represent Performance Analyzer metrics on which intermediate computations\nare performed. The intermediate nodes can be RCAs or other derived symptoms which helps in computation of the final\nRCA(s). The framework operates on a single analysis graph which is composed of multiple connected-components. Fig 1: A multi-level RCA graph composed of metrics and thresholds\nExecution of an RCA is the evaluation of the data-flow graph on the physical nodes of the cluster (Fig 2).\nWe evaluate the RCA directed acyclic graph (DAG) in a particular order such that all the data for the RCA is available\nwhen it is being evaluated. Nodes in the cluster can also communicate among themselves to share the output of graph\nevaluations. For simplicity, we let only the currently active master node listen to the output emitted by other\ndata nodes. Fig 2: Execution model of RCA graph across Elasticsearch nodes\nRCA to diagnose high heap usage\nMost Elasticsearch clusters perform reasonably well when they are configured with the right JVM heap pressure with\nright configuration settings. However, the heap pressure in Elasticsearch could shoot up for a variety of reasons\nsuch as organic growth in traffic, spike in customer requests, rogue queries etc. We‚Äôve leveraged the RCA framework\nand created two JVM RCAs to help you identify and diagnose heap pressure issues. Out of the box, you‚Äôll get access to\nthe HighHeapUsageClusterRca, which diagnose old and young generation issues in the JVM garbage collector. By querying\nthe RCA API, you‚Äôll be able to quickly identify if your GC is unhealthy and which consumers (fielddata cache/request\ncache, Lucene segment memories, etc.) are consuming the most memory. API Usage\nUse the API below on the elected master node to get a cluster level view of the JVM RCA output. curl --url \"localhost:9600/_opendistro/_performanceanalyzer/rca?name=HighHeapUsageClusterRca\" -XGET Sample RCA response from above API { \"HighHeapUsageClusterRca\": [ { \"rca_name\": \"HighHeapUsageClusterRca\", \"state\": \"unhealthy\", \"timestamp\": 1587426650942, \"HotClusterSummary\": [ { \"number_of_nodes\": 3, \"number_of_unhealthy_nodes\": 2, \"HotNodeSummary\": [ { \"host_address\": \"192.168.144.X\", \"node_id\": \"JtlEoRowSI6iNpzpjlbp_Q\", \"HotResourceSummary\": [ { \"resource_type\": \"old gen\", \"threshold\": 0.65, \"value\": 0.81827232588145373, \"avg\": NaN, \"max\": NaN, \"min\": NaN, \"unit_type\": \"heap usage in percentage\", \"time_period_seconds\": 600, \"TopConsumerSummary\": [ { \"name\": \"CACHE_FIELDDATA_SIZE\", \"value\": 5907025640 }, { \"name\": \"CACHE_REQUEST_SIZE\", \"value\": 28375 }, { \"name\": \"CACHE_QUERY_SIZE\", \"value\": 12687 }], }] }, { \"host_address\": \"192.168.144.Y\", \"node_id\": \"v2gdRXDFQHioIULQ5vn_7A\", \"HotResourceSummary\": [ { \"resource_type\": \"young gen\", \"threshold\": 400.0, \"value\": 512.8, \"avg\": NaN, \"max\": NaN, \"min\": NaN, \"unit_type\": \"promotion rate in mb/s\", \"time_period_seconds\": 600, }] }] }] }] } The JVM RCA API response returns a list of nodes in the cluster that are marked unhealthy. Each unhealthy node is\naugmented with a nested list of unhealthy JVM resources. From the list resource summaries, we can quickly identify the\nroot cause of the JVM issue. i.e. whether the heap contention is from the old generation or the young generation of the\nJVM heap and the resource metrics that breach the set thresholds.\nIn the sample response above, two out of three nodes in the cluster are under JVM pressure and one node has old\ngeneration contention while the other has young generation issue. Let‚Äôs assume that we set the GC parameter XX:CMSInitiatingOccupancyFraction to 75 when we start the ES process. This setting means that Major GC will kick in\nonce heap usage is above 75%. However, from the sample response, node with ip address ‚Äú192.168.144.X‚Äù which is\nexperiencing old generation contention has 81% occupancy even after a Major GC. This points to the fact that a huge\nchunk of old generation heap is occupied by long lived objects that can not be garbage collected. So our next question\nis what are those long lived objects? Luckily the TopConsumer field in this json output provides us that\ninformation and fielddata cache is the top consumer (5.9GB). So the easiest way to mitigate this issue is to clear the\nfielddata cache and set limits for the fielddata cache to say, 40% of node heap space to improve the cluster stability.\nConclusion\nThe new RCA engine enables you to quickly diagnose performance bottlenecks in your Elasticsearch cluster in an\nautomated fashion. It leverages deep metric insights provided by Performance Analyzer to isolate the root cause.\nWe plan to add RCAs for other commonly encountered bottlenecks in Elasticsearch clusters and a decision maker framework\nthat collates RCA outputs and provides unified recommendations in the future.\nWe encourage you to try out this solution on Open Distro for Elasticsearch and provide your valuable feedback to our\nengineering team at https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca. We also invite you to\ncontribute to this feature. Check out the contribution guidelines here to get more involved in the project.",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Open-Distro-for-Elasticsearch-1.9.0-is-released/",
    "title": "Open Distro for Elasticsearch 1.9.0 is now available",
    "content": "We are pleased to announce the release for Open Distro for Elasticsearch 1.9.0 that introduces Root Cause Analysis Engine in Performance Analyzer, batch actions like start, stop and delete for anomaly detectors in Anomaly Detection, support for remote cluster indexes in Anomaly Detection, and an ability to set index priority action in Index State Management feature. Open Distro for Elasticsearch 1.9.0 can be downloaded here.\nThe release consists of Apache 2.0 licensed Elasticsearch version 7.8.0, and Kibana version 7.8.0. This distribution also includes alerting, anomaly detection, index management, performance analyzer, security, SQL and k-NN plugins. Other components including SQL Workbench, SQL ODBC and JDBC drivers, SQL CLI client, and PerfTop, a client for Performance Analyzer are also available for download.\nDownload the latest packages\nYou can find Docker Hub images Open Distro for Elasticsearch 1.9.0 and Open Distro for Elasticsearch Kibana 1.9.0 on Docker Hub. Make sure your compose file specifies 1.9.0 or uses the ‚Äòlatest‚Äô tag.\nIf you‚Äôre using RPMs or DEBs, see our documentation on how to install Open Distro for Elasticsearch with RPMs and install Open Distro for Elasticsearch with Debian packages. A tarball is also available for testing and other applications.\nA Windows ready package supporting version 1.9.0 enables users to easily install Elasticsearch and Kibana on Windows. If you‚Äôre using Kubernetes, check out the Helm chart to install Open Distro for Elasticsearch.\nYou can find Open Distro for Elasticsearch security, alerting notification and job scheduler artifacts on Maven Central.\nYou can download the latest versions of Open Distro for Elasticsearch‚Äôs PerfTop client on npm.org, Open Distro for Elasticsearch‚Äôs latest SQL CLI client on PyPi. SQL drivers supporting ODBC and JDBC are also available.\nRelease Highlights\nIntroducing Root Cause Analysis Engine in Performance Analyzer to facilitate conducting root cause analysis (RCA) for performance and reliability problems in Elasticsearch clusters\nAllowing batch actions such as start, stop and delete on anomaly detectors as part of the Anomaly Detection feature Support for remote cluster indexes in Anomaly Detection\nIntroduce an ability to set index priority action so that users are allowed to set the order of index recovery in Index State Management\nPlease take a look at the 1.9.0 release notes or the latest features, enhancements, infra and documentation updates and bug fixes. Upgrade to 1.9.0 to leverage the latest features and bug fixes.\nCome join our community!\nThere are many easy ways to participate in the Open Distro community!\nAsk questions and share your knowledge with other community members on the Open Distro discussion forums.\nAttend our bi-weekly online community meetup to learn more about Elasticsearch, security, performance, machine learning and more.\nFile an issue, request an enhancement you need or suggest a plugin you need at github.com/opendistro-for-elasticsearch.\nContribute code, tests, documentation and even release packages at github.com/opendistro-for-elasticsearch. If you want to showcase how you‚Äôre using Open Distro, write a blog post for opensearch.org/blog. If you‚Äôre interested, please reach out to me on Twitter. You can find me at @vrphanse.\nWe also invite you to get involved in ongoing development of new Open Distro for Elasticsearch plugins, clients, drivers. Contribute code, documentation or upcoming features like Kibana Reports, and Historical Workbench in Anomaly detection.\nYou can also track upcoming features in Open Distro for Elasticsearch by watching the code repositories or checking the roadmap Thanks again for using and contributing to Open Distro for Elasticsearch and being part of the project‚Äôs growing community!",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Open-Distro-for-Elasticsearch-now-supports-integration-with-Tableau-and-Microsoft-Excel/",
    "title": "Open Distro for Elasticsearch now Supports integration with Tableau and Microsoft Excel",
    "content": "We now facilitate connecting Elasticsearch with two of the most powerful business intelligence and data visualization applications, Tableau and Microsoft Excel. To achieve this, we introduce the Open Distro for Elastic Search - Tableau integration and the Open Distro for Elastic Search - Excel integration in Open Distro for Elasticsearch SQL Engine.\nOpen Distro for Elasticsearch SQL Engine allows the use of Structured Query Language (SQL) to manifest search results in a tabular format with documents represented as rows, fields as columns, and indexes as table names, respectively, in the WHERE clause. It is powered by Open Distro for Elasticsearch, an Apache 2.0 licensed distribution of Elasticsearch. One of the key features of this engine is the Open Database Connectivity (ODBC) driver to help connect Elasticsearch to various business intelligence (BI) and analytics applications. We have expanded the capabilities of the ODBC driver via new integrations with business intelligence (BI) and data visualization applications such as Tableau and Microsoft Excel.\nUsers can now connect Elasticsearch with Tableau to create sophisticated visualizations on top of the search results obtained from Elasticsearch by leveraging the Open Distro for Elasticsearch - Tableau Integration. Similarly, they can connect Elasticsearch with Microsoft Excel to conduct a deeper analysis on data obtained from Elasticsearch using the Open Distro for Elasticsearch - Excel integration. Such an integration with Excel allows users to use advanced functionality like Power Query, a self-service business intelligence (BI) for Excel and export data in the format of choice such as CSV on the search results.\nOpen Distro for Elasticsearch - Tableau integration and Open Distro for Elasticsearch - Microsoft Excel integration are supported on any cluster running Open Distro for Elasticsearch 1.8 and above. To learn more about the integrations with Tableau and Excel, visit Open Distro for Elasticsearch - Tableau Integration Documentation and Open Distro for Elasticsearch - Excel Integration Documentation. To learn more about Open Distro for Elasticsearch, visit the website.",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Open-Distro-for-Elasticsearch-1.10.1-is-released/",
    "title": "Open Distro for Elasticsearch 1.10.1 is now available",
    "content": "We are pleased to announce the release of Open Distro for Elasticsearch 1.10.1. With this release we are adding support for several new features including a new command line interface and sample data for Anomaly Detection, an email destination in Alerting, a warmup API in k-NN, and an overhauled Kibana plugin for Security. Open Distro for Elasticsearch 1.10.1 and Open Distro for Elasticsearch Kibana 1.10.1 can be downloaded here.\nOpen Distro for Elasticsearch 1.10.1 includes version 7.9.1 of Open Source Elasticsearch and Kibana, plus Apache-2.0-licensed extensions that provide alerting, anomaly detection, index management, performance analysis, security, SQL, and k-NN features. Other components, including ODBC and JDBC drivers, a command-line SQL client, and a command-line performance visualization tool (‚ÄúPerfTop‚Äù) are also available to download.\nDownload the latest packages\nYou can find Docker Hub images Open Distro for Elasticsearch 1.10.1 and Open Distro for Elasticsearch Kibana 1.10.1 on Docker Hub. Make sure your compose file specifies 1.10.1 or uses the ‚Äòlatest‚Äô tag.\nIf you‚Äôre using RPMs or DEBs, see our documentation on how to install Open Distro for Elasticsearch with RPMs and install Open Distro for Elasticsearch with Debian packages. A tarball is also available for testing and other applications.\nA Windows package supporting version 1.10.1 enables users to install Elasticsearch and Kibana on Windows. If you‚Äôre using Kubernetes, check out the Helm chart to install Open Distro for Elasticsearch.\nYou can find Open Distro for Elasticsearch security, alerting notification and job scheduler artifacts on Maven Central.\nYou can download the latest versions of Open Distro for Elasticsearch‚Äôs PerfTop client on npm.org, Open Distro for Elasticsearch‚Äôs latest SQL CLI client on PyPi. SQL drivers supporting ODBC and JDBC are also available.\nRelease Highlights\nAnomaly Detection supports a command line interface that allows users to create, start, stop and delete detectors, and work with multiple clusters using named profiles.\nAnomaly Detection supports three different types of sample detectors and corresponding indices that allow users to detect sample anomalies using logs related to HTTP response codes, eCommerce orders, and CPU and memory of a host.\nThe Alerting feature now supports email destinations to send notifications without using a web hook.\nK-NN supports warmup API that allows users to explicitly load indices‚Äô graphs used for approximate k-NN search into memory before performing their search workload. With this API, users no longer need to run random queries to prevent initial latency penalties for loading graphs into the cache.\nThe updated Kibana plugin for Security streamlines security workflows, improves usability and adds audit and compliance logging configuration.\nSee the 1.10.1 release notes for a complete list of new features, enhancements, and bug fixes.\nCome join our community!\nThere are many easy ways to participate in the Open Distro community!\nAsk questions and share your knowledge with other community members on the Open Distro discussion forums.\nAttend our bi-weekly online community meetup to learn more about Elasticsearch, security, performance, machine learning and more.\nFile an issue, request an enhancement or suggest a plugin at github.com/opendistro-for-elasticsearch.\nContribute code, tests, documentation and even release packages at github.com/opendistro-for-elasticsearch. If you want to showcase how you‚Äôre using Open Distro, write a blog post at opendistro.github.io/for-elasticsearch/blog. If you‚Äôre interested, please reach out to me on Twitter. You can find me at @vrphanse.\nWe also invite you to get involved in ongoing development of new Open Distro for Elasticsearch plugins, clients, drivers. Contribute code, documentation or features to Open Distro for Elasticsearch.\nYou can also track upcoming features in Open Distro for Elasticsearch by watching the code repositories or checking the roadmap.\nThanks again for using and contributing to Open Distro for Elasticsearch and being part of the project‚Äôs growing community!",
    "keywords": [
      "odfe-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/developer-advocacy-with-open-distro/",
    "title": "Developer Advocacy with Open Distro for Elasticsearch",
    "content": "While Open Distro for Elasticsearch is an open-source, community-led project, we are gaining another full-time person devoted to the project ( me). Before I start contributing and interacting with the community, I wanted to take the time to introduce the role and myself.\n<!--more-->\nWhat is a developer advocate?\nThat‚Äôs a good question. Developer advocates are a link between a product or project and the technically-minded people who use it. While the title includes the word ‚Äúdeveloper,‚Äù it is a very broad application of the term ‚Äì it could be people who write software (software developers), those who run/administer the software (operations, devops practitioners, site reliability engineers, database administrators), or other technical end-users (data scientists, analytical professionals). Likely, if you are reading this, then you are someone that will work with a developer advocate like me.\nThe other term in the title is ‚Äúadvocacy.‚Äù Advocacy here has two meanings:\nTo advocate for the project. Developer advocates have goals oriented to the success of the project, and as such, want it to grow and become more mature. This might take the form of presenting on the project at conferences, writing about the project and helping guide the outward profile of the project in the larger community. You can think of this as growing the pie ‚Äì more users, more contributors, and more awareness.\nTo advocate for the developers. Developer advocates are users themselves of the project and don‚Äôt regularly contribute to the core of the software. They, however, work closely with the team that is contributing directly to help make decisions that are best for the users. In this way, developer advocates need to listen to and have deep empathy for the users and how they utilize the project.\nThe nice thing about these two meanings is that they are not in conflict but usually feed naturally into each other.\nThere are similar roles with similar titles (‚Äúdeveloper evangelist‚Äù, ‚Äúdeveloper relations engineer‚Äù, etc.) and, outwardly, seem to be doing similar tasks. The key difference between these roles and a developer advocate is that advocacy places equal weight on listening/empathy and success oriented tasks. Sometimes, however, these are just titles and the actual practice is the same, however we chose developer advocate in reference to Open Distro for Elasticsearch very intentionally ‚Äì this project is open source and is made of community and people as much as code ‚Äì listening and empathy are key.\nWho am I?\nI am a long time software developer who loves working with data. Most recently, I was doing advocacy for Redis and was involved with the launch of a Redis-based search engine. When I felt like I had done all I could in that role, I was thrilled to find out about Open Distro for Elasticsearch. Search and log analytics are endlessly fascinating to me, so I feel right at home on this project. I live in Edmonton, Alberta, Canada, and prior to the pandemic, I was on-the-road nearly half the time. So while Edmonton is home-base you‚Äôre likely to find me just about anywhere in the world (once when travel is safe again, naturally).\nWhat‚Äôs next?\nYou will probably see me start to participate in the community meetings, on stackoverflow, in the forums, or on github. In addition, may also get some direct communication from me as I start building the community. Feel free to reach out at kyledvs@amazon.com or on twitter @stockholmux or find me on GitHub. Super excited to get to know everyone!",
    "keywords": [
      "community-updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/community-meeting-early-nov/",
    "title": "Early November Community Meeting",
    "content": "We‚Äôve got a jam-packed agenda for this biweekly community meeting! Join us on Monday, November 2nd at 10am Pacific (UTC-7) on Zoom. Our focus topics will include:\nKibana Reports &amp; Notebooks\nPiped Processing Language\nHigh Cardinality Anomaly Detection\nAs always, we will go over the Open Distro for Elasticsearch roadmap and leave some time for questions and answers.\nAll are welcome to join us!",
    "keywords": [
      "meetings"
    ],
    "type": "News"
  },
  {
    "url": "/blog/fine-grained-access-control-for-search/",
    "title": "Using fine grained access control for search",
    "content": "Open Distro for Elasticsearch has extensive access control capabilities built right in. Of course, access control can prevent access to sensitive information, but it can also help you build applications that depend on Open Distro for Elasticsearch for search. Let‚Äôs explore this a little more. Say your application uses standard HTTP endpoints to serve data to end users. So, a web user loads a webpage, the client-side Javascript fires off a HTTP request to the application endpoint for a specific type of data, the data is returned to the client-side Javascript, and the browser renders it on screen for the user. Your server code is doing some minor handling of the HTTP route and parameters, making a request to Elasticsearch based on this and manipulating the returned results to make it easily useable by the client-side Javascript. Pretty typical stuff.\nWhen your application becomes sufficiently complex, you‚Äôll probably find the code is generally repetitious. You‚Äôll be writing almost the same methods over and over - listen for a HTTP request, process it, ask Elasticsearch, return results, yet with small, subtle differences. Not only is this boring to write, it represents a larger, harder to maintain codebase. So, how does access control fit into all this?\nBy varying the Open Distro for Elasticsearch users for types of different requests, you can change the data returned. In this scenario you have documents that represent items - these documents can contain not only public facing fields - price, colour, material, description, but also private fields like cost, suppliers, and performance. Your employees may need to see all the information from behind a special login while customers should only see the public information. Of course, you could write very similar server code for each, vary the _source in the Query DSL and reimplement loads of logic. Alternatively, you can let Open Distro do this work for you and gain the ability to centrally control field availability.\nLet‚Äôs take this example - running this in query in Kibana Dev Tools ( &lt;yourhost&gt;:&lt;yourport&gt;/app/dev_tools#/console) we will add a single item to the index ecommerce-items. PUT ecommerce-items/_doc/brown-mug\n{\n\"title\": \"Brown Mug\",\n\"description\": \"This mug features a large handle and a marbled, brown ceramic design.\",\n\"price\": 5.99,\n\"private\": {\n\"cost\": 1,\n\"supplier\": \"Mug World\"\n}\n} No matter what happens, we don‚Äôt want our customers to be able to see anything in the private object. We‚Äôll use Open Distro‚Äôs built-in access control functionality to make this happen.\nCreating a user &amp; role\nFirst up in Kibana we need to create a user. This user will be used by the application for serving public information, so we‚Äôll call it public-item-user. First, go to the Security item from the side menu then click Internal users in the secondary menu. Now, click on the Create internal user button. This will take you to &lt;yourhost&gt;:&lt;yourport&gt;/app/opendistro_security#/users/create.\nUnder the heading ‚ÄúCredentials‚Äù enter the username of public-item-user and a password twice for validation. Then click the Create button. At this point we have created a user but it can‚Äôt do anything in Open Distro. To enable it to be useful, we‚Äôll have to create and assign a role.\nFrom the side menu, go back to Security and then Roles from the secondary menu then click the Create role button. You should be at &lt;yourhost&gt;:&lt;yourport&gt;/app/opendistro_security#/roles/create. Under the ‚ÄúName‚Äù heading enter public-item-viewer into the ‚ÄúName‚Äù text box. Then under the heading ‚ÄúIndex Permissions‚Äù add the ecommerce-items in the ‚ÄúIndex‚Äù field and then indices:data/read/search into the ‚ÄúIndex Permissions‚Äù field. Finally, under the heading ‚ÄúField level security‚Äù go to the ‚ÄúExclude‚Äù field and enter private. When you‚Äôve done all this, go to the bottom and click the Create button.\nBefore proceeding, let‚Äôs take a moment to examine why indices:data/read/search is the right permission for this situation. It grants users with this role the ability to search documents but not to manipulate them. If you scroll through the possibilities in the ‚ÄúIndex Permissions‚Äù box you might notice the type crud and search which are permission groups. Permission groups bundle typically used permissions together as a shorthand; you could actually use these here, but you would also be granting far more permission than needed, so we‚Äôll stick with a highly specific permission that can do very little (more on that later).\nAt this point you have a user, public-item-user and a role public-item-viewer but they aren‚Äôt connected. In Open Distro this is called mapping. To map a role to a user, go back to Security and then select Roles and find our user ( public-item-viewer) and click on it (you may need to use the search box). It should bring you to &lt;yourhost&gt;:&lt;yourport&gt;/app/opendistro_security#/roles/view/public-item-viewer. Now go to the tab Mapped Users - the list will be empty so click Map users. At this point you‚Äôll be at the URL &lt;yourhost&gt;:&lt;yourport&gt;/app/opendistro_security#/roles/edit/public-item-viewer/mapuser. Under the heading ‚ÄúInternal users‚Äù go to the ‚ÄúInternal users‚Äù field and select public-item-user. Then click Map.\nNow your user ( public-item-user) has all the powers provided by the role public-item-viewer. Let‚Äôs test it out by going to Kibana in a private or incognito window. This trick will allow you to be logged into two users at the same time. Go to Dev Tools ( &lt;yourhost&gt;:&lt;yourport&gt;/app/dev_tools#/console) and enter the following query: GET /ecommerce-items/_search\n{\n\"query\": {\n\"match\": {\n\"title\": \"brown\"\n}\n}\n} The result will show: {...,\n\"hits\": [\n{\n\"_index\": \"ecommerce-items\",\n\"_type\": \"_doc\",\n\"_id\": \"brown-mug\",\n\"_score\": 0.13353139,\n\"_source\": {\n\"title\": \"Brown Mug\",\n\"description\": \"This mug features a large handle and a marbled, brown ceramic design.\",\n\"price\": 5.99\n}\n}]\n} Notice that the private object is gone. Our user and role has filtered this data right out.\nOne additional note here - we set up these two roles to only have access to this particular index. That means that if, by some mistake or happenstance the credentials are released for these users or an application bug somehow allows for passing in other indices, you‚Äôre covered. No other index managed by Open Distro would be at risk of disclosure as access control is denied when querying. You can think of this as an implementation of the principle of least privilege.\nTo set up a route for the employee section of your application, create a role called private-item-viewer that has permissions to see all information. Repeat the process above, except skipping the step where you exclude private in the ‚ÄúField level security section‚Äù. Then create a user called private-item-user with the role of private-item-viewer. All of the requests from to backend for employees would uses this user to make the requests, enabling them to have access to all the fields in the object private.\nOverview of the application, users / roles, &amp; search\nAs far as your application is concerned, you only need a minimal abstraction to provide different routes with different information. The authenticated route for employees needing to see the private information could reuse all the query preparation, request, serializing, deserializing and rendering code as the public route. Later when your data changes and you want to include or exclude other parts of the document, you don‚Äôt have to modify your code, just alter the roles and the application will serve only the fields you specify. Wrap up\nBy using the access control features of Open Distro for Elasticsearch you can reduce the amount of code needed to implement an application that serves differing data from the same index depending on the situation. The pattern outlined above moves the responsibility of data visibility from the application to the search engine. In this way your application can be simpler and you gain centralized, code-free control at the data level.\nThis pattern can be generalized to many other situations too. Say you have documents that are sensitive to two different groups and neither can see the whole document. Additionally, you have another group that needs to see the whole document, perhaps for security or compliance reasons. Using the same pattern of users, roles and permissions to provide differing visibility of the data with the same queries. But that is a story for another time‚Ä¶",
    "keywords": [
      "technical-posts"
    ],
    "type": "News"
  },
  {
    "url": "/blog/journey/",
    "title": "A journey of a thousand miles begins with a single step",
    "content": "Welcome to OpenSearch news. In this section we plan to update the community on new developments and call attention to great things happening around OpenSearch.\nWant to write an article? Contact kyledvs@amazon.com",
    "keywords": [
      "update"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-beta-announcement/",
    "title": "OpenSearch Beta 1",
    "content": "We are excited to release the OpenSearch Beta 1.0 (derived from Elasticsearch 7.10.2) and OpenSearch Dashboards Beta 1.0 (derived from Kibana 7.10.2). With this beta release, we have refactored all the Open Distro for Elasticsearch plugins to work with OpenSearch and provide the community with downloadable artifacts (Linux tars and Docker images) to run OpenSearch and OpenSearch Dashboards with these plugins installed. Features released with this beta include;\nAdvanced Security\nSQL Query Syntax\nAnomaly Detection\nAlerting\nPiped Processing Language\n‚Ä¶and more ( OpenSearch and OpenSearch Dashboards).\nWe would like to get your feedback from this beta release - please let us know if there are any features you would like or bugs you identify. You can submit pull requests, write documentation, open issues (either on OpenSearch or OpenSearch Dashboards), or simply read the news. If you would like to install or run this project please see the Developer Guide ( OpenSearch and OpenSearch Dashboards) to get started.\nPlugins and Components Available in Beta are: Anomaly Detection Anomaly Detection Dashboards Security Dashboards (FE) Security (BE) Performance Analyzer PerfTop K-NN Job Scheduler Index Management Index Management Dashboards Alerting Alerting Dashboard SQL/PPL OpenSearch Plugin Query Workbench Dashboards Plugin SQL JDBC Driver SQL CLI Client SQL ODBC Driver Notebooks OpenSearch Plugin Notebooks Dashboards Plugin Reporting Dashboards Plugin Reporting Opensearch Plugin Trace Analytics Dashboards Plugin Gantt Charts - Dashboards Visualization plugin Asynchronous Search OpenSearch CLI Common Utils Data Prepper Special Thanks\nWe would also like to extend a special thanks to the people already contributing to the project. We appreciate the additional support for this project from; AmiStrn Conan-Kudo rmuir sharp-pixel Hakky54 kyleconroy geekygirldawn jkowall ace03uec denysvitali jkowall galangel FreCap anthonylouisbsb aadrien accat adkalavadia alexz00 amitai aparo asfoorial BlackMetalz Bobpartb bradlee dawnfoster dirkhh erhan fabide frotsch GezimSejdiu GoodMirek hagayg horizondave horovits igorid70 janhoy jkeirstead jkowall Katulus lornajane Malini mattwelke mosajjal nickytd nknize Northern opensorcerer89 otisg ralph retzkek robcowart santiagobassett SergioFG shamil sksamuel spapadop sunilchadha tardyp Tom1 ttx tvc_apisani willyb How can you help?\nThe biggest thing you can do to help is join us! You can install ( OpenSearch and OpenSearch Dashboards) and start diving in! Feel free to take a look around at what we‚Äôve been up to, and then head over to look at the open issues. Or you could jump right in and start opening issues or contributing. For more details see here. When in doubt, open an issue For almost any type of contribution, the first step is opening an issue ( OpenSearch, OpenSearch Dashboards). Even if you think you already know what the solution is, writing down a description of the problem you‚Äôre trying to solve will help everyone get context when they review your pull request. If it‚Äôs truly a trivial change (e.g. spelling error), you can skip this step ‚Äì but as the subject says, when it doubt, open an issue. Look for the tag ‚Äúhelp wanted‚Äù If you‚Äôre excited to jump in, we‚Äôve marked a few issues that would be really helpful ( ‚ÄúHelp Wanted‚Äù). Questions? Feedback? Let us know in by opening an issue or posting in the forums. We are also having biweekly Community Meetings. The community meetings will include progress updates at every session as well as leaving time for Q&amp;A. The next community meeting is on May 18, 2021 at\n9:00 AM PDT.",
    "keywords": [
      "update"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-roadmap-announcement/",
    "title": "The OpenSearch roadmap is now available on GitHub",
    "content": "I wanted to share that a new public roadmap was published to the OpenSearch project‚Äôs GitHub. The roadmap shows the next several months of planned features and releases. Going forward, this roadmap will show what the maintainers and contributors of the various components of the project intend to work on and when that work aims to launch. What exactly is the public roadmap?\nThe roadmap shows the features anybody is working on, along with when they‚Äôre planning to include those features in a release. It also shows when the next several releases are targeting. I want to emphasize that there isn‚Äôt an ‚Äòinternal to Amazon‚Äô roadmap and ‚Äòexternal‚Äô roadmap. This is the roadmap that the current maintainers use for their work. As a development practice, one of the responsibilities for building new functionality will be to show those plans in the roadmap. This ensures there is a single place for the community of developers and users to see the project‚Äôs plans. Every roadmap item includes a link to a GitHub issue in the appropriate repo to track it. If you have feedback, requirements, questions, or suggestions about any particular feature, the GitHub issue is the best place to provide it.\nIf it isn‚Äôt clear, the dates and milestones reflect intentions rather than firm commitments. Dates may change as anyone learns more or encounters unexpected issues. When that happens, it‚Äôs not considered a failure. Anyone who builds software knows that the best-laid plans can change. The roadmap will help make changes more transparent so that anyone can plan around them accordingly. You‚Äôll notice that release milestones further in the future will have less granularity than ones in the near term. As those milestones are worked on you will see more granularity and some larger items split into multiple smaller incremental features.\nWhat gets included in the roadmap?\nThe roadmap does not include every change going into every release. That would be overwhelming. The roadmap focuses on new features and enhancements in the project. It does not focus on bugs. I also want to note that the opensearch-cli and data-prepper are released independently of the distribution. Because of this, the roadmap also doesn‚Äôt include them.\nHow do I contribute something and get it on the roadmap?\nThere is now an entry in the FAQ to provide guidance on that. By following the guidance in that FAQ, the maintainers of the repo you are interested in contributing to will work with you on how to get your contribution included.\nWith all that said, I‚Äôm really excited to get this roadmap opened up. I see this as step in this project operating with full transparency and community involvement. I look forward to the feedback!\nI want to give a special thanks to all of the people who helped me get this put together: sean-zheng-amazon CEHENKLE bbarani anirudha jcgraybill xinlamzn seanneumann vamshin lukkoor ketanv3 sandeshkr419 getsaurabh02 psychbot Pallavi-AWS",
    "keywords": [
      "update"
    ],
    "type": "News"
  },
  {
    "url": "/blog/my-first-steps-in-opensearch-plugins/",
    "title": "My First Steps in OpenSearch Plugins",
    "content": "Taking the leap, not the plunge Personally, I‚Äôve always wanted to contribute to an open-source project, but never found a way to incorporate it with my day-to-day work. Occasionally, I‚Äôd muster up the courage to clone a project I liked, seeking a good entry point to add some new feature or handle some issue. I thought that all I needed was to make a small contribution and everything else would just flow into place.\nTurns out, this may be true for small-scale projects, but for complex ones like OpenSearch this isn‚Äôt the case. While you may fix a minor bug, you‚Äôre not likely to know where to begin in order to fix a moderate bug or design a new feature. Searching for that entry point via the main() entrance mostly feels like wandering around an M.C. Escher print.\nHowever, some projects have an entrance unlike any other - a Pluggable Architecture.\nA pluggable architecture enables us to have a hands on experience with a logical sub section of a large-scale project. This effect makes such an immense project more approachable despite its overall size.\nIn this blog post, I will take you on a journey through my thought process and conclusions from building my first OpenSearch plugin. We will build a simple REST plugin together, install it into OpenSearch, and run it. By the end, I hope to convince you that it is a great starting point for you gaining hands-on experience required for joining the OpenSearch community of contributors, and a great starting point for the project itself. My First Steps in OpenSearch Plugins Taking the leap, not the plunge What are OpenSearch Plugins? Building a Simple OpenSearch REST Plugin Before we start - make sure you have these environment dependencies Defining Our Plugin Requirements for OpenSearch The build.gradle of Our Plugin Implementing the Plugin Code OpenSearch Plugin Interfaces Implementing Plugin logic Logic separation Testing Trying out our new plugin Building the plugin project Installing the plugin Running OpenSearch with our new plugin Uninstalling the plugin Key takeaways from creating my first plugin What are OpenSearch Plugins?\nSoftware plugins are, in essence, standalone components that add features and capabilities to an existing core program. Normally, when viewing a plugin‚Äôs code you wouldn‚Äôt see any point of entry (no main()). That‚Äôs because the plugin‚Äôs points of entry are the interface functions that it implements.\nOpenSearch plugins are no different.\nAs an analogy, think of how you can replace lenses and flash elements on a camera body. The core project is the part that takes the picture. It contains things a camera must have such as a light sensor and the software required to record the sensor‚Äôs information. Plugins then would be what you can connect to the camera body.\nTake for example, a wide angle lens to take pictures of landscapes. Maybe a cool feature would be to keep the camera still without touching it? Or taking it under water? You could install a tripod or a protective waterproof case respectively. Consider also that these plugins connect to the camera body in some way. To create one yourself you must first know 1) how to connect to the camera, and 2) how to interact with it via these connections ‚Äì this is called the plugin interface.\nIf you are designing a way for the camera to store more images on a new form of memory chip, then you don‚Äôt need to know anything about the interface of the lenses. However, you would have to know a great deal about how this camera sends/retrieves its recorded file via the memory chip connectors.\nIn other words, the plugin interface is not necessarily the same for every plugin.\nThe beauty of this architecture is that it enables you to customize the core program to your needs, without having to contribute to the core project directly (Imagine having to buy a new camera for every different lens instead of switching the lenses!).\nLet‚Äôs get back to OpenSearch plugins:\nSome plugins, for historic reasons, reside in the core project.\nAn example of one of these is the repository-s3 plugin that implements snapshot and restore capabilities using AWS‚Äôs S3 buckets as the repository.\nThey are in the open-source project repository. However, when we create the binaries of the OpenSearch project ( ~ gradle build) these plugins are not installed. This is by design ‚Äì ensuring the release artifact is not huge by default.\nIdeally, the OpenSearch project would be pluggable but contain no plugins in the project itself.\nAs mentioned previously, an important characteristic of a pluggable architecture is the plugin interface. This interface needs 1) to be as stable as possible and 2) to strive to be backwards-compatible throughout the different project versions. Otherwise, existing plugins will be very hard to maintain. This is similar to maintaining a public API.\nThis brings us to the plugins built by the community and completely independent of the main project.\nThe ability to add plugins offers some key benefits to the open source community:\nAdding new features to OpenSearch can be done independently of the core project ‚Äì it allows for a wide range of features to be developed with few (if any) conflicts in the code.\nWriting a plugin for OpenSearch requires going through the core project‚Äôs source code. By creating plugins, you are gaining the skills to be part of the OpenSearch community.\nOpenSearch plugins vary greatly in complexity and functionality, yet they all have common characteristics. Let‚Äôs get to know them by creating a simple REST plugin ‚Äì a plugin that implements a new REST endpoint in the OpenSearch project, reachable via an HTTP call to the running cluster.\nBefore we start - make sure you have these environment dependencies\nJava 14\nGradle 7.0+ (don‚Äôt forget to add them to $JAVA_HOME and $PATH)\nDefining Our Plugin Requirements for OpenSearch\nLet‚Äôs say we would like to add a new endpoint to OpenSearch called _plugins/hello_world with the following behavior: ‚ûú ~ curl -XGET 'localhost:9200/_plugins/hello_world'\nHi! Your plugin is installed and working:)\n‚ûú ~ curl -XPOST 'localhost:9200/_plugins/hello_world' -H 'Content-Type: application/json' -d '{\"name\":\"Amitai\"}'\nHi Amitai! Your plugin is installed and working:) That is all we should start with. Anything more ambitious would keep us from focusing on the basics of plugins.\nI am going to be writing the plugin using the plugin writing conventions suggested in the OpenSearch project‚Äôs opensearch-plugins repo.\nTo get started, create a new gradle project. Then, add the following directories and files, which we will edit along the way.\n(Alternatively you can clone the complete source code for this plugin here).\n|-- LICENSE.txt\n|-- NOTICE.txt\n|-- build.gradle\n`-- src\n|-- main\n| `-- java\n| `-- org\n| `-- opensearch\n| `-- rest\n| `-- action\n| |-- HelloWorldPlugin.java\n| |-- HelloWorldService.java\n| `-- RestHelloWorldAction.java\n|-- test\n| `-- java\n| `-- org\n| `-- opensearch\n| `-- rest\n| `-- action\n| `-- HelloWorldPluginTests.java\n`-- yamlRestTest\n|-- java\n| `-- org\n| `-- opensearch\n| `-- rest\n| `-- action\n| `-- HelloWorldClientYamlTestSuiteIT.java\n`-- resources\n`-- rest-api-spec\n|-- api\n| `-- _plugins.hello_world.json\n`-- test\n`-- _plugins.hello_world\n|-- 10_basic.yml\n`-- 20_hello_world.yml The build.gradle of Our Plugin\nAll OpenSearch plugins have the same general structure.\nThe plugin itself is a zip file that contains at the root:\nThe plugin jar files plugin-descriptor.properties [OPTIONAL] bin directory\n[OPTIONAL] config directory\n[OPTIONAL] plugin-security.policy Any resources a plugin requires are packaged into a resources jar. This is due to the fact that only jar files at the root are added to the classpath for the plugin.\nThe build.gradle file will build this structure for us when we are done, including creating the required plugin-descriptor.properties.\nHowever, since we need the OpenSearch project artifact for the gradle build process and the plugin interface, let‚Äôs clone the project and publish it to our Maven local. This last step is required since the artifact is not yet published to public repositories.\nIn this example I will be using release version 1.0.0-beta1, as described here: ‚ûú ~ git clone https://github.com/opensearch-project/OpenSearch.git\n‚ûú ~ cd OpenSearch\n‚ûú OpenSearch git:(main) git checkout 1.0.0-beta1 -b beta1-release\n‚ûú OpenSearch git:(beta1-release)./gradlew publishToMavenLocal -Dbuild.version_qualifier=beta1 -Dbuild.snapshot=false Copy the following to the build.gradle file: apply plugin: 'java'\napply plugin: 'opensearch.opensearchplugin'\napply plugin: 'opensearch.yaml-rest-test'\nopensearchplugin {\nname 'opensearch-rest-plugin'\ndescription 'Custom OpenSearch REST plugin for educational purposes'\nclassname 'org.opensearch.rest.action.HelloWorldPlugin'\nlicenseFile rootProject.file('LICENSE.txt')\nnoticeFile rootProject.file('NOTICE.txt')\n}\nbuildscript {\nrepositories {\nmavenCentral()\nmavenLocal()\n}\ndependencies {\nclasspath \"org.opensearch.gradle:build-tools:1.0.0-beta1\"\n}\n}\nrepositories {\nmavenLocal()\n}\n// disabling some unnecessary validations for this plugin\ntestingConventions.enabled = false\nloggerUsageCheck.enabled = false\nvalidateNebulaPom.enabled = false\ndependencies {\n// required for the yaml test to run\nyamlRestTestImplementation 'org.apache.logging.log4j:log4j-core:2.11.1'\n} A few things to note on the build.gradle: opensearchplugin section:\nEvery plugin must contain a file called plugin-descriptor.properties which contains some optional fields and some mandatory. In my example I included the mandatory ones only. You must provide: the full class path to the plugin file in your project ( classname), the plugin name ( opensearch-rest-plugin) and description, and the locations of your plugin‚Äôs license and notice files.\nThis section ensures that this file will be created in the plugin‚Äôs build process using the properties provided in the curly braces. org.opensearch.gradle:build-tools:1.0.0-beta1 The version at the end in this example is 1.0.0-beta1, the version must match the version of the OpenSearch project we are going to install our plugin into. We are getting this artifact from our local maven repo. LICENSE.txt and NOTICE.txt files:\nThese pertain to your plugin, and not to the core project.\nAn example for the License and Notice files if you‚Äôre interested in the Apache 2.0 license:\nExample for Notice.txt: This product includes software developed by The Apache Software\nFoundation (https://www.apache.org/). And a copy of the Apache 2.0 license can be found here.\nAdd your plugin‚Äôs license and notice.txt files to your project and edit them later: ‚ûú opensearch-rest-plugin touch \"./LICENSE.txt\"\n‚ûú opensearch-rest-plugin touch \"./NOTICE.txt\" I am not a lawyer. If you‚Äôre not sure what to put in the LICENSE.txt and NOTICE.txt files, please consult one.\nImplementing the Plugin Code\nWe are going to add the plugin under /src/main/java/org/opensearch/rest/action.\nThis is not an arbitrary directory! During the plugin installation your source code will be copied to this directory in the OpenSearch project, so you must select the relevant path depending on the type of plugin (more on this in a bit).\nOpen the project in your favorite IDE, and edit HelloWorldPlugin.java: /*\n* SPDX-License-Identifier: Apache-2.0\n*\n* The OpenSearch Contributors require contributions made to\n* this file be licensed under the Apache-2.0 license or a\n* compatible open source license.\n*/\npackage org.opensearch.rest.action;\nimport...\npublic class HelloWorldPlugin extends Plugin implements ActionPlugin {\n@Override\npublic List&lt;RestHandler&gt; getRestHandlers(final Settings settings,\nfinal RestController restController,\nfinal ClusterSettings clusterSettings,\nfinal IndexScopedSettings indexScopedSettings,\nfinal SettingsFilter settingsFilter,\nfinal IndexNameExpressionResolver indexNameExpressionResolver,\nfinal Supplier&lt;DiscoveryNodes&gt; nodesInCluster) {\nreturn singletonList(new RestHelloWorldAction());\n}\n} Notice we are extending Plugin and implementing the ActionPlugin. How did I reach the conclusion that I need to implement this particular interface? This is where we need to learn a little bit about the project we are plugging into.\nOpenSearch Plugin Interfaces\nWhen creating a plugin we would want to implement at least one plugin interface class. For our simple example it seems we need a REST layer, so one interface should do. These are the current plugin interfaces available, found at the plugins folder in the OpenSearch project:\n<!--- explain each one refine the explanations here! ---> ActionPlugin - Simply put, actions are things you can do over the API. This interface allows for extentind the REST API. AnalysisPlugin - Extends analysis functionality (i.e. tokenizer‚Äôs and char filters) on the index level. ClusterPlugin - Extends the cluster management behavior, such as shard allocation. DiscoveryPlugin - Extends OpenSearch‚Äôs host discovery functionality. EnginePlugin - For providing alternative engine implementations. This is used when a new index is created and affects how we read/write to that index. ExtensiblePlugin - Provides a callback for extensible plugins to be informed of other plugins which extend them. IndexStorePlugin - Provides alternative directory implementations (for the data being indexed). IngestPlugin - A powerful tool! You can use it to add custom ingest processors that are applied to the docs when they are ingested into OpenSearch. MapperPlugin - Extend this to add custom mappers. NetworkPlugin - Plugin for extending network and transport related classes. This interface is used on the transport layer between nodes. PersistentTaskPlugin - Plugin for registering persistent tasks executors. ReloadablePlugin - Plugins usually implement this interface in order to allow a plugin to reload its state. RepositoryPlugin - An interface for adding custom snapshot and restore repository implementations. ScriptPlugin - Extends the scripting functionality. SearchPlugin - Plugin for extending search time behavior (Aggregations, scoring,). SystemIndexPlugin - Plugin for defining system indices.\nSo, I wanted to find which of these is the right one for adding a REST call. Simply reading the javadoc of the classes is not always as forthcoming as you would expect. Check out the description over the ActionPlugin: An additional extension point for {@link Plugin}s that extends OpenSearch‚Äôs scripting functionality. Instead, the best way to know which plugin interface to implement is to follow a similar flow in the project.\nOff the top of your head, what is the most simple REST call you send to OpenSearch?\nFor me it is the /_cat endpoint. So I set out to understand the flow of a simple endpoint that I am familiar with.\nSearching the OpenSearch project for \"/_cat\" led me to a file called RestCatAction. Look at the files around this one, can you see that there is a naming convention/template? Rest + specific function + Action So a new endpoint like the one we wish to create would have to be in this path (now you know why i chose org/opensearch/rest/action), and have a name like RestHelloWorldAction. We will edit it later on.\nNext, I wanted to know how OpenSearch registers all these routes? How would my plugin‚Äôs new endpoint be properly exposed?\nIf you search for usages of the RestCatAction you will get to the ActionModule. This class has a method called initRestHandlers() which does exactly what I was looking for. If registering REST handlers is done here it stands to reason that REST handling for plugins is also done here. Simply searching for the word \"plugin\" led me to this block of code in the initRestHandlers() function: for (ActionPlugin plugin: actionPlugins) {\nfor (RestHandler handler: plugin.getRestHandlers(settings, restController, clusterSettings, indexScopedSettings,\nsettingsFilter, indexNameExpressionResolver, nodesInCluster)) {\nregisterHandler.accept(handler);\n}\n} And there you have it! ActionPlugin ‚Äôs have a function getRestHandlers() that returns the plugin REST handler of type RestHandler.\nIn conclusion:\nWe know we need to implement an ActionPlugin interface in order to be registered during the node startup.\nOur plugin needs to supply a RestHandler type of class. This means the new class we added ( RestHelloWorldAction) should extend the RestHandler class.\nImplementing Plugin logic\nNoticing that the majority of the REST action handlers are extending the BaseRestHandler which in turn extends the RestHandler led me to edit the RestHelloWorldAction.java file like this: /*...*/\npackage org.opensearch.rest.action;\nimport...\npublic class RestHelloWorldAction extends BaseRestHandler {\n@Override\npublic String getName() {\nreturn \"rest_handler_hello_world\";\n}\n@Override\npublic List&lt;Route&gt; routes() {\nreturn unmodifiableList(asList(\nnew Route(GET, \"/_plugins/hello_world\"),\nnew Route(POST, \"/_plugins/hello_world\")));\n}\n@Override\nprotected RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) throws IOException {\nString name = request.hasContent()? request.contentParser().mapStrings().get(\"name\"): \"\";\nreturn channel -&gt; {\ntry {\nchannel.sendResponse(HelloWorldService.buildResponse(name));\n} catch (final Exception e) {\nchannel.sendResponse(new BytesRestResponse(channel, e));\n}\n};\n}\n} This implementation adds the routes we defined earlier. It also prepares the request for handling.\nThe incoming request first lands on the REST layer and is handled later (async) by an event loop. The request is processed into the tcp layer and then the response is sent back via the REST layer. This is, of course, an oversimplification of how actions are handled in OpenSearch.\nif this asynchronous behavior and the way nodes communicate is a subject you would like to hear more about please let me know in the comments!\nLogic separation\nPlease note that I am adding a new class called HelloWorldService to implement the logic. It would be wrong in this case to handle the logic at the time the endpoint is called rather than when the system is ready to handle the request. This separation also allows us to test the plugin logic and the integration of the new route separately.\nThat‚Äôs what the HelloWorldService class is for -\nWe could edit the class to be something like this: /*...*/\npackage org.opensearch.rest.action;\nimport...\npublic class HelloWorldService {\npublic static RestResponse buildResponse(String name) {\nString space = name.isEmpty()? \"\": \" \";\nfinal String message = \"Hi\" + space + name + \"! Your plugin is installed and working:)\";\nreturn new BytesRestResponse(RestStatus.OK, message);\n}\n} Testing\nOpenSearch offers test cases for you to extend, that give you the ability to have common cluster configurations for many tests. Some of these offer a way to test your plugin as part of an OpenSearch instance. In order to test the integration with our new endpoint we can implement a YAML test. This type of test is recommended by the documentation for REST tests.\nIn order to run a YAML REST test we need to provide a resource containing the REST API spec adhering to the api spec requirements, and the YAML based tests describing the actions and the expected returned values.\nAs previously described, the YAML REST test file structure should be like this: -- src\n|\n`-- yamlRestTest\n|-- java\n| `-- org\n| `-- opensearch\n| `-- rest\n| `-- action\n| `-- HelloWorldClientYamlTestSuiteIT.java\n`-- resources\n`-- rest-api-spec\n|-- api\n| `-- _plugins.hello_world.json\n`-- test\n`-- _plugins.hello_world\n|-- 10_basic.yml\n`-- 20_hello_world.yml The 10_basic.yml will test that the plugin has been added, the other tests check if the rest endpoint is working as expected.\nWe can now edit the _plugins.hello_world.json to describe the new REST endpoint‚Äôs behaviour: {\n\"_plugins.hello_world\": {\n\"stability\": \"stable\",\n\"url\": {\n\"paths\": [\n{\n\"path\": \"/_plugins/hello_world\",\n\"methods\": [\n\"GET\",\n\"POST\"]\n}]\n},\n\"body\": {\n\"description\": \"The name to be included in the hello message\"\n}\n}\n} And now the YAML tests themselves: 10_basic.yml \"Test that the plugin is loaded in OpenSearch\":\n- do:\ncat.plugins:\nlocal: true\nh: component\n- match:\n$body: /^opensearch-rest-plugin\\n$/ 20_hello_world.yml ---\n\"Default with no name\":\n- do:\n_plugins.hello_world: {}\n- match: {$body: \"Hi! Your plugin is installed and working:)\" }\n---\n\"With name\":\n- do:\n_plugins.hello_world:\nbody:\nname: Amitai\n- match: {$body: \"Hi Amitai! Your plugin is installed and working:)\" } The only thing we are missing is to edit the HelloWorldClientYamlTestSuiteIT.java file. When this file is run it runs the YAML cases we defined using a client against a running OpenSearch cluster. HelloWorldClientYamlTestSuiteIT.java /*...*/\npackage org.opensearch.rest.action;\nimport...\npublic class HelloWorldClientYamlTestSuiteIT extends OpenSearchClientYamlSuiteTestCase {\npublic HelloWorldClientYamlTestSuiteIT(@Name(\"yaml\") ClientYamlTestCandidate testCandidate) {\nsuper(testCandidate);\n}\n@ParametersFactory\npublic static Iterable&lt;Object[]&gt; parameters() throws Exception {\nreturn OpenSearchClientYamlSuiteTestCase.createParameters();\n}\n} And that concludes the integration tests. We finish up by editing our unit test for the logic performed by the action: /*...*/\npackage org.opensearch.rest.action;\nimport...\npublic class HelloWorldPluginTests extends OpenSearchTestCase {\npublic void testBuildHelloWorldResponse() {\nString name = \"What's in a name?\";\nassertThat(HelloWorldService.buildResponse(name).content().utf8ToString(),\nequalTo(\"Hi \" + name + \"! Your plugin is installed and working:)\"));\n}\n} Running the tests is as easy as: ‚ûú opensearch-rest-plugin gradle check Building the plugin project\nNow that everything is prepared building the plugin is as easy as: ‚ûú opensearch-rest-plugin gradle build We can find our built plugin zip in the distributions folder, let‚Äôs take a look inside it: ‚ûú opensearch-rest-plugin vim build/distributions/opensearch-rest-plugin-0.0.1-SNAPSHOT.zip You should see something like this: plugin-descriptor.properties\nopensearch-rest-plugin-0.0.1-SNAPSHOT.jar\nNOTICE.txt\nLICENSE.txt Installing the plugin\nInstalling the plugin into OpenSearch will require getting a distribution of the project, we can create one from the cloned OpenSearch repo: ‚ûú OpenSearch git:(beta1-release)./gradlew localDistro And installing our plugin to OpenSearch: ‚ûú OpenSearch git:(beta1-release) cd build/distribution/local/opensearch-1.0.0-SNAPSHOT\n‚ûú opensearch-1.0.0-SNAPSHOT git:(beta1-release) bin/opensearch-plugin install file:///full/path/to/opensearch-rest-plugin/build/distributions/opensearch-rest-plugin-0.0.1-SNAPSHOT.zip Running OpenSearch with our new plugin\nNow we can finally fire up OpenSearch and try out our new plugin! ‚ûú opensearch-1.0.0-SNAPSHOT git:(beta1-release) bin/opensearch After it starts running open a new terminal tab and run the following: ‚ûú ~ curl -XGET 'http://localhost:9200/_plugins/hello_world' Hi! Your plugin is installed and working:) Or you can add your name by running this: ‚ûú ~ curl -XPOST 'localhost:9200/_plugins/hello_world' -H 'Content-Type: application/json' -d '{\"name\":\"Amitai\"}' Hi Amitai! Your plugin is installed and working:) AMAZING!!!\nUninstalling the plugin\nThis is required if you want to make changes to the plugin and install it again. Installing a plugin with the same name twice results in an error. ‚ûú opensearch-1.0.0-SNAPSHOT git:(beta1-release) bin/OpenSearch-plugin remove opensearch-rest-plugin -&gt; removing [opensearch-rest-plugin]... I won‚Äôt lie to you, this simple plugin had a steep learning curve for understanding the full flow. I had everything and anything go wrong in the first try. It is important to remember that this is totally normal for large scale projects.\nHaving said that, here are some of the things I learned along the way:\nThere are complex plugins and there are simple plugins, but they are all created using this thought process:\nWhat has a similar logic flow in the OpenSearch project to the logic I need my plugin to implement? -&gt; this gives you the plugin interface you are looking for.\nWhat methods does this plugin interface expose?\nWhat additional classes do you need to implement in order to satisfy these methods?\nAdding a plugin that has to do with REST handling essentially has taught me all the basics of REST handling in OpenSearch. To top it off - creating this plugin boosted my confidence to go and explore other aspects of the OSS project.\nIn this simple plugin use case we got to see the REST layer implementation, add an endpoint to the OpenSearch project and interact with it. In the following posts we will implement more complex plugins, getting to know other sections of this great project.\nI would like to thank Sarat Vemulapalli from AWS for his feedback and assistance on this post.\nThis post is contributed by Amitai Stern from Logz.io and is co-published on Logz.io blog",
    "keywords": [
      "technical-posts"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-release-candidate-announcement/",
    "title": "OpenSearch Release Candidate (RC1) is now available",
    "content": "Today I am excited to announce the Release Candidate (RC1) for version 1.0.0 of both OpenSearch (derived from Elasticsearch 7.10.2) and OpenSearch Dashboards (derived from Kibana 7.10.2). The Release Candidate includes downloadable artifacts (Linux tars and Docker images) as well as a number of OpenSearch plugins (listed below). So what does a release candidate mean exactly? This Release Candidate is a version of the project that is feature complete and passing automated testing with the intent to validate expected functionality before moving to a General Availability (GA) launch. The goal with this Release Candidate is to share the current release with the community and solicit a final round of testing and feedback before the GA release. Between now and the 1.0.0 release of OpenSearch and OpenSearch Dashboards, you should expect no further changes apart from critical bug fixes and the release of additional artifacts (see below). What is included in this Release Candidate? There is a long list of enhancements and fixes that are part of this release including span filtering support in Trace Analytics, tenant support in Notebooks, K-NN field level algorithm selection, support for index management transforms, and support for scheduling and tenants in reporting. Below you‚Äôll find the detailed highlights for this release: OpenSearch OpenSearch Dashboards Anomaly Detection Anomaly Detection Dashboards Security Security Dashboards Performance Analyzer PerfTop K-NN Job Scheduler Index Management Index Management Dashboards Alerting Alerting Dashboards SQL/PPL OpenSearch Plugin Query Workbench Dashboards Plugin SQL JDBC Driver SQL CLI Client SQL ODBC Driver Notebooks OpenSearch Plugin Notebooks Dashboards Plugin Reporting Opensearch Plugin Reporting Dashboards Plugin Trace Analytics Dashboards Plugin Gantt Charts Dashboards Visualization Plugin Async-Search Common-Utils You can find all the changes across the project in the consolidated release notes. What does this Release Candidate not include? Minimum Artifacts - Based on community feedback, GA will include the release of standalone, minimal downloadable artifacts without OpenSearch plugins (alerting, AD, security, etc.). Additional Artifacts - This release candidate will only include the current artifacts (Linux tars and Docker images). GA will additionally include RPM (X64), Windows (X64), and DEB (X64). OpenSearch 1.1 will add Tar (ARM64), DEB (ARM64), RPM (ARM64), MacOS (M1 ARM), and MacOS (X64). Clients with License Checks - There are known issues with license checks which are causing incompatibilities (e.g. Beats 7.13). A solution for the OpenSearch community is targeted for GA. How can you help? The best thing you can do is run the Release Candidate through its paces. Every bit of testing and/or feedback the community can provide helps ensure that this Release Candidate is performing as expected and the GA release includes the best possible product. Couple specific places to start;\nInstall the Release Candidate and open an issue to report any bugs you discover.\nTest rolling upgrades from existing Elasticsearch and ODFE versions to OpenSearch RC1 in a sandbox cluster.\nTest clients! This will help identify what is working (and not working) so issues can be opened and prioritized.\nUse our docs and highlight any additional content you‚Äôd like to see! Submit a PR or open an issue for any content you think would be valuable.\nBefore installing or upgrading to OpenSearch 1.0.0 or OpenSearch Dashboards 1.0.0, please read the release notes ( OpenSearch and OpenSearch Dashboard). While this 1.0.0 Release Candidate is suitable for testing by the community, you should not use a release candidate in a production environment - this Release Candidate is provided for testing and validation purposes only. Speaking of help‚Ä¶ I would like to pass our thanks to those currently contributing to OpenSearch! aadrien, accat, ace03uec, adkalavadia, alexz00, AmiStrn, amitai, anthonylouisbsb, aparo, asfoorial, astateofmind, Bhupendra, BlackMetalz, Bobpartb, bradlee, brandtj, Conan-Kudo, dawnfoster, denysvitali, dirkhh, erhan, erickg, fabide, FreCap, frotsch, galangel, geekygirldawn, GezimSejdiu, ginger, GoodMirek, hagayg, Hakky54, horizondave, horovits, igorid70, janhoy, jkeirstead, jkowall, jkowall, justme, Katulus, kyleconroy, lornajane, Malini, mattwelke, mosajjal, nickytd, Northern, opensorcerer89, oscark, otisg, ralph, retzkek, rmuir, robcowart, santiagobassett, SergioFG, shamil, sharp-pixel, sksamuel, spapadop, sunilchadha, tardyp, Tom1, ttx, tvc_apisani, and willyb If I have missed anyone on this list, please reach out and I will very gratefully make an addition! What‚Äôs next? Let‚Äôs work together to test for bugs and identify feature gaps while working towards a GA release. The goal is to release the GA version of 1.0.0 on July 12, 2021 (check out the public roadmap). Stay tuned to the forums, GitHub, the community meeting, and our blog for the latest updates and announcements.\nIn the meantime, join us building OpenSearch (if you haven‚Äôt already)! Feel free to take a look around at what the community has been up to, check out the public roadmap, and then head over to look at the open issues. Or you could jump right in and start opening issues or contributing. For more details see here.",
    "keywords": [
      "update"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-backwards-compatibility-faq/",
    "title": "OpenSearch 1.0 Backwards Compatibility FAQ",
    "content": "In introducing OpenSearch we said:\nThe Amazon OpenSearch Service APIs will be backwards compatible with the existing service APIs to eliminate any need for customers to update their current client code or applications. Additionally, just as we did for previous versions of Elasticsearch, we will provide a seamless upgrade path from existing Elasticsearch 6.x and 7.x managed clusters to OpenSearch.\nToday we would like to provide mode detail, and clarify what the above statement, and compatibility in general, mean. Upgrading from Elasticsearch OSS and Kibana OSS or Open Distro for Elasticsearch (ODFE) to OpenSearch and OpenSearch Dashboards is like upgrading between versions of Elasticsearch OSS and Kibana OSS. Specifically: OpenSearch supports rolling upgrades and restart upgrades from Elasticsearch OSS 6.8.0 through Elasticsearch OSS 7.10.2 to OpenSearch 1.0. OpenSearch Dashboards supports restart upgrades from Kibana OSS 6.8.0 through Kibana OSS 7.10.2 to OpenSearch Dashboards 1.0. All 1.x versions of ODFE similarly support upgrades to OpenSearch and OpenSearch Dashboards 1.0. We‚Äôve also added an Upgrading section to OpenSearch FAQs, and are updating the OpenSearch docs. Additional questions are welcome on the forums. Please also do open backwards compatibility bugs in GitHub issues.\nFinally, we would like to note that both spellings of backward s compatibility and backward compatibility are allowed, but we prefer backwards because we created GitHub labels as backwards-compatibility and don‚Äôt want to go change everything.",
    "keywords": [
      "technical-posts"
    ],
    "type": "News"
  },
  {
    "url": "/blog/data-into-opensearch/",
    "title": "How do you plan on getting data into OpenSearch?",
    "content": "I expect very few people will only use OpenSearch and OpenSearch Dashboards. Sure, you might do a little testing with the sample data in OpenSearch Dashboards, but really you‚Äôre going to be using something to help you get some data into that cluster. There is a ton of existing, compatible software that can help you do just that - agents, client libraries for programming languages, and data pipeline tools.\nHaving a good end-to-end experience is important to the success of OpenSearch. Consequently, the team is looking to invest time and resources in testing, documenting and potentially contributing back to the tools to ensure that you have a smooth journey from agent to dashboard. However, like most things in life, the testing and documentation resources are not infinite and the team is looking to make the investment into the most used tools first.\nIn order to get a grasp on the usage of these tools Eli from the product team has put together a poll in our forum. It breaks down the known tools and for each tool it allows you to indicate:\nYou‚Äôre currently using,\nYou‚Äôre not using but interested,\nYou‚Äôre not using and not interested.\nSo, head on over to the forums and give mark your usage and interest. We‚Äôll keep the survey active for 7 more days.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/feature-highlight-reporting/",
    "title": "OpenSearch Reporting 101",
    "content": "The OpenSearch Reporting feature helps users generate reports from Dashboards, Visualizations, and the Discover panel and export them to PDF, PNG, and CSV file formats to easily share. In this blog post we talk about how to use the reporting feature, how it is implemented and how it was built with numerous contributions from the open source community. Last, we discuss future plans to improve it. What is Reporting in OpenSearch? Reporting is a form of organizing data into formal summaries; to make reports visually appealing Reporting also supports charts and graphs. Reports are classified into two primary types, visual reports (i.e. PDF and PNG) and data reports (CSVs). How can reports be generated? Reports are generated manually on OpenSearch Dashboards, via scheduled triggers, or through system triggers like alerts or API calls. Anatomy of the Reporting feature on OpenSearch The reporting feature is composed of two plugins; an OpenSearch Dashboards plugin which controls most of the user experience and the OpenSearch plugin which provides scheduling features and secures access to user data with OpenSearch Security policies. The initial request for comments (RFC) for Reporting can be found here. Quick Start User Guide\nInstallation\nReporting comes packaged with the OpenSearch and OpenSearch Dashboards downloads. You can follow the installation instructions on opensearch.org/downloads How do I create a report? Visual reports are created inside Visualizations or Dashboards. CSV reports are created from the Discover tab. Scheduled reports are configured from inside the Reporting UI under OpenSearch Plugins. Development internals\nThe reporting feature uses a custom minimal build of headless-chromium. Chromium is used as the rendering engine to generate visual reports. Below is are multiple flow diagrams for the various actions of Reporting. Creating a report definition: When a user defines a report in OpenSearch Dashboards, the Reporting Dashboards plugin saves that definition in an index in OpenSearch. Downloading a report: When a user downloads a report, the OpenSearch Dashboards Reporting plugin fetches the data from OpenSearch. After the data is fetched, the report is generated and downloaded to the users browser. Running a reporting job: When a schedule report begins, data is fetched from OpenSearch, next the OpenSearch Dashboards Reporting plugin generates the report. Once the report is generated, it is sent to the configured destination in the reporting policy and the job status is updated and stored back in OpenSearch. Community contributions\nReporting was built by multiple contributors from the community (some PRs listed below). Our team presented the initial Reporting contributions in a community meeting with the Reports RFC. Our team went over the initial design and requirements, and our high level plan to develop the feature. After the community meeting, some of the attendees expressed interest in contributing and helped build the CSV functionality. Since the initial release, there have been numerous other community contributions to improve and customize the project (see below). We thank all the contributors who have helped build the Reporting feature! CSV APIs endpoints for data reports. #50 Improve quality of rendered PDFs #354 passing default proxy-authentication headers #329 Ignore custom commented areas. #314 Add dynamic wait to allow page content to render #331 Use commonlyUsedRanges from Settings instead of a constant value #352 Add i18n translation support #362 Improve sanity tests #361 and more‚Ä¶\nIf you‚Äôre interested in contributing please reach out on GitHub issues or the community forum. The more formal contribution guidelines are documented here.\nFuture project goals\nCreate integrations with Alerting, Notebooks and other plugins\nEnable support for slack, chime, webhooks, pager-duty, and other destinations via the Notifications plugin.\nRemove the hard dependency on headless-chromium by enabling support for a Java-based rendering engine.\nEnable APIs for programatic reporting",
    "keywords": [
      "feature"
    ],
    "type": "News"
  },
  {
    "url": "/blog/introducing-community-projects/",
    "title": "Introducing the community projects page",
    "content": "Many open source projects benefit from the ecosystem of software created on top of or for the project. As an example, the Node Package Manager (NPM) makes JavaScript more approachable. It hosts a variety of modules that solve problems people face during the development of JavaScript software. Users can quickly search for these modules install them and incorporate them directly into their JavaScript project. Having a similar ecosystem for OpenSearch will help all of us get more out of our own OpenSearch deployments. As a first step, I wanted to share that opensearch.org now has a community projects page. The goal of this page is to highlight projects built by the OpenSearch community for the OpenSearch community. What kinds of projects are appropriate to highlight? As mentioned above, the goal of this page is to highlight projects built by the OpenSearch community for the OpenSearch community. Any project that helps people use or get more value out of OpenSearch is appropriate for the page, regardless of the project license. Projects could be OpenSearch plugins, OpenSearch Dashboards plugins, client libraries, ingestion tools, and more. For example, the first project highlighted is the OpenSearch Plugin Template (by AmiStrn). Each project has a name, description, icon (optional), license, owner, and download link (optional). How do I get a project added to the page? Getting a project added is as simple as creating a pull request (PR). Just follow the instructions in the community projects page to know where to add your project information and submit a PR. Once you have created the PR, one or more of the OpenSearch website maintainers will review it, and work with you to resolve any issues. Once the PR is merged, the website will be updated to include your project. Where do we go from here? This is a first step in highlighting OpenSearch community projects. Overtime this page will evolve to have search functionality, tags, project types and more. The ultimate goal is for all of us to have a catalog of projects that any of us can easily search through and incorporate into our OpenSearch and OpenSearch Dashboards deployments. If you‚Äôve got something you‚Äôd like to add, go ahead and open a PR on the website repo or reach out on the forums.\nI‚Äôd like to thank the people involved in creating the first version of this page: AmiStrn for volunteering to have the Plugin Template added aparo for volunteering to have the Flattened Data Type plugin added stockholmux for reviewing the blog post and project page PR kgcreative for styling the page",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/how-to-upgrade-from-opendistro-to-opensearch/",
    "title": "How To: Upgrade from Open Distro to OpenSearch",
    "content": "As general availability for OpenSearch and OpenSearch Dashboards is fast approaching, we wanted to provide some guidance on upgrading Open Distro for Elasticsearch (Open Distro) 1.13 to OpenSearch 1.0, along with some steps to help you prepare for the upgrade. While most of you will already have the experience of upgrading Elasticsearch clusters, we wanted to provide a refresher on the upgrade process, and make some specific call-outs regard the upgrade to OpenSearch from Open Distro.\nPreparing for the Upgrade\nIf you have an Open Distro for Elasticsearch cluster and want to upgrade to OpenSearch, there are a few things you should do to prepare for the upgrade. Preparing for your upgrade ahead of time ensures that you can enjoy the new features and improved usability of OpenSearch as efficiently as possible.\nFirst, it is recommended to take a snapshot before you continue to the actual upgrade, since all versions of Open Distro support taking snapshots of the cluster. This is especially important in the snapshot upgrade we‚Äôll cover later. To take a snapshot, use the following command: # via Dev Tools on Kibana\nPUT /_snapshot/my_backup/opendistro_backup?wait_for_completion=true\n# via curl, assuming the cluster is on 9200 port\ncurl -XPUT \"localhost:9200/_snapshot/my_backup/opendistro_backup?wait_for_completion=true\"\n# via curl, assuming the cluster is on 9200 and security is enabled\ncurl -XPUT -k -u 'admin:admin' 'https://localhost:9200/_snapshot/my_backup/opendistro_backup?wait_for_completion=true\" Note: For rest of the blog post we will have commands represented for Dev Tools on Kibana/OpenSearch Dashboards for simplicity, but all the above formats will work for all the commands.\nSecond, you should verify the version of your existing cluster, and follow the recommended upgrade path for version compatibility (see table below). While all Open Distro 1.x versions can be upgraded to OpenSearch, you may need to upgrade to Open Distro 1.13.2 first. All the Open Distro plugins are upgrade compatible to their OpenSearch equivalent. Upgrade support from Open Distro 1.x to OpenSearch Open Distro ES Recommended Upgrade Path 1.0\n7.0.1\nRestart/Rolling Upgrade to Open Distro 1.13\n1.1\n7.1.1\nRestart/Rolling Upgrade to Open Distro 1.13\n1.2\n7.2.1\nRestart/Rolling Upgrade to Open Distro 1.13\n1.3\n7.3.2\nRestart/Rolling Upgrade to Open Distro 1.13\n1.4\n7.4.2\nRestart/Rolling Upgrade to Open Distro 1.13\n1.5\n7.5.2\nRestart/Rolling Upgrade to Open Distro 1.13\n1.6\n7.6.1\nRestart/Rolling Upgrade to Open Distro 1.13\n1.7\n7.6.1\nRestart/Rolling Upgrade to Open Distro 1.13\n1.8\n7.7.1\nRestart/Rolling Upgrade to Open Distro 1.13\n1.9\n7.8.0\nRestart/Rolling Upgrade to Open Distro 1.13\n1.10.0\n7.9.1\nRestart/Rolling Upgrade to Open Distro 1.13\n1.11.0\n7.9.1\nRestart/Rolling Upgrade to Open Distro 1.13\n1.12.0\n7.10.0\nRestart/Rolling Upgrade to Open Distro 1.13\n1.13.2\n7.10.2\nRestart/Rolling Upgrade to OpenSearch 1.0 And finally, now that you are ready for the upgrade, you should download the latest version of OpenSearch from the OpenSearch downloads page, and if you need help with installation instructions follow the install and configure guide on the documentation website.\nTypes of Upgrade\nNow that you have taken your snapshot, and validated the recommended upgrade path for your existing cluster, we can move on the the upgrade itself. We are going to provide instructions on 3 types of upgrades. Snapshot Upgrade: A snapshot upgrade involves taking a snapshot of the current cluster, spinning up a new cluster, and then restoring from the snapshot. Depending on how you do this, there may or may not be downtime. Restart Upgrade: A restart upgrade involves taking down the whole cluster, upgrading it, and then starting it back up. This will include downtime. Rolling Upgrade: A rolling upgrade involves taking down a cluster node by node, upgrading incrementally, and then starting them back up. This method will not incur downtime.\nApproach #1: Snapshot Upgrade\nThe first approach to upgrading from Open Distro to OpenSearch is to take a snapshot of your data (indices), create an OpenSearch cluster, restore the snapshot on the new cluster, and point your clients to the new host. While this approach means running two clusters in parallel, it provides you with an opportunity to validate that the OpenSearch cluster is working in a way that meets your needs prior to modifying the current Open Distro cluster.\nVerify that the existing cluster is green and healthy. See the Validation section below for additional guidance.\nTake a snapshot in Open Distro. PUT /_snapshot/my_backup/opendistro_backup?wait_for_completion=true Spin up a new cluster using OpenSearch.\nRestore from the snapshot taken in step 2. POST /_snapshot/my_backup/opendistro_backup/_restore Verify the indices are searchable and indexable in OpenSearch after restore.\nAnd that‚Äôs it! Your snapshot upgrade is complete.\nApproach #2: Restart Upgrade\nAnother approach is restarting your whole cluster while upgrading it to OpenSearch. This method incurs downtime, and traffic from clients should be stalled until the new cluster is up.\nVerify that the existing cluster is green and healthy. See the Validation section below for additional guidance.\nDisable shard allocation: during upgrade we do not want any shard movement as the cluster will be taken down. PUT _cluster/settings\n{\n\"persistent\": {\n\"cluster.routing.allocation.enable\": \"primaries\"\n}\n} Stop indexing, and perform a flush: as the cluster will be taken down, indexing/searching should be stopped and _flush can be used to permanently store information into the index which will prevent any data loss during upgrade. POST /_flush Shutdown all the nodes. # if you are running Elasticsearch using systemd run:\nsudo systemctl stop elasticsearch.service\n# if you are running Elasticsearch using SysV init:\nsudo -i service elasticsearch stop\n# if you are running Elasticsearch as daemon:\nkill $(cat pid) Upgrade all the nodes (install OpenSearch):\nExtract the zip in a new directory.\nCopy over the data directory from Elasticsearch to OpenSearch, or set path.data in config/opensearch.yml pointing to that directory.\nSet path.logs in config/opensearch.yml, pointing to a directory where you want to store logs.\nVerify that the existing cluster is still green and healthy.\nStart each upgraded node: if the cluster has dedicated master nodes, start them first, and make sure the master is elected before data nodes are started. You can monitor the health of the cluster as follows. GET _cluster/health Re-enable shard allocation: PUT _cluster/settings\n{\n\"persistent\": {\n\"cluster.routing.allocation.enable\": null\n}\n} Verify that the indexed data in Open Distro is now searchable and indexable in OpenSearch.\nYou did it! Your cluster is now upgraded via a Restart Upgrade.\nApproach #3: Rolling Upgrade\nThis last approach upgrades an existing cluster node by node to OpenSearch 1.0.0, while keeping the cluster active during the upgrade. It is important to note that while search operations during rolling upgrades are supported, indexing operations are not recommended.\nVerify that the existing cluster is green and healthy. See the Validation section below for additional guidance.\nDisable shard allocation: during upgrade we do not want any shard movement as the cluster will be taken down. PUT _cluster/settings\n{\n\"persistent\": {\n\"cluster.routing.allocation.enable\": \"primaries\"\n}\n} Stop indexing, and perform a flush: as the cluster will be taken down, indexing/searching should be stopped and _flush can be used to permanently store information into the index which will prevent any data loss during upgrade. POST /_flush Shutdown a single node: first data nodes and later master nodes. # if you are running Elasticsearch using systemd run:\nsudo systemctl stop elasticsearch.service\n# if you are running Elasticsearch using SysV init:\nsudo -i service elasticsearch stop\n# if you are running Elasticsearch as daemon:\nkill $(cat pid) Upgrade the node which was shutdown: i.e install OpenSearch.\nExtract the zip in a new directory.\nCopy over the data directory from Elasticsearch to OpenSearch or set path.data in config/opensearch.yml, pointing to that directory.\nSet path.logs in config/opensearch.yml, pointing to a directory where you want to store logs.\nVerify that the existing cluster is still green and healthy.\nWait for the node to recover: before upgrading the next node, wait for the cluster to finish shard allocation and the cluster to be green: GET _cat/health?v=true Re-enable shard allocation: once all the data nodes are upgraded you can re-enable the shard allocation. PUT _cluster/settings\n{\n\"persistent\": {\n\"cluster.routing.allocation.enable\": null\n}\n} Verify that the indexed data in Open Distro is now searchable and indexable in OpenSearch.\nRepeat: repeat steps 4-7 until all the nodes are upgraded (data nodes first and then master nodes). Note: Old master nodes can communicate with new data nodes but new masters cannot communicate with old data nodes.\nPhew! You have upgraded to OpenSearch via a Rolling Upgrade.\nValidation\nOpenSearch plugins based on the Open Distro for Elasticsearch plugins are included in OpenSearch 1.0, and are functionally backwards compatible with their predecessors.\nVerify all the cluster health, indices, settings etc are seamlessly migrated and working in OpenSearch.\nYou can verify via these APIs, which are just few of many different ways: # get cluster health\nGET _cluster/health\n# get nodes in a cluster\nGET _cat/nodes\n# get node health\nGET _cat/health?v=true As with all software, upgrade is a critical path for the community and customers. If you need help as always open an issue and label them backwards-compatibility, 1.0.0:\nFor OpenSearch: OpenSearch issues.\nFor a plugin: use the individual plugin repository.\nFor all the plugins: OpenSearch plugins issues.\nIf you must rollback your upgrade for any reason, the simplest path is to spin up an Open Distro cluster and restore it using the snapshot you took earlier. You could follow the same process listed in the snapshot upgrade path above.\nRecommendations\nNow that you read through most of the good stuff, we have some additional recommendations to ease your upgrade:\nSnapshot Upgrade: this is the safest approach if you can spin up multiple clusters, and move your customers to the new endpoint after the upgrade. We recommend this approach for beginners.\nRestart Upgrade: this is a moderately difficult approach, as it takes down the whole cluster. Ensuring everything works is performed at the very end. We recommend this approach for intermediate users.\nRolling Upgrade: This is the most complex approach, but requires no downtime and provides cluster health feedback at every step of the process. This method takes down one node at a time for an upgrade. We recommend this approach for advanced users.\nUpgrading OpenSearch Dashboards\nSimilarly to Kibana OSS, OpenSearch Dashboards currently does not support rolling upgrades, requiring a Restart Upgrade. To upgrade to OpenSearch Dashboards, stop all Open Distro Kibana instances, deploy a new OpenSearch Dashboards instance, and direct traffic to it.\nGoing Forward\nOpenSearch can be upgraded using a Rolling Upgrade for most minor and major versions, thus upgrading going forward will not usually interrupt your service.\nFor more information on upgrading please see our Upgrading FAQs. If you have questions about backward compatibility, check out the Backwards Compatibility FAQ blog post. If you‚Äôre looking for a great ‚ÄúGetting Started‚Äù guide, I highly recommend Gedalyah‚Äôs Install and Configure OpenSearch.\nHope this blog post is helpful! We plan to have a follow-up post on upgrading from Elasticsearch OSS to OpenSearch, next. If you have any feedback/suggestions on new topics please don‚Äôt hesitate to open issues or post to the forums.\nThanks to the OpenSearch team for making seamless upgrades happen.",
    "keywords": [
      "technical-posts"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-general-availability-announcement/",
    "title": "1.0 is released!",
    "content": "Today is a really good day for the OpenSearch project. Since its start, the goal has been to quickly produce a version of OpenSearch that‚Äôs ready to use in production environments. After a ton of work by the OpenSearch community, we‚Äôre thrilled to announce the first general availability (GA) release of OpenSearch 1.0. Everyone in this highly-engaged and diverse community should be proud of the accomplishment in reaching this milestone together.\nOpenSearch is a community-driven, open source search and analytics suite derived from Apache 2.0 licensed Elasticsearch 7.10.2 &amp; Kibana 7.10.2. It consists of a search engine daemon (OpenSearch), a visualization and user interface (OpenSearch Dashboards), and advanced features from Open Distro for Elasticsearch like security, alerting, anomaly detection and more.\nToday‚Äôs release is the culmination of several stages of development. When the repositories were initially opened, the project team recapped the entire process to that point. Since then the pace of change on OpenSearch has picked up, with the addition of the plugins from the Open Distro for Elasticsearch project, as well as key updates provided by contributors across the community. These updates include: Continual removal of proprietary code and marks. While the initial pre-Alpha effort removed the vast majority of the now redundant proprietary bits and references, additional cleaning occurred throughout the rest of development. You can now use OpenSearch with confidence that it‚Äôs free of proprietary code and references. Upgrading. OpenSearch should work equally well for you whether you‚Äôre building an application from scratch or upgrading an existing workload from an earlier version of Elasticsearch. With this in mind, the team spent time making upgrades as easy as possible. You can upgrade from Elasticsearch to OpenSearch exactly the same way you would upgrade between Elasticsearch versions in the past. Compatibility. The ever-growing cornucopia of tools and libraries that work with OpenSearch make it an even more useful search and analytics suite. The community of users has been instrumental in pointing out and helping the developers on the project understand what‚Äôs needed to ensure continued compatibility with their favorite tools. Maximizing compatibility required making changes to how versions are reported as well as maintaining important API features used by various libraries. Testing. With a project as complex as OpenSearch, robust testing ensures that small changes don‚Äôt cascade into unintended problems. The team paid special attention to making sure OpenSearch has a modern and flexible testing infrastructure that will serve the project for years to come.\nThe GA version of OpenSearch 1.0 also has a few enhancements since the beta:\nSupport for the ARM64 architecture for Linux,\nMinimal artifacts for embedding of OpenSearch and OpenSearch Dashboards into existing products and services,\nData stream support for OpenSearch Dashboards,\nSpan attribute visibility and filtering in the Trace Analytics plugin,\nScheduling and tenant support in the Reporting plugin.\nYou can see all the changes in the release notes. Thank you! This release includes a number of great contributions. From the new plugins to improvements to the CX, you make OpenSearch better every day. So without further ado, a shout-out to everyone who‚Äôs contributed to 1.0: aadrien, abbashus, accat, ace03uec, aditjind, adkalavadia, adnapibar, aetter, afazel, akbhatta, alexz00, amistrn, amoo-miki, anan, ananzh, andy_leong, andy840314, anirudha, aparo, arunabh23, asfoorial, ashwinkumar12345, ashwinpankaj, astateofmind, AvianDo, awshurneyt, badbybirth, bbarani, Bhupendra, BlackMetalz, Bobpartb, boktorbb-amzn, bradlee, brandtj, bukhtawar, camerski, cehenkle, chenqi0805, chloe-zh, chrisdeeming, cjcjameson, cliu123, conan-kudo, dai-chen, dansimpson, davidcui1225, dbbaughe, dblock, debjanibnrj, devardee, dhiamzn, dhruvil7doshi, dirkhh, eirsep, elb3k, elfisher, erhan, erickg, fabide, flyhigh72, frotsch, Gagi, gaiksaya, GalAngel, galangel, geekygirldawn, getsaurabh02, gezimsejdiu, ginger, giu85, GoodMirek, gzurowski, hagayg, hakky54, hardik-k-shah, harmishlakhani, harold-wang, horizondave, horovits, hsiang9431-amzn, hxiao608, hyandell, igorid70, itiyamas, jainankitk, jamesiri, janhoy, jayeshathila, jcgraybill, jgough, jkeirstead, jkowall, jmazanec15, jonahcalvo, joshuali925, jotok, justme, kaituo, Katulus, kavilla, ke4qqq, keithhc2, ketanv3, kevinhwang, kgcreative, khushbr, kkhatua, krishna-ggk, kyleconroy, annie3431, lezzago, liujoycec, lizsnyder, lobdelle, lopqto, lornajane, lukkoor, maheshmr, makarthikeyan1, Malini, MartiniGuy, mattsb42-aws, mattwelke, mch2, meetshah777, mengweieric, mgodwan, micrictor, mihirsoni, mkcg, mmakaria, mosajjal, nickytd, nico, nisheedh, nknize, Northern, ohltyler, opensorcerer89, oscark, otisg, palashhedau, penghuo, peternied, peterzhuamazon, piyushdaftary, platzd, praveensameneni, psychbot, qreshi, ralph, rbridle, retzkek, rguo-aws, rmuir, robcowart, rtarek, rursprung, SalvoDiMa91, sandeshkr419, santiagobassett, saratvemulapalli, seanneumann, sejli, seraphjiang, SergioFG, setiah, shamil, sharp-pixel, shdubsh, shivani, shwetathareja, siddharthlatest, skkosuri-amzn, sksamuel, soosinha, sothawo, spapadop, spbjss, spotaws, sreekarjami, sruti1312, ssgao, stockholmux, sujithvm, sunilchadha, taewooKim, tardyp, tgurr, thalurur, TheAlgo, thealgo, itiyamas, tlfeng, tmarkley, Tom1, ttx, ttx, tvc_apisani, vachashah, vamshin, vengadanathan-s, vijayanb, vrozov, weicongs-amazon, willyb, wkruse, wnbts, wqixian, wrijeff, wusheng, xuezhou25, yadavcbala, yilintao-amzn, yizheliu-amazon, ylwu-amzn, yoavwe, yu-sun-77, yujias0706, yuxitang-amzn, zengyan-amazon, zhanghg08, zhongnansu A special thanks goes to the OpenSearch partners who have helped make OpenSearch 1.0 possible: Aiven, Appbase.io, Bitergia, Bonsai, ElastiFlow, Eliatra, GSI Technology, Instaclustr, Logz.io, Opster, SearchBlox Software, Inc., Titaniam, and Wazuh.\nIf anyone is missing from these lists, please reach out and we will very gratefully make an addition! So what‚Äôs next? Now that OpenSearch has added test automation, built infrastructure, made updates to ensure the code is suitable for production use, and thoroughly tested, the project will be able to deliver on a more regular release cadence. Exciting things are coming for the project:\nTo ensure using OpenSearch is as easy to use as possible the intention is to release additional artifacts including RPM x64, DEB x64, DEB ARM64, RPM ARM64, MacOS ARM64, MacOS x64, and Windows x64 among others.\nThe project will make security investments such as more granular security for Index State Management and increasing SAML attribute flexibility.\nObservability features like bucket level alerting and out of the box security and operational analytics experiences will be added to make it easier to analyze your application and infrastructure data.\nOpenSearch will support cross cluster replication to allow you to deploy for higher availability.\nTo help you better understand your data, ML features such as FAISS library integrations and root cause analysis support will be added.\n‚Ä¶among many others on the roadmap.\nThe next major milestone is OpenSearch 1.1, which is expected to release on August 30. In addition to more artifact coverage (e.g. RPM X64, DEB X64, DEB ARM64), OpenSearch 1.1 will include improvements to anomaly detection (ex. universal flow), indexing pressure protection, and notification support among others. Check out the public roadmap for more details. How can you help? For almost any type of contribution, the first step is opening an issue ( OpenSearch, OpenSearch Dashboards). Even if you think you already know what the solution is, writing down a description of the problem you‚Äôre trying to solve will help everyone get context when they review your pull request. If it‚Äôs truly a trivial change (e.g. spelling error), you can skip this step ‚Äì but when in doubt, open an issue. If you‚Äôre excited to jump in, check out the ‚Äúhelp wanted‚Äù tag in issues. Questions? Feedback? Let us know by opening an issue or posting in the forums. There are also biweekly Community Meetings. The community meetings will include progress updates at every session and include time for Q&amp;A. The next community meeting is on Monday, July 26, 2021, from 10:00 AM to 11:00 AM Pacific time.",
    "keywords": [
      "updates"
    ],
    "type": "News"
  },
  {
    "url": "/blog/feature-highlight-opensearch-dashboards-notebooks/",
    "title": "Feature Deep Dive: OpenSearch Dashboards Notebooks",
    "content": "OpenSearch Dashboards Notebooks lets you easily combine live visualizations, narrative text, and SQL and Piped Processing Language (PPL) queries so that you can tell your data‚Äôs story. You can interactively explore data by running different visualizations and share them with team members to collaborate. Notebooks can help with a variety of use cases such as creating postmortem reports, designing operations run books, building live infrastructure reports, and even documentation. With OpenSearch 1.0, Notebooks is now production ready. Additionally, multiple enhancements were introduced such as support for multi-tenancy, query languages like SQL and PPL and an integration with reporting.\nIn this blog we will explore popular use cases for Notebooks, how the feature works, and how to get started with it.\nWhat problem does OpenSearch Dashboards Notebooks solve?\nBefore Notebooks, there was no built-in way to combine text, queries, visualizations, and dashboards to build contextual views of data stored in OpenSearch. This made it difficult to convey important results and summaries in a way that can be consumed easily. As an example, when creating a post mortem report for a customer issue, you had to collaborate with multiple stakeholders like support engineers, devops, developers, and your management. It involved relying on multiple tools like text editors, wikis, notes, and screenshots to create documents and reports. This process was time consuming and the documents and reports generated contain static data and visualizations. You could not tell the full story from the data and had to share generic dashboards and visualizations that left interpretation open to your stakeholders.\nHow does OpenSearch Dashboards Notebooks solve the problem?\nNotebooks was built to address this problem by enabling you to interactively and collaboratively develop rich reports backed by live data, and queries in cells or paragraphs that can combine markdown, SQL and PPL queries, and visualizations with support for multi-timelines so that you can easily tell a story. They can be developed, shared as a PDF, PNG or an OpenSearch Dashboards endpoint, and refreshed directly from OpenSearch Dashboards to foster data driven exploration and collaboration between you and your stakeholders. In this release, we integrated Notebooks with other OpenSearch features and libraries like the SQL and PPL query engine, reporting engine, and OpenSearch Dashboard APIs. We used the nteract library to render paragraphs and markdown.\nHow do you use OpenSearch Dashboards Notebooks?\nTo get started, choose Notebooks within OpenSearch Dashboards.\nStep 1: Create a notebook\nChoose Create notebook and enter a descriptive name.\nChoose Create.\nChoose Actions to rename, duplicate, or delete a notebook. Step 2: Add paragraphs\nParagraphs combine code blocks and visualizations for describing data.\nAdd a code block\nCode blocks support markdown, SQL, and PPL languages.\nSpecify the input language on the first line using %[language type] syntax. For example, type %md for markdown, %sql for SQL, and %ppl for PPL.\nSample markdown block %md\nAdd in text formatted in markdown. Sample SQL block %sql\nSelect * from opensearch_dashboards_sample_data_flights limit 20; Sample PPL block %ppl\nsource=opensearch_dashboards_sample_data_logs | head 20 Add a visualization\nTo add a visualization, choose Add paragraph and select Visualization.\nIn Title, select your visualization and choose a date range. You can choose multiple timelines to compare and contrast visualizations.\nTo run and save a paragraph, choose Run. Paragraph actions\nYou can perform the following actions on paragraphs:\nAdd a new paragraph to the top of a report,\nAdd a new paragraph to the bottom of a report,\nRun all the paragraphs at the same time,\nClear the outputs of all paragraphs,\nDelete all the paragraphs. Sample notebooks\nWe prepared the following sample notebooks that showcase a variety of use cases:\nUsing SQL to query the OpenSearch Dashboards sample flight data.\nUsing PPL to query the OpenSearch Dashboards sample web logs data.\nUsing PPL and visualizations to perform sample root cause event analysis on the OpenSearch Dashboards sample web logs data.\nTo add a sample notebook, choose Actions and select Add sample notebooks. Create a report\nYou can use notebooks to create PNG and PDF reports:\nFrom the top menu bar, choose Reporting actions.\nYou can choose to Download PDF or Download PNG.\nReports generate asynchronously in the background and might take a few minutes, depending on the size of the report. A notification appears when your report is ready to download.\nTo create a schedule-based report, choose Create report definition. For steps to create a report definition, see the create report documentation.\nTo see all your reports, choose View all reports. How do I contribute?\nIf you‚Äôre interested in contributing please reach out on GitHub issues or the community forum. The more formal contribution guidelines are documented in the contributing guide.\nThank you\nWe would like to thank @ps48, @joshuali925, davidcui1225, chloe-zh and lvndc for their contributions to this feature.",
    "keywords": [
      "feature"
    ],
    "type": "News"
  },
  {
    "url": "/blog/streaming-analytics/",
    "title": "Streaming Analytics in OpenSearch",
    "content": "This post introduces an updated Random Cut Forest library, RCF 2.0. OpenSearch uses RCF 1.0 in its existing High Cardinality Anomaly Detection (HCAD) framework [ 1, 2, 3]. The sequel discusses the perspective motivating RCF 2.0.\nAnomaly detection is a quintessential search problem. A typical use case corresponds to a high cardinality dataset, where some attributes split the data into a large number of individual, and potentially incomparable, time series and one simultaneously monitors each of those time series. Anomalies are often only explainable in the context of past data specific to each time series. The use case exemplifies the needle-in-an-unfamiliar-haystack search task. The HCAD in OpenSearch [ 2, 3] provides users an out of the box solution for this use case. See [4] for discussions on using the feature in OpenSearch.\nHowever a truly high cardinality scenario requires that models are stored on disk and loaded into memory on demand to save heap space. Loading the models repeatedly however, hearkens back to rebuilding models on every data value which is a bottleneck for scale. Imagine rebuilding hash tables every time a new value is seen ‚Äî further, imagine rebuilding an ensemble of 50 such hash tables with the same entries in them!\nRCF is a subclass of Random Forests and some of the critical criteria for choosing the RCF models in OpenSearch correspond to light footprint, accuracy, explainability as well as the ability of fast streaming updates where models are not rebuilt from scratch on every new observation [1]. Technically, RCF uses the theme of stochastic coupling in a strong sense ‚Äî where assuming one has sampled from a distribution of trees, the update preserves the distributional assumption. Just as streaming algorithms can support continuous learning without additional expansive pipelines, RCF 1.0 supports some examples of continuous learning over Random Forests. However as is typical in many streaming algorithms, the existing RCF 1.0 library does not address stop-store-reload-and-go behavior well. Trees are more complicated data structures in comparison to hash tables and the repeated reloading of ensemble forests impacts scalability. Furthermore there are multiple potential tradeoffs between representation and update efficiency for trees. RCF 2.0 threads that tradeoff needle ‚Äì on synthetic benchmarks, available with the library, the model size reduces by 5-10x and speed of serialization/deserialization increases by an order of magnitude in comparison to RCF 1.0, thereby improving the scalability of HCAD and potentially reducing cost.\nRCF 2.0 provides a new compact, efficiently serializable ensemble forest that supports the application where the forest itself can be a streamed input in addition to new data. Before discussing the improvements, let us reconsider RCFs in greater detail for a moment. The central tenet behind RCF had been to ‚Äúmake ingest faster and move complexity to the inference step‚Äù [5]. This was achieved by stochastic coupling mentioned earlier. RCF 2.0 doubles down on the same idea, and ‚Äúattempt to make updates information theoretically leaner‚Äù instead of just improving computational speed. Random Forests are traditionally viewed as top-down partitioning algorithms; a collection of points are recursively partitioned based on some criterion. RCF 1.0 espoused a similar conceptual top-down view for updates, but inference (by necessity) was bottom up. RCF 2.0 reuses stochastic coupling to simulate the top down update via a bottom up process ‚Äî ensuring that the end result of the new update is distributionally indistinguishable from the conceptual top-down model update. As a consequence, RCF 2.0 avoids the necessity of storing bounding boxes at every node, and can recreate exactly those bounding boxes which are necessary. This provides a significant reduction of heap memory and allows greater multitenancy of models. On the aforementioned synthetic benchmarks, the heap memory improvement ranges from 1.5x to 7x, with corresponding changes in throughput (based on input parameters, not necessarily linear). In the specific parameter settings of HCAD in OpenSearch, there appears to be a 2-4x improvement in heap size, allowing greater scalability. Stay tuned for the upcoming releases for more specific details.\nRCF 2.0 provides a few additional benefits. A model can be restored to exactly replicate an unrestored model which may be of independent use from a broader systems perspective. From an inference perspective, the functions remain mostly unchanged ‚Äî the extrapolation is refactored to expose intermediate computations which act as if providing a conditional field which is likely to be use in the future. We reiterate that anomaly detection is typically the mere tip of the proverbial iceberg and the starting point of inquiry [5]. A typical use case of detection is followed up with variety of explanatory queries such as predicted/imputed value, density around the point, nearest neighbor ‚Äî and indeed multitudes of good anomaly detection algorithms exist which leverage these explanatory facets to detect anomalies. However this provides a very interesting opportunity. Suppose that there is an unspecified statistical measure that indicates anomalousness in some scenario; suppose further we have a reasonably good dynamic anomaly detection algorithm over streaming data ‚Äî then it stands to reason that the dynamic data structures and information used by said streaming anomaly detector should contain relevant information about that unspecified measure. This therefore can be a reasonable recipe to provide a streaming algorithm for that same said measure by reusing the dynamic data structure with minimal extra effort. And drawing on inspirations from Random Forests, this opens up a broad array of possibilities.The RCF library provides some examples of dynamic imputation, dynamic density estimation and dynamic near neighbors, as mentioned above. The interested reader can try out the Apache 2.0 library available at [6].\nThis task of finding a vanishing-needle-in-an-everchanging-haystack; deriving ephemeral insights from data, possibly with some approximation, as the data is being observed and transformed on the fly, is a core aspiration of both streaming analytics and OpenSearch. Size and scalability of ML/Analytics models remains a challenging frontier and insights from streaming algorithms provides useful tools. The sequential decision making in streaming, where we decide on processing the current input completely before seeing the next input, is the ultimate what-you-compute-is-what-you-have in the context of analytics. Costs, latencies, and general behavior is significantly more predictable with a streaming approach and the possibility of dynamic computation of different measures provides exciting opportunities ahead for analytics in OpenSearch. 1: https://aws.amazon.com/blogs/opensource/introducing-real-time-anomaly-detection-open-distro-for-elasticsearch/ 2: https://aws.amazon.com/blogs/big-data/using-random-cut-forests-for-real-time-anomaly-detection-in-amazon-elasticsearch-service/ 3: https://aws.amazon.com/blogs/big-data/a-deep-dive-into-high-cardinality-anomaly-detection-in-elasticsearch/ 4: https://opensearch.org/docs/monitoring-plugins/ad/index/ 5: https://www.opensearch.org/blog/odfe-updates/2019/11/random-cut-forests/ 6: https://github.com/aws/random-cut-forest-by-aws",
    "keywords": [
      "feature"
    ],
    "type": "News"
  },
  {
    "url": "/blog/community-clients/",
    "title": "Carrying forward the OpenSearch client libraries as a community",
    "content": "Note: This post was updated in Dec 2021 to reflect the current status of the new clients. Software projects often provide language specific client libraries to make it easy to integrate applications. OpenSearch is built to be wire compatible with open source Elasticsearch 7.10.2 so that anyone can leverage the existing clients, connectors, and tools with minimal modifications to their deployments.\nStarting with version 7.14, multiple of the clients maintained by Elastic contain new logic that rejects connections to OpenSearch clusters or to clusters running open source distributions of Elasticsearch 7. This includes the Apache-licensed distribution provided by Elastic, as well as community distributions like Open Distro for Elasticsearch.\nFor the time being, people who use open source Elasticsearch, Open Distro, or OpenSearch should avoid upgrading to version 7.14 of these client libraries as this may break applications. Please see this documentation for recommended versions of client libraries that have been tested to work with open source Elasticsearch, Open Distro, and OpenSearch.\nOver the next few weeks, the OpenSearch project will add new open source clients for connecting applications to any OpenSearch or Elasticsearch version 7 cluster. These clients will be derived from the last compatible version of the corresponding Elastic-maintained client before product checks were added. The list of clients includes: elasticsearch-py ‚Üí opensearch-py elasticsearch-java ‚Üí opensearch-java elasticsearch-net go-elasticsearch ‚Üí opensearch-go elasticsearch-js ‚Üí opensearch-js elasticsearch-ruby ‚Üí opensearch-ruby eland elasticsearch-php ‚Üí opensearch-php elasticsearch-rs ‚Üí opensearch-rs elasticsearch-perl elasticsearch-specification elasticsearch-hadoop If there is a client library that you do not see, but would like to contribute to or maintain, please post a request in the forums.\nAs stated in OpenSearch‚Äôs 6th principle of development ‚ÄúGreat open source software is built together, with a diverse community of contributors,‚Äù and so we are seeking co-maintainers for each of these client libraries. Maintainers are active and visible members of the community, and have maintain-level permissions on a repository. They use those privileges to serve the community and evolve the software in the repository they maintain. As an OpenSearch project maintainer, you agree to advance the mission of the project and their repo, and to uphold the project‚Äôs Code of Conduct. It‚Äôs up to you.\nShould you take on this responsibility, you won‚Äôt be alone‚ÄîAWS will contribute engineers to support each library as well. In addition, AWS will ensure there is continuity should any maintainers step down in the future. If you‚Äôre interested in maintaining a client library, you‚Äôll find an open issue in each repo where volunteers are being solicited.\nI‚Äôd like to give a big thanks to the people who have already stepped up to help progress and maintain the forks of the clients: madhusudhankonda robcowart svencowart robsears deztructor axeoman paulborgermans Shyim",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/partner-highlight-titaniam/",
    "title": "OpenSearch Partner Highlight: How Titaniam Arcus can further secure your OpenSearch Deployment",
    "content": "Over the past few years there have been numerous security breaches reported in the news. These types of incidents are top of mind as people want to ensure the software and services they build are secure. OpenSearch provides an out-of-the-box security plugin so that developers can build OpenSearch deployments securely. The out-of-the-box features include: TLS for the REST API, node-to-node communication, and OpenSearch Dashboards Built-in authentication with support for Active Directory, LDAP, SAML OpenID, and more Role-based access control with index-level, document-level, and field-level security Audit Logging OpenSearch Dashboards multi-tenancy This blog dives into how Titaniam can further strengthen the OpenSearch security posture with the Titaniam Arcus plugin.\nEncryption is an essential data protection tool in the security toolbox. It is because of encryption that we all can sleep at night knowing our valuable data is secure while flowing through networks and being stored at rest. However, the ability to encrypt data has traditionally been limited to data-at-rest (file system) and data-in-transit (TLS). When it comes to actually utilizing data, say for instance when data is being indexed, searched and analyzed (i.e. data-in-use), it is processed in clear text. Modern day attackers can exploit stolen credentials to get to the data just like how your application would access the data. In this attack vector both data-at-rest or data-in-transit encryption do not help.\nNowhere is this more relevant than in the world of enterprise search. Conducting search and analytics on vast quantities of data requires indexing and persisting of this data in clear text. Search and analytics solutions are often the targets for data hungry ransomware and extortion actors, who either look for misconfigured clusters or steal admin credentials. Once inside, they exfiltrate and use this data to extort their victims and their victims‚Äô customers and partners; sometimes leaking and selling the data to other cyber criminals on the dark web.\nThe Titaniam Arcus Plugin for OpenSearch enables sensitive data to be indexed and searched while always keeping the data in FIPS 140-2 certified encryption format. Here is how it works:\nAll sensitive data is preprocessed and encrypted prior to being indexed.\nQueries are intercepted and reformulated to execute in encrypted space without any data decryption whatsoever.\nTitaniam Arcus supports most types of queries - term, prefix, wildcard, match, match-phrase, match-phrase-prefix, range, term (CIDR) etc.\nQuery results are natively released in encrypted form. Here is an example of query results: Fig1. Titaniam Arcus for OpenSearch returning results in encrypted form Titaniam Arcus does all the above without significantly impacting ingest and search performance ‚Äì typically up to about 10% when ingesting data and 2-3% when searching. All this means that even if attackers find their way to your OpenSearch deployment, the data they exfiltrate would be encrypted and unusable to the attacker. So how does a legitimate user get clear text out of OpenSearch with Titaniam Arcus enabled? There are several controlled release processes including direct allowlisting and controlled release via pre-integrated proxy or translation service. All release configurations are defined at the granular field-level, and you can set up different fields to behave differently. Fig2. Titaniam Panther translating the results from Arcus Titaniam Arcus comes with a rich key management infrastructure including index specific keys, keystore integrations, key rotation, field-level key derivation and integrations to major key vaults. If you are a SaaS operator or Managed Service Provider, Titaniam‚Äôs index-specific keystore capability allows you to offer the Bring Your Own Key capability to your customer.\nFor more information, you can visit the OpenSearch Arcus page on Titaniam Labs.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/what-is-semver/",
    "title": "OpenSearch Versioning, or What is SemVer anyway?",
    "content": "I think people often use Semantic Versioning (a.k.a SemVer) intuitively even if they don‚Äôt know the term. So, let‚Äôs take a look at what SemVer is and what it means for software like OpenSearch.\nSemVer is a way of versioning software. Versioning software has been done in a variety of ways since the dawn of computing: date-based versioning (Windows 95), major/minor (MS DOS 6.22), and even some idiosyncratic versions (dBase III+), but as time went along the idea of a three component version became more predominant: e.g version 3.2.1. S emantic versioning is a formalized three component version: the first component is the ‚Äòmajor‚Äô, the second is a ‚Äòminor‚Äô, and the third is a ‚Äòpatch‚Äô.\nQuick overview\nEach one of these components has meaning and provides the user with an understanding of the version. Let‚Äôs start from the right with ‚Äòpatch‚Äô. A patch version fixes something, for example a security patch. It doesn‚Äôt add or remove any features and doesn‚Äôt change any data formats or APIs. It should be considered an extremely low risk upgrade (and maybe even a risk to not upgrade). You can usually skip a patch version and still upgrade (the exceptions being where a patch fixes an issue in the upgrading process itself).\nA minor version is one that provides something new, but it doesn‚Äôt change the data formats or APIs. A minor upgrade should be a low risk upgrade, but you shouldn‚Äôt feel compelled to upgrade since the data formats and APIs are the same. You just might miss out on a feature, a performance improvement, or a bug fix but otherwise it‚Äôs no damage. You don‚Äôt generally have to upgrade to each minor version, so you could skip from 1.1.0 to 1.3.0 safely.\nMajor versions are breaking changes. They provide new functionality and break the API or data format. Major version upgrades are higher risks in upgrades that might require you to switch tools or client libraries. You‚Äôll want to upgrade in sequence to avoid any potential breakage in the upgrade process (1.x.x ‚Üí 2.x.x ‚Üí 3.x.x, but usually not 1.x.x directly to 3.x.x)\nThese changes can be consolidated, so it‚Äôs valid for major releases to have new, unbreaking features and fixes, just not the other way around. For example, version 3.0.0 could include many fixes, features, and breaking changes, but 3.0.1 could not include any breaking changes or features when compared to 3.0.0.\nThe intuitive and the unintuitive\nSemVer aside, when software breaks on a minor or patch version people tend to get cross - it seems wrong that 1.1.0 to 1.2.0 would mean that you need to carefully test. Indeed, this logic is baked into tools you use. It‚Äôs not uncommon for package managers to allow you to auto-upgrade or install the latest minor or patch on every fresh install. And going from 1.0.0 to 2.0.0 is something that most people would understand to not automate - for good reason.\nUnintuitively, SemVer does not indicate the drastic-ness of the change. Let‚Äôs take a few scenarios:\nA patch version is released that fixes a major conceptual misunderstanding. This patch version could change thousands of lines of code, yet as long as it‚Äôs not a new feature and doesn‚Äôt change the API or data formats, it‚Äôs still a valid patch release.\nA minor version includes dozens of new features. As long as the existing APIs and data formats are not disturbed by the the new features, this is still a valid minor release.\nAn API endpoint is misspelled and is corrected. Under SemVer, this correction might only be a single byte change, but it would be considered a major release. Granted, if spelling issues in the API are being released, this is indicative of other quality issues that probably also need attention!\nThat‚Äôs the biggest mental hurdle - a major version does not indicate a major leap forward or lots of new features. It just means that it‚Äôs breaking compatibility.\nOne question that comes up occasionally is why minor and patch are isolated from one another - if a feature doesn‚Äôt break anything then why does it need a different type of release? One of the major reasons is that new additions to the code have a higher risk of causing problems. Consider that adding a new feature could take up significantly more resources. Adding new features over fixing existing features carries inherently more risk.\nA minor release also might not contain any new features under one specific circumstance. SemVer states that deprecations trigger a new minor release. So, the APIs do function between these versions, just stating that a feature or part of the API is deprecated would be enough to cut a new minor version.\nSemVer at play in OpenSearch\nBecause OpenSearch is still at 1.0.0, there is no history to draw upon as examples. However, looking at some planned changes in the OpenSearch roadmap there are a few examples of changes that will have to be in patch, minor, and major versions:\nIn security-dashboards-plugin#805, there is an issue where the tenant name displayed in the UI could reflect an out-of-date value. This is planned to be released in 1.0.1. This is a good example of something that could be contained in a patch release as it only fixes and existing feature and it doesn‚Äôt break the data format nor the API. OpenSearch#846 introduces a new tool for migrating Elasticsearch nodes to OpenSearch. The 1.1.0 release will carry this feature. Because it‚Äôs a new feature, it can‚Äôt be in a patch version but yet it doesn‚Äôt break the data format or API, so the upgrade risk is still low and it could go in a minor release.\nWhile OpenSearch 2.0.0 is still many months from release, OpenSearch#472 describes changing the API from ‚ÄòMaster‚Äô to a new, as yet undetermined, term. While this will probably not have many actual logic changes, this does have a change on the externally facing APIs, so it has to go into a major version, 2.0.0.\nThese are, of course, forward looking and could move around in actual version numbers but because of SemVer, the requirements stay the same. However, because of consolidation, all of these changes could end up in the next major version.\nOpenSearch &amp; SemVer\nFirst, the software OpenSearch was forked from does not follow semantic versioning, despite using a three component version. As a result, Open Distro did not follow semantic versioning. OpenSearch, from the outset, has used semantic versioning.\nThe OpenSearch plugins and OpenSearch Dashboards plugins do not quite follow semantic versioning though. These plugins use a four component version number (1.2.3.4), with the fourth component being build metadata. At the time of writing, this build metadata doesn‚Äôt follow the SemVer spec syntactically but it‚Äôs conceptually compatible. Additionally, each plugin is developed somewhat independently, so each one may have different build metadata. These plugins are released in coordination with the OpenSearch and OpenSearch Dashboards distribution that does follow semantic versioning, so that‚Äôs what you should be most concerned about.\nOne consequence of SemVer is that OpenSearch may end up with a much higher version number over time - again this doesn‚Äôt indicate re-writes or major changes, but it‚Äôs really a way for the developers of the project to communicate directly and concisely to the user why the version was released.\nAs a user, this means that OpenSearch will be released predictably and you can judge if a release is an upgrade path for your situation just by glancing at the version number compared to yours. For the developers of OpenSearch it gives clear instruction on what constitutes a major, minor, and patch version.\nNow that you‚Äôre armed with this knowledge, it‚Äôs a great time to take a look at the roadmap and see what is in store for the OpenSearch project!",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/a-query-or-there-and-back-again/",
    "title": "A query, or There and Back Again",
    "content": "OpenSearch is a distributed, open source search and analytics suite used for a broad set of use cases like real-time application monitoring, log analytics, and website search. And it‚Äôs easy to imagine why you might want to query information from OpenSearch in those use cases. But you might not know how those queries actually work. Well, that‚Äôs what we‚Äôre going to explore in this blog! In particular, we‚Äôre going to take a closer look at how a query works by following a query through OpenSearch.\nHigh-level concepts\nIn order to understand how a query works, you‚Äôll need a high-level understanding of OpenSearch itself. To start, OpenSearch is document oriented; meaning the most basic unit for OpenSearch is a document. Rather than information stored as rows and columns as you see in tabular data, OpenSearch stores data as JSON documents. By default, documents added to an OpenSearch store are indexed. Indices are logical partitions of documents and the largest unit of data in OpenSearch. To index these documents, OpenSearch leverages the Apache Lucene search library which uses an inverted index ( Wikipedia). An inverted index is a data structure that stores mappings of content to the location in a document (or set of documents). Rather than search each document and text directly, OpenSearch searches the index/indices allowing it to achieve it‚Äôs fast search responses.\nA node is an instance of OpenSearch and a cluster is a collection of one or more OpenSearch nodes with the same cluster name. There are several different types of OpenSearch nodes that we can discuss in future blog posts (stay tuned), but at a high-level OpenSearch functionality is built off the distribution of tasks and work among all the nodes in a cluster. OpenSearch allows for indices to be subdivided into multiple shards and each shard is part of the OpenSearch index. You can copy those index shards as replica shards (or just replicas) which serve as redundant copies of data, increasing both resiliency and capacity for read requests.\nA Query‚Äôs Journey\nNow that we have reviewed OpenSearch components, let‚Äôs follow a query through OpenSearch. At a very high-level, a OpenSearch query can be broken down into two major phases; the query phase and the fetch phase.\nQuery Phase\nIn this phase, the query provided to OpenSearch is broadcasted to a copy of every shard across the entire index. Once received, the query is executed locally. The result is a priority queue of matching, sorted documents for each shard. This priority queue is simply a sorted list of the top n matching documents with top being determined by relevance and n being determined by pagination parameters set by the user (or the default if not set by the user). Relevance in this case is a score of how well each document it matches the query. The individual shards are responsible for the actual matching process as well as the scoring. So for example, if you have a three node cluster and wanted to search for ‚ÄúHamster‚Äù, you could write a query like this; curl -X GET \"localhost:9200/_search?q=Hamster&amp;pretty\" Once OpenSearch has your query, OpenSearch takes the following steps;\nThe API or client sends your search query to Node 1.\nNode 1 sends the search request to a primary or replica shard for each shard in an index.\nEach shard executes the search locally and creates a locally sorted queue.\nEach shard returns doc ID and sort value of the all the documents in its local queue.\nNode 1 as the coordinator node, merges these values into a globally sorted list. Figure 1: Query Phase Note on coordinating nodes: Once OpenSearch learns about the query, the query can be sent to any available data node and that node can become the coordinating node for that query. These nodes delegate client requests to the shards on the data nodes, collects and aggregates the results into one final result, and sends this result back to the client. Often larger clusters will have dedicated coordinating nodes that manage search volume.\nFetch Phase\nNow that the query phase has identified the documents that satisfy the request, OpenSearch needs to actually retrieve the documents. For the fetch phase, the coordinating node used the globally sorted priority list generated in the query phase to build the GET requests needed for the query. Using the same three node cluster as the earlier example, OpenSearch needs to collect all the results for your query (i.e. ‚ÄúHamster‚Äù) and return them to you.\nThe coordinating node uses the global list to identify which documents are needed.\nThe coordinating node issues multiple GET requests to the relevant shards.\nEach individual shard loads the request document and returns them to the coordinating node.\nOnce all the documents are returned, the results are returned to the client. Figure 2: Fetch Phase With the results returned, OpenSearch has completed the query!\nConclusion\nAnd there you go, you now have a basic understanding of how an OpenSearch query works! What‚Äôs next? Well, we are planning two additional posts to bring your conceptional understanding to the next level;\nA Deeper Look at Queries: A look at how query scoring, routing, and balancing work.\nSo you want to write an OpenSearch query: An overview of query basics such as Boolean Operators, Ranges Queries, Fields, Fuzzy Queries, and Wildcards.\nIf you‚Äôre interested in contributing please reach out on GitHub issues or the community forum. The more formal contribution guidelines are documented in the contributing guide. Sources https://opensearch.org/docs/opensearch/index-data/\nhttps://opensearch.org/docs/opensearch/cluster/\nhttps://opensearch.org/docs/opensearch/ux/",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/data-prepper-roadmap/",
    "title": "Data Prepper roadmap is now publicly available on GitHub",
    "content": "This blog has been updated for technical accuracy on 17 Nov 2022. The roadmap for Data Prepper is publicly available. Data Prepper is a component of the OpenSearch Project that accepts, filters, transforms, enriches, and routes data at scale. It is a key component of the trace analytics feature, which helps developers use distributed trace data to find and fix performance problems in distributed applications. The roadmap outlines a plan to enable Data Prepper to ingest logs and metrics from telemetry data collection agents, such as Fluent Bit and the Open Telemetry Collector. Data Prepper is the single data ingestion component for log and trace data pipelines that can scale to handle stateful processing of complex events such as trace data, aggregation transforms, and log-to-metric calculations.\nData Prepper Overview\nData Prepper is a data ingestion component of the OpenSearch Project that pre-processes documents before storing and indexing in OpenSearch. To pre-process documents, Data Prepper allows you to configure a pipeline that specifies a source, buffers, a series of processors, and sinks (Figure 1). Once you have configured a data pipeline, Data Prepper takes care of managing source, sink, and buffer properties and maintaining state across all instances of Data Prepper on which the pipelines are configured. A single instance of Data Prepper can have one or more configured pipelines. A pipeline definition requires at least a source and sink attribute to be configured and uses the default buffer and no processor if they are not configured. Figure 1: Data Prepper pipelines and attributes Source is the pipeline‚Äôs input component. It defines the mechanism through which a pipeline will consume records. The source component could consume records either by receiving over HTTP/HTTPS or reading from external endpoints like Fluent Bit, Beats, OpenTelemetry collectors, Kafka, Amazon SQS, and Amazon CloudWatch. Buffer is a temporary store of data and can be either in-memory or disk based. The default buffer will be an in-memory queue and is the only option available in the initial release. Sink is the pipeline‚Äôs output component. It defines one or more destinations to which a pipeline will publish the records. A sink destination can be services like OpenSearch, Amazon S3, or another pipeline. By using another pipeline as the sink, customers can chain multiple Data Prepper pipelines. Processors are intermediary processing units that filter, transform, and enrich the records into the desired format before publishing to the sink. The processor is an optional component of the pipeline, and if not defined, the records will be published in the format as defined in the source. You can have more than one processor, and they are executed in the order they are defined in the pipeline specification. For example, a pipeline might have one processor that removes a field from the document, followed by another processor that renames a field.\nData Prepper supports trace analytics data processing, as shown in Figure 2, and includes support for OpenTelemetry collector as a source, a raw-trace processor for trace data processing, and a service-map processor for service map creation. Figure 2: Trace analytics data processing pipeline Roadmap\nThe roadmap for Data Prepper is published in the Data Prepper OpenSearch Project repository on GitHub. It gives you visibility on plans for new features, enhancements, and bug fixes, and we look forward to your feedback on it. In late 2021, the focus is on log collection from multiple sources through Data Prepper where they can be ingested, viewed, and analyzed in OpenSearch Dashboards. The Data Prepper 1.2 release provides users the ability to send logs from Fluent Bit to OpenSearch or Amazon OpenSearch Service and use Grok to enhance the logs. These logs can then be correlated to traces coming from the OTEL Collectors to further enhance deep diving into your service problems using OpenSearch Dashboards. Below are the key features that we are targeting for the Data Prepper 1.2 release:\nHTTP Source Plugin (support for Fluent Bit)\nGrok processor\nLogstash template support for Grok\nSample code and tutorials on creating processors\nIf you want to add something that is not in the public roadmap for OpenSearch Data Prepper, that‚Äôs a perfect opportunity to contribute! We are looking for contributors to help develop new processors and new source plugins and accelerate development of processors and source plugins on the roadmap. This public roadmap follows the same principles in the blog post outlining the OpenSearch public roadmap. It is worth emphasizing a few points:\nDate and milestones reflect intentions rather than firm commitments. Dates may change as anyone learns more or encounters unexpected issues. The roadmap will help make changes more transparent so that anyone can plan around them accordingly.\nYou can create a feature request in the relevant GitHub repo for the feature and socialize the request. A maintainer or someone else in the community may pick up this feature and work on it. As progress is made, the maintainers will help get the feature onto the roadmap.\nYou can build a feature yourself. To do this create a proposal as a GitHub issue in the relevant repo and use the feature request. Offer your commitment to build it. The maintainers will work with you to figure out how best to proceed. See OpenSearch FAQ 1.19 for more details.\nWould you like to contribute? Maybe you don‚Äôt know where to start. The Data Prepper maintainers curate good first issues. These are issues that we believe new contributors could take on. Please comment on the issue if you would like to work on it.\nLearn more about Data Prepper by accessing the code, examples, and documentation in the Data Prepper GitHub repository. You also can connect with us and provide feedback in our forum category.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-py-js-go/",
    "title": "OpenSearch clients in Python, Go, and Node.js",
    "content": "Last month, the project announced the intention to release OpenSearch specific clients and today the first batch are ready for production use. You can get opensearch-py from PyPI, install opensearch-js from npm and start using opensearch-go.\nLet‚Äôs take a brief look at how to get going on these client libraries. You can find more info on the README file for each repo.\nPython\nTo install opensearch-py, you can use pip: $ pip install opensearch-py From there you can import it into your code: from opensearchpy import OpenSearch Node.js\nInstalling the JavaScript client in Node.js is through npm: $ npm i @opensearch-project/opensearch You can use it in your script by requiring the module: const { Client } = require('@opensearch-project/opensearch') Golang\nYou can install opensearch-go as a dependency with go get: $ go get github.com/opensearch-project/opensearch-go@1.0 The golang implementation is separated into distinct packages ( opensearchapi for the OpenSearch API, opensearchtransport for transport and connection, and opensearchutil for helper functions): import (...\nopensearch \"github.com/opensearch-project/opensearch-go\"\nopensearchapi \"github.com/opensearch-project/opensearch-go/opensearchapi\"\nopensearchtransport \"github.com/opensearch-project/opensearch-go/opensearchtransport\"\nopensearchutil \"github.com/opensearch-project/opensearch-go/opensearchutil\"...) How do these clients fit in? opensearch-py, opensearch-js, and opensearch-go are derived from elasticsearch-py, elasticsearch-js, and go-elasticsearch respectively and will work with OpenSearch and open source Elasticsearch. Each of these clients follows syntax of the previous projects closely and moving your custom code over should be a matter of changing over the naming.\nWhat‚Äôs next?\nThe project team is moving forward in the public for the Java client and will soon make public repos available for.NET, Ruby, Rust, Perl, and PHP clients as well as the Hadoop and HDFS connectors. Additionally, the Python DSL query builder library has been identified as a target for OpenSearch.\nHow can you help?\nOpenSearch is a community-driven project and your input is always valued and appreciated - feel free to add an issue or pull request on GitHub or post in the forum category on clients. If these clients are important to you or your work, consider taking on a maintainer role where you can have a real impact on the ongoing direction and implementations. Together, let‚Äôs build a great set of clients!\nAcknowledgements\nThe following people contributed to opensearch-py, opensearch-js, or opensearch-go: rushiagr, axeoman, vchrombie, deztructor, VijayanB, peternied, peterzhuamazon, ananzh, boktorbb-amzn, hansott, and henvic.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/building-opensearch-1-1-distributions/",
    "title": "Building OpenSearch 1.1 Distributions using Automation",
    "content": "The Open Distro for Elasticsearch was a package containing open source Elasticsearch and a dozen open source plugins. Open Distro‚Äôs release process began by picking up a stable version of Elasticsearch OSS, incrementing all the plugin version numbers, and going through a development cycle. Plugins went from -SNAPSHOT to -beta, then -rc, then finally to a release.\nThis worked well until the OpenSearch fork. It was decided that OpenSearch would be a product with two distributions at launch: a full distribution that included a dozen plugins, and a minimal distribution without any plugins. This made sense because most users either wanted a well-tested distribution with everything, or preferred to assemble their favorite flavor of all the parts.\nThe full distribution of OpenSearch 1.0 was similar to Open Distro, but with one major difference - the OpenSearch fork of Elasticsearch OSS was developed in parallel with its plugins. The complete distribution of OpenSearch and OpenSearch Dashboards effectively became two monoliths that spanned multiple GitHub repositories. Using OpenSearch as an example, one could not develop all the moving parts simultaneously, while having stable dependencies of OpenSearch, common-utils, or job-scheduler, to name a few.\nStarting with OpenSearch 1.1, the entire distribution is built from source, end-to-end, orchestrated in this Jenkinsfile by invoking a manifest-based workflow. This includes a release and a snapshot build for x64 and arm64 CPU architectures, executed as follows.\nGiven an input manifest committed to GitHub, build all components from source, and generate a build manifest. bundle-workflow/build.sh manifests/1.1.0/opensearch-1.1.0.yml Use the build manifest produced above to assemble a distribution. bundle-workflow/assemble.sh artifacts/manifest.yml Artifacts are published to ci.opensearch.org. For example, here‚Äôs the x64 build manifest and a distribution manifest from build #405, which is OpenSearch 1.1. If you examine the latter, you‚Äôll find a link to opensearch-1.1.0-linux-x64.tar.gz and opensearch-min-1.1.0-linux-x64.tar.gz. These are unsigned and untested outputs from daily CI, and therefore should not be used in production.\nIn parallel, a snapshot workflow builds and publishes -SNAPSHOT maven artifacts to aws.oss.sonatype.org.\nA test workflow is executed for each build, and consists of integration, backwards compatibility and performance tests. The test orchestrator pipeline will be a topic for a future blog post.\nTested artifacts are signed and promoted to a release. The signing step takes the manifest file created from the build step, and signs all its component artifacts. Packages are manually uploaded to opensearch.org, while maven artifacts are staged and then promoted to Maven Central.\nHere‚Äôs a quick cheat-sheet for reproducing the build process for a complete distribution of OpenSearch 1.1. You will need additional tools, including maven and cmake, to compile some of the components. git clone https://github.com/opensearch-project/opensearch-build cd opensearch-build\ngit checkout 1.1.0./bundle-workflow/build.sh manifests/opensearch-1.1.0.yml./bundle-workflow/assemble.sh manifests/opensearch-1.1.0.yml This system will be reused for OpenSearch Dashboards in 1.2, see opensearch-build#158.\nThe signing process continues, for now, to rely on an existing signing system and all our jobs run from a private Jenkins setup. This is entirely for historical reasons. The OpenSearch team at AWS relies on a significant amount of costly hardware procured and managed by AWS (e.g. I use a c5.18xlarge instance for development). We are in the process of making the Jenkins instance public, open-sourcing its CDK setup, and open-sourcing all remaining tools.\nThe build process described above runs parallel to development of individual components. It‚Äôs always challenging to code against moving dependencies, and we are starting to think about the long term future of these systems. We‚Äôd like to untangle the monolith, enable plugins to declare a compatibility matrix, and much more. In the meantime, we welcome anyone who would like to brush up their Python skills by contributing to the opensearch-build scripts, and could use some help bringing OpenSearch and OpenSearch Dashboards to various OS-specific distribution mechanisms, such as Windows, similar to this community port to FreeBSD.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Launch-Announcement-1-1/",
    "title": "OpenSearch 1.1.0 is here!",
    "content": "We are excited to announce the 1.1.0 release of OpenSearch, OpenSearch Dashboards, and the OpenSearch Project plugins (available to download today). What‚Äôs Included in OpenSearch 1.1.0? This release includes dozens of new features and hundreds of improvements ( release notes), and we wanted to highlight some of the most exciting new features in this release:\nCross-Cluster Replication (CCR): With this feature, you will be able to deploy OpenSearch clusters across different servers, data centers, or even regions and setup a more fault-tolerant configuration. CCR provides low latency replication for indices with sequential consistency.\nLucene 8.9 Upgrade: Lucene 8.9 includes sorting improvements when building BKD indexes that can boost index building by 35x and speed up merging sorted fields and term vectors for smaller segments. It also includes a fix for a performance regression in boolean queries that was introduced in 8.8. Full details are available in the Lucene 8.9 change log.\nCLI OpenSearch Upgrade Tool: While upgrading to OpenSearch from a compatible version of Elasticsearch, there are some manual tasks that need to be performed. The opensearch-upgrade tool included in the distribution automates those tasks by importing the existing configurations and applying them to the new installation of OpenSearch. For more information see the readme.\nAnomaly Detection Upgrades: OpenSearch now has a unified workflow for realtime and historical anomaly detection within the same detector. You can now click through a single workflow to perform all of the necessary steps to create and run anomaly detection jobs. Anomaly Detection has also been upgraded to use an updated Random Cut Forest (RCF) algorithm and saw improvements to high cardinality anomaly detection.\nBucket Level Alerting: With Bucket Level Alerting, you can configure alerting policies that evaluate against aggregations grouped by a unique field value. For example, if you have an index that is ingesting health logs a number of different hosts, with Bucket Level Alerting, you could configure a monitor to alert when any host has a metric, like CPU or memory, that exceeds a defined threshold. What‚Äôs next? With the launch of 1.1.0, OpenSearch is already racing forward. There are a number of upcoming feature and enhancements including:\nA shard level back pressure framework is being added to improve OpenSearch indexing reliability.\nA number of new observability features are being added to help you analyze trace and log data.\nOpenSearch‚Äôs k-NN plugin will add support for the updated FAISS algorithm that improves performance.\nAnomaly detection will add visibility to which signals contributed to specific anomalies.\nSee additional active features on the project roadmap. How can you contribute? The OpenSearch community continues to grow and we invite new users and members to contribute! For almost any type of contribution, the first step is opening an issue ( OpenSearch, OpenSearch Dashboards). Even if you think you already know what the solution is, writing down a description of the problem you‚Äôre trying to solve will help everyone get context when they review your pull request. If it‚Äôs truly a trivial change (e.g. spelling error), you can skip this step ‚Äì but when in doubt, open an issue. If you‚Äôre excited to jump in, check out the ‚Äúhelp wanted‚Äù tag in issues. Do you have questions or feedback? If you‚Äôre interested in learning more, have a specific question, or just want to provide feedback and thoughts, please visit OpenSearch.org (https://opensearch.org/), open an issue on GitHub, or post in the forums. There are also regular Community Meetings that include progress updates at every session and include time for Q&amp;A. Thank you! We knew OpenSearch would need to build a great open source community to succeed and we‚Äôre so excited about the progress! Not only is OpenSearch seeing some awesome contributions across the project but the community partners continue to grow (5 more partners have joined since 1.0.0). As always, everyone should be incredibly proud of the accomplishment of reaching 1.1.0 together.\nThank you for your continued support.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/cross-cluster-replication-intro/",
    "title": "Introduction to Cross-Cluster Replication",
    "content": "Earlier this year while working on Open Distro the team announced the experimental release of cross-cluster replication. I am happy to announce the general availability of cross-cluster replication for OpenSearch. This post provides a brief overview of the feature and the thought process behind the design and implementation.\nOverview\nToday, OpenSearch is often used in mission-critical applications which require replication across clusters. Scenarios where you might need cross-cluster replication: Disaster Recovery (DR) / High Availability (HA) - To build tolerance for outages or complete failure of OpenSearch clusters, cross-cluster replication is an important building block. With cross-cluster replication, you can create an alternate clusters that continuously replicates indices from the primary cluster without the need of any third-party technologies. Data Proximity - OpenSearch is also widely used as a search engine for product/service search by organizations with global end user base. Having a centralized cluster can add to the query latency for end users far from the cluster and thus impact end user experience. With cross-cluster replication you can now replicate indices to another cluster in a location closer to the end user for data proximity. These secondary clusters can reduce latency for search requests enhancing customer experience.\nHistorically, you have been solving for these use cases by creating a second cluster, fork their input data streams to the two clusters, and place a load balancer in front of the two domains to balance incoming search requests.. However, this adds complexity and costs as it requires multiple third-party technologies to monitor and make corrections for data discrepancy between the two clusters. Often requiring manual intervention that adds to the operational burden. Native support for cross-cluster replication reduces cost and also removes complexities and operational overhead.\nSome important terminologies before diving deeper. Leader Index - Index which is being replicated is called a leader index. Leader Cluster - Cluster where the leader index resides. Follower Index - Index which replicates leader index Follower Cluster - Cluster where the follower index resides. Sequential consistency - Query performed on the leader and the follower index after the same number of operations applied in the same order will yield the same result. Global Checkpoint - is a sequence number for which all active shards histories are aligned till this point.\nThe cross-cluster replication team was guided by the following principles while designing and implementing the feaure. Secure: Cross-cluster replication should offer strong security controls for all flows and APIs. Accuracy: There must be no difference between the intended contents of the follower index and the leader index. Performance: Replication should not impact indexing rate of the leader cluster. Eventual Consistency: The replication lag between the leader and the follower cluster should be under a few seconds. Resource usage: Replication should use minimal resources.\nCross-cluster replication on OpenSearch supports an active-passive model for replication. The leader index is active as it can receive both writes and read requests. The follower index is passive as it can only receive read or search requests. Cross-cluster replication provides granular control, allowing you to choose the index you want to replicate. The granular control also makes it possible to replicate selected indices from one cluster to another and some indices in the opposite direction.\nThe cross-cluster replication feature is implemented as an OpenSearch plugin that exposes APIs to control replication, spawns background persistent tasks to asynchronously replicate indices, and utilizes snapshot repository abstraction to facilitate bootstrap. Replication relies on cross-cluster connection setup from the follower cluster to the leader cluster for connectivity. The cross-cluster replication plugin also optionally offers seamless integration with the OpenSearch Security plugin. You can encrypt cross-cluster traffic via the node-to-node encryption feature and control access for replication activities via the security plugin.\nCross-cluster replication provides granular control so that you can choose which index you want to replicate. While starting replication, apart from the index details, you can also specify specific settings for the follower index that you want to override from the leader index, for example the number of replicas. Once initiated documents, settings and mappings from the leader index are replicated to the follower index. At the start of replication, the follower index goes through a bootstrapping phase where the data from the leader index is copied over (akin to how replicas are created). After the bootstrapping phase, replication enters the syncing phase where all changes to the leader index are replicated on the follower. The plugin also provides auto-follow, which lets you automatically replicate indices from the leader to the follower cluster if they match a specific pattern. While replication is ongoing, there could be scenarios where it encounters intermittent issues. Cross-cluster replication has inbuilt retry mechanism in case of errors. However, there could be scenarios when it is unable to recover and throws exception.\nDuring the implementation the team consciously tried to minimize the impact of replication activities on the leader cluster while also trying to minimize the lag between the two indices. The lag can be monitored with the help of the status API that gives both a high level view (index level) and a detailed view (shard level) of the ongoing replication. Cross-cluster replication is designed to support sequential consistency. This in a way guarantees the consistency between the leader index and the follower index. In plain speak, if you are to perform 1,000 operations on the leader index in a particular oder and you perform the same set of operations on the follower index in the same order, any search request executed on both the indices will provide the same result. While there is no easy way to monitor the operations and its sequences for an index in OpenSearch, the closest you have is the Global Checkpoint which provides a sort of marker that guarantees that all operations up to the checkpoint has been processed by all active shards (both primary and replicas). The status API also provides metrics aggregated at the index level to give a sense of the replication lag.\nSummary\nWith the release of cross-cluster replication as part of OpenSearch 1.1.0, the team hopes to establish the foundation for the use cases mentioned in the beginning of the post. As you can imagine, cross-cluster replication is both critical and complex and this is just the beginning of the cross-cluster replication journey. The intention is to create the springboard for future enhancements like automatic failover, full-cluster replication, active-active replication, and many more. You are invited to evaluate, provide feedback and also collaborate to make cross-cluster replication better.",
    "keywords": [
      "cross-cluster"
    ],
    "type": "News"
  },
  {
    "url": "/blog/partner-highlight-aiven/",
    "title": "Partner Highlight: Aiven offers OpenSearch as a service",
    "content": "Aiven has recently announced Aiven for OpenSearch as the newest addition to their managed database services platform. As an existing provider of open source Elasticsearch as a service, the move to OpenSearch was a logical next step. Customers can stay with Aiven and have an open source upgrade path ahead of them with easy migration of existing Elasticsearch services to Aiven for OpenSearch.\nAiven has been a vocal supporter of OpenSearch since before the project had a name. We knew we wanted to be hands-on in our support, both in the community and in the code itself, and to offer OpenSearch on our platform since many of our customers are as enthusiastic about open source as we are!\nYou can find different members of the Aiven team in different places around the OpenSearch project. Over in our OSPO (Open Source Program Office) we have Andriy working on OpenSearch itself as a Software Engineer. Some of our Python experts ( Denis and Aleksei) are maintainers on the client library opensearch-py.\nWe are also excited to contribute to the ecosystem as a whole, and I personally have enjoyed making small contributions to project repositories and participating on the forums. The Developer Relations team also has responsibility for the Aiven for OpenSearch docs, and most of the information and examples there are just as applicable to OpenSearch deployments on other platforms. Olena is giving talks about OpenSearch and always adding more content.\nWe really appreciate the hard work required to make OpenSearch a reality. It was important to us at Aiven to be able to support open source and provide an upgrade path for everyone who wanted it. The journey is far from over; in fact it has just begun. We look forward to helping to build a thriving open source project with a governance model that enables collaboration and ensures a long and healthy future.\nIf you‚Äôd like to try OpenSearch on Aiven, then sign up for our free trial and let us know what you think? We are proud to be part of the OpenSearch landscape, both as providers of OpenSearch, and as contributors to the project itself.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/alerting-intro/",
    "title": "Introduction to OpenSearch Alerting",
    "content": "Log analytics has grown to be one of OpenSearch‚Äôs popular use cases as it is able to easily ingest, secure, search, visualize, and analyze log data. The automated alerting feature can further help you by automatically detecting problems from your ingested data. When a problem is detected, you can send an alert to external systems like Slack, email, and more. For example, you might want to create an alert and notify a Slack channel if your application logs see more than five HTTP 503 errors within an hour. In this blog we walk through setting up an alerting policy and discuss how the alerting plugin works.\nLet‚Äôs setup an alerting monitor\nBefore beginning, if you don‚Äôt have an OpenSearch cluster setup, we recommend setting up a cluster using OpenSearch with Docker Compose.\nNow that you have a running OpenSearch cluster, let‚Äôs set up a monitor. The first step is to choose an index to be monitored, either use an existing index or create a new one. To create a sample index, run the below curl from the terminal. curl -X PUT -u admin:admin --insecure 'https://localhost:9200/sample-logs' -H 'Content-Type:application/json' -d '{\n\"mappings\": {\n\"properties\": {\n\"@timestamp\": {\n\"type\": \"date\"\n},\n\"host_address\": {\n\"type\": \"text\",\n\"fields\": {\n\"keyword\": {\n\"type\": \"keyword\",\n\"ignore_above\": 256\n}\n}\n},\n\"http_code\": {\n\"type\": \"integer\"\n},\n\"message\": {\n\"type\": \"text\"\n}\n}\n}\n}' Next, you need to create monitors to receive automated alerts on data ingested in this index. You can create a monitor using the Alerting UI in OpenSearch Dashboards or the REST API. The UI provides an interactive way to create a monitor and provides a validation feature that checks for the monitor‚Äôs query correctness and performance. On the other hand, the API is for programatic configuration. For example, you might use the API to build automation scripts for sharing configurations across multiple clusters. To create the monitor for the ‚Äòsample-logs‚Äô index, run the curl command below. curl -XPOST -u admin:admin --insecure 'https://localhost:9200/_plugins/_alerting/monitors?pretty' -H 'Content-Type:application/json' -d'{\n\"name\": \"sample-monitor\",\n\"enabled\": true,\n\"inputs\": [\n{\n\"search\": {\n\"indices\": [\n\"sample-logs\"],\n\"query\": {\n\"size\": 0,\n\"aggregations\": {},\n\"query\": {\n\"bool\": {\n\"filter\": {\n\"range\": {\n\"@timestamp\": {\n\"from\": \"||-1h\",\n\"to\": \"\"\n}\n}\n},\n\"must\": {\n\"term\": {\n\"http_code\": 403\n}\n}\n}\n}\n}\n}\n}],\n\"schedule\": {\n\"period\": {\n\"interval\": 10,\n\"unit\": \"MINUTES\"\n}\n},\n\"triggers\": [\n{\n\"name\": \"count-trigger\",\n\"severity\": \"1\",\n\"condition\": {\n\"script\": {\n\"source\": \"ctx.results[0].hits.total.value &gt; 5\",\n\"lang\": \"painless\"\n}\n},\n\"actions\": []\n}]\n}' A few important details for the monitor definition: Schedule: The schedule defines how often the monitor runs. The value you choose should be based on how often you want to alert on the indexed data, the indexing rate, and the maximum delay you expect between generating a documents and it reaching OpenSearch. Indices: You must provide one or more OpenSearch indices to run the monitor query on. You can also use a wildcard (*) in index name if you want to monitor data from multiple indexes. These monitored indices must contain time-series data with at least one date field. Extraction query: The extraction query is an OpenSearch DSL query that the monitor runs on the defined schedule. There are many options for writing the DSL query. Our sample uses a bool query that combines a range filter clause on the time field and a must clause on http_code. Trigger: A trigger is a condition that, if met, generates an alert. At least one trigger needs to be defined per monitor to generate an alert. To create a trigger you need to define a trigger condition and an alert severity. The alert severity allows you to highlight the significance of the alert. The trigger condition uses an inline plainless script that returns true or false. When it returns true, the condition has been met. It is worth noting that the results from the extraction query can be used in triggers via the ctx.results[0] variable. In our example, the trigger returns true when the extraction query ‚Äúhits‚Äù count is greater than 5. Actions: When a trigger condition is met it runs a set of actions (e.g. sending a notification to a Slack channel). Actions are optional.\nNow that you ran the command to create a monitor, you can check that it was created by running the following command: curl -X GET -u admin:admin --insecure 'https://localhost:9200/_plugins/_alerting/monitors/_search?pretty' -H 'Content-Type:application/json' -d'{\n\"query\": {\n\"match\": {\n\"monitor.name\": \"sample-monitor\"\n}\n}\n}' How and When does this monitor run?\nNow that you have setup the sample monitor, let‚Äôs discuss how it works. Monitors are run as periodic jobs on OpenSearch data nodes. The below picture illustrates four runs of our example monitor. Assuming the monitor was created at 8:00am, the first run ( run-1) starts at 8:00am. It returns search hits for the documents whose timestamps are between 7:00am and 8:00am. Similarly, run-2 runs at 8:10am and returns search hits between 7:10am and 8:10am. For run-1 the trigger condition is not met as there are only 5 documents that match the criteria. However, run-2 at 8:10am creates an alert because the trigger condition is met with 6 documents returning from the query. During run-3 the trigger condition is still met and alert remains active. Finally, during run-4 the trigger condition is no longer met, and so the alert state changes from ‚ÄúActive‚Äù to ‚ÄúCompleted‚Äù. Note that if there is significant delay in indexing documents, you might miss alerts. For example in the figure below, if new documents with timestamps between 7:00am and 7:10am arrive after 8:00am those wont get counted in the extraction query results of run-2, run-3, run-4 Now that you have a working monitor, let‚Äôs trigger an alert. The below command ingests data into the sample-logs index. Make sure to change the timestamp to be in the last one hour to generate alerts. curl -X POST -u admin:admin --insecure 'https://localhost:9200/sample-logs/_doc?pretty' -H 'Content-Type:application/json' -d '{\n\"@timestamp\": \"2021-10-11T10:30:12\",\n\"host_address\": \"10.12.120.228\",\n\"http_code\": 403,\n\"message\": \"Hello! This is a sample response.\"\n}‚Äò Next you can check for alerts from either the alerting UI, or you can use the API commands below: curl -X GET -u admin:admin --insecure 'https://localhost:9200/_plugins/_alerting/monitors/alerts`?pretty`' If you have an alert, you can acknowledge it so that it does not continue to send notifications using the alerting UI or the below command: curl -X POST -u admin:admin --insecure 'https://localhost:9200/_plugins/_alerting/monitors/&lt;monitor_id&gt;/_acknowledge/alerts?pretty' -H 'Content-Type:application/json' -d'{\n\"alerts\": [\"`&lt;alert_id&gt;`\"]\n}' The lifecycle of an alert\nNow that you have triggering an alert, let‚Äôs dive a bit deeper into the lifecycle of an alert. The below picture shows the alert states as vertices and state changes as edges. The alert state is evaluated every time its monitor runs. First, if the configured actions for that trigger are successfully executed, the alert is created in the ACTIVE state and stored in the.opendistro-alertings-alerts index.\nIf the alert fails to execute an action it will be created in ERROR state.\nOnce ACTIVE, the alert state can be changed to the ACKNOWLEDGE state by a user taking action to acknowledge it. This is intended to be used to stop notifications while remediating the alert cause or fixing an alert in the ERROR state.\nIf the corresponding monitor or trigger is deleted, the alert enters into DELETE state and is moved to.opendistro-alertings-alert-history-* indices.\nIf the corresponding trigger conditional evaluates to false in the next monitor execution, then the alert state is moved to COMPLETED. Once completed, the alert is stored in the.opendistro-alertings-alert-history-* indices.\nBy default the alert is stored in the cluster for 60 days, you can change this by using plugins.alerting.alert_history_retention_period setting. The complete list of settings can be found in the alert settings documentation.\nConclusion\nOpenSearch alerting provides a powerful mechanism to continuously monitor large volumes of data, and now you know how to use the feature. Give it a try with your own use cases and don‚Äôt hesitate to provide feedback. Stay tuned for the next post where we will discuss security with alerting, and dive into how to alert on OpenSearch audit logs.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/automated-testing-for-opensearch-releases/",
    "title": "Testing Automation for OpenSearch Releases",
    "content": "OpenSearch releases many distributions across multiple platforms as part of a new version. These distributions are of two types - the default distribution that includes all plugins, and the min distribution without any plugins. They go through a rigorous testing process across multiple teams, before they are signed off as ‚Äúrelease ready‚Äù. It includes unit testing, integration testing to validate the behavioral integrity, backward compatibility testing to ensure upgrade compatibility with previous versions, and stress testing to validate the performance characteristics. Once successfully tested, they are marked ready for the release.\nThe rigorous testing process provides good confidence in the quality of the release. However, so far it has been manual and non-standardized across plugins. Each plugin team validated their component by running tests on the distribution and provided their sign-off manually. With dozens of OpenSearch plugins released as part of default distribution, the turn around time for testing was high. Also, lack of a continuous integration and testing process lead to late bug discoveries which further added to release times. The Automated testing framework solves these problems by simplifying and standardizing the testing process across all components of a release distribution.\nThe way it works is once a new bundle is ready, the build-workflow (explained in this blog), kicks off the test-orchestrator-pipeline with input parameters that uniquely identify the bundle in S3. The test-orchestrator-pipeline is a Jenkins pipeline to orchestrate the test workflow, consisting of three test suites - integ-test (integration testing), bwc-test (backward compatibility testing), perf-test (performance testing), to run in parallel. Each of these test suites is a Jenkins pipeline that executes the respective test type.\nLike build-workflow, these test workflows are manifest-based workflows. integ-test suite reads bundle manifest file to identify the type and components of a bundle under test. It pulls all maven and build dependencies for running integration tests on the bundle from s3 (these dependencies are built as part of build-workflow and re-used for testing). After pulling the dependencies, it runs integration tests for each component in the distribution, based on the component test config defined in the test-manifest file. It spins a new dedicated local cluster to test each test config and tears down the cluster after the test run completes. The test and the cluster logs are published to S3 after the test workflow completes. bwc-test suite runs similar to integ-test suite, for backward compatibility tests. Currently, it only supports backward compatibility tests for OpenSearch and anomaly-detection plugin, but there‚Äôs ongoing effort to add more plugins. perf-test suite runs performance testing with rally tracks on a dedicated external cluster. This piece is currently in development. Once all test suites complete, the notifications job sends out notifications to all subscribed channels. Figure 1 illustrates how different components of the test workflow interact with each other. Figure 2 shows a sample test report generated by the integration testing workflow on a release candidate build. Figure 1: Automated test workflow explained Figure 2: A sample test reported generated by integ-test workflow. with-security config denotes the test run with security plugin enabled, without-security config denotes the test run without security plugin enabled. This testing automation helps make the release process faster by providing a quick feedback loop to surface issues sooner. It also standardizes the testing process and enforces strict quality controls across all components. The code is entirely open source and development work is being tracked on this project board. We welcome your comments and contributions to make this system better and more useful for everyone.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-security-concepts/",
    "title": "Partner Highlight: Eliatra presents OpenSearch Security Concepts",
    "content": "Eliatra provides OpenSearch Support, Professional Services and Custom Feature Development. Our team has been working with open source search engine technology like Lucene and Elastic products since the very beginning. We have extensive ecosystem knowledge. This makes us the perfect partner for AWS and OpenSearch, not only as a contributor but also to offer full support and professional services.\nFor many years Elasticsearch had no built-in security. This led to numerous security breaches with millions of sensitive data leaked. Luckily, OpenSearch provides a strong and reliable security model out-of-the-box.\nIn this article, we will first explain some of the core concepts of OpenSearch security. Additional parts of this article can be found on the Eliatra blog, we walk you through a basic OpenSearch security setup using the demo configuration and the internal user database.\nOpenSearch Security Basic Concepts\nThe security model of OpenSearch is based on the following core concepts: TLS encryption makes sure that no one can sniff or modify any data in motion. Users define who has access to an OpenSearch cluster. Before interacting with the cluster, any user has to authenticate first. Roles are used to implement authorisation. Any authenticated user can have one or more roles. Roles define what permissions a user has for particular indices. Permissions define what a user is allowed to do. For example, the permissions of a user may grant READ access to data, but do not allow WRITE or DELETE operations.\nApart from these basic security controls, OpenSearch also supports advanced features like LDAP authentication. You can find articles about it on the Eliatra blog too.\nTLS Encryption - REST Layer\nOpenSearch uses TLS to secure and encrypt traffic between users and OpenSearch on the REST layer. This is much like accessing your online bank account with a browser using HTTPS: All traffic between your browser and the online banking server is encrypted. In addition, the browser checks and validates the server‚Äôs TLS certificate. By validating the TLS certificate you can be sure you are sending the data to the right entity, and not a malicious website.\nThe same happens when you send a request to OpenSearch, for example by using a browser or tools like curl. The HTTP traffic is encrypted, so no one can sniff or modify your data while it is being transmitted.\nTLS Encryption - Transport Layer\nBesides the REST layer which uses the HTTP protocol for communication, there is another internal layer where data is exchanged. This is the so-called transport layer. The transport layer is responsible for any traffic that is exchanged between the nodes in the cluster ( ‚Äúinter-node traffic‚Äù).\nOpenSearch is a distributed system, so most operations like indexing data or querying for data affect multiple nodes in the cluster. As with the REST layer, OpenSearch uses TLS on the transport layer to encrypt and secure traffic, and to make sure only that trusted nodes are allowed to join your cluster.\nBy combining TLS encryption on both layers, all data in motion is end-to-end encrypted. Users, Roles, and Permissions\nRequest Flow\nOpenSearch implements a role-based security model and supports multiple types of authentication and authorisation backends. The specific details on how a request is handled may vary depending on what technology you use, e.g. Active Directory, LDAP, OIDC, SAML, etc. However, the general request flow always follows the same pattern:\nAny request that hits OpenSearch has to carry some sort of user credentials. For example, username and password as HTTP Basic Authentication header, a JSON web token or a Kerberos ticket.\nOpenSearch extracts this information from the request and validates it using an authentication domain. For example, the internal user database, an LDAP server, or Active Directory.\nIf the credentials are valid, OpenSearch fetches the so-called backend roles of the user from an authorisation domain. Again, this could be LDAP roles, JWT claims, SAML assertions, or roles from the internal user database. This step is optional.\nBased on the user name and/or the backend roles, the user is assigned one or more security roles\nThe security roles finally define what the user is allowed to do. You can set up access permissions for each index separately, or use wildcards and regular expressions to apply permissions to multiple indices at once. User Management\nOpenSearch supports multiple ways of managing users. Probably the easiest way is to use the built-in internal user database. The internal user database stores users and hashed passwords directly in a protected OpenSearch index. You can use the OpenSearch securityadmin CLI or the REST API to directly create, modify and delete users.\nOpenSearch also supports a wide variety of other authentication domains, like:\nLDAP / Active Directory\nKerberos\nJSON web tokens\nOIDC / SAML\nProxy authentication\nTLS client certificates\nOpenSearch also provides the capability of chaining or combining authentication domains. For example, you can configure LDAP as first authentication domain, and the internal user database as second authentication domain. In this case, if the LDAP user authentication fails, OpenSearch will try the internal user database next. This provides for great flexibility, making it possible to implement most basic to very complex use-cases.\nAuthentication and authorization domains are configured in the file config.yml. After making changes here, do not forget to apply the changes by using the securityadmin CLI to upload changes.\nDefining Security Roles\nA security role defines what the bearer of this role is allowed to do. A role can grant access permissions for:\nCluster actions\nIndices\nDashboard tenants\nCluster actions are actions that are not tied to specific indices, but rather operate on the cluster level. For example, health checks, cluster monitor actions, or cluster admin actions. They also include msearch and bulk actions.\nIndex-level permissions can be granted by index or by index pattern. For example, you can assign READ permissions for all indices starting with logstash like: my-role:...\nindex_permissions:\n- index_patterns:\n- \"logstash-*\"\nallowed_actions:\n- \"READ\" OpenSearch comes with a number of pre-defined sets of permissions, like READ, WRITE, CRUD, etc. These permission sets are called action groups and should cover most use cases.\nIf required, you can also use single actions and even define your own action groups for re-use across roles.\nSecurity roles are configured in the file roles.yml. After making changes here, do not forget to apply them by using the securityadmin CLI.\nMapping Security Roles to Users\nWe have now discussed user management and role management. The last question is how to assign our roles to users.\nIf you use the internal user database, then you can directly assign security roles to users. Again, you can use the securityadmin CLI or the REST API. For example: new-user:\nhash: \"...\"\nopensearch_security_roles:\n- \"my-role\"\n- \"my-other-role\" Here, user myuser has two roles, my-role and my-other-role.\nThis is nice and straightforward. However, if your user base grows, or if you use LDAP or Active Directory, you may want to leverage the user mapping feature of OpenSearch.\nThink of it like adding users to groups first, and then assign one or more security roles to those groups. This introduces a layer of indirection but provides more flexibility when changing permissions. Let‚Äôs say you have an LDAP and all employees that work in DevOps are members of this group: cn=devops,ou=it,dc=example,dc=com You can use the name of this group to automatically assign security roles to all members. To do so, add an entry in the role_mapping.yml configuration file like: my-devops-security-role:\n- \"cn=devops,ou=it,dc=example,dc=com\" Now all members of the LDAP group cn=devops,ou=it,dc=example,dc=com will be assigned to the security role my-devops-security-role.\nAdmin TLS Certificates\nThe last concept we want to cover is admin TLS certificates. In contrast to other security solutions, OpenSearch does not have the notion of a root user that has unlimited permissions. Instead, OpenSearch uses a client TLS certificate, the so-called admin TLS certificate.\nThis is a normal TLS certificate, signed by your root CA or intermediate CA. If a request to OpenSearch sends an admin TLS certificate, all permissions to the cluster are granted.\nUsing a TLS certificate for granting full access to the cluster provides a higher level of security than using a username/password combination.\nThis post is contributed by Anton Rubin from Elitra and is co-published on the Eliatra blog",
    "keywords": [
      "technical-posts"
    ],
    "type": "News"
  },
  {
    "url": "/blog/moving-from-opensource-elasticsearch-to-opensearch/",
    "title": "Moving from open source Elasticsearch to OpenSearch",
    "content": "Blog refreshed for technical accuracy on 16 Nov 2022 The process of upgrading from open source Elasticsearch to OpenSearch varies depending on your current version of Elasticsearch, installation type, tolerance for downtime, and cost sensitivity. Rather than recommended steps for every situation, we provide general guidance on the process.\nThis blog post is a refresh of the upgrade process.\nWhat is an open source Elasticsearch to OpenSearch upgrade?\nAn upgrade means moving a cluster to either a new major or latest minor version of the major version. Major version upgrades come with breaking changes, and the upgrade should follow the recommended migration path. Minor version upgrades are relatively simple.\nWhen upgrading from Elasticsearch to OpenSearch, settings and indexes are upgraded:\nCluster settings: All dynamic settings that are supported in the new cluster will be migrated, and the ones that were deprecated will be archived.\nIndexes: All compatible indexes (that is, all your data) will be readable/writable by the new software version.\nStatic settings defined in the opensearch.yml or elasticsearch.yml files are not moved automatically during an upgrade. Each version of OpenSearch is bundled with the opensearch-upgrade tool. The tool connects to an Elasticsearch cluster and moves the static settings, and the cluster and the settings are defined in the elasticsearch.yml to opensearch.yml files automatically. If you use ElasticSearch keystore to store secret values, you also can use the opensearch-upgrade tool to automate the migration.\nPreparing for the upgrade\nIf you run open source Elasticsearch or older minor versions of OpenSearch and want to upgrade to the latest OpenSearch version, you‚Äôll need to take the following actions.\nFirst, back up your data by taking a snapshot of your existing cluster (You can follow the approach outlined in Upgrade to OpenSearch.\nSecond, verify version compatibility between the existing cluster and the version to which you are migrating. OpenSearch was forked from the last open source version of Elasticsearch, 7.10.2.\nIndexes are compatible with current and previous major versions (OpenSearch 1.x or later can read/write indexes from open source Elasticsearch 6.x and 7.x).\nWire compatibility works with all major versions and the minor verison(s) of the latest major version. OpenSearch 1.x or later can join an open source Elasticsearch 6.8.x and 7.x cluster.\nIndexes that were created prior to open source Elasticsearch 6.x must be reindexed or deleted in order to upgrade the cluster to OpenSearch. The cluster will fail to start if you have incompatible indexes.\nUse the Reindex API to migrate your data from indexes created in versions prior to 6.x to a new version. Here‚Äôs an example: POST /_reindex\n{\n\"source\":{\n\"index\":\"my-old-index\"\n},\n\"dest\":{\n\"index\":\"my-new-index\"\n}\n} Lastly, download the latest version of OpenSearch from the OpenSearch downloads page. If you need help with installation, see the guidance in Install and Configure OpenSearch.\nAs described in a previous blog post, open source Elasticsearch is the foundation for Open Distro, and it has the same upgrade paths. You can follow through the steps for the path you choose to upgrade your cluster.\nSupport matrix for open source Elasticsearch to OpenSearch Open Source Elasticsearch Recommended Upgrade Path 7.0 to 7.10.2\nRestart/Rolling upgrade to OpenSearch 1.x\n6.8\nRestart/Rolling upgrade to 7.10.2\n6.0 to 6.7\nRestart/Rolling upgrade to 6.8\n5.6\nRestart/Rolling upgrade to 6.8\n5.0 to 5.5\nRestart/Rolling upgrade to 5.6 Clients and tools\nIf you are using tools like Beats, Logstash, Fluentd, and Fluent Bit and you want to continue using them, see Agents and Ingestion Tools. Some clients and tools have version checks built in and do not work out of the box. OpenSearch has an intermediate solution. It is built with a compatibility flag that returns to version 7.10.2, and you‚Äôll need to set the cluster setting as follows: PUT _cluster/settings\n{\n\"persistent\": {\n\"compatibility\": {\n\"override_main_response_version\": true\n}\n}\n} OpenSearch also provides clients for several popular programming languages, including Python, NodeJS, and Go.\nGoing forward\nNow that you have the necessary information about migrating from open source Elasticsearch to OpenSearch, you can decide the suitable path for your workload. Again, remember to take a backup before starting the upgrade process.",
    "keywords": [
      "technical-posts"
    ],
    "type": "News"
  },
  {
    "url": "/blog/bwc-testing-for-opensearch/",
    "title": "Backwards Compatibility Testing for OpenSearch",
    "content": "Backwards Compatibility (BWC) testing is used to test and determine the safe upgrade paths from a supported BWC version to the current version. The framework in OpenSearch allows you to run these BWC tests for all supported BWC versions in OpenSearch allowing safe upgrade paths between versions. This framework is now extended to work for plugins which can introduce their BWC tests without creating individual frameworks of their own. This post provides details on the framework in OpenSearch for backwards compatibility.\nOpenSearch 1.1.0 was tested with versions 7.10.2 and 1.0.0 for backwards compatibility using the framework described in this post.\nThe Framework\nAs a general idea for BWC tests, the framework supports spinning up a test cluster with a supported BWC version, then upgrade the nodes to the current version and test various features and functionalities as a result of the upgrade. This allows for testing the compatibility of the code between versions.\nTest Cluster Setup\nThe test clusters are available via opensearch.testclusters gradle plugin. A cluster is an instance of OpenSearchCluster and it contains a list of nodes which are instances of the OpenSearchNode class which provides the methods to start, stop, restart, upgrade, install plugins etc. The version of the nodes can be set by providing a list of versions to the testclusters. The node can then be upgraded to a newer version provided by using the nextNodeToNextVersion method on the concerned testcluster or by using goToNextVersion method which upgrades all the nodes in a cluster. testClusters {\n\"mycluster\" {\nversions = [\"7.10.2\", project.version]\nnumberOfNodes = 4\n}\n} An example testcluster initialization is given above. Here the testcluster is named mycluster with 4 nodes. Two versions are provided and the first version is the version that gets installed by default when a node starts. It upgrades to the next version using the method mentioned above.\nTypes Of BWC Tests\nAll the BWC tests in OpenSearch are located inside the qa module. The following types of tests are available for BWC: Mixed Cluster: For a mixed cluster scenario, a test cluster is spun up with multiple nodes of a supported BWC version, then one node upgraded to the current version of OpenSearch using the nextNodeToNextVersion method resulting in a cluster with a few nodes still on the older version. The corresponding tests are run for the old cluster and for the newly created mixed cluster automating the mixed cluster upgrade path.\nLocated in qa/mixed-cluster Run using./gradlew:qa:mixed-cluster:vM.N.b#mixedClusterTest Rolling Upgrade: For a rolling upgrade scenario, a test cluster is spun up with multiple nodes of a supported BWC version and each node is upgraded one by one to a newer version of OpenSearch using the nextNodeToNextVersion method resulting in a fulling upgraded cluster achieved via sequential upgrade. The corresponding tests are run for the old cluster and for each node after it undergoes the upgrade.\nLocated in qa/rolling-upgrade Run using./gradlew:qa:rolling-upgrade:vM.N.b#upgradedClusterTest Full Cluster Restart: For a full cluster restart, a test cluster is spun up with multiple nodes of a supported BWC version and all the nodes are upgraded using the goToNextVersion method resulting in a fully upgraded cluster by restarting. The corresponding tests are run for the old cluster and for the upgraded cluster.\nLocated in qa/full-cluster-restart Run using./gradlew:qa:full-cluster-restart:vM.N.b#upgradedClusterTest Snapshot Upgrade: For a snapshot upgrade, two test clusters are created (one with the older version and one with a newer version) and the snapshot restoration is tested between the clusters. Each step tests the creation of repository and snapshot and restoring the snapshot.\nLocated in qa/repository-multi-version Run using./gradlew:qa:repository-multi-version:vM.N.b#Step4NewClusterTest The backwards compatibility tests in OpenSearch can be run for versions ranging from 7.10.0 to the current project version.\nTo run all the backwards compatibility tests:./gradlew bwcTest A specific version can be tested as well. For example, to test bwc with version 7.10.2 run:./gradlew v7.10.2#bwcTest BWC Tests For Plugins\nThe framework for BWC from OpenSearch has been extended to be used for plugins as part of an effort to increase test automation for various upgrade paths. The plugins can use the testclusters plugin from OpenSearch for a test cluster setup and provide the plugin to be installed on the nodes of the cluster. testClusters {\n\"mycluster\" {\nversions = [\"7.10.2\", project.version]\nnumberOfNodes = 4\nplugin(provider(new Callable&lt;RegularFile&gt;(){\n@Override\nRegularFile call() throws Exception {\nreturn new RegularFile() {\n@Override\nFile getAsFile() {\nreturn fileTree(\"`src/test/resources/org/opensearch/ad/bwc/anomaly-detection`/1.13.0.0/\").getSingleFile()\n}\n}\n}\n}))\n}\n} For the example given above, the test cluster consists of 4 nodes which are initialized with 7.10.2 version when started with the BWC version 1.13.0.0 of plugin anomaly-detection installed on all the nodes. For anomaly-detection, the backwards compatibility tests run for testing the upgrade from 7.10.2 versioned test cluster with 1.13.0.0 plugin to 1.1.0 versioned test cluster with 1.1.0.0 plugin.\nMixed cluster, rolling upgrade and full restart upgrade scenarios are supported for the BWC tests in plugins and the nodes in the cluster can be upgraded using the methods on the test cluster: upgradeNodeAndPluginToNextVersion for a single node upgrade at a time and upgradeAllNodesAndPluginsToNextVersion for upgrading all the nodes at the same time. The tests corresponding to each step can be configured to run similarly as explained above according to each upgrade scenario.\nSummary\nBackwards compatibility especially for plugins has been a major concern during releases since it involved manual testing to determine safe upgrade paths. With the extended framework for BWC, plugins can now implement their BWC tests thus alleviating the need for manual efforts. All pull requests in OpenSearch now run the BWC tests making sure all the changes adhere to the backwards compatibility standards. For more information on the CI runs, please check out an example anomaly-detection CI run.\nFor more information on BWC testing, please check out OpenSearch/TESTING.md and opensearch-plugins/TESTING.md.\nClosing on a high note, we hope this blog post adds light to the backwards compatibility framework and we are continuing to invest efforts in automating backwards compatibility for OpenSearch. The release automation currently built as part of opensearch-project/opensearch-build#90 runs the bwc tests for OpenSearch as part of the release process. Going further, we are working to add bwc tests for all plugins ( opensearch-project/opensearch-plugins#77), automate running the plugin bwc tests and run bwc tests for the bundle as part of the release process. Please refer to opensearch-project/opensearch-build#90 for the next steps.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/partner-highlight-instaclustr/",
    "title": "Partner Highlight: Instaclustr adds OpenSearch to its managed service offerings",
    "content": "Instaclustr is pleased to announce the general availability of OpenSearch and OpenSearch Dashboards on the Instaclustr SaaS Platform. OpenSearch is another key open source technology that joins our suite of managed offerings. Instaclustr also provides Enterprise Support for organizations who choose to run OpenSearch themselves.\nWe are pleased to support the OpenSearch initiative and believe that it is vital that the community work together to ensure a viable, open alternative to increasingly proprietary offerings from Elastic.co.\nOur Managed OpenSearch offering builds on the operational knowledge developed from our Open Distro for Elasticsearch offering. In March 2020, Instaclustr was the first company to offer Managed Service for Open Distro for Elasticsearch besides AWS‚Äô own service (as the original sponsors of the Open Distro). Instaclustr Managed OpenSearch is available on AWS, GCP, Azure and your own infrastructure (including on-prem).\nIf you are running Elasticsearch clusters and are concerned about the licence changes we highly encourage you to trial an OpenSearch cluster and experiment with it. You can begin a free trial of Instaclustr Managed OpenSearch here.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/real-time-and-historical-ad/",
    "title": "Anomaly Detection for Historical Data and Real-time Streaming",
    "content": "You can leverage anomaly detection to analyze vast amount of logs in many different ways. Some analytics approaches require real-time detection, such as application monitoring, event detection, and fraud detection. Others involve analyzing past data to identify the trends and patterns, isolate the root cause and prevent them from happening again in the future. The anomaly detection plugin in OpenSearch already supports real-time streaming data. In OpenSearch 1.1.0, the team introduced support for anomaly detection on historical data. Often-times, the historical and real-time use cases work hand-in-hand. You may find real-time anomalies in your data, and decide to run historical detection to search for similar patterns in the past. Or, you may first search for anomalies in the past data, then decide to configure anomaly detection to monitor your system for anomalies in real-time. OpenSearch 1.1.0 also streamlines the anomaly detection configuration with the new unified flow that allows you to configure an anomaly detector once that can then be applied to both real-time or historical analysis. In this blog, I will review each of the steps in the unified workflow with an example.\nConsider an index service_1_metrics which contains different performance metrics for a service named service_1. The index service_1_metrics contains the fields host, cpu_utilization, mem_utilization, and timestamp, where host is the specific host that‚Äôs reporting the metrics, cpu_utilization is the current CPU utilization percentage reported by that host, mem_utilization is the current memory utilization percentage reported by that host, and timestamp is the time when the metrics were collected. Now, suppose you are interested in setting up anomaly detection on the CPU utilization and memory utilization metrics.\nIn Step 1, you define the basic detector configuration - a detector is an individual anomaly task. You can have multiple detectors running in your cluster at any time. The detector settings include the detector name and description, details about the data source, and detector interval. Here, you would select the service_1_metrics index, and perhaps add some preprocessing to the data by adding a data filter to ignore any documents in the index where host is null. You would also select timestamp as the configured timestamp field. Under ‚ÄúOperation settings‚Äù, you can tune how often you want to collect results with the ‚ÄúDetector interval‚Äù setting. Suppose the service_1_metrics collects results from all hosts every minute, and indexes a new document for each host every minute. In this case, using the default interval of 10 minutes is reasonable. This means the detector will aggregate the real-time source data and produce anomaly results every 10 minutes. In Step 2, you configure your features and other model settings. Up to five features can be configured per detector. In this example, you would want to create two features - one for analyzing the CPU utilization metrics, and one for analyzing the memory utilization metrics. Under ‚ÄúCategorical fields‚Äù, you can optionally select a field to partition the source data on. For example, by selecting the host field, a separate set of anomaly results will be produced for each unique host that is found in the index, allowing for a more fine-grained analysis of the data. By not selecting any categorical fields, all of the data will be treated as a single time series, and a single set of anomaly results will be produced.\nAfter making selections, you can optionally preview how the detector may perform by clicking ‚ÄúPreview anomalies‚Äù, which will sample existing source data and produce sample anomaly results. In Step 3, you select the anomaly detection jobs you want to run upon creation - real-time or historical. If historical analysis is selected, you can configure the date range to run the detection on. Suppose the service_1_metrics index has 30 days of data on it already - you may decide to run a historical analysis on the last 30 days to see if any historical anomalies were detected in the data. In Step 4, you can review and edit any of the selections made in the previous steps. When clicking ‚ÄúCreate detector‚Äù, the detector will be created, and any selected real-time or historical analysis will begin running. Note that these can always be started or restarted at a later time as well. After creation, real-time results can be viewed under the ‚ÄúReal-time results‚Äù tab, and historical results can be viewed under the ‚ÄúHistorical analysis‚Äù tab. The ‚ÄúDetector configuration‚Äù tab contains the configuration used and shared by any existing real-time and historical anomaly detection jobs. The user interface for viewing historical results is similar to the real-time results, but with a few extra helpful features:\nYou always have the option to run a new historical analysis and select a new date range, by clicking on the ‚ÄúModify historical analysis range‚Äù button, highlighted in red below. Note that only the latest historical analysis job will be persisted and visible on this page. Previous runs and anomaly results will be discarded. As the historical analysis is running, the page will automatically refresh to show partial results, along with a progress bar. When the detection job is finished, you can zoom and view the anomaly results, as well as aggregate them on-the-fly by clicking on the ‚ÄúDaily max‚Äù / ‚ÄúWeekly max‚Äù / ‚ÄúMonthly max‚Äù buttons. Currently, only max anomaly grade aggregations are supported. There are a few other changes to the Anomaly Detection Dashboards Plugin in 1.1.0 worth mentioning:\nOn the detector list page, an additional column related to historical analysis has been added. If there is any existing historical analysis found for a detector, a ‚ÄúView results‚Äù link will appear, which will navigate to the ‚ÄúHistorical analysis‚Äù tab on the detector‚Äôs detail page. The sample detectors page has been moved into a new ‚ÄúOverview‚Äù page, which provides additional high-level information about anomaly detection. This page can always be accessed by clicking on the ‚ÄúAnomaly detection‚Äù title in the side navigation bar. Conclusion\nIn this blog post, I discussed two new features released in OpenSearch 1.1.0 - anomaly detection for historical data and universal flow in OpenSearch Dashboards that enables you to just configure once and detect anomalies on real-time and historical data. Anomaly detection for historical data helps you analyze and identify trends and patterns in past data. You can use these insights to make better decisions, improve planning, and boost the overall operational efficiency of your applications.\nThe team welcomes you to come join in and contribute with us in building many more exciting features in anomaly detection and machine learning. You can learn more by visiting the Anomaly Detection OpenSearch Plugin and Anomaly Detection OpenSearch Dashboards Plugin GitHub repositories.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/launch-announcement-1-2-0/",
    "title": "OpenSearch 1.2.0 is out now!",
    "content": "With this latest version of the OpenSearch distribution (OpenSearch, OpenSearch Dashboards, as well as plugins and tools) you can enjoy a number of new features and enhancements as well as improvements to stability and efficiency. A few highlights include: New Observability Interface: A new interface design for Observability in OpenSearch Dashboards that makes it easier to analyze and manage log and trace data. This new design includes an event explorer that makes it easy to search through log data across indexes with support for Piped Processing Language (PPL) querying. Additionally, there is a new PPL-based chart builder to construct visualizations from your log queries. Lastly, the new interface allows you to correlate your log and trace data: Feature Attribution in Anomaly Detection: A new API is available to provide an attribution ratio for each input feature to help you understand how they contributed to the anomaly. With this data, you can more quickly identify the cause of the anomaly. Shard-level indexing back-pressure: Optimize indexing back-pressure with new shard-level memory accounting as well as throughput, and last successful request factors. ‚ÄòMatch‚Äô query support in SQL and PPL: The match query type returns documents that match a provided text, number, date, or Boolean value for a specified field. The Match search query type is now supported in both query languages. More efficient k-NN dense vectors: k-NN now has support for the Faiss library, allowing you to expand the size of feature vectors. The Faiss library brings efficient similarity search and clustering of dense vectors and allows for search in data sets of vectors in sizes larger than what fits in memory. Custom Dashboards Branding: Align the look and feel of OpenSearch Dashboards to your own brand by providing a configurable logo, favicon, title, and more. What‚Äôs next?\nWith 1.2.0 launched, OpenSearch 1.3.0 is already in development! There are a number of upcoming feature and enhancements being worked on. A few highlights include:\nCustom GeoJson support in region maps\nSupport for additional input types in alerting Drag and drop visualization creation A new application analytics view If you are curious, feel free to take a look at the project roadmap where you can find out the planned features and fixes with linked issues where you can provide feedback.\nHow can you contribute?\nWe would love to see you contribute to OpenSearch! For almost any type of contribution, the first step is opening an issue. Even if you think you already know what the solution is, writing a description of the problem you‚Äôre trying to solve will help everyone get context when they review your pull request. If it‚Äôs truly a trivial change (e.g. spelling error), you can skip this step ‚Äì but when in doubt, open an issue. If you‚Äôre excited to jump in, check out the ‚Äúhelp wanted‚Äù tag in issues.\nDo you have questions or feedback?\nIf you‚Äôre interested in learning more, have a specific question, or just want to provide feedback and thoughts, please visit OpenSearch.org, open an issue on GitHub, or post in the forums. There are also regular Community Meetings that include progress updates at every session and include time for Q&amp;A.\nThank you!\nWe knew OpenSearch would need to build a great open source community to succeed and we‚Äôre so excited about the progress! Not only is OpenSearch seeing some awesome contributions across the project but the community partners continue to grow (with 8 new partners since 1.0.0) and the recent launch of the testimonials page that includes Dow Jones, Goldman Sachs, Quantiphi, Rackspace Technology, SAP, Wipro and Zoom. As always, everyone should be incredibly proud of the accomplishment of reaching 1.2.0 together.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/setup-multinode-cluster-kubernetes/",
    "title": "Setup OpenSearch multi-node cluster on Kubernetes using Helm Charts",
    "content": "Introduction\nOpenSearch can operate as a single-node or multi-node cluster. Production setup typically requires a multi-node cluster. In this tutorial, you will learn how to setup a multi-node cluster of OpenSearch using Helm and configure OpenSearch Dashboards to access the cluster. This will setup a three-node cluster that has one dedicated master node, one dedicated coordinating node, and one data node that are used for ingesting data. So, let‚Äôs start setting up the OpenSearch stack on K8s.\nStep 1: Set up Kubernetes\nFor demo purposes, I am using Docker Desktop for running a local Kubernetes (abbreviated as k8s) cluster. The Helm chart version used for this tutorial for OpenSearch is 1.2.4 and 1.0.6 for OpenSearch Dashboards. You can use anything you prefer, be it minikube, kind, etc. You an also setup Kubernetes clusters in AWS, Google Cloud, Azure or any other cloud provider and set up the context on your local environment. If you don‚Äôt have kubectl installed follow the instructions to install. It will help in managing all the deployments in Kubernetes clusters.\nThis tutorial uses ‚Äúcontexts.‚Äù A context is a group of access parameters. Each context contains a Kubernetes cluster, a user, and a namespace. The current context is the cluster that is currently the default for kubectl.\nYou can check your current context by running: &gt; kubectl config current-context To switch to a different context say ‚Äúdocker-desktop‚Äù, run the following command and you will be good to go. &gt; kubectl config set-context docker-desktop A minimum of 4GiB of memory is required. I would recommend to have 8 GiB of memory available for this setup to avoid any intermittent failures.\nInstall Helm in your environment for managing charts deployment by following the instructions from the Helm website. Step 2: Set up values for different nodes\nCopy the values.yaml file from the Helm repo.\nCopy the contents of the values.yaml file the into three different pod configuration files named master.yaml, data.yaml and client.yaml respectively.\nOpen the master.yaml file and change the configurations to the below values. clusterName: \"opensearch-cluster\"\nnodeGroup: \"master\"\nmasterService: \"opensearch-cluster-master\"\nroles:\nmaster: \"true\"\ningest: \"false\"\ndata: \"false\"\nremote_cluster_client: \"false\"\nreplicas: 1 Save the file and close.\nOpen the data.yaml file and change the following configurations: clusterName: \"opensearch-cluster\"\nnodeGroup: \"data\"\nmasterService: \"opensearch-cluster-master\"\nroles:\nmaster: \"false\"\ningest: \"true\"\ndata: \"true\"\nremote_cluster_client: \"false\"\nreplicas: 1 The setup creates 1 data pod replica. Save the file and close.\nOpen the client.yaml file and change the following configurations: clusterName: \"opensearch-cluster\"\nnodeGroup: \"client\"\nmasterService: \"opensearch-cluster-master\"\nroles:\nmaster: \"false\"\ningest: \"false\"\ndata: \"false\"\nremote_cluster_client: \"false\"\nreplicas: 1 Save the file and close it.\nStep 3: Deploy\nAdd the Helm repository helm repo add opensearch https://opensearch-project.github.io/helm-charts/ Run the helm install command three times once for each custom YAML file. helm install opensearch-master opensearch/opensearch -f usr/data/master.yaml\nhelm install opensearch-data opensearch/opensearch -f usr/data/data.yaml\nhelm install opensearch-client opensearch/opensearch -f usr/data/client.yaml Wait for the deployment to complete. Use the command: kubectl get pods to inspect the progress of the deployment.\nThe output shows READY 1/1 once the deployment succeeds. Install OpenSearch Dashboards with the default configuration by doing a Helm install. helm install dashboards opensearch/opensearch-dashboards Repeat step 3 to inspect the deployment for OpenSearch Dashboards.\nStep 4: Play with the cluster\nTo access the cluster locally use kubectl to forward it to port 9200 using the below command. kubectl port-forward opensearch-cluster-master-0 9200 Open a different tab in the terminal and run the following command to check your cluster is spinning curl -XGET https://localhost:9200 -u 'admin:admin' --insecure To access the OpenSearch Dashboards URL locally, forward it to port 5601. kubectl get pods After getting the pod name do a port-forward to 5601 by running the following command kubectl port-forward dashboards-opensearch-dashboards-575ddd7c4b-cc28b 5601 Output: Visit the url http://localhost:5601/ and use username and password as ‚Äúadmin‚Äù to play around with OpenSearch Dashboards\nIn this tutorial you learned about different tools which are used for managing Kubernetes clusters like Helm, kubectl etc. After completing the tutorial you should be able to have a multi-node cluster up and running on your Kubernetes cluster. You can leverage the different features in the Helm charts to setup your production stacks which can be used for logging, monitoring and for a number of other purposes. One thing to note there might be newer releases in the official Helm charts so syntax may vary from the blog, please refer the repository‚Äôs README for more details on that. Happy helming!",
    "keywords": [
      "technical-posts"
    ],
    "type": "News"
  },
  {
    "url": "/blog/plugins-intro/",
    "title": "Introduction to OpenSearch Plugins",
    "content": "OpenSearch enables enhancing core features in a custom way via plugins. For example, plugins could add custom mapping types, engine scripts, etc. In this blog post we wanted to unbox how plugins load, install, and run in OpenSearch.\nPluggable Architecture\nThe modular architecture in OpenSearch makes it easier to develop on a large codebase (4.5MM lines). The blog post from OpenSearch partner Logz.io describes why pluggable architecture is important and how plugins can be developed.\nThe Plugin architecture is designed to enable solving specific problems and extending generic features. For example, Anomaly Detection reads time stream data ingested and finds anomalies. Another example is Job Scheduler plugin which schedules and runs generic jobs.\nPlugins are of various types, generally categorized as:\nAnalysis: Used for researching of data available within the cluster,\nDiscovery: Used for easy recognition of nodes in various platforms,\nIngest: Used for pre/post-processing data during ingestion,\nMappers: Helps extend/create data fields,\nSnapshot/Restore: Used to create a snapshot and restore data.\nTo develop these plugins, the codebase has well defined interfaces to solve specific sub-set of problems.\nExtension Points\nThe architecture is designed for plugins to hook onto various points within the OpenSearch codebase. Plugins can subscribe to relevant notifications/events via these extension points.\nThe Plugin.java file defines a list default extension points.\nExtension points enable plugins to hook into various events within the cluster and data lifecycles in OpenSearch.\nThe default extension points are defined by Plugin.java abstract class: getFeature - Implement a custom feature and respond to cluster state API. createGuiceModules - Implement node level dependency injection modules via Guice. getGuiceServiceClasses - Node level services which will be automatically called with node state changes. createComponents - Custom component implemented and its lifecycle being managed by OpenSearch. additionalSettings - Implement additional node level settings. getNamedWriteables - Custom parsers the plugin would use for transport messaging. getNamedXContent - Custom parsers the plugin would use for NamedObjects. onIndexModule - Index level extension point, called before an index is created. getSettings - Implement additional cluster level settings. getSettingsFilter - Implement additional cluster level settings filter. getSettingUpgraders - Implement setting upgraders. getIndexTemplateMetadataUpgrader - An extension to modify index template metadata on startup. getExecutorBuilders - Implement custom thread pools for executions. getBootstrapChecks - Add additional bootstrap checks when OpenSearch node initializes. getRoles - Implement additional DiscoveryNodeRole‚Äôs. getAdditionalIndexSettingProviders - Implement additional index level settings for newly created indices.\nCustom plugin interfaces can define new extension points for plugins to hook onto. For example, the Engine Plugin interface can be used to provide additional implementations to the core engine, expose a hook to node bootstrap to load the custom engineFactory and the Index Service overrides it if plugin chooses to override.\nHow do plugins work?\nAs you might have used plugins in the OpenSearch distribution. Plugins are installed and loaded when OpenSearch starts.\nInstalling a plugin\nThe OpenSearch distribution comes with a tool./bin/opensearch-plugin which installs a plugin. PluginCli reads and validates plugin-descriptor.properties file packaged with every plugin. For example, the OpenSearch security plugin defines the plugin-descriptor.properties file which defines a bunch of parameters, and the tool verifies if it is using the compatible version of OpenSearch, and the dependencies are present.\nThe tool verifies the plugin-security.policy file, defined by the plugin which needs additional security permissions. For example, the OpenSearch security plugin defines many permissions like file read/write, classloading or networking that it needs through the plugin-security.policy file. These permissions are managed via Java Security Manager (more details later in this post). After the tool validates the plugin, it copies all jars into the plugins directory. By default, the OpenSearch Minimum distribution does not package any plugins including the native plugins which exist in the OpenSearch codebase.\nLoading a plugin\nPlugins run within the same process as OpenSearch. As the OpenSearch process bootstraps, it initializes PluginService via Node.java. All plugins are class-loaded via loadPlugin during the bootstrapped of PluginService.\nIt checks the plugins directory and loads the classpath where all the plugin jars and their dependencies are already installed by the opensearch-plugin install tool. ~/opensearch-1.1.0/plugins$ ls\nopensearch-alerting opensearch-asynchronous-search opensearch-index-management opensearch-knn opensearch-performance-analyzer opensearch-security\nopensearch-anomaly-detection opensearch-cross-cluster-replication opensearch-job-scheduler opensearch-notebooks opensearch-reports-scheduler opensearch-sql As the plugins are class-loaded during the node bootstrap, the extension points (defined by the plugin interface) initialize the data structures.\nThis design of loading plugins during the node bootstrap prevents them being loaded on the fly and cannot be hot-swapped. Each node within the cluster has to be restarted to reload a new plugin.\nPlugins vs Modules\nAs you might have noticed, OpenSearch defines plugins and modules differently. The main difference is modules are automatically loaded in an OpenSearch node and are packaged with opensearch-min artifact. On the flipside, plugins are not automatically packaged and have to be manually installed. ~/opensearch-1.1.0/modules$ ls\naggs-matrix-stats geo ingest-geoip lang-expression lang-painless opensearch-dashboards percolator reindex transport-netty4\nanalysis-common ingest-common ingest-user-agent lang-mustache mapper-extras parent-join rank-eval repository-url Modules essentially are plugins and they implement plugin interfaces, but the only logical difference is that they are loaded by default which makes them special. All core extendible features are built via modules for extensibility.\nJava Security Manager\nJava applications are prone to have vulnerabilities on a remote cluster or by a DDoS attack. To prevent this, JVM can be run in a sandbox mode which will prevent, for example: access to the local hard disk or the network. All of these are handled by the Security Manager.\nHow Security Manager is used in OpenSearch?\nAs OpenSearch bundles a few plugins, every plugin can define its own custom security policy file which will be installed at the same time when OpenSearch is installing the plugin.\nSecurity Manager is initialized in Opensearch.java, and the plugins that require security polices have a custom policy file called plugin-security.policy.\nThe getPolicy() method will take care of assigning the initial default policies required for the plugins and setPolicy() method will assign the custom policies of the plugins present in plugin-security.policy.\nEach custom security policy file is signed and has a codebase which is a signed key between OpenSearch and the plugin.\nEach security policy can be attached via gradle plugin opensearch.opensearchplugin in the build.gradle file of the plugin. opensearchplugin {\ndescription 'The HDFS repository plugin adds support for Hadoop Distributed File-System (HDFS) repositories.'\nclassname 'org.opensearch.repositories.hdfs.HdfsPlugin'\n} The security policy has a notion of per-user policies and it is useful in the context of manually configuring the application deployment on a single specific computer, but it is hard to use in the generic case. Benefits of Security Manager in OpenSearch Prevent access to OpenSearch cluster with all permissions. Any intruder can add new classes, change private members or access the ClassLoaders. The Security Manager takes care of it by creating a sandbox environment. exitVM permission is allowed for a few special classes, for other classes the process will kill and exit the cluster. This method calls checkPermission with the RuntimePermission and throws a SecurityException.\nSecurity Manager Policy allows installing a plugin through plugin-security.policy which consists of dynamic configuration and dependencies required for a plugin to run.\nPlugins can ask for AllPermission of the OpenSearch cluster. The Security Manager takes care of AllPermission and makes sure to check it as a part of Bootstrap.\nPlugins can create a plugin-security.policy file and write dynamic configuration and permissions required to run from the OpenSearch Cluster.\nClosing Notes\nWe hope this post helped explain how plugins work within OpenSearch. We would love to see you get your hands dirty, and develop a new plugin for OpenSearch.\nLooking forward, we are thinking about solving the limitations in plugin architecture and would ask for your feedback in OpenSearch#1422.\nIn the coming days, look out for a follow-up post soon on intro to plugins with OpenSearch Dashboards.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/distributed-tracing-pipeline-with-opentelemetry/",
    "title": "Building a distributed tracing pipeline with OpenTelemetry Collector, Data Prepper, and OpenSearch Trace Analytics",
    "content": "Over the past few years, the importance of observability when developing and managing applications has spiked with the spotlight firmly on micro-services, service mesh. Distributed services can be unpredictable. Despite our best efforts, failures and performance bottlenecks in such systems are inevitable and difficult to isolate. In such an environment, having deep visibility into the behavior of your applications is critical for software development teams and operators.\nThe landscape for observability tools continues to grow as we speak. This growth is particularly large for metrics, error logging, and distributed traces. These can provide valuable information to optimize service performance and troubleshoot issues to make a service more reliable. With this in mind, it makes sense to create a distributed tracing pipeline to ingest, process, and visualize tracing data with query/alerting.\nAt Dow Jones, we started on a similar journey as we continue to move our applications and microservices to our service mesh based on EKS and Istio. Our requirements formulated from the generated service mesh telemetry data was to have a distributed tracing pipeline that could scale to the amount of our traces, work for applications regardless of host, and the language of implementation. OpenTelemetry‚Äôs full support for traces, metrics, and logs, as well as, provide a single implementation married perfectly into this idea.\nOpenTelemetry (OTEL) is formed by the merging of OpenTracing and OpenCensus. It is a CNCF incubating project and the second most active in terms of contributions. OTEL, since its inception, aimed to offer a single set of APIs and libraries that standardize how you collect and transfer telemetry data. Tracing was the first component to reach stable status for the OpenTelemetry project. Since then, almost all trace components but collector has reached stable maturity level, metrics are well on their way, and initial work on logs has also started. At Dow Jones, we launched a distributed tracing pipeline based on the OpenTelemetry project. I will be going over a few of the design decisions we implemented in our setup and source code to DIY.\nTracing Pipeline Components\nWith OpenTelemetry as the core of our pipeline, we still needed to decide on the sink for our traces, tools to visualize and query traces, and tools to export traces in order to create a Federated architecture. After much deliberation, we decided on the following components: OpenTelemetry for creation, propagation, collection, processing, and exporting of trace data. OpenSearch as the sink for the traces\nOpenSearch is a community-driven, open source search and analytics suite derived from Apache 2.0 licensed Elasticsearch 7.10.2 &amp; Kibana 7.10.2. Additionally, it comes with managed Trace Analytics plugin, a visualization and user interface, and OpenSearch Dashboards. Data Prepper and Jaeger to re-format OpenTelemetry trace data and export it to OpenSearch in formats that both the OpenSearch dashboard and Jaeger understand. This should go away soon as both Jaeger and OpenSearch Dashboards will natively be able to use OpenTelemetry Protocol (OLTP) traces. Jaeger tracks and measures requests and transactions by analyzing end-to-end data from service call chains, making it easier to understand latency issues in microservice architectures. It‚Äôs exclusively meant for distributed tracing with a simple web UI to query traces and spans across services. Jaeger enables views of individual traces in a waterfall-style graph of all the related trace and span executions, service invocations for each trace, time spent in each service and each span, and the payload content of each span, including errors.\nData Prepper is a new component of OpenSearch that receives trace data from the OpenTelemetry collector, aggregates, transforms, and normalizes it for analysis and visualization in OpenSearch Dashboards. Trace Analytics Plugin for OpenSearch to visualize and query traces\nThe Trace Analytics plugin also aggregates trace data into Trace Group views and Service Map views, to enable monitoring insights of application performance on trace data. By going beyond the ability to search and analyze individual traces, these features enable developers to proactively identify application performance issues, not just react to problems when they occur.\nArchitecture\nOnce we got the components for the pipeline penciled, the remaining work to get them to play nice with each other and scale to our requirement. Since then, the tracing pipeline has been launched in production and processes 4-6 TB of traces daily. In a nutshell, the flow is as follows: ‚Üí Applications use OpenTelemetry libraries/API to instrument traces and send it to open telemetry agents. ‚Üí OpenTelemetry agents process/batches and send traces from microservices to the openTelemetry gateway collector. ‚Üí The collector processes/samples the traces and export them to backends which in our case are Data Prepper and Jaeger collector. These backends transform and export the data to OpenSearch. ‚Üí Trace Analytics plugin for OpenSearch Dashboards and Jaeger visualizes trace data from OpenSearch and allows querying on it. Let‚Äôs break it down further by the flow of traces through the distributed tracing pipeline ‚Ä¶\nBreakdown\nStep 1: Creating and Propagating traces\nCreating traces\nWe need to create/propagate traces to be able to use a distributed tracing pipeline. OpenTelemetry provides a collection of tools such as API, SDK and integrates with popular languages and frameworks to integrate with the greater OpenTelemetry ecosystem, such as OTLP and the OpenTelemetry collector.\nOpenTelemetry provides a status page to keep track of its multiple tools as they go stable. Additionally, it provides documentation on how to create distributed traces for your service both manually or with auto instrumentation The documentation should get you started with creating standard OTEL traces for your service.\nPropagating traces\nOnce we have traces created for services, context propagation is required to convert these traces to distributed traces. Context propagation facilitates the movement of context between services and processes. Context is injected into a request and extracted by a receiving service. It is then used to parent new spans. That service may then make additional requests and inject context to be sent to other services‚Ä¶and so on.\nThere are several protocols for context propagation that OpenTelemetry recognizes. W3C Trace-Context HTTP Propagator B3 Zipkin HTTP Propagator This works well for our service mesh PaaS as Istio leverages Envoy‚Äôs distributed tracing feature to provide tracing integration out of the box. Specifically, Istio provides options to install various tracing backends and configure proxies to send trace spans to them automatically. It requires an application to propagate the B3 Zipkin HTTP Propagator headers so that when the proxies send span information, the spans can be correlated correctly into a single trace. It natively works with OpenTelemetry as well, since this is a supported context propagation protocol in OpenTelemetry.\nData Collection\nOnce we have the tracing data created and propagated through services, the OpenTelemetry project facilitates the collection of telemetry data via the Open Telemetry collector. The OpenTelemetry collector offers a vendor-agnostic implementation on how to receive, process, and export telemetry data. It removes the need to run, operate, and maintain multiple agents/collectors in order to support open-source observability data formats exporting to one or more backends. In addition, the collector gives end-users control of their data. The collector is the default location where instrumentation libraries export their telemetry data to.\nOpenTelemetry binary can be deployed in two primary deployment methods. For production workloads, it is recommended to go with a mix of both methods.\nThe two primary deployment methods: Agent: A collector instance running with the application or on the same host as the application (e.g. binary, sidecar, or daemonset). Gateway: One or more collector instances running as a standalone service (e.g. container or deployment) typically per cluster, data center or region.\nWe will be deploying a mix of both the agent and the gateway in our setup.\nStep 2: OpenTelemetry Agents\nWe will be deploying OpenTelemetry agents as daemonset to receive, process, and export trace from every EKS worker node. The Agent is capable of receiving telemetry data (push and pull based) as well as enhancing telemetry data with metadata such as custom tags or infrastructure information. In addition, the agent can offload responsibilities that client instrumentation would otherwise need to handle including batching, retry, encryption, compression, and more.\nThe agent can be deployed either as a daemonset or as a sidecar in a Kubernetes (k8s) cluster. This step may be skipped (not recommended), if you are creating this pipeline in a test environment and would rather send traces straight to the open telemetry collector, which is deployed as a Kubernetes deployment (horizontally scalable). Click to expand: K8s Manifest for OpenTelemetry Agent --- apiVersion: v1 kind: ConfigMap metadata: name: otel-agent-conf namespace: tracing labels: app: opentelemetry component: otel-agent-conf data: otel-agent-config: | receivers: otlp: protocols: grpc: http: exporters: otlp: endpoint: \"otel-collector.tracing:4317\" tls: insecure: true sending_queue: num_consumers: 20 queue_size: 10000 retry_on_failure: enabled: true loadbalancing: protocol: otlp: # all options from the OTLP exporter are supported # except the endpoint tls: insecure: true sending_queue: num_consumers: 20 queue_size: 10000 retry_on_failure: enabled: true resolver: dns: hostname: otel-collector.tracing port: 4317 processors: resource: attributes: - key: k8s.cluster.region value: \"region-name\" action: insert - key: k8s.cluster.name value: \"cluster-name\" action: insert - key: k8s.cluster.env value: \"environment-name\" action: insert # The resource detector injects the pod IP # to every metric so that the k8sattributes can # fetch information afterwards. resourcedetection: detectors: [\"eks\"] timeout: 5s override: true memory_limiter: check_interval: 1s limit_percentage: 50 spike_limit_percentage: 30 extensions: memory_ballast: size_in_percentage: 20 service: pipelines: traces/1: receivers: [otlp] processors: [memory_limiter, batch, resourcedetection, resource] exporters: [loadbalancing] --- apiVersion: apps/v1 kind: DaemonSet metadata: name: otel-agent namespace: tracing labels: app: opentelemetry component: otel-agent spec: selector: matchLabels: app: opentelemetry component: otel-agent template: metadata: annotations: prometheus.io/scrape: \" true\" prometheus.io/port: \" 8888\" prometheus.io/path: \" /metrics\" labels: app: opentelemetry component: otel-agent spec: containers: - command: - \" /otelcontribcol\" - \" --config=/conf/otel-agent-config.yaml\" image: otel/opentelemetry-collector-contrib:0.37.1 name: otel-agent env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP # This is picked up by the resource detector - name: OTEL_RESOURCE value: \" k8s.pod.ip=$(POD_IP)\" resources: limits: cpu: 500m #TODO - adjust this to your own requirements memory: 500Mi #TODO - adjust this to your own requirements requests: cpu: 100m #TODO - adjust this to your own requirements memory: 100Mi #TODO - adjust this to your own requirements ports: - containerPort: 55680 # Default OpenTelemetry receiver port. hostPort: 55680 - containerPort: 4317 # New OpenTelemetry receiver port. hostPort: 4317 volumeMounts: - name: otel-agent-config-vol mountPath: /conf livenessProbe: httpGet: path: / port: 13133 # Health Check extension default port. readinessProbe: httpGet: path: / port: 13133 # Health Check extension default port. volumes: - configMap: name: otel-agent-conf items: - key: otel-agent-config path: otel-agent-config.yaml name: otel-agent-config-vol --- <!-- --> Step 3: OpenTelemetry Gateway\nThe OpenTelemetry agent forwards the telemetry data to an OpenTelemetry collector gateway. A Gateway cluster runs as a standalone service and offers advanced capabilities over the agent, including tail-based sampling. In addition, a Gateway cluster can limit the number of egress points required to send data, as well as consolidate API token management. Each collector instance in a Gateway cluster operates independently, so it can be scaled with a simple load balancer. We will deploy gateway as a Kubernetes deployment type.\nK8s deployments are highly elastic and can be automatically scaled up and down via Horizontal Pod Autoscale depending on the amount of traces flowing through the pipeline. The agent-gateway architecture also allows us to use Tail Sampling Processor with more availability in this setup. Tail Sampling Processor enables us to make more intelligent choices when it comes to sampling traces. This is especially true for latency measurements, which can only be measured after they are complete. Since the collector sits at the end of the pipeline and has a complete picture of the distributed trace, sampling determinations are made in opentelemetry collector to sample based on isolated, independent portions of the trace data.\nToday, this processor only works with a single instance of the collector. The workaround is to utilize Trace ID aware load balancing to support multiple collector instances and avoid a single point of failure. This load balancer exporter is added to the agent configuration. It is responsible for consistently exporting spans and logs belonging to a trace to the same backend gateway collector for tail based sampling.\nFiltering traces for tail based sampling\nWe will use a combination of filters to sample the traces. The filters for tail sampling are positive selections only, so if a trace is caught by any of the filter, it will be sampled; Alternatively, if no filter catches a trace, then it will not be sampled. The filters for tail based sampling can be chained together to get the desired effect: latency: Sample based on the duration of the trace. The duration is determined by looking at the earliest start time and latest end time, without taking into consideration what happened in between. probabilistic: Sample a percentage of traces. status_code: Sample based upon the status code ( OK, ERROR or UNSET) rate_limiting: Sample based on rate of spans per trace per second string_attribute: Sample based on string attributes value matches, both exact and regex value matches are supported\nSince there is no blocklist for filters, we will be using a workaround to sample all but cherrypick services that we do not want sampled. we will be using the string_attribute filter to create a negative filter. The filter will be dropping specific service traces that we do not want sampled and sample all other traces. This can be done using the following: ‚Üí In the agent config, add a span attribute for all traces with a key/value pair (e.g - retain_span/false) using attribute processor. ‚Üí In the agent config, add another attribute processor now to override that value to false for cherrypicked services that we don‚Äôt want to be sampled. ‚Üí In the gateway config, use the string attribute filter on tail based sampling with the key: retain_span and value: true. All traces without retain_span: true attribute will not be sampled. Click to expand: K8s Manifest for OpenTelemetry collector --- apiVersion: v1 kind: ConfigMap metadata: name: otel-collector-conf namespace: tracing labels: app: opentelemetry component: otel-collector-conf data: otel-collector-config: | receivers: otlp: protocols: grpc: http: processors: # This processor inspects spans looking for matching attributes. # Spans that DONT MATCH this will have a new attribute added named retain-span # This is then used by the tail_sampling processor to only export spans that # have the retain-span attribute. # All conditions have to match here to be excluded. attributes/filter_spans1: exclude: match_type: strict attributes: - {key: \"foo\", value: \"bar\"} actions: - key: retain-span action: insert value: \"true\" attributes/filter_spans2: include: match_type: regexp regexp: # cacheenabled determines whether match results are LRU cached to make subsequent matches faster. # Cache size is unlimited unless cachemaxnumentries is also specified. cacheenabled: true span_names: [\"serviceA*\"] actions: - key: retain-span action: update value: \"false\" # Any policy match will make the trace be sampled!, enable regex didnt caused nothing to match tail_sampling: decision_wait: 10s expected_new_traces_per_sec: 300 policies: [ { name: policy-retain-span, type: string_attribute, string_attribute: {key: retain-span, values: ['true']} }, { name: rate-limiting-policy, type: rate_limiting, rate_limiting: {spans_per_second: 35} }, { name: probabilistic-sampling, type: probabilistic, probabilistic: {sampling_percentage: 50} }] # The k8sattributes in the Agent is in passthrough mode # so that it only tags with the minimal info for the # collector k8sattributes to complete k8sattributes: passthrough: true memory_limiter: check_interval: 1s limit_percentage: 50 spike_limit_percentage: 30 extensions: memory_ballast: size_in_percentage: 20 exporters: logging: loglevel: info otlp/data-prepper: endpoint: data-prepper-headless:21890 tls: insecure: true jaeger: endpoint: \"http://jaeger-collector.tracing.svc.cluster.local:14250\" tls: insecure: true service: pipelines: traces/1: receivers: [otlp] processors: [memory_limiter, k8sattributes, attributes/filter_spans1, attributes/filter_spans2, tail_sampling] exporters: [jaeger, otlp/data-prepper] --- apiVersion: v1 kind: Service metadata: name: otel-collector namespace: tracing labels: app: opentelemetry component: otel-collector spec: ports: - name: otlp # Default endpoint for OpenTelemetry receiver. port: 55680 protocol: TCP targetPort: 55680 - name: grpc-otlp # New endpoint for OpenTelemetry receiver. port: 4317 protocol: TCP targetPort: 4317 - name: metrics # Default endpoint for querying metrics. port: 8888 selector: component: otel-collector --- apiVersion: apps/v1 kind: Deployment metadata: name: otel-collector namespace: tracing labels: app: opentelemetry component: otel-collector spec: selector: matchLabels: app: opentelemetry component: otel-collector minReadySeconds: 5 progressDeadlineSeconds: 120 replicas: 4 #TODO - adjust this to your own requirements template: metadata: annotations: prometheus.io/scrape: \" true\" prometheus.io/port: \" 8888\" prometheus.io/path: \" /metrics\" labels: app: opentelemetry component: otel-collector spec: containers: - command: - \" /otelcontribcol\" - \" --log-level=debug\" - \" --config=/conf/otel-collector-config.yaml\" image: otel/opentelemetry-collector-contrib:0.37.1 name: otel-collector resources: limits: cpu: 2 #TODO - adjust this to your own requirements memory: 4Gi #TODO - adjust this to your own requirements requests: cpu: 1 #TODO - adjust this to your own requirements memory: 2Gi #TODO - adjust this to your own requirements ports: - containerPort: 55680 # Default endpoint for OpenTelemetry receiver. - containerPort: 4317 # Default endpoint for OpenTelemetry receiver. - containerPort: 8888 # Default endpoint for querying metrics. volumeMounts: - name: otel-collector-config-vol mountPath: /conf livenessProbe: httpGet: path: / port: 13133 # Health Check extension default port. readinessProbe: httpGet: path: / port: 13133 # Health Check extension default port. volumes: - configMap: name: otel-collector-conf items: - key: otel-collector-config path: otel-collector-config.yaml name: otel-collector-config-vol --- <!-- --> Formatting and Exporting Traces\nWe now have distributed traces created, context propagated, and sampled using tail based sampling. We need to format the trace data in a way that our query engine can analyze it. At Dow Jones, we have some teams that use Jaeger and others use OpenSearch Dashboards to query and visualize this data. With OpenTelemetry, we can push the data to multiple backends from the same collector. At present, there is one caveat, both Trace Analytics OpenSearch Dashboards plugin and Jaeger need OpenTelemetry data transformed to be able to visualize it mandating the need for last mile server-side components: Jaeger collector Jaeger is actively working toward the future Jaeger backend components to be based on OpenTelemetry collector. This integration will make all OpenTelemetry collector features available in the Jaeger backend components. Till this is experimental, Jaeger collector is required to transform the traces before shipping to OpenSearch so that Jaeger UI (query component) is able to consume and visualize the trace data. Jaeger collector is deployed as a deployment with a horizontal pod autoscaler. Click to expand: Helm Chart for Jaeger collector # All operations of service foo are sampled with probability 0.8 except for operations op1 and op2 which are probabilistically sampled with probabilities 0.2 and 0.4 respectively. # All operations for service bar are rate-limited at 5 traces per second. # Any other service will be sampled with probability 1 defined by the default_strategy. provisionDataStore: cassandra: false storage: type: elasticsearch elasticsearch: scheme: https usePassword: false host: \" opensearch-arn.us-east-1.es.amazonaws.com\" port: \" 443\" tag: 1.22.0 agent: enabled: false collector: autoscaling: enabled: true targetMemoryUtilizationPercentage: 80 service: serviceAccount: name: jaeger samplingConfig: |- { \"service_strategies\": [ { \"service\": \"foo\", \"type\": \"probabilistic\", \"param\": 0.8 }, { \"service\": \"bar\", \"type\": \"ratelimiting\", \"param\": 5 }], \"default_strategy\": { \"type\": \"probabilistic\", \"param\": 1.0 } } query: enabled: false <!-- --> Data Prepper Similar to Jaeger, to vizualize distributed traces through Trace Analytics in OpenSearch, we need to transform these traces for OpenSearch. Data Prepper is a key component in providing Trace Analytics feature in OpenSearch. Data Prepper is a data ingestion component of the OpenSearch project. It is a last mile server-side component that collects telemetry data from AWS Distro OpenTelemetry collector or OpenTelemetry collector. It then transforms it before storing and indexing in OpenSearch.\nTo pre-process documents, Data Prepper allows you to configure a pipeline that specifies a source, buffers, a series of processors, and sinks. Once you have configured a data pipeline, Data Prepper takes care of managing source, sink, buffer properties, and maintains state across all instances of Data Prepper on which the pipelines are configured. A single instance of Data Prepper can have one or more pipelines configured. A pipeline definition requires at least a source and sink attribute to be configured, and will use the default buffer and no processor if they are not configured. Data Prepper can be deployed as a deployment with a horizontal pod autoscaler. Click to expand: K8s Manifest for Data Prepper apiVersion: v1 kind: ConfigMap metadata: namespace: tracing labels: app: data-prepper name: data-prepper-config data: pipelines.yaml: | entry-pipeline: workers: 8 delay: \"100\" buffer: bounded_blocking: # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory. # We recommend to keep the same buffer_size for all pipelines. # Make sure you configure sufficient heap # default value is 512 buffer_size: 4096 # This is the maximum number of request each worker thread will process within the delay. # Default is 8. # Make sure buffer_size &gt;= workers * batch_size batch_size: 512 source: otel_trace_source: health_check_service: true ssl: false prepper: - peer_forwarder: discovery_mode: \"dns\" domain_name: \"data-prepper-headless\" ssl: false sink: - pipeline: name: \"raw-pipeline\" - pipeline: name: \"service-map-pipeline\" raw-pipeline: workers: 8 buffer: bounded_blocking: # Configure the same value as in otel-trace-pipeline # Make sure you configure sufficient heap # default value is 512 buffer_size: 4096 # The raw prepper does bulk request to your elasticsearch sink, so configure the batch_size higher. # If you use the recommended otel-collector setup each ExportTraceRequest could contain max 50 spans. https://github.com/opendistro-for-elasticsearch/data-prepper/tree/v0.7.x/deployment/aws # With 64 as batch size each worker thread could process upto 3200 spans (64 * 50) batch_size: 512 source: pipeline: name: \"entry-pipeline\" prepper: - otel_trace_raw_prepper: sink: - opensearch: hosts: - \"https://opensearch-arn.us-east-1.es.amazonaws.com\" insecure: false aws_sigv4: true aws_region: \"region-name\" trace_analytics_raw: true service-map-pipeline: workers: 1 delay: \"100\" source: pipeline: name: \"entry-pipeline\" prepper: - service_map_stateful: buffer: bounded_blocking: # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memory. # We recommend to keep the same buffer_size for all pipelines. # Make sure you configure sufficient heap # default value is 512 buffer_size: 512 # This is the maximum number of request each worker thread will process within the delay. # Default is 8. # Make sure buffer_size &gt;= workers * batch_size batch_size: 8 sink: - opensearch: hosts: - \"https://opensearch-arn.us-east-1.es.amazonaws.com\" insecure: false aws_sigv4: true aws_region: \"region-name\" trace_analytics_service_map: true data-prepper-config.yaml: | ssl: false --- apiVersion: v1 kind: Service metadata: namespace: tracing labels: app: data-prepper name: data-prepper-headless spec: clusterIP: None ports: - name: \" 21890\" port: 21890 targetPort: 21890 selector: app: data-prepper --- apiVersion: v1 kind: Service metadata: namespace: tracing labels: app: data-prepper name: data-prepper-metrics spec: type: NodePort ports: - name: \" 4900\" port: 4900 targetPort: 4900 selector: app: data-prepper --- apiVersion: apps/v1 kind: Deployment metadata: namespace: tracing labels: app: data-prepper name: data-prepper spec: replicas: 4 selector: matchLabels: app: data-prepper strategy: type: Recreate template: metadata: annotations: prometheus.io/scrape: \" true\" prometheus.io/port: \" 4900\" prometheus.io/path: \" /metrics\" sidecar.istio.io/inject: \" false\" labels: app: data-prepper spec: containers: - args: - java - -jar - /usr/share/data-prepper/data-prepper.jar - /etc/data-prepper/pipelines.yaml - /etc/data-prepper/data-prepper-config.yaml - -Dlog4j.configurationFile=config/log4j2.properties image: amazon/opendistro-for-elasticsearch-data-prepper:1.1.0 imagePullPolicy: IfNotPresent name: data-prepper resources: limits: cpu: 1 memory: 2Gi requests: cpu: 200m memory: 400Mi ports: - containerPort: 21890 volumeMounts: - mountPath: /etc/data-prepper name: prepper-configmap-claim0 - mountPath: config name: prepper-log4j2 restartPolicy: Always volumes: - name: prepper-configmap-claim0 configMap: name: data-prepper-config - name: prepper-log4j2 configMap: name: data-prepper-log4j2 --- apiVersion: v1 kind: ConfigMap metadata: namespace: tracing labels: app: data-prepper name: data-prepper-log4j2 data: log4j2.properties: | status = error dest = err name = PropertiesConfig property.filename = log/data-prepper/data-prepper.log appender.console.type = Console appender.console.name = STDOUT appender.console.layout.type = PatternLayout appender.console.layout.pattern = %d{ISO8601} [%t] %-5p %40C - %m%n rootLogger.level = warn rootLogger.appenderRef.stdout.ref = STDOUT logger.pipeline.name = com.amazon.dataprepper.pipeline logger.pipeline.level = info logger.parser.name = com.amazon.dataprepper.parser logger.parser.level = info logger.plugins.name = com.amazon.dataprepper.plugins logger.plugins.level = info --- <!-- --> Sink: OpenSearch\nAs you may have noticed in the architecture, we utilize OpenSearch to ship to and store traces. OpenSearch makes an excellent choice for storing and searching trace data, along with other observability data due to its fast search capabilities and horizontal scalability. Jaeger collector and Data Prepper can be configured to ship traces to the same OpenSearch cluster. Both deployments create their indexes with a mutually exclusive prefix for name. OpenSearch can be configured with index patterns to create seperate views:\nJaeger: jaeger-span* Trace Analytics: otel-v1-apm-span* Trace analytics and Jaeger UI (Query component) are configured to analyze their indexes based on prefix and do not collide with each other. This allows for a single OpenSearch service with multiple data formats existing side by side.\nStep 4: Visualization\nOnce you reach this point, the difficult part is over. You should have distributed traces created, context propagated, and sampled using tail based sampling, transformed, and exported to OpenSearch under different index patterns.\nNow is the part where you actually get to use the pipeline and visualize these traces to run queries, build dashboards, setup alerting. While Trace Analytics OpenSearch Dashboards plugin is a managed plugin that comes with OpenSearch, we host our own Jaeger Frontend/UI with a simple Jaeger UI to query for distributed traces with Traces View\nand Traces Detail View. We deploy Jaeger Query as a deployment within the EKS cluster. Jaeger Query can be deployed as a deployment with a horizontal pod autoscaler. Jaeger UI (Query) To visualize your traces with Jaeger we need to run a Jaeger Query. Jaeger-Query serves the API endpoints and a React/Javascript UI. The service is stateless and is typically run behind a load balancer: Click to expand: Helm Chart for Jaeger Query # All operations of service foo are sampled with probability 0.8 except for operations op1 and op2 which are probabilistically sampled with probabilities 0.2 and 0.4 respectively. # All operations for service bar are rate-limited at 5 traces per second. # Any other service will be sampled with probability 1 defined by the default_strategy. provisionDataStore: cassandra: false storage: type: elasticsearch elasticsearch: scheme: https usePassword: false host: \" opensearch-arn.us-east-1.es.amazonaws.com\" port: \" 443\" tag: 1.22.0 agent: enabled: false collector: enabled: false query: enabled: true agentSidecar: enabled: false service: type: LoadBalancer port: 443 annotations: service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \" \" service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0 config: |- { \"dependencies\": { \"dagMaxNumServices\": 200, \"menuEnabled\": true }, \"menu\": [ { \"label\": \"Open Telemetry Resources\", \"items\": [ { \"label\": \"Open Telemetry Client Instrumentation\", \"url\": \"https://opentelemetry.io/docs/\" }] }], \"archiveEnabled\": true, \"search\": { \"maxLookback\": { \"label\": \"7 Days\", \"value\": \"7d\" }, \"maxLimit\": 1500 } } spark: enabled: false esIndexCleaner: enabled: false esRollover: enabled: false esLookback: enabled: false <!-- --> Trace Analytics feature in OpenSearch/OpenSearch Dashboards Trace Analytics for Opensearch Dashboards can be leveraged without having to re-instrument applications. Once you have traces flowing through your pipeline, you should see indexes being created in OpenSearch under the otel-v1\\* index prefix. As this is created, trace analytics plugin within OpenSearch Dashboards should now have visualization for your traces, inclusive of service map, error/throughput graphs, and waterfall trace view for your services. Testing\nWe now have the distributed tracing pipeline. It is time to start an application that generates traces and exports them to agents. Depending on your application code and whether you want to do manual or automatic instrumentation, the Open Telementry Instrumentation Docs should get you started.\nIn this setup, OTEL agents are running as daemonset. We will be exporting traces from the service to the agent on the same host(worker-node). To get the host IP, we can set the HOST_IP environment variable via the Kubernetes downwards API. It can be referenced in our instrumentation as the destination address.\nMake sure you are injecting the HOST_IP environment variable in your application manifest: env:\n- name: HOST_IP\nvalueFrom:\nfieldRef:\nfieldPath: status.hostIP To setup the pipeline, you can apply the Kubernetes manifest files and helm chart(for Jaeger). Besides being in the body of this blog, the manifests have been added to opensearch-blog github repo for convenient access.\nNext Steps\nWe built a scalable distributed tracing pipeline based on the OpenTelemetry project running in an EKS cluster. One of the driving forces behind our move to OpenTelemetry is to consolidate to a single binary to ingest, process, and export, not just traces but metrics and logs as well (telemetry data). Telemetry data can be exported to multiple backends by creating pipelines for pushing to observability platform of choice.\nData Prepper 1.2 (December 2021) release is going to provide users the ability to send logs from Fluent Bit to OpenSearch and use Grok to enhance the logs. Logs can then be correlated to traces coming from the OTEL collectors to further enhance deep diving into your service problems using OpenSearch Dashboards. This should elevate the experience of deep diving into your service with a unified view and give a really powerful feature into the hands of developers and operators.\nSo the next step would naturally be to extend this pipeline to be more than just a distributed tracing pipeline and morph to a distributed telemetry pipeline! Comments, questions, corrections? Just let me know on Twitter, via email kuber.kaul@dowjones.com, or submit a github issue Thanks to EP team, Chris Nelligan.\nArchitecture images created using cloudcraft.io",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/introducing-opensearch-benchmark/",
    "title": "Introducing OpenSearch Benchmark",
    "content": "We would like to introduce OpenSearch Benchmark - a community driven, open source fork of Rally. The mission of the project is to provide a convenient and reliable performance testing framework for OpenSearch. OpenSearch Benchmark intends to be compatible with OpenSearch and Amazon OpenSearch Service. OpenSearch Benchmark is currently in alpha release and will be maintained under the Apache License, Version 2.0 (ALv2).\nWhy is this important for the OpenSearch community?\nUsers of OpenSearch often want to understand how their workloads perform across different settings, host types, and cluster configurations. OpenSearch Benchmark helps you to optimize OpenSearch cluster resource usage to reduce operating costs. A performance testing mechanism also empowers you to run periodic benchmarks which can be used to identify performance regressions and improve performance.\nSounds interesting? Here‚Äôs what you can do!\nWe encourage you to join us and be a part of the growing OpenSearch Benchmark community! Please install OpenSearch Benchmark and view the source on the OpenSearch Benchmark Github repository. We‚Äôd like you to participate in the ongoing discussions and if you want to request any changes(bugs, feature requests), please create an issue here.\nIf you‚Äôre interested in contributing to the project, we‚Äôd be more than happy to welcome you. There‚Äôs already a curated set of issues with the tag help wanted that you can get started on right away!",
    "keywords": [
      "update"
    ],
    "type": "News"
  },
  {
    "url": "/blog/connecting-java-high-level-rest-client-with-opensearch-over-https/",
    "title": "Connecting to OpenSearch over HTTPS using the Java High-Level REST Client",
    "content": "If you have ever used OpenSearch with a Java application, you might have come across the OpenSearch Java high-level REST client. The REST client provides OpenSearch APIs as methods, and makes it easier for a Java application to interact with OpenSearch using request/response objects.\nIn this blog, you will learn how to configure the Java high-level REST client to connect to OpenSearch over HTTPS. For demonstration purposes, I‚Äôll first setup an OpenSearch server with SSL certificates. If you already have one running, you may skip this step. Next, I‚Äôll walk you through the steps to configure and use the Java High level REST client in your Java application.\nSetup OpenSearch Server\nStart by downloading the latest Linux distribution from the OpenSearch website. I have used the 1.2.0 version which is the latest available version at the time of this writing. You can also download this version with wget command as shown below. wget https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.0/opensearch-1.2.0-linux-x64.tar.gz Next, unzip the downloaded tar.gz file. tar -xf opensearch-1.2.0-linux-x64.tar.gz The OpenSearch distribution comes pre-installed with the security plugin. As part of the initial setup, you are required to setup the SSL certificates for encrypting the client to node and the node to node communication channels. The distribution comes with a tool for setting up demo certificates for a quick getting started experience. However, it is highly recommended to use certificates from a trusted Certification Authority (CA). You could also setup self-signed certificates by following this documentation. For the purpose of this blog, I have used the demo certificates available with the install_demo_configuration.sh tool. cd opensearch-1.2.0/ # Provide executable permissions to the tool if missing chmod +x plugins/opensearch-security/tools/install_demo_configuration.sh # Install demo ssl certificates with install_demo_configuration.sh tool./plugins/opensearch-security/tools/install_demo_configuration.sh -y -i -s Once the certificates are setup, increase the default vm.max_map_count limit and start the OpenSearch cluster. # Increase mmap count limit sudo sysctl -w vm.max_map_count = 262144 # To start the OpenSearch cluster bin/opensearch At this point, your server is ready. You can verify using the below curl command. curl -k -XGET https://localhost:9200 -u 'admin:admin' You should see a response similar to - { \"name\": \"smoketestnode\", \"cluster_name\": \"opensearch\", \"cluster_uuid\": \"2oMfOxJKQcmrDZ1e0OgXKw\", \"version\": { \"distribution\": \"opensearch\", \"number\": \"1.2.0\", \"build_type\": \"tar\", \"build_hash\": \"15e9f137622d878b79103df8f82d78d782b686a1\", \"build_date\": \"2021-10-04T21:29:03.079792Z\", \"build_snapshot\": false, \"lucene_version\": \"8.9.0\", \"minimum_wire_compatibility_version\": \"6.8.0\", \"minimum_index_compatibility_version\": \"6.0.0-beta1\" }, \"tagline\": \"The OpenSearch Project: https://opensearch.org/\" } Now that you have setup the OpenSearch server, it‚Äôs time to move on to the client.\nSetup Java High Level REST Client\nThe OpenSearch Java High Level REST Client is available on Maven Central. Add it as a dependency to your Java application.\nFor Gradle build system, include the following dependency in your project‚Äôs build.gradle file: dependencies { implementation 'org.opensearch.client:opensearch-rest-high-level-client:1.2.0' } For Maven build system, include the following dependency in your project‚Äôs pom.xml file: &lt;dependency&gt; &lt;groupId&gt; org.opensearch.client &lt;/groupId&gt; &lt;artifactId&gt; opensearch-rest-high-level-client &lt;/artifactId&gt; &lt;version&gt; 1.2.0 &lt;/version&gt; &lt;/dependency&gt; Next, create an instance of RestHighLevelClient in your Java application and use that to create an index and ingest some data into OpenSearch. But before going there, hold on a sec! Remember, while setting up the server you configured SSL certificates to enable HTTPS (and disabled HTTP)? Now, since these server certificates are just demo certificates and not provided by any trusted Certificate Authority (CA), they won‚Äôt be trusted by your Java application to establish an SSL connection. In order to make it work, you‚Äôll need to add the root authority (that signed the server certificate) certificate to your application truststore. Let‚Äôs see how to configure the Java application truststore.\nJava applications (by default) use the JVM truststore, which holds certificates from the trusted Certified Authorities (CA), to verify the certificate presented by the server in an SSL connection. You can use the Java keytool to see the list of trusted CAs in your JVM truststore. keytool -keystore $JAVA_HOME /lib/security/cacerts -storepass changeit -list To use the RestHighLevelClient, you need to add the root CA certificate root-ca.pem to the application truststore. This tells your Java application to trust any certificates signed by this root authority. The install_demo_configuration.sh tool created the root-ca.pem file in opensearch-1.2.0/config/ directory while setting up the server. You can either add it to the JVM truststore, or add it to a custom truststore and use that custom truststore in the Java application. I used the custom truststore approach to keep the JVM truststore clean.\nUse the Java keytool to create a custom truststore and import the root authority certificate. The keytool does not understand the.pem format, so you‚Äôll have to first convert the root authority certificate to.der format using openssl cryptography library and then add it to the custom truststore using Java keytool. Most Linux distributions already come with openssl installed.\nStep 1: Convert the root authority certificates from.pem to.der format. openssl x509 -in opensearch-1.2.0/config/root-ca.pem -inform pem -out root-ca.der --outform der Step 2: Create a custom truststore and add the root-ca.der certs.\nAdding the root authority certificate to the application truststore tells the application to trust any certificate signed by this root CA. keytool -import root-ca.der -alias opensearch -keystore myTrustStore Confirm the action was successful by listing certs in truststore. The grep should be able to find opensearch alias if the certs were added successfully. keytool -keystore myTrustStore -storepass changeit -list | grep opensearch Step 3: Set the truststore properties in the Java application code to point to the custom truststore. // Point to keystore with appropriate certificates for security. System. setProperty ( \"javax.net.ssl.trustStore\", \"/full/path/to/myCustomTrustStore\"); System. setProperty ( \"javax.net.ssl.trustStorePassword\", \"password-for-myCustomTrustStore\"); Step 4: Create an instance of the client and connect to OpenSearch over HTTPS. // Create the client. RestClientBuilder builder = RestClient. builder ( new HttpHost ( \"localhost\", 9200, \"https\")). setHttpClientConfigCallback ( new RestClientBuilder. HttpClientConfigCallback () { @Override public HttpAsyncClientBuilder customizeHttpClient ( HttpAsyncClientBuilder httpClientBuilder) { return httpClientBuilder. setDefaultCredentialsProvider ( credentialsProvider); } }); RestHighLevelClient client = new RestHighLevelClient ( builder); // Create an index with custom settings. CreateIndexRequest createIndexRequest = new CreateIndexRequest ( \"custom-index\"); createIndexRequest. settings ( Settings. builder () //Specify in the settings how many shards you want in the index.. put ( \"index.number_of_shards\", 4). put ( \"index.number_of_replicas\", 1)); CreateIndexResponse createIndexResponse = client. indices (). create ( createIndexRequest, RequestOptions. DEFAULT); // Adding data to the index. IndexRequest request = new IndexRequest ( \"custom-index\"); //Add a document to the custom-index we created. request. id ( \"1\"); //Assign an ID to the document. HashMap &lt; String, String &gt; stringMapping = new HashMap &lt; String, String &gt;(); stringMapping. put ( \"message:\", \"Testing Java REST client\"); request. source ( stringMapping); //Place your content into the index's source. IndexResponse indexResponse = client. index ( request, RequestOptions. DEFAULT); Checkout this documentation for complete sample code.\nCongratulations! You have now successfully setup the Java high level REST client and connected to OpenSearch on a secure HTTPS channel.\nHope this post helped you in getting started with OpenSearch in your Java application. Moving forward, we are looking at ways in which we can further simplify the getting started experience for OpenSearch, and would like to seek your feedback in OpenSearch#1618.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/update-to-1-2-1/",
    "title": "Important: Update to OpenSearch 1.2.1",
    "content": "Update 2021-12-22\nOpenSearch version 1.2.3 is now available and addresses CVE-2021-44228, CVE-2021-45046, and CVE-2021-45105. Update your clusters to 1.2.3. See the release blog.\nUpdate Dec 14, 2021\nThe new CVE-2021-45046 affecting Log4j 2.15.0 was published earlier today, and OpenSearch 1.2.1 includes that version of Log4j. As with the previous issue, the team has not been able to reproduce this issue, nor any patterns thought to trigger this issue. Out of an abundance of caution, and, as indicated in the advisory, we will release a new version of OpenSearch shortly that uses Log4j 2.16.0 which is not affected by this new CVE. In addition, we are releasing a version of the Logstash OSS with OpenSearch Output Plugin bundle which resolves both CVE-2021-44228 and CVE-2021-45046. Open Distro 1.13.3 is not affected by CVE-2021-45046 as the JndiLookup mitigation resolves both this and the original security issue.\nTL;DR: Upgrade your clusters to 1.2.1.\nA recently published security issue ( CVE-2021-44228) affects several versions of the broadly-used Apache Log4j library. Some software in the OpenSearch project includes versions of Log4j referenced in this CVE. While, at time of writing, the team has not found a reproduceable example in OpenSearch of remote code execution (RCE) described in this issue, its severity is such that all users should take mitigation measures. As recommended by the advisory, the team has released OpenSearch 1.2.1, which updates Log4j to version 2.15.0. For those who cannot upgrade to 1.2.1, the Log4j website outlines additional measures to mitigate the issue. This patch release also addresses CVE-2021-4352 in the OpenSearch Docker distributions.\nOpenSearch 1.2.1 is available for both the full and minimal distributions. You can get version 1.2.1 on the downloads page and on Docker Hub.\nSome other software in the project also includes versions of Log4j that are referenced in the CVE and have been fixed or mitigated. Open Distro for Elasticsearch 1.13.3 Data Prepper 1.1.1 for OpenSearch and Data Prepper 1.0.1 for Open Distro Logstash OSS with OpenSearch Output Plugin Do you have questions or feedback?\nIf you‚Äôre interested in learning more, have a specific question, or just want to provide feedback and thoughts, please visit OpenSearch.org, open an issue on GitHub, or post in the forums. There are also regular Community Meetings that include progress updates at every session and include time for Q&amp;A.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Introducing-Data-Prepper-1.2.0-with-Log-Pipelines/",
    "title": "Introducing Data Prepper 1.2.0 with Log Pipelines",
    "content": "Development teams are using Data Prepper and OpenSearch to better understand their applications through Trace Analytics. Now teams can\nuse Data Prepper and OpenSearch for log analysis as well. You can start using Data Prepper 1.2.0 for\nlog ingestion today. This release enables log collection into OpenSearch from Fluent Bit, supports Logstash configuration files, and provides other improvements.\nLog Ingestion\nThe Problem with Logs\nOpenSearch is a great tool for searching log data and developers are already using it for log analysis.\nModern applications are distributed, often containing multiple services running across different servers.\nDevelopers want to analyze their log data centrally to more quickly address customer issues and improve\napplication performance, availability, and security.\nGetting log data into OpenSearch isn‚Äôt easy though. Application logs are generally unstructured data.\nThey are lines of text which humans can read, but are not great for searching and analysis at large scale.\nDevelopers need to structure their log data in OpenSearch, enabling then to search for key events. Say, for\nexample, finding slow HTTP requests during a certain time window.\nThe Solution\nData Prepper now supports log ingestion into OpenSearch using FluentBit. FluentBit\nis a popular Apache-licensed log forwarder. It runs alongside an application, reads log files, and forwards them\nto a destination over HTTP. Data Prepper receives these log lines using the new http source plugin.\nData Prepper saves each log line as an individual OpenSearch document.\nThis diagram outlines the basic architecture for log ingestion using FluentBit, Data Prepper, and OpenSearch. In this release, Data Prepper also provides a method to structure log data using the grok prepper. Pipeline authors can configure a grok prepper based on the known log format. Data Prepper will extract the specified parts of the log lines into specific fields in the\ndestination OpenSearch document. The grok prepper uses patterns to extract parts of incoming log data. It has a number of\npredefined patterns and allows authors to create custom patterns using regular expressions. Here are just a few common\npredefined patterns Data Prepper supports. INT - an integer value UUID - A UUID IP - an IP address in IPv4 or IPv6 IPORHOST - either an IP address or a hostname MONTH - the name of a Gregorian calendar month in the full name (January) or abbreviated form (Jan) LOGLEVEL - common log levels such as ERROR, WARN, DEBUG\nTo configure Data Prepper for log ingestion, you will create a pipeline with an http source, grok prepper, and opensearch sink.\nThe following diagram shows the flow of data through this type of pipeline. An Example\nI‚Äôm going to illustrate this solution using Apache HTTP logs as an example. Apache HTTP is a popular web server and it\nlogs HTTP requests in Common Log Format.\nBelow is a sample log line in the Common Log Format. 127.0.0.1 - admin [30/Nov/2021:11:34:28 -0500] \"GET /hello/server HTTP/1.1\" 200 4592 The line above has value information which developers want to put into fields in OpenSearch documents. Pipeline authors use\nthe grok prepper to extract the different parts of the log line into individual fields.\nTo extract these fields, a pipeline author can create a grok prepper with the following configuration. - grok:\nmatch:\nlog: [ \"%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)\"] The configuration above will add new fields to the OpenSearch document. The following table shows some of the fields that it adds from the example above. Document field name Meaning Document field value from the example log line above clientip\nThe requestor IP address\n127.0.0.1\nauth\nThe username of the requestor\nadmin\ntimestamp\nTimestamp of the request\n30/Nov/2021:11:34:28 -0500\nverb\nHTTP method/verb\nGET\nrequest\nThe request path\n/hello/server\nhttpversion\nThe version of HTTP used\nHTTP/1.1\nresponse\nHTTP status code in the response\n200\nbytes\nThe size in bytes of the response\n4592 The Apache Common Log format is a common format. Data Prepper actually has a shortcut pattern to perform the same extraction. A pipeline author can configure grok to extract data from Apache - grok:\nmatch:\nlog: [ \"%{COMMONAPACHELOG}\"] You can read the Log Analytics guide and try out a complete demo to start working with log ingestion today.\nCompatibility with Logstash Configuration Files\nSome teams are already using Logstash for log ingestion. Data Prepper 1.2.0 parses your existing\nLogstash configuration files and creates a similar pipeline. This new feature supports simple Logstash\nconfigurations. But, it is designed for extensibility. Expect to see expanded support for more complex\nLogstash configuration files in future versions. Data Prepper has a Logstash migration guide to help you start migrating.\nOther Improvements\nData Prepper 1.2.0 has a few other notable improvements. Default Data Prepper configuration: Data Prepper has a default configuration file which you can use to get started quickly. It runs the core API with SSL using a demo certificate and secures the core API with a default username and password. Configurable Authentication: The existing OTel Trace Source plugin already accepts OpenTelemetry Protocol trace data. Now both the existing OTel Trace Source and the new HTTP source plugins support configurable authentication. You can configure a username and password for HTTP Basic authentication. Or you can create a plugin with custom authentication. Plugin Framework: The new plugin framework improves the plugin author‚Äôs experience through a more flexible approach to creating plugins. It is also preparing future expansions for more community-driven plugins. Proxy Support for OpenSearch: You can now connect Data Prepper to an OpenSearch cluster via a proxy when your network requires it. Log4j Fix: This release also uses Log4j 2.16.0 which fixes CVE-2021-44228 and CVE-2021-45046. It also disables JNDI as a security hardening against similar exploits.\nNext Steps\nWith Data Prepper 1.2.0 out, the team is already working on Data Prepper 1.3.0.\nThis release will focus on more ways to enrich log data. Current community needs drive the list of prioritized features.\nWhat do you want to see in Data Prepper? You can impact the project roadmap in a few ways. First, comment on or up-vote issues in GitHub which you find valuable. Second, if you\ndon‚Äôt see it on the roadmap, please create a Feature request.\nFinally, Data Prepper is open-source and open to contributions.\nYou can develop the feature you are looking for. Express your interest in working an issue and a Data Prepper maintainer\nwill be happy to help you get started.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/update-1-2-2/",
    "title": "Update to OpenSearch 1.2.2",
    "content": "Update 2021-12-22\nOpenSearch version 1.2.3 is now available and addresses CVE-2021-44228, CVE-2021-45046, and CVE-2021-45105. Update your clusters to 1.2.3. See the release blog.\nUpdate your clusters to 1.2.2\nAs indicated in the previous blog post, CVE-2021-45046 was issued shortly following the release of OpenSearch 1.2.1. This new CVE advises upgrading from Log4j 2.15.0 (used in OpenSearch 1.2.1) to Log4j 2.16.0. Out of an abundance of caution, the team is releasing OpenSearch 1.2.2 which includes Log4j 2.16.0. While there has been no observed reproduction of the issue described in CVE-2021-45046, Log4j 2.16.0 takes much more extensive JNDI mitigation measures.\nIf you are currently using OpenSearch 1.2.1 or below you should upgrade to 1.2.2. If you cannot upgrade to OpenSearch 1.2.2, follow the mitigations as described on the Log4j security page. Either action should be taken as soon as possible.\nIf you are using Open Distro 1.13.3, released as a response to CVE-2021-44228, no further action is needed as the original mitigation also addresses CVE-2021-45046. You should update Data Prepper 1.0.x or 1.1.x to 1.0.2 or 1.2.0 respectively. Currently, Logstash OSS with OpenSearch Output Plugin requires mitigation by classpath removal and the team will release a new version of the distribution using Logstash 7.16.2 which includes Log4j 2.16.0 shortly. Update: The team has released a new version of Logstash OSS with OpenSearch Output Plugin that uses Logstash 7.16.2 and includes Log4j 2.16.0.\nYou can get OpenSearch 1.2.2 on the downloads page.\nDo you have questions or feedback?\nIf you‚Äôre interested in learning more, have a specific question, or just want to provide feedback and thoughts, please visit OpenSearch.org, open an issue on GitHub, or post in the forums. There are also regular Community Meetings that include progress updates at every session and include time for Q&amp;A.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/oracle-announcement/",
    "title": "Partner Highlight: Oracle offers OpenSearch as a service",
    "content": "Oracle Cloud Infrastructure - Welcome to the OpenSearch community!\nWe are excited to see Oracle Cloud Infrastructure‚Äôs (OCI) announcement of a fully managed search service based on OpenSearch. As Rob Walters, Vice President, Data Management at Oracle, puts it; ‚ÄúWe‚Äôre pleased to make OCI for OpenSearch, a fully managed search and analytics service, available to customers. Supporting the OpenSearch and open-source community is of utmost importance as we want customers everywhere to be able to deploy OpenSearch and Open Dashboards without having to worry about security, patching, updates, and backups. We look forward to contributing and working together with the OpenSearch community in the future.‚Äù In their announcement, OCI also published a reference architecture for aggregating logs with OCI for OpenSearch in the OCI Architecture Center. If you interested in taking a look, the OCI public beta is available now ( access form) with an upcoming global release in 2022. Congrats to everyone at OCI involved! We‚Äôre excited to welcome Oracle to the OpenSearch community and can‚Äôt wait to see what 2022 holds!\nThe goal of the OpenSearch project continues to be to make it easy for as many people and organizations as possible to use OpenSearch in their business, their products, and their projects. Whether you are an independent developer, an enterprise IT department, a software vendor, or a managed service provider, the ALv2 license ensures you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. As always, broad adoption benefits all members of the community.\nGet involved!\nWe would love to see you contribute to OpenSearch (if you‚Äôre not already)! For almost any type of contribution, the first step is opening an issue. Even if you think you already know what the solution is, writing a description of the problem you‚Äôre trying to solve will help everyone get context when they review your pull request.\nInterested in sharing your OpenSearch story? The community partners and testimonials pages continue to grow and we would be happy to add your story!\nDo you have questions or feedback?\nIf you‚Äôre interested in learning more, have a specific question, or just want to provide feedback and thoughts, please visit OpenSearch.org, open an issue on GitHub, or post in the forums. There are also regular community meetings that include progress updates at every session and include time for Q&amp;A.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/update-1-2-3/",
    "title": "Update to OpenSearch 1.2.3",
    "content": "Update your clusters to 1.2.3 CVE-2021-45105 for Log4j was issued after the release of OpenSearch 1.2.2. This CVE advises upgrading to Log4j 2.17.0. While there has been no observed reproduction of the issue described in CVE-2021-45105 in OpenSearch, we have released OpenSearch 1.2.3 which updates Log4j to version 2.17.0.\nIn addition to OpenSearch, additional artifacts were released with Log4j 2.17.0 updates including:\nData Prepper 1.2.1, which updates Log4j to version 2.17.0.\nOpen source Logstash 7.16.2 with opensearch-output plugin, which includes a fix for CVE-2021-45105 provided by the maintainers of Logstash.\nYou can get OpenSearch 1.2.3 and other updated artifacts on the downloads page.\nDo you have questions or feedback?\nIf you‚Äôre interested in learning more, have a specific question, or just want to provide feedback and thoughts, please visit OpenSearch.org, open an issue on GitHub, or post in the forums. There are also regular Community Meetings that include progress updates at every session and include time for Q&amp;A.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/calyptia-partner-blog-announcement/",
    "title": "Calyptia and OpenSearch Partnering to Add Connectors for Fluent Bit and Fluentd",
    "content": "We are excited to share that Calytpia and the OpenSearch project team are partnering to build OpenSearch connectors for Fluent Bit and Fluentd. These open source Cloud Native Computing Foundation (CNCF) graduated projects are commonly used for log collection, processing, and forwarding. What is Fluent Bit? Fluent Bit is a log processor and forwarder for collecting data, like logs, from different sources, enriching them with filters, and sending them to multiple destinations. It‚Äôs designed with performance in mind, meaning it is optimized for high throughput and low CPU and memory usage. It‚Äôs written in C and has a pluggable architecture supporting more than 70 plugins for inputs, filters, and outputs. What is Fluentd? Fluentd is a data collector for log data collection, processing, and forwarding. It‚Äôs written in Ruby and supports over 500 plugins including data sources, data output, parsers, formatters, and filters.\nAs maintainers for Fluentd and Fluent Bit, Calyptia brings a wealth of knowledge for the Fluent projects, and we look forward to working together to add OpenSearch connectors for these popular tools. As Eduardo Silva, Co-founder of Calyptia and Creator of Fluent Bit, puts it; ‚ÄúSince the launch of OpenSearch, we have seen consistently increasing demand for production-ready OpenSearch connectors for Fluent Bit and Fluentd. We are eager to be partnering with the OpenSearch project team to build these connectors and provide an end-to-end vendor neutral and open source solution for collecting, parsing, storing, and analyzing log data.‚Äù We are excited to have Fluent Bit and Fluentd join the range of tools like Data Prepper and the OpenSearch Logstash Output Plugin that help people collect, process, and sending data to OpenSearch. To learn more and track the progress on these connectors, you can checkout the Fluentd GitHub repo and Fluent Bit GitHub repo.\nGet involved!\nWe would love to see you contribute to OpenSearch, Fluent Bit, and Fluentd! For almost any type of contribution, the first step is opening an issue. Even if you think you already know what the solution is, writing a description of the problem you‚Äôre trying to solve will help everyone get context when they review your pull request.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/dashboards-plugins-intro/",
    "title": "Introduction to OpenSearch Dashboard Plugins",
    "content": "Plugins are fundamental to how OpenSearch works, and the similarity extends to OpenSearch Dashboards too. Infact almost everything that you see inside OpenSearch Dashboards is built inside a plugin. As a follow up to the blog post on how plugins work for OpenSearch, this post will explore how plugins work for OpenSearch Dashboards.\nWhat is a plugin\nTo understand what a plugin in OpenSearch Dashboards is, first it‚Äôs important to understand what makes up OpenSearch Dashboards. It has 3 main components:\nCore: The main runtime responsible for managing the lifecycle of the application and all its main services\nPackages: A set of static utilities that can be imported and used throughout the application (Both by core and plugins).\nPlugins: All other major functionality within OpenSearch Dashboards.\nPlugins are a way to extend and customize the core functionality of OpenSearch Dashboards. They do not need to be a part of the Dashboards repository, though many are! They makeup some of the core applications and services within it.\nPlugins are classes that can be loaded via the Dashboards plugin API to integrate with the core system via lifecycle methods. Plugins can consist of either client side code (public), server side code (server), or both. Plugins can also interact with each other and core from both client and server. Plugins must also contain a manifest file that describes a set of properties, both required and optional that core system can use to load and initialize the plugin correctly.\nA plugin is usually made up of two parts:\nManifest file ( opensearch_dashboards.json)\nPlugin definition that implements an instance of the Plugin class\nIf a plugin has a server and client side code, each section needs to describe the class separately. The interface for each remains the same. A typical plugin has the following folder structure: my_plugin/\n‚îú‚îÄ‚îÄ opensearch_dashboards.json\n‚îú‚îÄ‚îÄ public\n‚îÇ ‚îú‚îÄ‚îÄ applications\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ my_app\n‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.ts\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.ts\n‚îÇ ‚îú‚îÄ‚îÄ services\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ my_service\n‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.ts\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.ts\n‚îÇ ‚îú‚îÄ‚îÄ index.ts\n‚îÇ ‚îî‚îÄ‚îÄ plugin.ts\n‚îî‚îÄ‚îÄ server\n‚îú‚îÄ‚îÄ routes\n‚îÇ ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ collectors\n‚îÇ ‚îî‚îÄ‚îÄ register.ts\n‚îú‚îÄ‚îÄ saved_objects\n‚îÇ ‚îú‚îÄ‚îÄ index.ts\n‚îÇ ‚îî‚îÄ‚îÄ my_type.ts\n‚îú‚îÄ‚îÄ services\n‚îÇ ‚îú‚îÄ‚îÄ my_service\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ index.ts\n‚îÇ ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ index.ts\n‚îî‚îÄ‚îÄ plugin.ts More details on the plugin structure can be found in the Conventions readme under the core.\nManifest file\nThe role of the manifest file is to describe the set of required and optional properties of a plugin such as plugin name version and other required plugins.\nThe manifest file signature is defined by the interface PluginManifest { \"id\": \"plugin_id\", \"version\": \"1.0.0\", \"opensearchDashboardsVersion\": \"opensearchDashboards\", \"server\": true, \"ui\": true, \"requiredPlugins\": [ \"data\", \"savedObjects\"], \"optionalPlugins\": [] } Plugin definition\nAll the remaining plugin logic is exposed via a single class that implements the Plugin interface. This is done so that it is easy for other plugins and the core system to discover and use a plugins features using a well defined API.\nLifecycle methods\nEvery plugin has 3 life cycle methods that the core system calls during the lifecycle of a plugin.\nSetup: when the plugin is registered and initialized\nStart: is where any ‚Äúrunning‚Äù logic for your plugin would live. This only applies if you need to start listening for outside events (polling for work, listening on a port, etc.)\nStop: Used to cleanup runtime. // my_plugin/public/index.ts import { PluginInitializer } from '../../src/core/public '; import { MyPlugin, MyPluginSetup, MyPluginStart } from './plugin '; export const plugin: PluginInitializer &lt; MyPluginSetup, MyPluginStart &gt; = () =&gt; new MyPlugin (); export { MyPluginSetup, MyPluginStart } // my_plugin/public/plugin.ts import { CoreSetup, CoreStart, Plugin } from '../../src/core/public '; import { OtherPluginSetup, OtherPluginStart } from '../other_plugin '; import { ThirdPluginSetup, ThirdPluginStart } from '../third_plugin '; export interface MyPluginSetup { registerThing (...); } export interface MyPluginStart { getThing (): Thing; } export interface MyPluginSetupDeps { otherPlugin: OtherPluginSetup; thirdPlugin?: ThirdPluginSetup; // Optional dependency } export interface MyPluginStartDeps { otherPlugin: OtherPluginStart; thirdPlugin?: ThirdPluginStart; // Optional dependency } export class MyPlugin implements Plugin &lt; // All of these types are optional. If your plugin does not expose anything // or depend on other plugins, these can be omitted. MyPluginSetup, MyPluginStart, MyPluginSetupDeps, MyPluginStartDeps, &gt; { public setup ( core: CoreSetup, plugins: MyPluginSetupDeps) { // should return a value that matches `MyPluginSetup` } public start ( core: CoreStart, plugins: MyPluginStartDeps) { // should return a value that matches `MyPluginStart` } public stop () {... } } How does all this work?\nNow that I‚Äôve talked about what makes up a plugin, let‚Äôs take a look at how it all works. The job of discovering, initializing and running plugins within OpenSearch Dashboards is handled by the plugin service within the core system. It begins when you start OpenSearch Dashboards using yarn start. The core performs the following actions:\nRead the config file opensearch_dashboards.yml Discover the plugins and construct a dependency tree. Both core plugins (plugins in the projects source that come with OpenSearch Dashboards) and external plugins\nLoad plugin specific config (if present)\nRun the osd:bootstrap script within each plugin if its public code is not yet built.\nCompletes setup for each plugin loaded and starts the plugins\nThe core system also manages each plugins lifecycle.\nHow to use them?\nUnderstanding the specifics of how the core system sets up and handles these plugins can get a little confusing. Thankfully working with plugins does not require an in depth understanding of its inner workings. With the help of some useful packages, the process of creating, loading, distributing plugins is made much simpler.\nCreating a plugin\nTo create a plugin, we can use the @osd/plugin-generator package to bootstrap a plugin template for us. To use it you can run in your terminal the command node scripts/generate_plugin.js my_plugin_name # replace \"my_plugin_name\" with your desired plugin name It will ask you a few questions about the plugin you want to generate and scaffold a plugin template that is ready to run in the appropriate plugin directory.\nLoading a plugin\nTo load a plugin into an instance of OpenSearch Dashboards, the plugin must be placed in one of the configured paths that the core system will search during its plugin discovery phase. There are 3 default paths from which plugins can be discovered:./src/plugins./plugins../opensearch-dashboards-extra Placing the plugin in one of these paths will ensure that they are discovered by the core system. The default paths are defined by the env class within the @osd-config package and discovered using the plugin service‚Äôs discover method. Alternatively, we can also define an additional config parameter additionalPluginPaths to define other explicit plugin paths. This is however not recommended and will throw a warning in production.\nBuilding and distributing your plugin\nTo build the distributable archive of your plugin run yarn build Generated plugins receive a handful of scripts that can be used during development. Those scripts are detailed in the README.md file in each newly generated plugin, and expose the scripts provided by the OpenSearch Dashboards plugin helpers. It also contains a script to build a distributable archive of your plugin.\nExample plugins\nSometimes it‚Äôs also useful to see some example plugins that solve a specific problem following the recommended practices. OpenSearch Dashboards comes with some example plugins that help us better understand how plugins work, what are some the core plugins offered by OpenSearch Dashboards and how to use them. Passing an additional flag --run-examples during startup loads these plugins. Setting the --run-examples flag also adds the examples folder to the list of paths used for plugin discovery yarn start --run-examples You can find examples and their documentation in the examples folder.\nClosing notes\nI hope that this post was useful in understanding how plugins work in OpenSearch Dashboards, and hopefully this makes working with them less daunting. Let me or the team know if you have any feedback or would like new features in plugin architecture.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-as-a-service/",
    "title": "Partner Highlight: How to Offer OpenSearch as a Service using Virtuozzo DevOps PaaS",
    "content": "How to Offer OpenSearch as a Service using Virtuozzo DevOps PaaS\nOne of the great things about Platform as a Service is the flexibility it brings to service providers and end users. Really it‚Äôs a way to provide anything as a service, any application, and we‚Äôre pleased to announce that OpenSearch is the latest certified container available in the Virtuozzo DevOps PaaS solution.\nOpenSearch is a community-driven, open-source search engine for website search, application monitoring, data aggregation and analysis.\nIt provides a secure, high-quality search and analytics suite that can be easily modified and extended with additional functionality, and now it‚Äôs easy to deploy with automatic clustering using Virtuozzo DevOps PaaS.\nVirtuozzo DevOps PaaS is powered by Jelastic, a division of Virtuozzo. Read on to learn more about OpenSearch, how to deploy it with auto-clustering, and how to get started!\nOpenSearch Cluster Components\nVirtuozzo DevOps PaaS provides an OpenSearch Cluster using the following components united into a single auto-clustering solution: OpenSearch - an open-source search engine that provides a distributed, multitenant-capable full-text search OpenSearch Dashboards (optional) - visualization for data stored inside the OpenSearch nodes (derived from Kibana 7.10.2) Logstash (optional) - data processing Beats (optional) - can be installed as add-on for single-purpose data shippers that send data from the client nodes to Logstash or OpenSearch\nIn such a solution, data is gathered on client nodes by Beats Data Shippers, sent to Logstash for the required transformation, and stored in OpenSearch. OpenSearch Dashboard is the supplementary visualization tool. OpenSearch Cluster Installation\nThe OpenSearch Cluster creation within Virtuozzo DevOps PaaS is a straightforward and fully automated process that can be performed directly from the topology wizard.\nSelect the OpenSearch stack at the NoSQL database section. Auto-clustering will be enabled by default and provide the required configurations. You can also add the OpenSearch Dashboards and Logstash components by activating the respective options.\nMore detail on the OpenSearch components available in DevOps PaaS can be found in our OpenSearch Cluster documentation.\nReady to Get Started?\nIf you need a high-performance and easy-to-manage OpenSearch Cluster, give it a try at one of the Virtuozzo DevOps PaaS service providers Virtuozzo DevOps PaaS service providers.\nIf you are interested in offering OpenSearch Cluster as a service to your customers ‚Äì or any other flavor of PaaS ‚Äì we‚Äôd love to help. Get in touch to learn more!",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/data-prepper-1.2.1-performance-testing/",
    "title": "Data Prepper 1.2.1 Performance Testing",
    "content": "Following the launch of log pipeline in Data Prepper 1.2, the Data Prepper Team are excited to share the results of the Data Prepper performance testing suite. The goal is to create a tool that can simulate a set of real-world scenarios in any environment while maintaining compatibility with popular log ingestion applications. In all performance test results discussed below, the test environments and configurations are identical, except where the same option is not available for all applications. See testing limitations section for additional details.\nData Prepper 1.2.1 has 88% better sustained throughput than Logstash 7.13.2\nData Prepper 1.2.1 has 46% lower mean response latency than Logstash 7.13.2. Response latency here is, the amount of time it takes from when a request is made by the client to the time it takes for Data Prepper‚Äôs or Logstash‚Äôs response to get back to that client. Throughout the test, Data Prepper can consistently maintain a higher throughput. Logstash hit peak latency of 7,382ms Data Prepper‚Äôs peak latency was 5,276ms ¬† Data Prepper 1.2.1 Logstash 7.13.2 Throughput\n3.73 MB/s\n1.98 MB/s\nMean response (milliseconds) 53 ms\n99 ms\nTotal logs processed within 30 minute test\n68,166,000\n36,206,800 Data Prepper 1.2.1 processed 88% more logs than Logstash 7.13.2 within the 30-minute test limit.\nData Prepper Environment Logstash Environment Comparing the performance of the latest release of Data Prepper 1.2.1 against Logstash 7.13.2. The tests are configured to simulate 10 clients to send requests as frequently as possible. Each request was 19.2 KB and contained a batch of 200 logs. The test ran for 30 minutes, measuring the latency and throughput.\nApplication Configurations Property Data Prepper Logstash HTTP Source Thread\n4\n4\nMax Connection Count\n2,000\nn/a *\nRequest Timeout\n10,000\nn/a *\nBuffer Size\n2,000,000\nn/a *\nBatch Size\n5,000\n5,000\nWorker Thread\n12\n12\nSSL\nDisabled\nDisabled [*] Note, Max Connection Count, Request Timeout, Buffer Size are not configurable with Logstash 7.13.2 Data Prepper Scaling\nIn this simulation clients are ramped up from 1 to 60 over one hour to measure the impact concurrent clients have on performance. As the number of clients increases Data Preppers throughput remains constant, processing an average of 10,473 MB/m.\nIt‚Äôs important to note that Data Prepper 1.2.1 and Logstash 7.13.2 support different feature sets, and the performance test suite is targeted at common functionality. As Data Prepper adds more processors and sources, test cases will be added. The Data Prepper team will continue update the community with performance benchmarks.\nCandidates for future performance testing scenarios and improvements In a real-world deployment, if Data Prepper is unable to keep pace with the logs generated by the source application. Data Prepper will become a bottleneck causing backpressure on the source application. In the future performance tests will be enhanced to simulate backpressure and measure the impact.\nThe scope of this initial performance testing scenarios was focused on a common scenario, http source, grok processor, and an OpenSearch sink. As new features are added to Data Prepper in upcoming releases, performance testing simulations will be added to cover core functionality.\nPerformance suite source code is available on GitHub. To run the full test suite execute./gradlew ‚Äìrerun-tasks gatlingRun -Dhost=&lt;target url&gt;. After all tests have completed HTML reports will be created in./build/reports/gatling/&lt;simulation-name&gt;-&lt;unix-timestamp&gt;/index.html. Further instructions on running performance tests and Gatling are available in the repository readme.\nOn identical hardware, Data Prepper 1.2.1 maintained 88% faster throughput in the scenarios simulated and offered lower latencies. If there is a performance critical scenario you would like to see the results of consider opening a feature request. If you are interested in trying Data Prepper for yourself checkout the getting started guide.\nTable [1] - AWS Environment Details Name EC2 Instance Type Instance Count vCPU Memory (GiB) JVM Memory Limit (GiB) Data Prepper\nm5.xlarge\n1\n4\n16\n4\nData Prepper Prometheus + Grafana\nm5.xlarge\n1\n4\n16\n¬†\nData Prepper OpenSearch Cluster\ni3.xlarge\n3\n4\n30.5\n¬†\nLogstash\nm5.xlarge\n1\n4\n16\n4\nLogstash Prometheus + Grafana\nm5.xlarge\n1\n4\n16\n¬†\nLogstash OpenSearch Cluster\ni3.xlarge\n3\n4\n30.5\n¬†\nGatling\nm5.2xlarge\n1\n8\n32\n¬† Latest Performance Test Results\nFollow this link to see the Latest Performance Test Results",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/feature-deep-dive-opensearch-sql-basic-queries/",
    "title": "Feature Deep Dive: OpenSearch SQL - Basic queries",
    "content": "In this post, we‚Äôll explore some of the features and capabilities supported by OpenSearch SQL. We also briefly introduce how SQL engine works internally.\nWhat is OpenSearch SQL?\nOpenSearch SQL is similar to the SELECT statement in ANSI SQL, but it is purely designed for analyzing data in OpenSearch. In this document, we refer to it as OSQL (short for OpenSearch Structured Query Language). In order to make it easier for traditional SQL user to get started, we make the OSQL syntax conform to the subset of the ANSI SQL specification. In addition, we also disclosed the unique features of OpenSearch, for example, we support full-text search.\nHow to use OpenSearch SQL?\nFor OpenSearch users, you could use the familiar REST interface _plugin/sql with your query in HTTP body.\nFor DBMS users, you could use JDBC/ODBC driver to connect OpenSearch domain.\nFor Dashboards users, you could use OpenSearch Dashboard Query Workbench to easily run on-demand SQL queries and download results.\nAn additional OpenSearch SQL CLI tool is provided for interactive SQL execution.\nSome Examples\nBasic Queries ‚ÄúFind error logs where response code is 404 or 503‚Äù. You can run OSQL in Query Workbench. The results are in tabular format. You could download result in JSON, JDBC, CSV and Text format. More examples are on Github. Functions ‚ÄúFind error logs where response code is 404 or 503, How many distinct host‚Äù You could also run OSQL from a OpenSearch Dashboards notebook. In this case, there are four distinct hosts and the results are in table format. More aggregations and functions are supported. Explain ‚ÄúHow does OSQL execute in OpenSearch?‚Äù. The OpenSearch SQL explain endpoint returns the query execution plan. For example, here are the three query operators for the above query. ProjectOperator, execute in coordinate node, read output from child operator and project originCountry and count fields. FilterOperator, execute in coordinator node, read output from child operator and filter docs which count &gt; 500. OpenSearchIndexScan, it is a DSL query which will be executed in OpenSearch through search endpoint. Inside SQL Engines\nInternally, a query will go through five major components in the query engine. (1) Language Processor parses the query string by following the grammar and generates the AST (Abstract Syntax Tree). (2) Core Engine analyzes and optimizes the AST and builds the Logical Plan. (3) Storage Engine is a pluggable component which provides the catalog schema and storage specified optimization and implementation. (4) Execution Engine schedules and executes the physical plan. (5) Protocol parses the request and formats the response. How do I contribute?\nIf you‚Äôre interested in contributing please reach out on GitHub issues or the community forum. The more formal contribution guidelines are documented in the contributing guide.",
    "keywords": [
      "feature"
    ],
    "type": "News"
  },
  {
    "url": "/blog/partner-hidora/",
    "title": "Partner Highlight: Enjoy OpenSearch one click installation with Hidora",
    "content": "Hidora is an expert in providing custom solutions for all sizes of businesses. Hidora Cloud Hosting Solutions are just what you need if you want an efficient, secure and scalable cloud hosting solution that is tailored to your needs. We offer different plans that are scalable according to your specific requirements so you can find one that suits you best. Our PaaS help developers to host their applications in few minutes with DevOps philosophy.\nWith our partner Jelastic, we have integrated the deployment of the OpenSearch stack in every detail. We are currently able to offer you version 1.2.3. Also, redeploy functionality provides a simple update process. You can scale your OpenSearch cluster, extend the storage, add nodes in few seconds.\nOne click installation with the Topology wizard!\nTake advantage of our customization interface to define the architecture of your cluster: Activate or not OpenSearch-Dashboard and Logstash\nDefine how many nodes will be create on OpenSearch cluster\nAssign public IPv4 or IPv6 or manage access through our Shared Load Balancer (SLB)\nScale nodes resources by managing in real time Cloudlet assignation\nAfter being created, the OpenSearch node is available on port 9200 for internal communication between nodes of the environment and port 4848 for external services through the shared load balancer. The HTTP basic auth is used in both cases (password can be reset using the Reset Password button). All the interconnection configurations are done automatically - the solution is ready for usage out-of-the-box and does not need any manual configuration. Access to OpenSearch Dashboard is available by the same credentials as for the OpenSearch node. Enjoy one-click Beat agent addon\nYou already use Hidora for your accommodation and you want to monitor these with OpenSearch? We have an add-on to deploy agents in one click and thus report logs and metrics to the OpenSearch stack. Bringing a touch of observability to your environments has never been easier. Manage easily your Logstash pipelines\nUse internal file explorer to easily manage Logstash configuration. FTP addon can also be deployed to improve configuration management. Integrated monitoring for your environnement\nWatch your cluster through the monitoring integrated to our platform, you will also receive alerts when there is a shortage of resources. Need to go even further?\nWe provide secure SSH access to your platform as well as the ability to work with custom Docker image to fine tune your cluster. The OpenSearch stack benefits from all the functionality available for other certified templates - dashboard file manager, Web SSH, dashboard log viewer, etc. The next version of our OpenSearch service will see the appearance of observability modules with Data Prepper, trace analytics bringing OpenTelemetry standard compatibility. As usual, the deployment will be done in one click though our wizard.\nThe pricing model of Hidora is different from other cloud vendor. At Hidora, you pay only what you consume!\nFor example, you have created a 16Gb of RAM server, and you application consume only 2Gb of memory, you will pay only for 2Gb.\nDuring peak loads, Hidora allocates more resources for applications and takes them back when they are no longer needed. Our data centers are located in SafeHost Datacenters certified ISO9001, ISO27001, ISAE3402 &amp; PCIDSS which guarantee you exceptionally low risk of flooding, landslides or earthquakes. When it comes to confidentiality, Switzerland is one of the best countries for storing your data, due to the Data Privacy Law\nInterested in OpenSearch Swiss Cloud service? Discover an ultra-fast solution and deploy your environment today. Opensearch Swiss Cloud on Hidora.io",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/getting-started-with-fluentd-and-opensearch/",
    "title": "Getting started with Fluentd and OpenSearch",
    "content": "The OpenSearch project is, a community-driven open-source search and analytics suite derived from Apache 2.0 licensed Elasticsearch 7.10.2 and Kibana 7.10.2. In order to get started with OpenSearch you will need to get data into OpenSearch. A new method, built through a partnership of Calyptia and the OpenSearch project, is native plugins for Fluentd. Fluentd is an open-source log and metrics processor that is part of the Cloud Native Computing Foundation (CNCF).\nIn this getting started guide we go over how you can use the new Fluentd plugin to collect data from your infrastructure, containers, and/or network devices and bring them into OpenSearch.\nUse Cases\nFor users not familiar with Fluentd, one of its strengths is the ability to collect data from multiple sources through plugins and add formatting, processing, or enrichment before sending it out to multiple outputs. There are over 500+ community plugins for Fluentd that can read data including enterprise staples like Kafka, as well as data heavy backends like Hadoop and MongoDB.\nA few popular use cases for Fluentd that now support OpenSearch as a backend include:\nIntegration with reading and writing from Kafka\nIngesting long term data into long term data stores (E.g. Hadoop, Google Cloud storage, Amazon S3)\nMigrating from proprietary backends by routing data to multiple destinations\nListening on syslog for firewalls and networks devices and performing GeoIP or other enrichment\nRouting application logs from apps running in Kubernetes\nHow to get the new plugin?\nThere are two ways to get the new OpenSearch plugin that we will go over in this guide:\nCalyptia Fluentd packages\nDownloading as a Ruby gem\nOf course the new Fluentd OpenSearch plugin is fully Apache 2.0 open source and available here: https://github.com/fluent/fluent-plugin-opensearch.\nCalyptia Fluentd packages\nAs Calyptia, we maintain a distribution of Fluentd, named Calyptia Fluentd. These packages contain all the downstream dependencies of Ruby and SSL, without you needing to maintain that yourself.\nWith the latest 1.3.4 release of Calyptia Fluentd, the OpenSearch plugin is included by default.\nDownload and install the package Red Hat Enterprise Linux / CentOS / Amazon Linux Debian / Ubuntu Windows MacOSX Adding configuration\nWe can create a barebones configuration that allows us to send the message ‚Äúdummy‚Äù to OpenSearch. We can store this configuration under /etc/calyptia-fluentd/calyptia-fluentd.conf &lt;source&gt;\n@type dummy\ntag dummy\ndummy {\"hello\":\"world\"}\n&lt;/source&gt;\n&lt;match dummy&gt;\n@type opensearch\nhost localhost\nport 9200\nindex_name fluentd\n&lt;/match&gt; Running Calyptia Fluentd\nTo run Calyptia Fluentd and start sending data to OpenSearch we can use the command line or restart the services that are configured during installation. For example, on Mac we can run the following command: sudo launchctl load /Library/LaunchDaemons/calyptia-fluentd.plist Downloading as a Ruby gem\nIf you are already using Fluentd or another distribution you can also retrieve the OpenSearch plugin by downloading the Ruby Gem. The Ruby Gem can be downloaded by running the following command:\nRunning the gem install command gem install fluent-plugin-opensearch Adding a configuration We can create a barebones configuration that allows us to send the message ‚Äúdummy‚Äù to OpenSearch. We can store this configuration under /etc/fluent.conf &lt;source&gt;\n@type dummy\ntag dummy\ndummy {\"hello\":\"world\"}\n&lt;/source&gt;\n&lt;match dummy&gt;\n@type opensearch\nhost localhost\nport 9200`\nindex_name fluentd\n&lt;/match&gt; Running Fluentd\nOnce completed you will then be able to run the following command to use the configuration used above. fluentd -c /etc/fluent.conf Getting involved\nWith this getting started guide we talked through how you can retrieve Fluentd packages and get started with the Fluentd plugin for OpenSearch. There are tons of additional plugins for Fluentd to explore connecting to other sources and backends.\nTo join the conversation be sure to sign up to the Fluent Slack channel, as well as leave your feedback on the OpenSearch forums.",
    "keywords": [
      "technical"
    ],
    "type": "News"
  },
  {
    "url": "/blog/a-quick-opensearch-primer/",
    "title": "A Quick OpenSearch Primer",
    "content": "First time here?\nHave you only recently heard of OpenSearch and are curious just what it is? Here‚Äôs some help getting on your feet. First, about indexing and full text search as a core function. Also, how OpenSearch extends those things into something more than a search library. OpenSearch provides a basis for many useful workflows meant to analyze, visualize, and aggregate data.\nSo what is OpenSearch then?\nOpenSearch is an open-source project built on top of Apache Lucene, a powerful indexing and search library. Even if you‚Äôve never heard of an indexing library before, you‚Äôre still probably more familiar with indexing than you think. The quantity of data businesses maintain in this age necessitates it. Apache Lucene provides OpenSearch the ability to maintain large volumes of full documents, arbitrary text as well as non-textual data while maintaining several indices to make searching through each of them efficient.\nWhat is it used for?\nThe use cases for OpenSearch are many. Here are a few popular examples. Log ingestion takes advantage of the full-text search capabilities against the unpredictability of data in log files. Indexing all the various data types encountered makes finding specific tokens (say, an IP address or a specific error message from a specific host) in a collection of log lines a short task. In distributed applications, this is even more useful. Summarize and aggregate an entire cluster‚Äôs logs using OpenSearch Dashboards. Throw in a bit of anomaly detection and some visualizations for some extra insight. This example is a stacked graph of response codes over time. Certain events are configured to trigger an anomaly warning. It‚Äôs a short example of what visualizations are capable of. Building your own custom search engine is another use for which OpenSearch is fully capable. Using the OpenSearch client library gives you access to ranked search, field weighting, and many other features common for search engines. Results are scored based on the frequency of the terms searched, how many documents contain that term, and how many times they contain it, amongst other criteria. OpenSearch can weigh the results as well. For example, matches against a document‚Äôs title can be considered a more meaningful match than matching terms in a document‚Äôs content.\nSee for yourself.\nThe quickest way to get hands-on is to bring up OpenSearch. Inspect the sample datasets and visualizations to get a feel for it. It is the straightest path to exemplifying what all can be done. Visit the downloads page and follow the instructions to get started.\nAfter logging into OpenSearch Dashboards with admin:admin you‚Äôll see the home screen. Click ‚ÄòAdd Data‚Äô in the upper right or ‚ÄòAdd Sample Data‚Äô in the lower left - you‚Äôll be shown three sample data sets. There‚Äôs no harm in installing all three of them. There‚Äôs nothing like seeing it in action! The sample data sets offered come with dashboards that are excellent examples of the visualizations and how the information is structured. What you see here is the dashboard that comes with some test data for a fake airline company. Heat maps, stacked graphs, pie charts, gauges and bar charts are available, among others. A wide selection of visualizations are available. Arrange them on a Dashboard to get a view that‚Äôs just right. All set, but it‚Äôs just the start.\nHopefully some of the uncertainty about what OpenSearch is and what it does has been cleared up. Its base functionality has a wide variety of use cases, such as building a custom search engine. Also, ingesting logs for the ease of searching and aggregating. Both make use of the strong indexing and search library under the hood. OpenSearch Dashboards extends this even further by allowing you to build the exact view you want. Even ones that can warn you of anomalies in your data.\nA closing thought. Anyone can help. Really. This project wouldn‚Äôt be where it is today without the community. One of the project‚Äôs principles of development is to be ‚ÄúOpen source like we mean it.‚Äù The participation of anyone willing will always be welcome. Assistance and contributions of any kind are helpful whether you‚Äôre a blogger, coder, tech writer, graphic designer, thought leader or jack of all trades. Not all contributions are code! If you‚Äôve enjoyed this, become part of the community! Check out the project page on GitHub, and the public roadmap. Also, join the community meetings. Last but not least, get involved in discussions and chat with other community members in the OpenSearch Forums. All are welcome and come as you are.",
    "keywords": [
      "intro"
    ],
    "type": "News"
  },
  {
    "url": "/blog/shard-indexing-backpressure-in-opensearch/",
    "title": "Shard Indexing Backpressure in OpenSearch",
    "content": "In this post we wish to dive into the Shard Indexing Backpressure feature which got launched with OpenSearch 1.2.0 and how it can improve cluster reliability. The Indexing APIs in OpenSearch such as _bulk allows you to write data in the cluster, which is distributed across multiple shards, on multiple data nodes. However, at times indexing requests may suffer performance degradation due to a number of reasons including non-optimal cluster configuration, shard strategy, traffic spikes, available node resources and more. These issues are further exacerbated for larger multi-node cluster and indices with many shards. All of these could cause out-of-memory errors, long garbage collection (GC) pauses, and reduced throughput, affecting the overall availability of data nodes in addition to degrading performance. This in turn would impact the node‚Äôs ability to perform useful work. In addition, these node drop events could cascade due to a lack of effective backpressure which puts the entire cluster at risk.\nIndexing Backpressure offers a real-time solution to address such issues, particularly for multi-node clusters with large data sets that have high throughput requirements. Indexing backpressure offers selective load-shedding of indexing requests when performance thresholds are exceeded. For example, in the case of too many stuck requests or request in the cluster running too long, indexing backpressure would kick in. This not only protects clusters from these types of cascading failures, but also ensures that the cluster can continue to perform as expected by only isolating the impacted paths. By tracking real-time performance on per shard basis in the background and evaluating them against dynamic thresholds, Indexing Backpressure applies appropriate in-time rejections decision at the shard-request level whenever a shard or a node suffers a duress situation affecting its ability to keep up with the other shards/nodes in the cluster. This prevents wide failures while also guaranteeing fairness in request execution. What is Shard Indexing Backpressure protection and why is it important? OpenSearch provides few control mechanisms today to prevent a data node meltdown for bulk indexing requests. These are essentially achieved by queue rejections, circuit breakers, and node-level memory limits for indexing traffic. However, these gating mechanisms are all static and reactive. Thread pool queue sizes are static in nature and do not change dynamically in real time. Every incoming request in the queue consumes resources differently. Circuit Breakers can track real-time memory utilization but most of them apply blanket level rejections without any fairness or identifying resources required for the requests. Node level Indexing Pressure limit framework is designed to reject overwhelming indexing requests based on some pre-defined static limits on nodes. It is similar to the blanket level rejections offered by circuit breakers.\nTherefore, there is a need for smarter rejection mechanism at a granular level i.e. per shard request. This should help identify the underlying reason (slow indexing requests or stuck indexing tasks) for a performance breach in a cluster. Once the issue is detected, there is a need for an effective propagation of backpressure, transferring the load from the overwhelmed node to other nodes that are still healthy. Thus the healthy nodes are capable of taking smarter backoff and short circuit decisions while processing these requests.\nHow this feature solves the problem\nShard Level Indexing Backpressure introduces dynamic rejections of indexing requests when there are too many stuck or slow requests in the cluster, breaching key performance thresholds. This prevents the nodes in cluster to run into cascading effects of failures under arising due to: Performance Degradation ‚Äì Indexing performance for a shard starts to indicate slowness overtime, such as due to configuration changes, resource contention, or poison-pill requests. Slow Nodes ‚Äì Entire data node starts to slowdown due to degraded hardware such as disk, network volumes, or software bugs such as thread-pool blocks, mutex locks. Stuck Requests (Blackhole) - Requests get stuck indefinitely and takes forever to responds back due to a slow downstream node. However the upstream node continues to accept requests faster than they are being processed. Surge in traffic ‚Äì Sudden surge or spike in the traffic to a leading quick build up across the nodes. Skewness in shard distribution ‚Äì Improper distribution of shards leading to hot spots, bottlenecks, affecting the overall performance.\nBelow are some of the key features offered by the shard level indexing backpressure, which helps address the above scenarios:\nGranular tracking of indexing tasks performance, at every Shard level, for each Node role i.e. coordinator, primary and replica.\nFair and Selective rejections by discarding the requests intended only for problematic path touching impacted indices or shards, while still allowing other requests to continue without any downtime.\nRejections thresholds are governed by combination of configurable parameters (such as memory limits on node) and dynamic parameters (such as latency increase, throughput degradation).\nNode level and Shard level indexing pressure statistics are exposed to users through stats API.\nControl knobs to tune the key performance thresholds which governs the rejection decisions, to address any specific cluster requirements.\nControl knobs to run the feature either in Shadow-Mode or Enforced-Mode. Shadow-mode is a dry-run mode where only rejection breakdown metrics are published while no actual rejections are performed.\nStep-by-step walk through\nShard level indexing backpressure focuses on dynamically assigning memory quota from the total memory pool to each shard based on its operational needs (i.e. the incoming workloads). A combination of shard level memory utilization and throughput enables OpenSearch to decide whether it should process a request. This also prevents other nodes in the cluster from running into cascading failures due to the performance degradation caused by slow node, stuck tasks, resource intensive requests, traffic surge, skewed shard allocation, etc. Shard level indexing backpressure depends on primary and secondary parameters to make decisions.\nPrimary Parameters\nPrimary parameters, are leading indicators for node duress and are governed through soft limits. We have two such primary parameters: Shard memory limit breach: The shard memory limit is breached if the allocated shard level memory limit is exceeded and there is a need to increase the allocation. At any given point the memory distribution algorithm will try to maintain that the current memory utilization of shard approximately 85% of the assigned shard memory. If the occupancy goes beyond 95%, this parameter is assumed to be breached. Node memory limit breach: The node memory limit is breached if the total allocated node level memory limit for indexing requests is exceeded. At any point the current memory utilization at the node level goes beyond 70% of total assigned node memory for indexing work, this parameter is assumed to be breached.\nBreach of primary parameters reflect no actual rejections yet, but triggers an evaluation of the secondary signals.\nSecondary Parameters\nThese parameters are based on granular, shard level performance such as request throughput and successful completion of requests. While these shard states are updated with every request, they are evaluated only when the primary parameters are breached. We have introduced two secondary parameters: Throughput: The framework tracks request latency per unit byte at the shard level and any substantial decrease in the shard throughput against its historic performance is considered a degradation. Successful Request: The framework also tracks successful indexing requests at the shard level and signals if there are large number of pending requests. This is used to detect memory build-ups due to stuck requests proactively and invoke the required backpressure.\nIndexing backpressure takes effect when one of the primary parameters and secondary parameters‚Äô thresholds are breached on the node. This results in rejection of new incoming requests/tasks for the affected shards on the node.\nThe mechanism used by indexing backpressure is fair. The selective rejections of requests are intended only for problematic index or shard, while still allowing others to continue. Below is the shard state transition diagram which covers these scenarios. Stats API For Shard Indexing Backpressure The performance and rejection statistics collected by the framework are exposed through REST APIs for users to have added visibility. GET /_nodes/_local/stats/shard_indexing_pressure Returns the Node level stats for indexing request rejections. GET /_nodes/stats/shard_indexing_pressure Returns the shard level breakup of indexing stats and rejections for every node. This only includes the hot shards (i.e. shards which have active writes currently going on them). GET /_nodes/_local/stats/shard_indexing_pressure?include_all Returns the Shards level breakup of indexing stats and rejections for every node. This includes all the shards, which either have active writes or had recent writes on them. GET /_nodes/_local/stats/shard_indexing_pressure?top Returns the top level aggregated indexing stats on rejections. Here the per shard statistics are not provided.\nSample response {\n\"_nodes\": {\n\"total\": 1,\n\"successful\": 1,\n\"failed\": 0\n},\n\"cluster_name\": \"runTask\",\n\"nodes\": {\n\"q3e1dQjFSqyPSLAgpyQlfw\": {\n\"timestamp\": 1613072111162,\n\"name\": \"runTask-0\",\n\"transport_address\": \"127.0.0.1:9300\",\n\"host\": \"127.0.0.1\",\n\"ip\": \"127.0.0.1:9300\",\n\"roles\": [\n\"data\",\n\"ingest\",\n\"master\",\n\"remote_cluster_client\"],\n\"attributes\": {\n\"testattr\": \"test\"\n},\n\"shard_indexing_pressure\": {\n\"stats\": {\n\"[index_name][0]\": {\n\"memory\": {\n\"current\": {\n\"coordinating_in_bytes\": 0,\n\"primary_in_bytes\": 0,\n\"replica_in_bytes\": 0\n},\n\"total\": {\n\"coordinating_in_bytes\": 299,\n\"primary_in_bytes\": 299,\n\"replica_in_bytes\": 0\n}\n},\n\"rejection\": {\n\"coordinating\": {\n\"coordinating_rejections\": 0,\n\"breakup\": {\n\"node_limits\": 0,\n\"no_successful_request_limits\": 0,\n\"throughput_degradation_limits\": 0\n}\n},\n\"primary\": {\n\"primary_rejections\": 0,\n\"breakup\": {\n\"node_limits\": 0,\n\"no_successful_request_limits\": 0,\n\"throughput_degradation_limits\": 0\n}\n},\n\"replica\": {\n\"replica_rejections\": 0,\n\"breakup\": {\n\"node_limits\": 0,\n\"no_successful_request_limits\": 0,\n\"throughput_degradation_limits\": 0\n}\n}\n},\n\"last_successful_timestamp\": {\n\"coordinating_last_successful_request_timestamp_in_millis\": 1613072107990,\n\"primary_last_successful_request_timestamp_in_millis\": 0,\n\"replica_last_successful_request_timestamp_in_millis\": 0\n},\n\"indexing\": {\n\"coordinating_time_in_millis\": 96,\n\"coordinating_count\": 1,\n\"primary_time_in_millis\": 0,\n\"primary_count\": 0,\n\"replica_time_in_millis\": 0,\n\"replica_count\": 0\n},\n\"memory_allocation\": {\n\"current\": {\n\"current_coordinating_and_primary_bytes\": 0,\n\"current_replica_bytes\": 0\n},\n\"limit\": {\n\"current_coordinating_and_primary_limits_in_bytes\": 51897,\n\"current_replica_limits_in_bytes\": 77845\n}\n}\n}\n},\n\"total_rejections_breakup\": {\n\"node_limits\": 0,\n\"no_successful_request_limits\": 0,\n\"throughput_degradation_limits\": 0\n},\n\"enabled\": true,\n\"enforced\": true\n}\n}\n}\n} How do I contribute to the feature?\nIf you have any feedback/suggestions on new topics please don‚Äôt hesitate to open issues or post to the forums.\nIf you‚Äôd like to contribute, a great place to start would be the links below.\nMeta Issue and Related PRs: https://github.com/opensearch-project/OpenSearch/issues/478 https://github.com/opensearch-project/OpenSearch/pull/1336",
    "keywords": [
      "feature"
    ],
    "type": "News"
  },
  {
    "url": "/blog/roadmap-proposal/",
    "title": "Update: OpenSearch Proposed 2022 Release Schedule",
    "content": "As part of thinking about releases for this year, I‚Äôve been trying to sketch out a schedule of major and minor release dates through 2023. I wanted to share with you what I have so far.\nConsiderations\nBefore getting to the dates themselves, here are the things I was thinking about when I picked them. The OpenSearch Project uses SemVer. As a reminder, the OpenSearch project assigns version numbers to releases following the semantic versioning convention. This means the project won‚Äôt introduce an incompatible change without also incrementing the major version number. Everyone is excited about Lucene 9.0.0. Lucene 9.0.0 released on December 7, 2021. It includes several new features and performance improvements that the team would like to make available to users of OpenSearch, including K-NN support, Vectors, Big Endian, faster numeric indexing, faster sorting, concurrent merge scheduler, and prototype Java Jigsaw module support. Starting to use Lucene 9.0.0 as soon as possible is a priority. It will take a few releases to leverage the full value of it, but adding it now is so exciting and has loads of potential for OpenSearch. The project will follow a ‚Äúrelease train‚Äù model for minor-version releases, and a ‚Äútentpole‚Äù model for major-version releases. The team wants to produce minor-version releases on a ‚Äúrelease-train‚Äù schedule: every six weeks a new release comes out, and it includes all the new features that are ready at the time of release. Having a set release schedule makes sure OpenSearch is releasing in a predictable way and prevents a backlog of unreleased changes. In contrast, major-version releases will take place when there is a critical mass of new features that would create incompatibilities, since that‚Äôs the only opportunity to release them. These are called ‚Äútentpole‚Äù features because getting one of these in might hold a release. If any of the tentpole changes runs into serious problems, the contributors working on it will need to add a comment on their meta tracking issue with an alert. At that point, the contributors working on the issue and I can decide whether to cut scope, remove the feature from the release, or move the release date depending on the type and severity of the problem.\nProposed schedule\nTaking the above into consideration, here‚Äôs how I think the majors and minors would lay out on the calendar. If you‚Äôre planning on submitting a code change, note the corresponding feature freeze dates (the date when no more changes can be added so the release can be tested). Version Feature Freeze Release Date 1.3.0\nMarch 9th\nMarch 15th\n1.3.2\nApril 29 2022\nMay 05 2022\n2.0.0 RC-1\nCore: March 21st\n¬†\n¬†\nPlugins and Clients: April 18th April 26th April 28th\n2.0.0 GA May 2nd May 17th May 12th May 24th\n1.3.3\nJune 3rd\nJune 9th\n2.1.0 June 23rd June 30th June 30th July 7th\n1.3.4 July 1st July 8th July 7th July 14th\n2.2.0\nAugust 4th\nAugust 11th\n1.3.5\nAugust 16th\nAugust 23rd\n3.0.0 RC\nOpenSearch Core/Dashboards: August 11th\n¬†\n¬†\nPlugin and Clients:\nSeptember 14th\n1.3.6\nSeptember 2nd\nSeptember 8th\n2.4.0\nSeptember 22nd\nSeptember 29th\n1.3.7\nSeptember 30th\nOctober 6th\n1.3.8\nNovember 4th\nNovember 10th\n2.5.0\nDecember 1st\nDecember 8th\n3.0.0 GA\nJanuary 10th, 2023\nJanuary 19th 2023 (Updated April 2022 to include patch releases to 1.x, Moved the 2.0-rc date by 2 days, moved 3.0-RC out 1 week)\n(Updated May 2022 to move RC GA date out 12 days)\n(Updated June 2022 to move 2.1 date out 5 days which bumps 1.3.4)\nYou‚Äôll notice I‚Äôve included a ‚ÄúPreview Release‚Äù for our major releases in 2022 to help folks get an early look. What the exact release process will look like and how feedback for the previews can be identified is yet-to-be determined, but as the first major releases I want to make sure everyone has a chance to try them out early. I‚Äôve only planned dates for one Release Candidate per major release, but if we need more we would add them.\nAlso, I‚Äôd like to increase how often OpenSearch is able to release minor version from 6 weeks to 4 by the end of the year and reduce the length of feature freeze required, but that will depend on our ability to add automation. For now I‚Äôve planned with 6 week intervals, but when the team is ready I‚Äôll reset the dates.\nThemes for the Major releases\n2.0.0\nAs mentioned above, the primary driver for the team to have an earlier 2.0.0 release is so that OpenSearch can get support for Lucene 9.0.0 in earlier. This change will likely enable some use of new Lucene features within the plugins, like KNN and Vector Field types. This release also allows for other breaking changes to be ready earlier, like inclusive naming.\n3.0.0\nSince Lucene 9 will be released with 2.0, the 3.0 release will contain the bulk of breaking changes planned for 2022. That list is not written in stone, but we already looking ahead to things like segment replication, inbuilt security and sandboxing. I‚Äôm excited to work with you to make those improvements, and to see your thoughts on how to make things even better.\nOne final note‚Ä¶.\nLike all schedules, it‚Äôs possible that it will need to change based on real world conditions cough log4j cough. As the year goes along, if better ways to do things are found, or someone has a brilliant idea for a feature, those changes will be made. If nothing else, in January of 2023 everyone can look back at this schedule with a wry smile at the bold confidence of February 2022.\nFeedback?\nPlease make it known in the forums! Specifically:\nDoes this list of considerations make sense? What else should be taken into consideration?\nAre there any errors in the way the dates are laid out?\nWhat would a useful ‚Äúpublic preview‚Äù look like to you? What would you want to get out of it?\nAs the public roadmap is populated with features, I look forward to also hearing your feedback on what‚Äôs going in as well.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/auto-backport-in-opensearch/",
    "title": "Auto Backport in OpenSearch",
    "content": "Backporting is a common process in OpenSearch in order to maintain release branches separate from main and to be ready for a release. This blog post talks about how the team navigated backports manually in OpenSearch and pursued automation to streamline the process and use it across various repos.\nMotivation Backporting is the process of cherry-picking specific commits from the main branch on to release branches according to semantic versioning. Previously in OpenSearch, to backport commits, developers had to cherry-pick the commits from the main branch on to a release branch which causes manual effort and can lead to missed backports before a release.\nIn order to alleviate the manual effort, the team built a backport GitHub Action with custom features to automate the end-to-end process of backporting.\nChallenges and Learnings\nThe backport GitHub Action creates a backport pull request (PR) automatically when appropriate labels are added to the original PR. Upon merging of the original PR, the GitHub Action will open backport PR by creating a new branch, cherry-picking the original commit on it and pushing this branch to create a PR.\nIn this blog post I would like to talk about the challenges that came along while integrating auto-backports and the approaches taken to resolve them incrementally.\nThe backport workflow did not have the permissions to create PRs from a backport branch using the GITHUB_TOKEN. This happened since the GITHUB_TOKEN has read permissions for all scopes unless specified otherwise in the workflow. This was resolved by adding a permissions key in the workflow file in order to provide the required scope (in this case related to pull-requests). Additionally, these updated permissions do not work when used on pull_request target events since such events can only get write permissions if the PR is within the source repo; cross-repo PRs do not get write permissions. For this purpose, pull_request_target event can be used.\nThe original backport GitHub Action created backport branches of the format backport-${pullRequestNumber}-to-${base} which conflicted with other branch protection rules. Using the custom branch name feature on the GitHub Action allows to maintain the correct permissions for custom branches on the repo. As an example, if the branch name prefix passed is backport/backport-${pullRequestNumber}, the workflow will create a branch named backport/backport-${pullRequestNumber}-to-${base} for the auto-backport.\nFor PRs generated automatically for backports, it is necessary to have all the required continuous integration (CI) workflows running just as a normal PR. This helps validate the backport PRs against release branches preventing an incompatible merge. However, according to GitHub‚Äôs documentation, when an action in a workflow is performed using the GITHUB_TOKEN, it cannot trigger a new workflow run. This means that when the backport GitHub Action runs on the original PR and creates a backport PR from the workflow, another workflows cannot be triggered. Creating a simple GitHub App with the required permissions solved this problem by allowing it to make the backport PRs instead of using the GITHUB_TOKEN.\nThe branches created by the backport workflow are opened against the repository. Once the backport PR is merged, such branches are left over and need to be deleted manually from the repo. Incorporating functionality where the source branch of the backport PR is auto deleted after the it is merged would make the process end-to-end automated. This helps to maintain branches efficiently on the repo and allows the backport workflow to clean up after itself. The auto delete workflow on the release branches cleans up the auto backport branches after the PRs are merged.\nCurrent Features\nThe current state of auto-backport after these learnings is:\nAuto-backports are triggered by labelling a PR with a backport label of pattern backport &lt;release-branch&gt;. Once this PR is merged and labelled, the auto-backport workflow can run to generate a backport PR.\nA GitHub App is used to create backport PRs so that CI workflows can trigger on such PRs.\nThe backport branches from such PRs are deleted automatically once the backport PR is merged. Closing\nThese type of automation processes help the project get to releases faster with minimum friction. Through this blog, I wanted to shed some light the how the waters were navigated by the team to adopt backport automation and also share these learnings with the community. I hope this blog post helps other open source projects with similar automation. Your feedback and thoughts on this type of automation is always appreciated.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/fine-dining-for-indices/",
    "title": "Fine Dining for Indices",
    "content": "Is Your Cluster Eating Healthy?\nThere are a lot of reasons you might want to fine-tune indices on an OpenSearch cluster. Every workflow has different requirements, and the default behavior of OpenSearch might not suit your use case. Before ingesting with reckless abandon to fill an index with data, enjoy some insight on tuning an index. A small bit of foresight can help fine tune a cluster for speed or space efficiency to suit a particular workflow.\nAre You Going To Eat All of That?\nBy default, all of the fields in an ingested document become part of the index. If they are not part of the field mapping for that index, a mapping will be added. Conversely, only fields that are part of the index can be queried and returned in search results after the document is ingested. By using an index mapping API call before you ingest data, control is taken over which fields become part of the index and which fields do not. Just one form of caloric restriction - consider this dev tool snippet: PUT dynamic_mapping_index\n{\n\"mappings\": {\n\"properties\": {\n\"eat_me\": {\n\"type\": \"text\"\n},\n\"dont_index_me\": {\n\"type\": \"text\",\n\"index\": false\n},\n\"drink_me\": {\n\"type\": \"text\"\n}\n}\n}\n} dynamic_mapping_index will refuse to index any ingested fields called dont_index_me. If you later decide to retrieve the document you will still find the fields and values stored in the _source object. This tradeoff is one of efficiency. The size of your index will be somewhat proportional to the amount of data you store in it. Deciding not to index certain fields means a smaller index, which means a smaller cluster.\nThe Proof is in the Pudding\nHow does this work in action? Remember, the field dont_index_me is explicitly not part of the index because of the mapping property of index: false. So, when you add a document with this field it isn‚Äôt being added to the index but is being stored. POST dynamic_mapping_index/_doc\n{\n\"eat_me\": \"iron ration\",\n\"drink_me\": \"potion of index familiarity\",\n\"dont_index_me\": \"I'm expecting this field to be ingested, but because I'm familiar with my data I know beforehand I don't need it. It will not be indexed and won't be returned in search queries. It will be retrievable in the `_source` object of this document is ever retrieved.\",\n\"some_other_field\": \"This field is not in the mapping to exemplify Dynamic Mapping.\"\n} Let us try a search query using the field dont_index_me: GET dynamic_mapping_index/_search\n{\n\"query\": {\n\"match\": {\n\"dont_index_me\": \"what?!\"\n}\n}\n} You‚Äôll be met with an error message. {\n\"error\": {\n\"root_cause\": [\n{\n\"type\": \"query_shard_exception\",\n\"reason\": \"failed to create query: Cannot search on field [dont_index_me] since it is not indexed.\",...\n}],...\n\"status\": 400\n} However, retrieve the document you stored and you‚Äôll see that dont_index_me is in the _source object. GET dynamic_mapping_index/_doc/xxxxxxxxxxx\n{...\n\"found\": true,\n\"_source\": {\n\"eat_me\": \"iron ration\",\n\"drink_me\": \"potion of index familiarity\",\n\"dont_index_me\": \"I'm expecting this field to be ingested, but I know beforehand I don't need it. It will not be indexed and won't be returned in search queries. It will be retrievable in the `_source` object if this document's id is retrieved\",\n\"some_other_field\": \"This should exemplify dynamic mapping when we check the mapping later.\"\n}\n} If you look at the mapping for dynamic_mapping_index - GET dynamic_mapping_index/_mapping\n{\n\"dynamic_mapping_index\": {\n\"mappings\": {\n\"properties\": {\n\"dont_index_me\": {\n\"type\": \"text\",\n\"index\": false\n},\n\"drink_me\": {\n\"type\": \"text\"\n},\n\"eat_me\": {\n\"type\": \"text\"\n},\n\"some_other_field\": {\n\"type\": \"text\",\n\"fields\": {\n\"keyword\": {\n\"type\": \"keyword\",\n\"ignore_above\": 256\n}\n}\n}\n}\n}\n}\n} Wait a second - ingesting a field that wasn‚Äôt explicitly mapped added a new mapping. some_other_field wasn‚Äôt there in the mapping until a new document was added that contained that field. This exemplifies a great dining tip:\nPlease Ingest Responsibly!\nOpenSearch will index every single field encountered in an ingested document. If a new field is ingested, OpenSearch will do its best to determine the data type and update the index mapping accordingly. This is called ‚ÄúDynamic Mapping‚Äù and is also the default behavior. This is where being intimate with your data set is very helpful. Being too granular can cause what is known as a mapping explosion, where the number of fields in an index causes storage and retrieval to be bound and slow.\nIf your diet is particularly strict, you can ignore fields that are not part of an explicit mapping. Consider the previous example with another option added: PUT explicit_mapping_index\n{\n\"mappings\": {\n\"dynamic\": false,\n\"properties\": {\n\"eat_me\": {\n\"type\": \"text\"\n},\n\"drink_me\": {\n\"type\": \"text\"\n},\n\"dont_index_me\": {\n\"type\": \"text\",\n\"index\": false\n}\n}\n}\n} Note the addition of dynamic: false. With this option in place, your mapping will never be automatically updated when ingesting a field that has not been seen before. Let‚Äôs repeat our previous experiment with this new explicitly mapped index. POST explicit_mapping_index/_doc\n{\n\"eat_me\": \"iron ration\",\n\"drink_me\": \"potion of index familiarity\",\n\"dont_index_me\": \"I'm expecting this field to be ingested, but because I'm familiar with my data I know beforehand I don't need it. It will not be indexed and won't be returned in search queries. It will be retrievable in the `_source` object if this document is ever retrieved.\",\n\"some_other_field\": \"This field is not in the mapping to exemplify Dynamic Mapping.\"\n} Remember with the dynamically mapped index, a new entry was added to the mapping when a new field was ingested. Checking our mapping now shows that some_other_field was not added automatically. GET explicit_mapping_index/_mapping\n{\n\"explicit_mapping_index\": {\n\"mappings\": {\n\"dynamic\": \"false\",\n\"properties\": {\n\"dont_index_me\": {\n\"type\": \"text\",\n\"index\": false\n},\n\"drink_me\": {\n\"type\": \"text\"\n},\n\"eat_me\": {\n\"type\": \"text\"\n}\n}\n}\n}\n} Can I Keep the Recipe?\nBy default the entire document is ingested to the _source object. _source can be disabled (it is by default enabled) when creating your index, usually for storage space efficiency. Ingested documents are indexed, but the originally submitted document is not stored. In this case, if you decide not to index every field encountered, the data will not be part of the index, nor will it be retrievable from the _source object.\nConsider this index creation API call that would disable saving the _source object: PUT no_source_index\n{\n\"mappings\": {\n\"_source\": { \"enabled\": false},\n\"dynamic\": false,\n\"properties\": {\n\"eat_me\": {\n\"type\": \"text\"\n},\n\"drink_me\": {\n\"type\": \"text\"\n},\n\"dont_index_me\": {\n\"type\": \"text\",\n\"index\": false\n}\n}\n}\n} And again, the same experiment: POST no_source_index/_doc\n{\n\"eat_me\": \"iron ration\",\n\"drink_me\": \"potion of index familiarity\",\n\"dont_index_me\": \"I'm expecting this field to be ingested, but because I'm familiar with my data I know beforehand I don't need it. It will not be indexed and won't be returned in search queries. It will be retrievable in the `_source` object if this document is ever retrieved.\",\n\"some_other_field\": \"This field is not in the mapping to exemplify Dynamic Mapping.\"\n} No source field! GET no_source_index/_doc/xxxxxxxxxxx\n{\n\"_index\": \"no_source_index\",\n\"_type\": \"_doc\",\n\"_id\": \"u7jIZn8BGDFfrMtSqjOH\",\n\"_version\": 1,\n\"_seq_no\": 0,\n\"_primary_term\": 1,\n\"found\": true\n} Save the Leftovers?\nThere are also other options for keeping only specific pieces of the _source material after indexing. This retains the _source document, but not in its entirety. While this may reduce the size of the resulting _source object, you will lose the data you exclude if it were to be re-indexed, an operation that takes all of the documents _source objects in an index and re-processes them into a new index.\nOne more index: PUT exclude_index\n{\n\"mappings\": {\n\"_source\": {\n\"includes\": [\"eat_me.*\",\"drink_me.*\"],\n\"excludes\": [\"*.msg\", \"*.salt\"]\n},\n\"dynamic\": false,\n\"properties\": {\n\"eat_me\": {\n\"type\": \"text\"\n},\n\"drink_me\": {\n\"type\": \"text\"\n},\n\"dont_index_me\": {\n\"type\": \"text\",\n\"index\": false\n}\n}\n}\n} A dash of data: POST exclude_index/_doc\n{\n\"eat_me\": \"iron ration\",\n\"secret_ingredient\": {\n\"msg\": \"yes I'm MSG, the flavor enhancer.\",\n\"salt\": \"I'm salt. Also a flavor enhancer.\"\n},\n\"drink_me\": \"potion of index familiarity\",\n\"dont_index_me\": \"I'm expecting this field to be ingested, but because I'm familiar with my data I know beforehand I don't need it. It will not be indexed and won't be returned in search queries. It will be retrievable in the `_source` object if this document is ever retrieved.\",\n\"some_other_field\": \"This field is not in the mapping to exemplify Dynamic Mapping.\"\n} What‚Äôs leftover in _source? GET exclude_index/_doc/xxxXxXXxXxXx\n{...\n\"found\": true,\n\"_source\": {\n\"drink_me\": \"potion of index familiarity\",\n\"eat_me\": \"iron ration\"\n}\n} The _source is filtered down to only the elements described in the include and exclude stanzas in the index mapping.\nNutritional Information Available\nIf you‚Äôre worried about the size of your index, don‚Äôt forget that you can always check the amount of disk space its using with an API call. Need a fun at-home experiment? Try defining two exact indices, one with _source and one without. How does their resource usage compare?\nHere‚Äôs how our examples are adding up so far: GET /_cat/indices/dynamic_mapping_index?format=json\n[\n{\n\"health\": \"green\",\n\"status\": \"open\",\n\"index\": \"dynamic_mapping_index\",\n\"uuid\": \"9g7x9PRoSyGcQp-3UMrP6g\",\n\"pri\": \"1\",\n\"rep\": \"1\",\n\"docs.count\": \"1\",\n\"docs.deleted\": \"0\",\n\"store.size\": \"10.1kb\",\n\"pri.store.size\": \"5kb\"\n}]\nGET _cat/indices/explicit_mapping_index?format=json\n[\n{\n\"health\": \"green\",\n\"status\": \"open\",\n\"index\": \"explicit_mapping_index\",\n\"uuid\": \"D66KzvYJSme5WZQU28YQDw\",\n\"pri\": \"1\",\n\"rep\": \"1\",\n\"docs.count\": \"1\",\n\"docs.deleted\": \"0\",\n\"store.size\": \"8.3kb\",\n\"pri.store.size\": \"4.1kb\"\n}]\nGET _cat/indices/exclude_index?format=json\n[\n{\n\"health\": \"green\",\n\"status\": \"open\",\n\"index\": \"exclude_index\",\n\"uuid\": \"3T4shiqERUS5qJTDm6qTWQ\",\n\"pri\": \"1\",\n\"rep\": \"1\",\n\"docs.count\": \"1\",\n\"docs.deleted\": \"0\",\n\"store.size\": \"13.8kb\",\n\"pri.store.size\": \"6.9kb\"\n}] Flavor To Taste, and Bon Apet√≠t!\nA lot of index functionality is controlled by these small, sometimes overlooked stanzas of configuration. Storing the source document, deciding whether to index or not index fields, or even whether new fields should be added on the fly are all things that have a lasting impact on the way it behaves. The way that OpenSearch comes out of the box is suitable for a lot of tasks, but there‚Äôs no replacement for having intimacy and forethought with your own data. Getting comfortable with the index configuration options will help ensure your cluster is ingesting properly.\nHow Was The Service Today?\nAs a community its important to know the team is advocating for all of the users. Please get involved and speak up if you find anything that would help you get more out of OpenSearch. Whether via the GitHub Project, the community forums or any of the other ways you can connect with the community.",
    "keywords": [
      "intro"
    ],
    "type": "News"
  },
  {
    "url": "/blog/launch-announcement-1-3-0/",
    "title": "OpenSearch 1.3.0 is out now!",
    "content": "With this latest version of the OpenSearch distribution (OpenSearch, OpenSearch Dashboards, as well as plugins and tools) you can enjoy a number of new features and enhancements as well as improvements to stability and efficiency.\nOne of the major new additions to 1.3.0 is a set of tools for observability between logs, metrics, and other live data. When critical, event-based information is correlated with other application or system data and reported centrally, availability issues are easier to identify and resolve. Try these new tools out (see Observability, below) and let the community know what you think.\nHere is a list of major and minor feature enhancements that have been included in this release:\nIndex State Management Continuous Mode in Transform: With this release, you can now run transforms in a continuous mode based on a schedule. This enables you to keep an incremental transform job running on a group of source indices actively ingesting new data. At each run, only the modified buckets will be transformed, reducing the overhead for users who want to keep their transform up-to-date.\nObservability\nUsers can now create custom Observability Applications to view the availability status of all their systems, where log events can be combined with trace and metric data, into a single view of system health empowering developers and IT Ops to resolve issues faster and with fewer escalations. App Analytics: In the past, users had to collate logs, traces, and metrics in separate views which made application monitoring difficult. With App Analytics Dashboards, users can now view application logs, traces, and metrics in one view instead of moving between different visualizations. Trace ID Correlation: Users who regularly track events across applications will now be able to use correlation ID (based on the Open Telemetry specification) to tie events together when viewing events and in-context visualizations. Live Tail: Previously, when users were watching a live event take place, they had to manually refresh their view. Users can now configure the interval in which content is refreshed saving the hassle of manually refreshing. Field Insights and Enhanced Visualization Support: Visualization types such as pie and heat maps were added as well as the ability to add named threshold markers as lines on the visualizations. When users want to know more about a metric that they are looking at, they can select ‚ÄúView surrounding events‚Äù to get a correlated picture. In the event a user needs a reference point for a future meeting, visualizations can now be saved to notebooks for convenience. SQL and PPL PPL Runtime Fields: Users may opt out of formatting the schema upon writing the index (schema-on-write). This opt out can be used to speed up indexing time, or for additional presentation flexibility. Users who want to define their schema upon querying the index (schema-on-read), can now use regex and parse commands to further format their schema using PPL. Machine Learning Support in PPL (Piped Processing Language): Users can now process arbitrary observability events using Anomaly Detection based on Random Cut Forest (RCF) and K-means commands in PPL. Improved Aggregate Functions Support in SQL and PPL: Users expect to use aggregate functions in SQL and PPL. OpenSearch SQL and PPL now supports ORDER BY and default query limit size. Support for Group Field and Span in Stats Command: Users can now use group fields together and can span by a specified interval using the Stats command in PPL. Also, users can now create multiple time series‚Äô with a single command. Comma-Separated Index Matching: Users can now query multiple indices using a comma separated value using PPL. CAST Function Supported in PPL: Users can now change datatypes using the CAST function in PPL. Support IN Clause in SQL and PPL: Users can now use the IN clause within SQL and PPL to select from within a value list. Support date_nanos in SQL and PPL: Users can now search on indices with the field type of data_nanos.\nAlerting Cluster Metric Monitors: Users can now create monitors within Alerts which help administrators feel more confident about managing the health of their clusters. The new monitor reports on dimensions including, but not limited to CPU usage, JVM memory usage, and total number of documents coming into the cluster.\nAnomaly Detection New Detector Validation: When setting up a new anomaly detector, the parameters specified will be validated to ensure that the criteria can initialize and that the detector will perform. The validation presented in the user interface and API will prevent detectors from being created which will not initialize.\nML Commons Machine Learning (ML) Commons: A new solution that makes it easy to develop new machine learning features. It allows engineers to leverage existing open source machine learning algorithms and reduce the efforts to build them from scratch. It also removes the necessity from engineers to manage the machine learning tasks which will help to speed the feature developing process. K-Means and Random Cut Forest Algorithm Support:\nK-means, an oft-used ‚Äúclustering‚Äù algorithm, is now supported in OpenSearch.\nRandom Cut Forest (RCF), an unsupervised algorithm for detecting anomalous data points within a data set, is now supported in OpenSearch..\nBoth are available in ML Commons and can be accessed via the PPL user interface.\nIn planned future enhancements (see What‚Äôs Next, below), the OpenSearch 2.0.0 release will add distribution support for DEB X64 and RPM ARM64 in addition the those already offered.\nYou can keep informed about upcoming distributions on the distribution roadmap.\nRPM &amp; Debian Distributions\nAs of yesterday (March 16), the team identified the roadmap for 1.3.0 included references to RPM and Debian distributions. In late January, the distribution efforts started to be tracked on a separate board as distributions were decoupled from version releases. In this transition, the cards for RPM and Debian distributions were never removed from the 1.3.0 roadmap. The team regrets this error and have put measures into place to ensure that the roadmap will be more accurate moving forward.\nWhat‚Äôs Next?\nWe have a number of features in-progress (see the OpenSearch Roadmap). Below we‚Äôve highlighted a few: Document Returning Alerts: The new monitor type makes it easy to execute rules against log groups such as flow logs and DNS logs. Instead of a summary of the alerts triggered like in query or bucket based alerts, the monitor returns document ids for additional analysis and review. Search Backpressure: This feature aims to enhance the overall resiliency of OpenSearch and will introduce constructs to have fair rejections, minimize wasted work, improved search request cost estimation, and adds the ability to stabilize a cluster when under duress. Search Memory Tracking: The tasks framework already tracks latency and has some context about the query/work being done. The goal of this feature is to enhance this to start tracking additional stats of memory and CPU consumed per task allowing the tracking the cluster-wide resource consumption by a query. Drag and Drop: The new drag and drop experience will allow users of OpenSearch Dashboards to create data visualizations and gather insights without preselecting the visualization output and with the flexibility to change visualization types and index patterns on the fly.\nIn addition, OpenSearch 2.0.0 is already in development! The primary driver for the team to have an earlier 2.0.0 release is so that OpenSearch can get support for Lucene 9.0.0 in earlier. This change will likely enable some use of new Lucene features within the plugins, like KNN and Vector Field types. This release also allows for other breaking changes to be ready earlier, like inclusive naming. A few highlights include: Lucene 9.0.0: Lucene 9.0.0 includes several new features and performance improvements ( Lucene 9.0.0 Documentation) that OpenSearch would like to make available to users, including K-NN support, Vectors, Big Endian, faster numeric indexing, faster sorting, concurrent merge scheduler, and prototype Java Jigsaw module support. Starting to use Lucene 9.0.0 as soon as possible is a priority. It will take a few releases to leverage the full value of it, but adding it with 2.0.0. is so exciting and has loads of potential for OpenSearch. Node.js Upgrade: OpenSearch Dashboards needs to upgrade the Node.js version from the current version, 10.24.1, which is no longer in support to a newer version. The target version of node for the upgrade will be v14.18.1. Node v14 which will be in LTS until 2023.\nIn addition to the above features, this release will replace non-inclusive terminology (e.g., master, blacklist, etc.) throughout OpenSearch with inclusive ones (e.g., leader, primary, allowlist). If you are curious, feel free to take a look at the project roadmap where you can find out the planned features and fixes with linked issues where you can provide feedback. Additionally, please take a look at the proposed 2022 release schedule.\nHow can you contribute?\nWe would love to see you contribute to OpenSearch! For almost any type of contribution, the first step is opening an issue. Even if you think you already know what the solution is, writing a description of the problem you‚Äôre trying to solve will help everyone get context when they review your pull request. If it‚Äôs truly a trivial change (e.g. spelling error), you can skip this step ‚Äì but when in doubt, open an issue. If you‚Äôre excited to jump in, check out the ‚Äúhelp wanted‚Äù tag in issues.\nDo you have questions or feedback?\nIf you‚Äôre interested in learning more, have a specific question, or just want to provide feedback and thoughts, please visit OpenSearch.org, open an issue on GitHub, or post in the forums. There are also regular Community Meetings that include progress updates at every session and include time for Q&amp;A.\nThank you!\nFrom the team at the OpenSearch Project:\nThank you to all of the community members for their dedication to this open source effort. The long term goal at the outset was to collaborate with developers and build the most extensible and innovative logging and search tool in the world. That singular mission has not changed, and we could not do it without input and contribution from the open source community. A tip of the hat to you all; you‚Äôre a critical component for our mission to succeed!",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Introducing-Data-Prepper-1.3.0-with-New-Aggregation-Processor/",
    "title": "Introducing Data Prepper 1.3.0 with New Aggregation Processor",
    "content": "Data Prepper is an open source data collector for trace and log data that can filter,\nenrich, and aggregate data for downstream analysis and visualization. Data Prepper now\nhas more support for log enrichment and aggregation with the new features released in\nData Prepper 1.3.0. This release includes new processors to mutate fields on events,\naggregate distinct log events, drop certain events, and more.\nIn this post, we‚Äôd like to introduce some of the new enrichment processors that are\npart of Data Prepper 1.3.0. The Release Notes include the full list of processors as well as links to their documentation.\nMutate and String Processors\nThe mutate event processors offer the opportunity to rename, copy, add, or even delete entries in events. Using the Add Entry Processor,\nfor example, would allow a user to add in an entry to their data to help debug the flow of data:... processor: - add_entries: entries: - key: \" debug\" value: true... The mutate string processors offer tools to manipulate strings in the incoming data. Say there was a need to split\na string into an array. Simply add this processor:... processor: - split_string: entries: - source: \" message\" delimiter: \" &amp;\"... and an entry named message with a string such as \"a&amp;b&amp;c\" would transform into [\"a\", \"b\", \"c\"]. Currently,\nthere is support for lowercase, uppercase, trim, substitute, and split.\nThe Logstash conversion tool has been updated to support conversion from Logstash‚Äôs mutate filter into these new processors.\nFiltering with the Drop Processor\nData Prepper now supports a new drop processor which can filter out specified log events. Say you are collecting web request logs and only wish to store\nnon-successful requests. You could create a pipeline which drops any requests where the response is less than 400\nso that only HTTP status codes 400 and above remain. The following example pipeline shows how you could configure this. log-pipeline: source: http: ssl: false processor: - grok: match: log: [ \" %{COMMONAPACHELOG}\"] - drop: drop_when: \" /response &lt; 400\" sink: - opensearch: hosts: [ \" https://opensearch:9200\"] index: failure_logs In the sample above, one of the processors is drop. The drop_when property defines a condition to determine\nwhich Events to drop from the pipeline. This condition is /response &lt; 400.\nExtracting Key-Value Pairs from Strings\nOften log data includes strings of key-value pairs. One common scenario is an HTTP query string. For example, if a\nweb user queries for a pageable URL, the HTTP logs might have the following HTTP query string: page=3&amp;q=my-search-term. If you wish to perform analysis using the search terms, you may wish to extract the\nvalue of q from a query string. Data Prepper‚Äôs new key-value processor provides robust support for extracting keys and values from strings like these.\nThe following example shows how you could use the new split_string processor and key_value processor to get query\nparameters from an Apache log line. processor: - grok: match: message: [ \" %{COMMONAPACHELOG}\"] - split_string: entries: - source: request delimiter: \"?\" - key_value: source: \" /request/1\" field_split_characters: \" &amp;\" value_split_characters: \" =\" destination: query_params Setting Timestamps on Events\nData Prepper provides a new date processor to allow pipeline authors to configure the timestamp. This gives pipeline authors a couple options. The first option\nis to parse a field in the current Event according to a date-time pattern. This is useful if your log events already\nhave timestamps in them. You can configure the timezone as well in case the timestamps come from other timezones. The\nsecond option is to use the time that Data Prepper receives events as the timestamp for events. You may wish to use\nthis when receiving log data that does not have a timestamp.\nAggregate Processor\nUsers often want to aggregate data from different Events over a period of time. This is important in order to reduce\nunnecessary log volume, and to handle use cases like multi-line logs that come in as separate Events. The aggregate processor is a stateful processor that groups Events together based on the values for a set of specified identification_keys,\nand performs a configurable action such as remove_duplicates or put_all on each group. You can use existing actions, or you can create your own actions\nas Java code to perform custom aggregations.\nState in the aggregate processor is stored in memory. For example, in order to combine four Events into one, Data Prepper\nneeds to retain pieces of the first three Events. The state of an aggregate group of Events is kept for a configurable\namount of time. Depending on your logs, the aggregate action being used, and the amount of memory available, the\naggregation could take place over a long period of time.\nIn Data Prepper 1.3, two actions are being released with the aggregate processor: remove_duplicates and put_all.\nHowever, creating custom actions is extremely simple. If you are interested in learning more about creating custom actions, read the aggregate README.\nAdditionally, please create a Github issue for any actions you would like Data Prepper to support.\nAt the moment, the aggregate processor is only useful for single-node clusters of Data Prepper. However, utilizing the peer forwarder to aggregate over multiple-node clusters is planned for a future release of Data Prepper.\nThe following pipeline configuration extracts fields of sourceIp, destinationIp, and port using the grok processor, and then aggregates on those fields over a period of 30 seconds using the aggregate processor and\nthe put_all action. At the end of the 30 seconds, the aggregated log is sent to the OpenSearch sink. aggregate_pipeline: source: http: ssl: false processor: - grok: match: log: [ \" %{IPORHOST:sourceIp} %{IPORHOST:destinationIp} %{NUMBER:port:int}\"] - aggregate: group_duration: \" 30s\" identification_keys: [ \" sourceIp\", \" destinationIp\", \" port\"] action: put_all: sink: - opensearch: hosts: [ \" https://opensearch:9200\"] index: aggregated_logs Given the following batch of logs: { \"log\": \"127.0.0.1 192.168.0.1 80\", \"status\": 200 } { \"log\": \"127.0.0.1 192.168.0.1 80\", \"bytes\": 1000 } { \"log\": \"127.0.0.1 192.168.0.1 80\" \"http_verb\": \"GET\" } The grok processor will extract the identification_keys to create the following logs: { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"port\": 80, \"status\": 200 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"port\": 80, \"bytes\": 1000 } { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"port\": 80, \"http_verb\": \"GET\" } And when the group is concluded after a duration of 30 seconds from the time that the first log is\nreceived by the aggregate processor, the following aggregated log will be shipped to the sink: { \"sourceIp\": \"127.0.0.1\", \"destinationIp\": \"192.168.0.1\", \"port\": 80, \"status\": 200, \"bytes\": 1000, \"http_verb\": \"GET\" } Other Improvements\nIn addition to the new features already described, Data Prepper 1.3.0 has a few other improvements.\nMany OpenSearch users setup rolling indexes based on time to help reduce storage costs. You can now configure Data Prepper to use a date and time pattern in your index names for log-based indexes. Data Prepper can also convert index names with date-time patterns from your Logstash configuration files.\nData Prepper now uses the term ‚ÄúProcessor‚Äù instead of ‚ÄúPrepper‚Äù in pipelines. This disambiguates the Data Prepper product from the processors which provide enrichment and transformation.\nData Prepper is internally migrating plugins to the new Event model. Once completed, generic processors will be able to work for any Event type including traces. This release includes some work toward that goal allowing some trace sources and sinks to work with Events.\nLooking to the Next Release\nThis release allows Data Prepper to solve more log use-cases for developers and teams. Data Prepper 1.4 has other important features coming. We‚Äôd especially like to highlight the following significant changes.\nData Prepper will begin to support metrics thanks to a community contribution.\nMany users have asked for a way to route different Events to different Sinks. Data Prepper‚Äôs Conditional Routing will allow users to route based on our new Data Prepper Expression syntax.\nYou can see the Data Prepper roadmap to see other\nupcoming changes. If there are any features on the roadmap that you are most interested in, please comment on the\nissue to help the team prioritize issues. You can also request any changes by creating a GitHub issue. This project is open source\nand we are happy to accept community contributions.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/getting-started-with-fluent-bit-and-opensearch/",
    "title": "Getting started with Fluent Bit and OpenSearch",
    "content": "We recently announced the release of Fluent Bit 1.9, and while there are a number of new features and enhancements to its already impressive speed, scale, and efficiency, one feature we are really excited about is the OpenSearch plugin for Fluent Bit. Fluent Bit is an Apache 2.0 open source lightweight log and metric processor that can gather data from many sources, while the OpenSearch project is a community-driven open-source search and analytics suite derived from Elasticsearch 7.10.2 and Kibana 7.10.2.\nIn this Getting Started Guide we cover:\nThe new plugin with integration to OpenSearch.\nA deployment scenario on top of Kubernetes.\nBonus! Using some of Fluent Bit‚Äôs new 1.9 features.\nUse Cases for Fluent Bit\nFluent Bit is a graduated sub-project under the Cloud Native Computing Foundation (CNCF) Fluentd project umbrella. Similar to the parent project, Fluent Bit has hundreds of integrations to common tools such as Kafka, Syslog, Loki, as well as to services like Datadog, Splunk, and New Relic. Now with Fluent Bit 1.9, OpenSearch is included as part of the binary package.\nThe main difference between Fluent Bit and Fluentd is that Fluent Bit is lightweight, written in C, and generally has higher performance, especially in container-based environments. However, both projects integrate well with each other depending on your architecture and observability pipeline requirements.\nA few popular use cases supported with Fluent Bit:\nAvoiding vendor lock-in from vendor focused agents (Splunk forwarder, Datadog agent, Elastic Beats) - Fluent Bit is completely vendor agnostic.\nCollecting container logs from Kubernetes applications and sending them to multiple locations - Fluent Bit integrates with all major back-ends and can direct data to multiple locations.\nRetrieving data from syslog or network devices and performing lookups for the indicator if compromise scenarios.\nSending data to OpenSearch\nTo make use of the latest OpenSearch output plugin we will first need to get the latest version of Fluent Bit - version 1.9. There are a couple of ways to run Fluent Bit that are covered in Fluent Bit‚Äôs [Getting Started Guide], and in this tutorial, we will focus on two methods:\nInstall as a package on Linux.\nKubernetes deployment.\nInstall as a package on Linux\nWe can grab the latest version of Fluent Bit by running the following one line install script on top of our Linux OS. In my case, I am using Ubuntu 20.04 LTS: curl https://raw.githubusercontent.com/fluent/fluent-bit/master/install.sh | sh Once installed we can modify the configuration under /etc/td-agent-bit/conf/td-agent-bit.conf to the following: [INPUT]\nName cpu\nTag cpu\n[OUTPUT]\nName opensearch\nMatch *\nHost 192.168.2.3\nPort 9200\nIndex my_index\nType my_type In this scenario, we use the CPU log metric input plugin to retrieve CPU metrics and send them to OpenSearch.\nWithin a few minutes we should be able to see the CPU metrics under the my_index index within OpenSearch.\nKubernetes deployment\nAnother extremely popular method of deploying Fluent Bit is on top of Kubernetes. Many cloud providers already bundle Fluent Bit as part of their Kubernetes services, however, you can run Fluent Bit as a daemonset to collect application logs, events, and route them to whichever backend you need such as OpenSearch. A DaemonSet in Kubernetes ensures that a single instance of Fluent Bit is present on each Kubernetes node such that it can collect all the container application logs. Additionally, users generally deploy with the Kubernetes filter which allows all container application logs to be enriched with Kubernetes metadata (e.g. namespace, pod, container_id), which can help with debugging and troubleshooting.\nThe simplest way to deploy a DaemonSet is by leveraging the Fluent helm chart. Helm is a package manager for Kubernetes and makes deploying flexible, repeatable, and easy to maintain.\nWe can first grab the source code for the chart: git clone Then we can modify the values.yaml file and change the output section: [OUTPUT]\nName opensearch\nMatch *\nHost 192.168.2.3\nPort 9200\nIndex my_index\nType my_type Once completed we can run the following helm command, with the current directory being fluent-bit helm chart: helm deploy fluent-bit After deploying we can then check OpenSearch to see all our incoming logs as well as all the enrichment performed with the Kubernetes filter.\nConclusion and Getting Involved\nWith this Getting Started Guide we covered how you can start using Fluent Bit 1.9 to start sending logs to OpenSearch immediately from your Windows, Linux, Container, Mac, or Kubernetes environments. There are many other scenarios to explore and we welcome users to join the conversation on the Fluent Slack Channel or the OpenSearch forums.",
    "keywords": [
      "technical"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-java-runtime/",
    "title": "Using Different Java Runtimes with OpenSearch",
    "content": "At the time of the fork OpenSearch inherited bundling OpenJDK 15, and eight releases have used AdoptOpenJDK 15.0.1+9 as the default runtime, replaced with Adoptium (Temurin) 11.0.14.1+1 in OpenSearch 1.3.0. This change was primarily driven by the fact that JDK 11 is a Long-Term Support (LTS) release, and JDK 15 is not. LTS releases of JDKs focus on stability, therefore you can expect future versions of OpenSearch to always default to bundling a LTS JDK. Version Bundled JDK (Linux) Tested JDKs 1.0.0 - 1.2.4 AdoptOpenJDK 15.0.1+9\n11, 14, 15 1.3.0 Adoptium (Temurin) 11.0.14.1+1\n8, 11, 14 This, however, doesn‚Äôt tell the whole story. The OpenSearch distribution is comprised of the engine and a dozen plugins. Both the engine, and each plugin, are built and tested with a range of JDKs (a subset of 8, 11 LTS, 14, 15, and 17), across multiple operating systems (Linux, Windows, FreeBSD, and MacOS). Then, the complete distribution is rebuilt from source, and tested with the bundled JDK. Finally, while OpenSearch was claiming compatibility with JDK 8, CI didn‚Äôt include tests with that version for most plugins, nor were they actually built to target JDK8 in most components.\nVersions 1.0 to 1.2.0\nThe complete distribution of OpenSearch 1.0 through 1.2.4 was built with JDK 14, and tested with the bundled JDK 15. Various individual components were built and tested individually with JDK 14.\nVersion 1.3.0\nIn 1.3.0 JDK 14 was replaced with a LTS version 11 for both builds and releases. All components build and test with JDK 8, 11, and 14.\nThe parent issues for this change were opensearch-build#64 and opensearch-build#74. The implementation in OpenSearch engine was OpenSearch#940, and was followed by plugins, e.g. security#1580. The source and target Java versions were lowered back to 8 in OpenSearch#2321 and any incompatible code in the engine and plugins was fixed. Version 1.3.0 now reliably runs on JDK8.\nOriginally, there was a plan to upgrade the bundled JDK to 17 in this version, but the team ran into a number of issues. Engineers decided to downgrade to JDK 11 in OpenSearch#2301, and deferred upgrading to JDK 17 to 2.0.0 via opensearch-plugins#129.\nCustomizing the OpenSearch Runtime\nBy default, any OpenSearch distribution consults JAVA_HOME first in order to find out the Java runtime to run on. If JAVA_HOME is not set, OpenSearch will try to fallback to the bundled JVM runtime if available.\nOpenSearch 1.3.0 is also introducing support for a new environment variable OPENSEARCH_JAVA_HOME that takes precedence over JAVA_HOME. This can be useful for systems with multiple applications co-located with different JVMs, or in migration scenarios with several instances of OpenSearch running on the same machine. The environment setting propagates to plugins that launch Java processes, such as performance-analyzer. See OpenSearch#1872 for details.\nVersion 2.0.0\nOpenSearch 2.0 will upgrade Lucene to 9.0, which requires JDK 11 or newer. Furthermore, given that Java 8 support ends in March 2022, OpenSearch 2.0 will drop support for JDK 8.\nIn 2.0.0 the complete distribution will be built with JDK 11, and tested with the bundled JDK 17. All individual components will be built and tested individually with JDK 11 and 17. Building and testing with JDK 14 and 15 is up for a discussion in opensearch-plugins#132.\nThese changes were made in OpenSearch#1368, 2007, 2025, and 2407. The remaining work in plugins is for adding support for JDK 17 in opensearch-plugins#129, upgrading to Gradle 7 in opensearch-plugins#107, and removing support for Java 8 in opensearch-plugins#110.\nBenchmarking JDKs\nBefore switching JVMs Engineers wanted to understand the performance impact of upgrading to JDK 17. Benchmarking tests were run across JDK 8, 11, 14, 15, and 17 with both the x64 and ARM versions of OpenSearch-min. Latency and throughput were evaluated. JDK 17 consistently outperformed the rest of JDKs. JDK 15 was the closest to the metrics of JDK 17, followed by JDK 8 and 11. Meanwhile, JDK 14 was significantly slower than the other choices. Based on the results, JDK 17 was decisively the best option for running OpenSearch and the team is looking forward to shipping it by default in OpenSearch 2.0. See OpenSearch#1276 for details and numbers.\nNo-JDK Distributions\nReleasing a no-JDK distribution of OpenSearch is tracked in opensearch-build#99. The current work-around is to download the existing distribution, remove the JDK and set JAVA_HOME or OPENSEARCH_JAVA_HOME. This is not ideal, so please add your +1 to the issue if you need this feature.\nPlatform Specifics\nFreeBSD\nFreeBSD packages are available for OpenSearch from textproc/opensearch. These packages do not bundle a version of Java, and depend on one of the Java versions installed on FreeBSD.\nUsers building their own packages can customize the version of Java the package will depend on by setting the java version in the DEFAULT_VERSIONS environment variable when building, e.g. DEFAULT_VERSIONS=java=15. See opensearch-build#101 and Releasing for FreeBSD for more information.\nArch Linux\nThe community-contributed Arch Linux distribution of OpenSearch has been using Java 11 for both versions 1.2.3 and 1.2.4.\nWindows and MacOS\nAt this moment there‚Äôs no official Windows or MacOS distribution of OpenSearch. However, this project does support building and assembling OpenSearch for Windows and MacOS, with some caveats. The version of JDK used and the configuration options are the same as on Linux. See opensearch-build#33, #37 and #38 for details and how you can contribute.\nDocumentation\nThe compatibility documentation has been updated to reflect the above changes in documentation-website#459.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/launching-open-source-operator-for-openSearch/",
    "title": "Introducing open-source Kubernetes Operator for OpenSearch",
    "content": "Overview\nSince the launch of the OpenSearch project, a fully open-source K8s Operator has been a popular request with the community. We are happy to announce the release for the OpenSearch Operator.\nThe OpenSearch Operator is a fully open-source Kubernetes Operator, licensed as Apache 2.0, and is used for automating the deployment, provisioning, management, and orchestration of OpenSearch clusters and OpenSearch dashboards.\nThe Operator development is being led by Opster with partners including SUSE Rancher, Maibornwolff, AWS, Logz.io, and more.\nOperator Capabilities\nThe Operator enables high-level API use for easily running advanced OpenSearch operations on Kubernetes.\nWith this release, the Operator allows for management of multiple OpenSearch clusters and OpenSearch dashboards. Using the Operator makes scaling up and down, version upgrades, rolling restarts, adjustment of memory and disk resources on the nodes, securing deployments, and managing certificates simplified and streamlined.\nIn the future releases, it would also allow for advanced shard allocation strategies, monitoring with Prometheus and Grafana, control of shard balancing and allocation (For example, based on AZ/rack awareness, hot/warm) and auto scaling based on usage load and resources.\nGetting Started\nThe Operator is available here.\nStep 1: Installation\nIn order to install the Operator, you can use two methods:\nHelm chart installation.\nLocal installation\nHelm chart installation\nArtifact Hub is a web-based application that enables finding, installing, and publishing packages and configurations for CNCF projects, including publicly available distributed charts Helm charts.\nThe deployment process is very simple with Artifact Hub, you can follow these instructions to pull and install the OpenSearch Operator under opensearch-operator-system namespace.\nLocal installation\nClone the repo.\nRun make build manifests to build the controller binary and the manifests.\nStart a Kubernetes cluster (e.g, with k3d or minikube) and make sure your ~/.kube/config points to it.\nRun make install to create the CustomResourceDefinition (CRD) in the Kubernetes cluster.\nThere is no special recommendation on which method to use; both of them will install the OpenSearch Operator deployment and CRD on your kubernetes cluster. If you would like to explore the source code, you can use method #2.\nStep 2: Deploying a new OpenSearch cluster\nGo to opensearch-operator and use opensearch-cluster.yaml as a starting point to define your cluster.\nNote that the clusterName is also the namespace that the new cluster will reside in. Then run: kubectl apply -f opensearch-cluster.yaml The current installation deploys with the default demo certificate provided by OpenSearch, which is ideal for demo purposes but not safe for production use. We recommend replacing the demo certificates with trusted certificate authority (CA)-provided certificates for any production use.\nStep 3: Deleting an OpenSearch cluster\nIn order to delete the cluster, please delete your OpenSearch cluster resource; this will delete the cluster namespace and all its resources. kubectl get opensearchclusters --all-namespaces kubectl delete opensearchclusters my-cluster -n &lt;namespace&gt; Moving Forward\nThe joint team is hard at work further developing the Operator and adding even more powerful capabilities. In the meantime, please feel free to test the Operator, contribute feedback, or contribute to the development of the Operator. If there are more features you‚Äôd like to see or contribute, please feel free to create an issue.\nAbout Opster Opster‚Äôs products and services optimize the performance of Elasticsearch and OpenSearch deployments, improve stability, and reduce hardware costs.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Partner-Highlight-Pureinsights-adds-OpenSearch-to-its-Discovery-Platform/",
    "title": "Partner Highlight: Pureinsights adds OpenSearch to its Discovery Platform",
    "content": "Pureinsights has deep expertise in the design, implementation and management of open source search projects. Our team has undertaken hundreds of projects using Solr and Elasticsearch and we have developed technology assets that enhance these search engines most notably around adding fine grain document understanding pipelines and machine learning model based document vectorizers. On the query side we enable Google BERT based FAQs and Featured Snippets (Extractive Answers) as well as Knowledge Graph Answers (via NEO4J) to OpenSearch applications. We now embrace and add OpenSearch to our stable.\nOur Kubernetes, cloud-based Discovery Platform provides several components that amplify open source search applications including data connectors, content processing, a knowledge graph and a customizable Google like user interface. We significantly enhance the user experience of OpenSearch applications through add-on AI technologies such as Natural Language Processing (NLP) and Machine Learning (ML) Google BERT based models. We are pleased to announce that OpenSearch will be the default search engine of choice for the Pureinsights Discovery Platform.\nPureinsights are a big supporter of the OpenSearch initiative, and we look forward to being an active member of the community. The response to the launch of OpenSearch has been encouraging and we are enthusiastic about its future. We have already seen a slew of new features being added to OpenSearch and the recent 1.3 release enables us to enhance our own internal live tele-metrics tracking. As strong advocates for open source, we look forward to collaborating and contributing towards the project.\nFor further information you can reach us here.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/partner-highlight-titaniam-byok/",
    "title": "OpenSearch Partner Highlight: BYOK for B2B SaaS Operators using OpenSearch",
    "content": "We recently learnt that a number of our prospects were running their B2B SaaS platform on top of Elasticsearch. Nearly all of them asked if we also support AWS OpenSearch. Many of them are running older versions of Elasticsearch and are evaluating if they should upgrade to a more recent version or migrate to OpenSearch to stick to Apache license. This is about to play out in large scale in the upcoming months. SaaS companies operate on their customers‚Äô data, and they have to take all the precautions within their reach to protect that data.\nIn my previous OpenSearch Partner blog ( link), we reviewed all the great security features that are part of OpenSearch (inter-node TLS, authentication, RBACs, Audit logging and OSD multi-tenancy) and a high level overview of Titaniam‚Äôs data-in-use encryption plugin for OpenSearch.\nTitaniam plugin for OpenSearch can be installed on your OpenSearch cluster in a few minutes. Once installed, it encrypts the designated fields in selected indices before OpenSearch indexes the data, while preserving all its awesome search capabilities. You can designate which fields in your index need to be protected and how it will be released in search results.\nThis blog addresses an important need for B2B SaaS providers who is already running on an OpenSearch backend or strongly considering it: BYOK (Bring Your Own Key).\nProblem\nYou are a B2B SaaS provider. You have filled out that long security questionnaire as part of the presales process. You have faced questions like\nHow do you protect my data?\nCan your devops staff see my data?\nYou may point to encryption built into the storage layer - like AWS S3's SSE-S3 or EBS volume encryption. These encryption solutions protect the data from being viewed by AWS datacenter employees, not your devops team.\nYour enterprise prospects know this as well. Especially the ones from regulated industries such as banking, finance, healthcare. With ransomware attacks and data breaches on the rise, they demand more. They press you to ship your SaaS software for on-premise deployment. What do you do?\nShipping SaaS software for on-premise deployment is a nightmare. SaaS Software is built to run on a very specific software stack by a team of specialists (your devops ninjas). It does not distribute well or lend itself to be run by enterprise IT. Dependencies on cloud stacks, distributed computing, cost optimizations that only work at scale - impedance mismatches start to count up. Building out the platform support matrix will bury your engineering team on low value tasks (supporting Oracle 12g, Oracle Linux version 7.1).\nAs a SaaS vendor you enjoy several advantages: fewer release branches, agility in upgrading and patching, rich instrumentation, visibility into usage, direct line to customer challenges etc. Dollar for dollar, your SaaS revenue is 5-10x more worthy than on-prem revenue towards your company's valuation. Shipping your software for on-prem deployment grinds away that advantage.\nYou'd like to be able to give definitive answers to your prospects:\n\"We encrypt your data with the highest data protection standards (NIST FIPS 140-2).\"\n\"Our devops team cannot see your data in clear text.\"\n\"As our premium enterprise customer, you can supply and control the encryption keys. You can revoke them whenever you want.\" aka BYOK - Bring Your Own Key.\nHowever, implementing this takes massive engineering effort. And you would rather continue to invest in areas of your core competency than in specialized encryption.\nTitaniam can help. Titaniam is the preeminent provider of encryption solutions protecting big data stores such as AWS S3 and search engines such as AWS OpenSearch. All Titaniam offerings come with consistently implemented BYOK capabilities giving your customers a simple and seamless key management experience. You can roll out Titaniam and secure your SaaS backend in a matter of weeks with minimal engineering effort allowing your team to focus on what they do the best - building and running your software. See my last blog for how Titaniam protects the data in your OpenSearch ( link)\nThis document\nOutlines the BYOK aspects of the Titaniam encryption and\nPresents you a solution architecture\nBYOK for OpenSearch\nBYOK in Titaniam for OpenSearch is built further upon the existing strengths of searchable encryption. It is built using two simple premises - index specific encryption keys and ability to read those individual keys from any location.\nAs a SaaS operator it requires just one thing - maintain a separate search index (or a set of indices) for each customer. This is anyway a good practice from both security and maintenance stand point; it allows for independent upgrades, encryption key rotation, etc. Titaniam plugin comes with a key registry where you can register which index needs to be secured using which encryption key (i.e. which customer it belongs to) and where to read each key from. Titaniam reads the encryption keys directly from your customers keystore. Titaniam supports common key stores such as AWS Secrets Manager, Hashicorp Vault etc.\nSteps to enabling data protection with BYOK for your OpenSearch based application.\nInstall Titaniam Plugin for OpenSearch\nChoose which fields to protect. In a typical OpenSearch index, not all documents need to be secured and searched. You choose specific fields to secure.\nTest out your front end application works seamlessly.\nCreate a key registry - specify which index should use which key and provide Titaniam with the credentials (in encrypted form) to access the corresponding keystores.\nStart loading data and querying it. Fig1. BYOK in Titaniam Plugin for OpenSearch Your customers can supply their key in any key store of their choice by putting it in a simple JSON structure like this. {\n\"key\":\"CjWe57Pl4sDn...7cu8Z1WC/H5tkgmpgXbo968WI\",\n\"disabled\":\"false\"\n} To disable access to their keys, they just have to turn the second attribute to true. Titaniam plugin checks the various customers' keystores periodically (e.g. every minute) to see if any customer has revoked a key. If it finds a revoked key, it will add that information to the log and disable search and decryption on that index.\nConsistency in BYOK\nUnlike other point solutions, Titaniam fulfills the BYOK promise in a consistent manner across all its products. Titaniam can use the same (or same set of) customer controlled keys to encrypt customer's data, whether it is stored in AWS S3 or AWS OpenSearch. Your enterprise SaaS customers will immediately gain control over all their data held by you, their SaaS vendor.\nTitaniam handles not just the well-trodden happy path, but all the scenarios that occur with distributed key management such as network fault tolerance, key revocation and reinstatement, key rotation, usage report and dashboards.\nTitaniam supports a range of keystores for the customers to work with, such as AWS Secrets Manager, Hashicorp Vault etc. so that your customers do not have to open up new cloud accounts or install anything unnatural to manage encryption keys.\nAbout Titaniam\nTitaniam is a leading provider of encryption based data protection for modern big data stores. At the heart of Titaniam's encryption capability is searchable encryption which allows the data to be searched and aggregated without decryption. To find out more about us and our complete product portfolio visit our website ( https://titaniam.io).",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/release-candidate-2-0-available/",
    "title": "OpenSearch 2.0.0 release candidate is available and ready for testing and feedback",
    "content": "I am excited to share that the release candidate (RC1) for version 2.0.0 of OpenSearch and OpenSearch Dashboards is now available for testing and validation. As a release candidate, this version of the project is feature complete and has passed automated testing, but we‚Äôll continue to add bug fixes until the full ‚ÄúGeneral Availability‚Äù (GA) release date.\nThe request to you, as members of the OpenSearch project community, is to deploy the release candidate in a test environment and open issues to report any problems you discover. Your support for testing and feedback helps ensure that the candidate can become the best possible product it can be ‚Äì so please, go kick the tires and tell us what you think.\nOpenSearch 2.0.0 RC1 includes a number of new features and enhancements, such as Lucene 9, Document Level Alerting, and more. Before installing or upgrading to OpenSearch 2.0.0 RC1 or OpenSearch Dashboards 2.0.0 RC1, please read the release notes. We‚Äôd also love your feedback on the OpenSearch documentation. If you see content that‚Äôs missing or could be improved, please submit a PR or open an issue.\nYou can download the release candidate here. As a reminder, this release candidate is provided for testing and validation purposes only. Please don‚Äôt use this release candidate in a production environment.\nFinally, because the release date for 2.0.0 RC1 was delayed, the GA version of 2.0 is now scheduled for May 24 to allow more time for community feedback. You can keep up to date with the latest on the 2.0 release here.\nThank you, and happy testing!",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearchcon-2022/",
    "title": "OpenSearchCon 2022",
    "content": "Be part of the first annual OpenSearchCon brought you by the OpenSearch project!\nDon‚Äôt miss this free-to-attend, one-day conference that brings together users, developers, and technologists from across the open-source community for a day of learning, collaboration, and innovation with the the people and organizations that are shaping the future of the OpenSearch project. Explore real-world successes and new applications, connect with peers and partners who can help you solve today‚Äôs search challenges, and unlock the next phase of your OpenSearch journey.\nThis event will be held on Wednesday, September 21, 2022 in Seattle at the beautiful Fremont Studios! Register now Be there when the OpenSearch project leaders, contributors and partners take the stage to showcase how OpenSearch is changing the face of the open-source community. Hear from open-source experts and industry innovators as they dive deep into the topics that will enable you to maximize your use of open-source search and analytics software to ingest, search, visualize, and analyze data. You‚Äôll also learn more about the OpenSearch roadmap for 2022 and beyond, how to get the most out of your project contributions, as well as gain insight into the future of machine learning within the framework of the OpenSearch project.\nYou can check out the full schedule, list of speakers and full list of topics we‚Äôll be covering by visiting the conference website. View the sessions and schedule now Several OpenSearch partners are also exhibiting at the one-day event, providing attendees the opportunity to connect and learn more about the applications and services these partners have built around OpenSearch. This includes how they support organizations that are new to OpenSearch with everything from initial set up and implementation to fine tuning specific features based on individual use cases. Discover how these partners can help you build and deliver applications at scale, better understand and observe behaviors of applications and infrastructure, and ensure security and compliance while protecting against cyber attacks.\nView a full list of OpenSearchCon exhibitors and take advantage of the opportunity to stop by for quick discussions and demonstrations. See who is exhibiting at OpenSearchCon 2022 Interested in sponsoring this one-day inaugural event? As a sponsor of #OpenSearchCon 2022, you‚Äôll have a front-row seat to connect with other open-source community members, expand your professional network, and build relationships while conducting peer-to-peer discussions, delivering short demonstrations, and gaining valuable insight from the community at large. Contact us to learn more. Attendance is limited at #OpenSearchCon 2022 so sign up soon! COVID-19 protocols available on main event page Joining us at #OpenSearchCon? Be sure to share this on social media!",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/oracle-cloud-infrastructure-search-service/",
    "title": "Partner Highlight: Oracle Cloud Infrastructure Search Service with OpenSearch is GA!",
    "content": "Late last year, we shared the news that Oracle joined the OpenSearch community and launched the public beta of their new service, OCI Search Service with OpenSearch. Today we‚Äôd like to congratulate them on the global availability of their service in all thirty OCI regions. The offering is a fully managed search service based on OpenSearch. The service automates common operational activities like patching, updating, upgrading, resizing, and backups. Using the reference architectures, customers will have access to a validated architecture to deploy and configure the service for log aggregation and in-application search.\nWe are excited for them as they move from public beta to general availability and even more excited to see their commitment to the OpenSearch community. Of course, you know Oracle and their long legacy in databases and their contribution to the open-source community through Java and MySQL, so it is great to see them actively supporting the OpenSearch project. We expect to see additional open source partnerships from them in the future.\nOracle‚Äôs mission is ‚Äúto help people see data in new ways, discover insights, unlock endless possibilities‚Äù and delivering this cloud-based search service with OpenSearch is inline with that mission. Their Gen 2 cloud is a strong foundation with its built-in security to mitigate threats, improved automation, scalability, and high performance.\nDuring the beta customers were able to easily migrate the Elasticsearch infrastructure to OpenSearch. Because their private endpoints use the same APIs as Elasticsearch, it was simply a matter of ingesting data and pointing to the new endpoints. In fact, one of their customers ingested 300 million items into their OpenSearch cluster and is now enabled for analysis and action with OpenSearch Dashboards.\nIt‚Äôs a good day for open source and the OpenSearch community as another hyperscale cloud provider embraces OpenSearch. Congrats again. To learn more about OCI Search Service with OpenSearch go here.\nGet involved!\nWe would love to see you contribute to OpenSearch (if you‚Äôre not already)! For almost any type of contribution, the first step is opening an GitHub issue. Even if you think you already know what the solution is, writing a description of the problem you‚Äôre trying to solve will help everyone get context when they review your pull request.\nInterested in sharing your OpenSearch story? The community partners, community projects, and testimonials pages continue to grow and we would be happy to add your story!\nDo you have questions or feedback?\nIf you‚Äôre interested in learning more, have a specific question, or just want to provide feedback and thoughts, please visit OpenSearch.org, open an issue on GitHub, or post in the forums. There are also regular community meetings that include progress updates at every session and include time for Q&amp;A.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/fluentcon-announcement/",
    "title": "FluentCon EU Announcement",
    "content": "The Fluent community is excited to partner with the OpenSearch community in hosting another installment of FluentCon. The conference is a co-located event alongside KubeCon Europe on May 16th in Valencia, Spain, as well as virtually from your laptop. Similar to previous FluentCons, this event will feature both general sessions as well as in-depth workshops focused on core use cases, strategies for managing observability at scale, and conversation on how we build forward.\nWe are also excited to have the OpenSearch community front and center, with a keynote around getting started with a keynote by Eli Fisher titled End-to-End Log Analytics: Using the OpenSearch Project and Fluent.\nYou can check out the rest of the FluentCon EU 2022 schedule that includes talks by other community members such as IBM, Microsoft, Asana, Medtronic, Amazon Web Services.\nIf this is your first FluentCon or KubeCon event experience, we welcome you to check out FluentCon NA session recordings from 2021. These include amazing presentations from users at Microsoft, Amazon Web Services, Fidelity, Neiman Marcus, Greynoise Intelligence, and more.\nRegistration\nUsers attending in-person can add a FluentCon Ticket to your KubeCon Europe pass as part of registration, for an additional fee, and users attending virtually can access FluentCon on the day of the event.\nGetting Involved\nCan‚Äôt make the conference but still want to get involved? We will discuss all the sessions in the next Fluent Community meeting and will post updates throughout the week in the Fluent Slack channel.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/tag-youre-it/",
    "title": "Building My Own Content Compass",
    "content": "Tag, You‚Äôre It!\nI feel a personal and vested interest in ensuring that the forums here at OpenSearch are an information-rich environment. They‚Äôre a great tool for directly interacting with users and also for seeing first hand what people need help with. To best leverage them as a tool, I need to generate some actionable data from the forum threads that can be a compass for content generation, and I‚Äôd also love for everyone to see what I‚Äôm up to.\nTags have been appearing on forum posts retroactively almost as far back as the beginning of the year. I feel no shame here in admitting that it was a boring, lifeless toil for me, all this tagging by hand (let me know if you think I‚Äôve tagged your post incorrectly.) I‚Äôve done my best to guess the intent of any new posts, but I don‚Äôt think I can get it right every time. So here‚Äôs what we can all do: tag your posts in the forum. Help us target the right areas of learning content. My part will be faithfully adding tags to threads that don‚Äôt have any.\nHow can I tell if I‚Äôm doing it right?\nI know that the community will be best served when these questions can be answered:\nAre gaps in documentation being identified and filled in?\nAre the learning materials in the right format?\nAre the learning materials of sufficient technical depth?\nAre the learning materials being released at the right pace?\nI foresee an issue right away. In order to answer these questions, I need data! The data will have the purpose of guiding what kinds of content will be produced to best help everyone. Who doesn‚Äôt love documentation and learning materials? So I‚Äôm pretty sure I need some data (technically, I think we all need some data!)\nSo what‚Äôs the plan?\nI only want to start with some basic correlations, and the question I want to answer is basic enough: ‚ÄúWhat forum topic tags appear the most in each category of the forum?‚Äù I tallied up the number of topics tagged in February: 148 tags. Not bad. Then I counted again per category. The top 3 categories were ‚Äú OpenSearch &amp; OpenSearch Dashboards ‚Äù, ‚Äú Security ‚Äù, and ‚Äú Open Source Elasticsearch and Kibana ‚Äù (38, 38, and 20 threads with a tag, respectively).\nThis is a fine measure of activity and tells me where we might want to spend some time creating content. The content produced may not be specifically targeted though, and I had a specific question to answer. So, what are people specifically having trouble with? I tallied again by category but also by tag this time. The result painted an entirely different picture. Security / configure was number 1 (17 topics in the ‚Äúsecurity‚Äù category with the tag ‚Äúconfigure‚Äù). This lined up with anecdotes from the forum. I found the correlation satisfying‚Äîall I did was count some things! Now I not only have an idea of where to target some content, but I‚Äôm also becoming aware of specific trouble points and where high areas of activity are. Even better, I can answer the first of my four questions with a bit of evidence.\nIterate and Increment\nI have just taken you through one full iteration of ‚Äúthe plan.‚Äù Tags and the categories to which they belong are aggregated in order to prioritize documentation and learning materials. The great thing about this is having some data to support why certain documentation issues are being filed. Obviously, this is mostly an exercise in counting. Which is fine, as long as each iteration leads to actions that have impact. It might even be through this process that I learn what has the most impact, and I‚Äôll be able to answer more of the initial four questions as well as answer with more data.\nI‚Äôll be taking these counts monthly, but I think I can do more. A few lines of code, and I could dashboard what our current trends are, and then I wouldn‚Äôt have to count anything by hand! It may not be a typical use case for OpenSearch, but that doesn‚Äôt mean we can‚Äôt use it to accomplish our goal. I‚Äôll share the results of that journey in a future update. Stay tuned.\nCare to help?\nI can‚Äôt stress enough that anyone can help guide efforts toward improving learning content. If you see specific areas that you think are lacking documentation or you want some learning content of any kind, please remember that we are a truly open-source project. Please file an issue under the documentation-website repo describing the documentation you‚Äôd like to see. We welcome feedback from anyone on any issue with our documentation.",
    "keywords": [
      "intro"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opentelemetry-metrics-for-data-prepper/",
    "title": "Metrics Ingestion with Data Prepper using OpenTelemetry",
    "content": "Data Prepper offers great capabilities for ingesting traces and logs into OpenSearch.\nUntil now, that leaves out the third pillar of observability: metrics.\nData Prepper 1.4.0 introduces support for metrics received via OpenTelemetry.\nThis feature aligns nicely with the OpenTelemetry traces support in Data Prepper.\nWhy Ingest Metrics in OpenSearch?\nAt a first glance, OpenSearch offers great support for storing and analyzing any structured observability data.\nIt provides powerful search and aggregation capabilities.\nExcellent visualizations support users in generating insights.\nThe integration of Vega visualizations into the OpenSearch Dashboards lift the metrics use-case to a whole new level.\nIt allows for even more powerful and interactive visualizations of the ingested metrics.\nOpenSearch already comes with monitoring plugins for alerting and anomaly detection, that nicely extends to metrics.\nHowever, OpenSearch was not built as a time series database.\nStoring metrics will be a lot less efficient with regard to resource consumption in comparison to specialized solutions.\nYou will need to configure index rollups or retention using index policies if you want to store metrics for longer periods of time.\nOpenSearch can shine on special use-cases: high-cardinality data sets.\nMetrics at least consists of a name, a timestamp and a value.\nAdditionally, attributes can be attached that describe the nature or origin of the metrics better.\nIn time series databases, the number of different values of such attributes is called cardinality.\nMany of those specialized solutions become much less efficient if faced with high-cardinality attributes.\nThis can cause higher storage volumes and higher response times.\nOpenSearch is much less affected by the cardinality.\nLet‚Äôs say you count the number of your service request by a service key you issued to your customers.\nDepending on the number of service keys, you might get a high-cardinality metric.\nWith a growing number of customers, the performance of your time series database might get worse and worse.\nThis effect will be almost negligible in OpenSearch.\nIn summary, storing metrics in OpenSearch allows you to create powerful visualizations and use anomaly detection and alerting.\nYou will be able to augment logs and traces by a careful selection of metrics to enhance your overall observability.\nYou need to be mindful of the overall event rate, though.\nBut you do not need to plan ahead for cardinality.\nSolution Architecture\nData Prepper now supports metrics ingestion into OpenSearch using the OpenTelemetry protocol.\nThere are many implementations capable of sending metrics using this protocol.\nThe OpenTelemetry Collector was chosen as an agent for metrics acquisition in the following diagram.\nIt can pull several metrics endpoints or act as a receiver for others.\nMetrics are sent to Data Prepper using the gRPC-based OpenTelemetry protocol.\nData Prepper receives the metrics, dissects and maps their data points, and saves each data point as an individual OpenSearch document.\nThe diagram outlines the basic architecture for metrics ingestion using OpenTelemetry collector, Data Prepper, and OpenSearch. The OpenTelemetry metrics data model is described in great detail in its specification.\nVery briefly, metrics are divided into 5 different types:\nSum\nGauge\nHistogram\nExponentialHistogram\nSummary\nEach metric type has a specific data point format.\nFor example, sums and gauges use single value data points, while histograms and summaries used nested arrays to represent buckets of data.\nCorrelation between different metrics as well as traces and logs is provided by attributes and trace/span ids.\nIn OpenTelemetry the correlation to traces or spans is achieved with exemplars.\nExemplars are a collection of data samples, that contributed to the calculation of an aggregated metric.\nLet‚Äôs say you are reporting the average response time of your application every 30 seconds.\nOpenTelemetry lets you add the response time of a number of requests during that period, that were used to calculate the average.\nOne such entry called an exemplar would contain the trace and span id of the request.\nSupport for ExponentialHistograms and exemplars at the data points is not yet included in Data Prepper 1.4 but will follow in a future version.\nThe Data Prepper OpenTelemetry metrics support consists of two plugins:\nThe OTel Metrics source to deserialize the gRPC data,\nThe OTel Metrics processor to preprocess the data points with regard to the metrics type.\nThe following diagram shows a prototypical pipeline: Configuration\nSetting up the OpenTelemetry metrics support is straight-forward.\nYou just need to configure the source and processor plugins: metrics-pipeline: source: otel_metrics_source: processor: - otel_metrics_raw_processor: sink: - opensearch: hosts: [ \" https://opensearch.local:9200\"] username: username password: password index: metrics-otel-v1-%{yyyy.MM.dd} The metric source supports the same configuration as the OpenTelemetry trace source.\nOutlook\nYou can start analysing your metrics data in OpenSearch with the simple configuration outlined above.\nData Prepper supports the full feature set of OpenTelemetry metrics, except for ExponentialHistograms and exemplars, both of which will be added soon.\nThe generated documents allow for easy visualizations as well as advanced use-cases in OpenSearch.\nEnhance your observability with anomaly detection and alerts based on your custom metrics.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/introducing-logstash-input-opensearch-plugin-for-opensearch/",
    "title": "Introducing logstash-input-opensearch plugin for OpenSearch",
    "content": "Overview\nFollowing the launch of logstash-output-opensearch plugin, the OpenSearch project team has released the logstash-input-opensearch plugin on Github as well as Ruby Gems.\nIn this post, we will talk about the new input plugin for Logstash. We will show how it works with OpenSearch by giving an example on how to read data from OpenSearch, perform a transformation, and index back to OpenSearch. This use case is useful in the event you want to migrate from one OpenSearch major version to another (e.g. 1.3 ‚Üí 2.0).\nWhat are Logstash plugins?\nTo understand what a plugin is in Logstash, it‚Äôs important to understand what makes up the stages for Logstash plugins: Inputs are used to get data into Logstash. There could be 1 or more inputs (e.g., S3, Kinesis, Kafka). Filters are buildings block for processing the events received from the input stages. Filters are not mandatory and there could be zero or more filters (e.g., Mutate, GeoIP). Outputs are final stage of the Logstash and it can have one or more output to send the processed data from filters (e.g., OpenSearch).\nHow do logstash-input-opensearch plugins work?\nPlugins are configured within the Logstash config file. There are sections for Input, Filter, and Output. Once configured (see below), Logstash will send a request to the OpenSearch cluster and read the data as per the specified query in the inputs section. Once data is read from OpenSearch, you can optionally send it to next stage Filter for doing a transformation such as add a geolocation associated with an IP address. In this example, we won‚Äôt use the Filter plugin. Next up is the Output plugin. From the output section of the config file, you can either send the data back to same OpenSearch cluster or different clusters if desired.\nSetup Logstash and plugin\nLogstash Installation\nThis step can be skipped if you already have OSS version of logstash installed. Otherwise follow the steps below:\nDownload logstash-oss-with-opensearch-output-plugin (this example uses the distro for macos-x64). For other distros please refer the artifacts here. wget https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-7.16.3-macos-x64.tar.gz Extract the downloaded tar ball. tar - zxvf logstash-oss-with-opensearch-output-plugin-7.16.3-macos-x64.tar.gz\ncd logstash-7.16.3/ Install logstash-input-opensearch plugin../bin/logstash-plugin install --preserve logstash-input-opensearch Once installation is done, you will see the message as below: Validating logstash-input-opensearch\nInstalling logstash-input-opensearch\nInstallation successful Let‚Äôs get into action and see how the plugin works\nIn the introduction you have seen that Logstash processing has 3 stages. These stages are wrapped as part of config file. Let‚Äôs work on creating config to search data from one index in OpenSearch as my_index, and write the data into another index as my_index_new. Create a new file and add below content, save the file as logstash-input-opensearch.conf after replacing the OpenSearch HOST, PORT, USERNAME, PASSWORD, and INDEX with your OpenSearch cluster URL and credentials. input {\nopensearch {\nhosts =&gt; [\"https://HOST:PORT\"]\nuser =&gt; \"USERNAME\"\npassword =&gt; \"PASSWORD\"\nindex =&gt; \"my_index\"\nquery =&gt; '{ \"query\": { \"match_all\": {}} }'\n}\n}\noutput {\nopensearch {\nhosts =&gt; [\"https://HOST:PORT\"]\nuser =&gt; \"USERNAME\"\npassword =&gt; \"PASSWORD\"\nindex =&gt; \"my_index_new\"\nssl =&gt; true\necs_compatibility =&gt; \"disabled\"\n}\n} Using the above configuration, the match_all query filter is triggered and data is loaded once. schedule setting can be used to periodically schedule ingestion using cron syntax.\nExample: schedule =&gt; \"* * * * *\" Adding this to the above configuration loads the data every minute.\nStart Logstash\nYou can run the logstash with below command:./bin/logstash -f logstash-input-opensearch.conf Once you run the command as above, logstash will search the data from source index, write to destination index and shutdown the logstash. [2022-05-06T20:14:28,965][INFO][logstash.agent] Successfully\nstarted Logstash API endpoint {:port=&gt;9600,:ssl_enabled=&gt;false}\n‚Ä¶\n‚Ä¶\n[2022-05-06T20:14:38,852][INFO][logstash.javapipeline][main] Pipeline terminated {\"pipeline.id\"=&gt;\"main\"}\n[2022-05-06T20:14:39,374][INFO][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=&gt;:main}\n[2022-05-06T20:14:39,399][INFO][logstash.runner] Logstash shut down. Summary\nUsing logstash-input-opensearch plugin you can read data from one OpenSearch cluster and write to same or another OpenSearch cluster. This is helpful if you are planning to reindex the data or looking to move data from one cluster to another cluster. You can also request any changes by creating a GitHub issue. This project is open source and we are happy to accept community contributions.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-2-0-is-now-available/",
    "title": "OpenSearch 2.0 is now available!",
    "content": "OpenSearch 2.0 is now generally available! This release incorporates user feedback and contributions from across the OpenSearch community to deliver a wealth of new capabilities and performance enhancements. We‚Äôre grateful for the collaborative effort of the community to build a distributed search and analytics toolset with the features, usability, and open-source flexibility that developers can rely on to create their most innovative solutions yet.\nHere‚Äôs a look at some of the new features and enhancements you can benefit from with OpenSearch 2.0. You‚Äôll find the full release notes here. Lucene 9.1 With the 2.0 release, OpenSearch has upgraded to Lucene 9.1 ( Lucene 9.1 documentation). The move to the latest version of Lucene affords a number of exciting advancements for this release and will continue to offer more value in future releases. For 2.0, this upgrade enables the following enhancements: Performance optimizations delivered with Lucene 9.1 include 10‚Äì15% faster indexing of multi-dimensional points and sorting on fields indexed with points that is now several times faster. Lucene 9‚Äôs ConcurrentMergeScheduler now assumes fast I/O, which likely improves indexing speed in cases where heuristics would incorrectly detect whether or not the system had modern I/O. Lucene 9 also changed all file formats from big-endian order to little-endian order, speeding up decoding of postings lists. Java Jigsaw module support means that with version 9.1, Lucene JARs are now proper Java modules, with module descriptors and dependency information. This aligns with the continuing evolution of OpenSearch to make the toolset more modular and extensible. Document-level Alerting Document-level alerting ( see GitHub issue) allows users to create monitors that can generate alerts per document. Commonly used in security detection, these monitors use a similar approach as other types of alerting monitors available in OpenSearch: query level and bucket level. While those alerts use a summarized view of the data, document-level alerting can issue alerts on each document in the index, spotlighting which specific documents or records are triggering an alert in the monitor and avoiding monitoring gaps or data overlaps based on timestamps. Notifications A new Notifications plugin ( see GitHub issue) adds a unified notifications system to OpenSearch. Users no longer need to configure and manage notification channels for each plugin independently; with version 2.0, the Notifications plugin provides a centralized location to set up and manage notifications for relevant OpenSearch plugins. For example, in addition to managing notifications for the Alerting plugin, this plugin can generate a notification when a scheduled action is completed in the Index State Management (ISM) plugin. ML Commons Upgrades Introduced with version 1.3, the ML Commons plugin ( see GitHub repo gains two new algorithms in version 2.0 to extend OpenSearch‚Äôs machine learning (ML) functionality to additional workloads, reduce the effort required to build ML features, and centralize computation, resource management, and security for ML processes. New algorithms for linear regression and localization join existing algorithms for kmeans and Random Cut Forests to provide a comprehensive foundation for building and training ML models. The addition of linear regression aims to simplify development of ML models for predictive analysis; with localization, users can get a head start on developing ML approaches that reveal the key contributors to anomalies or any events that are detected, facilitating analyses and visualizations for root cause analysis and other use cases.\nReplacing Non-inclusive Terminology\nThis release replaces non-inclusive terminology (such as master, blacklist) throughout OpenSearch with inclusive terminology (such as cluster manager, allowlist). ( See issue in GitHub).\nRPM Package Manager\nVersion 2.0 follows version 1.3.2 to include the availability of RPM Package Manager distribution ( see GitHub issue). This simplifies installation of the OpenSearch distribution for Red Hat Linux-based operating systems. You can view compatible Linux versions here.\nBreaking Changes and Continuing Support\nOpenSearch follows Semantic Versioning, or SemVer, so breaking changes are only included in major version releases, like this one. For version 2.0, the list of incompatible changes includes updates like the Lucene upgrades and inclusive terminology mentioned above as well as breaking changes in the Destination API that follow from the addition of the Notifications plugin, along with others. For a comprehensive list of breaking changes in version 2.0, refer to the documentation.\nPlease note that we are continuing to support the OpenSearch 1.x release line. For more information about deprecation support, see section 3.28 of our FAQ.\nContributing to the Project\nYour thoughts and contributions make a real impact on the OpenSearch project! From Lucene upgrades to new plugins to ML advancements, this release includes many valuable contributions from the OpenSearch community. We extend our deepest thanks to everyone who contributed to OpenSearch 2.0.\nIf you‚Äôre interested in learning more, have a specific question, or just want to offer your feedback, please visit OpenSearch.org, open an issue on GitHub for OpenSearch or OpenSearch Dashboards, or post in the forums. There are also regular Community Meetings that include updates and time for Q&amp;A in every session.\nFor almost any type of contribution, opening an issue is the first step. If you‚Äôre eager to jump in, check out issues with the ‚Äúhelp wanted‚Äù label. Get Started You can download OpenSearch 2.0 here! Be sure to take a look at the release notes and updated documentation as you get started.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/wtit/",
    "title": "Partner Highlight: Using OpenSearch for F5 & NGINX with WorldTech IT",
    "content": "WorldTech IT is excited to join the OpenSearch community as users, contributors, and official OpenSearch partners. As the leader in Professional and Managed Services integrating AWS with F5 and NGINX solutions, we rely on OpenSearch for use within our Cloud.Red platform. Before we get into how we use and promote OpenSearch, you may be wondering where F5 and NGINX come into the picture.\nAbout WorldTech IT\nAs a top-tier F5 Guardian Partner, we specialize in designing application delivery solutions that leverage the best of F5 and NGINX. Our key focus areas are traffic optimization, high availability, automation, IAM, and security. From on-premise to cloud migrations, health-checks, custom SaaS solutions, or our Always-On program ‚Äì our expert team of F5 engineers brings more experience, depth of engineering bench, and focus around F5 technology than any other consulting company.\nOur customers needed a monitoring and alerting platform for F5 BIG-IP. When the competition wasn‚Äôt cutting it, Cloud.Red was born. In addition to monitoring and alerting, Cloud Red has grown to include observability and tooling to run everyday operational tasks and lifecycle management. Need scheduled backups of all your F5 devices with cloud storage options? Maybe you have a large environment and want to access all your devices from one screen with SSO? Cloud.Red can do all of that and more ‚Äì making management at scale a reality.\nHow we use OpenSearch with F5 &amp; NGINX\nOpenSearch has been integral to making Cloud.Red a reality, providing an expansive feature set, such as role-based access (security), observability tooling, alerting, and anomaly detection. It is one of the cornerstones of the platform, allowing for the collection of logs, statuses, and statistics, helping present a clearer picture of our customers‚Äô F5 BIG-IP system heath.\nOpenSearch will continue to serve as an integral component of the Cloud.Red platform, as we roll out features for NGINX products and dive deeper into application and security analytics.\nOpen-Source and Community\nWorldTech IT has been involved with the OpenSearch community since the early versions of Open Distro. From its infancy, we have appreciated the project‚Äôs ability to provide indispensable features with a permissive license. It has allowed us to test, build, and deploy, unencumbered by restrictions seen in other software stacks.\nSince early on, we have been testers of new OpenSearch builds and collaborators in the community. We truly appreciate the many ways to collaborate with the project, such as the forums, GitHub, and community meetings.\nWe also appreciate the greater community, such as partners like Calyptia. We had the pleasure of collaborating with Calyptia on testing and improving Fluent Bit and Fluentd for OpenSearch. Our continued collaboration is powering our transition from legacy tools to ones more compatible with the OpenSearch ecosystem.\nWorking with WorldTech IT\nIf you are interested in collaborating on an OpenSearch project or looking for F5 &amp; NGINX expertise, contact us on our site or hit us up through our OpenSearch partner portal.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-2-0-clients-released/",
    "title": "OpenSearch 2.0 Clients Released",
    "content": "On the heels of the OpenSearch 2.0 release, the OpenSearch team is excited to announce that all OpenSearch clients have been released to support version 2.0. Not only were all clients updated, but Vacha Shah went above and beyond to improve each client repo by improving test coverage for multiple versions, support to test against unreleased versions, and adding backport functionality. Vacha made it so easy that the co-maintainer of the PHP repo provided the following quote:\n‚ÄúThe effort required to launch the PHP client was minimal thanks to the great communication on GitHub and contribution by Vacha Shah.‚Äù - Soner Sayakci\nA big thanks to external contributors Soner Sayakci (PHP) and Thomas Farr (Rust), as well as internal contributor Anan Zhuang (NodeJS) for their help as co-maintainers of their respective client repos. In addition, thanks to Sarat Vemulapalli, Vijayan Balasubramanian, and dB who helped with code reviews. Below is the full list of clients which were updated: Java High Level Python Rust Node.js Low Level Python Ruby Golang PHP For OpenSearch 3.0, the team has a goal to release clients the same day the rest of the release goes live.\nGet Involved\nDo you have a passion for a client listed above? Don‚Äôt see a client which you think should be part of the list? Interested in contributing to the project? Open up an issue on Github and let‚Äôs talk about how you can contribute to the OpenSearch community.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/S3-Log-Ingestion-Using-Data-Prepper-1.5.0/",
    "title": "S3 log ingestion using Data Prepper 1.5.0",
    "content": "Data Prepper is an open-source data collector for data ingestion into OpenSearch. It currently supports trace analytics\nand log analysis use cases. Earlier this year Data Prepper added log ingestion over HTTP using tools such as Fluent Bit.\nAnd a recent community submission added OpenTelemetry metrics ingestion in Data Prepper 1.4.0.\nToday, the Data Prepper maintainers announce the release of Data Prepper 1.5.0 with support for Amazon Simple Storage Service (Amazon S3) as a source.\nThe current environment\nMany teams use cloud object stores such as Amazon S3 for storing logs. AWS services often write valuable logs to Amazon S3 that\ncustomers want to analyze. For example, Application Load Balancer writes access logs to S3. As part of a\ncomprehensive log solution, teams want to incorporate this log data along with their application logs.\nIt‚Äôs not only AWS services writing logs to S3. S3 is a highly available service offering that does a fantastic job of\ntaking in large volumes of data. Because it does this so well, some application developers are sending their logs to S3.\nRight now, getting this log data out of S3 is complicated, and developers are writing their own code to read from S3. Much\nof this is duplicated code for receiving S3 Event Notifications and then parsing S3 objects. And developers may also encounter\nissues with the size and scale of some files.\nGo where the logs are\nTo solve these recurring issues for teams, Data Prepper 1.5.0 adds support for Amazon S3 as a source of log data. S3 has a\nfeature called S3 Event Notifications that Data Prepper leverages to get log data. With this feature, Amazon S3 can send\nnotifications to configured destinations whenever objects in an S3 bucket change. For example, if a new object is\ncreated, S3 will send this notification. You can configure an S3 bucket to send event notifications to an Amazon Simple Queue Service (Amazon SQS) queue whenever new objects are written to S3. You then configure Data Prepper\nto use that SQS queue for receiving event notifications.\nData Prepper polls the SQS queue to receive event notifications. For any newly created object, Data Prepper then gets that object out of S3\nand parses it into events. Initially, Data Prepper can read two types of formats:\nSingle-line logs - These are logs where a single log line indicates the same event.\nJSON objects - Data Prepper expects a common JSON pattern where the JSON structure has one large JSON array of smaller objects. Data Prepper will create a single event from each smaller object.\nAdditionally, Data Prepper supports either uncompressed data or Gzip-compressed data.\nAn example\nEarlier, I mentioned supporting Application Load Balancer access logs. These logs are saved as traditional logs.\nEach network request is a single line in the log file, and that line follows a specific format. Additionally, Application Load Balancer logs are stored with\ngzip compression in S3.\nThere is a lot of rich information in these logs. You can determine whether requests are HTTP, HTTPS, or gRPC, and both the time that\nthe entire request took to process and the time your application took to process it are available. Also, you can get\nthe final status code, User-Agent headers, and AWS X-Ray trace information. Much of this information is available\nin your application logs, but you can also find out why an Application Load Balancer failed a request before sending it your application.\nThe following is an example Application Load Balancer log. http 2022-06-22T20:18:15.398914Z app/awseb-AWSEB-1HEOQDG4Y6178/7714ad4a617cc2b1 72.29.185.16:4131 172.31.10.112:80 0.000 0.001 0.000 403 403 236 119 \"GET http://myexample.com:80/orders/ad2f0b17-c32e-450c-b702-58037795939a HTTP/1.1\" \"python-requests/2.23.0\" - - arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/awseb-AWSEB-7GYGTFHE3TC80/285a7d99a833502a \"Root=1-62b32917-528df45625f90fd94b858e78\" \"-\" \"-\" 0 2022-06-22T20:18:15.397000Z \"forward\" \"-\" \"-\" \"172.31.10.104:80\" \"403\" \"-\" \"-\" Data Prepper can read these objects using the S3 Source, perform grok processing on the data, and create a document in\nOpenSearch that is much richer than just a single log line. The following pipeline shows how you could do this.\nThe first part is the s3 source. It is new in Data Prepper 1.5.0.\nAfter that comes the processor chain. It has three grok processors. The first one breaks up the log line into\ndifferent parts. The second two break up some parts even further to produce more fine-grained information. Then the data\nprocessor adds a timestamp representing the time the data was received from S3.\nAfter all of this, the data is sent to the configured OpenSearch cluster via the opensearch sink. log-pipeline:\nsource:\ns3:\nnotification_type: \"sqs\"\ncompression: \"gzip\"\ncodec:\nnewline:\nsqs:\nqueue_url: \"https://sqs.us-east-1.amazonaws.com/12345678910/ApplicationLoadBalancer\"\naws:\nregion: \"us-east-1\"\nsts_role_arn: \"arn:aws:iam::12345678910:role/Data-Prepper\"\nprocessor:\n- grok:\nmatch:\nmessage: [\"%{DATA:type} %{TIMESTAMP_ISO8601:time} %{DATA:elb} %{DATA:client} %{DATA:target} %{BASE10NUM:request_processing_time} %{DATA:target_processing_time} %{BASE10NUM:response_processing_time} %{BASE10NUM:elb_status_code} %{DATA:target_status_code} %{BASE10NUM:received_bytes} %{BASE10NUM:sent_bytes} \\\"%{DATA:request}\\\" \\\"%{DATA:user_agent}\\\" %{DATA:ssl_cipher} %{DATA:ssl_protocol} %{DATA:target_group_arn} \\\"%{DATA:trace_id}\\\" \\\"%{DATA:domain_name}\\\" \\\"%{DATA:chosen_cert_arn}\\\" %{DATA:matched_rule_priority} %{TIMESTAMP_ISO8601:request_creation_time} \\\"%{DATA:actions_executed}\\\" \\\"%{DATA:redirect_url}\\\" \\\"%{DATA:error_reason}\\\" \\\"%{DATA:target_list}\\\" \\\"%{DATA:target_status_code_list}\\\" \\\"%{DATA:classification}\\\" \\\"%{DATA:classification_reason}\"]\n- grok:\nmatch:\nrequest: [\"(%{NOTSPACE:http_method})? (%{NOTSPACE:http_uri})? (%{NOTSPACE:http_version})?\"]\n- grok:\nmatch:\nhttp_uri: [\"(%{WORD:protocol})?(://)?(%{IPORHOST:domain})?(:)?(%{INT:http_port})?(%{GREEDYDATA:request_uri})?\"]\n- date:\nfrom_time_received: true\ndestination: \"@timestamp\"\nsink:\n- opensearch:\nhosts: [ \"https://localhost:9200\"]\nusername: \"admin\"\npassword: \"admin\"\nindex: alb_logs You can use Data Prepper‚Äôs grok processor, mutate processor, and other processors to\ningest from other log sources as well.\nPossible future extensions\nThis initial version only supports two codecs: single-line logs and JSON. There may be opportunities to add other\ncodecs, such as multiline logs, or Apache Parquet. Additionally, Data Prepper may benefit from a core concept of codecs\nwhich can be shared across different Sources and Sinks.\nPlease comment on this GitHub feature request to\nadd the concept of codecs to Data Prepper.\nOther changes Disabling index management: Data Prepper can manage OpenSearch indexes. This can make it easier to get started with trace analytics in Data Prepper and OpenSearch. However, some teams have security requirements that prevent Data Prepper from having the necessary permissions to create Index State Management (ISM) policies, templates, or indexes. These teams may wish to manage indexes directly in OpenSearch. Data Prepper now allows for disabling any form of index management that reduces the permissions that Data Prepper needs on the OpenSearch cluster. Custom metrics tags: Data Prepper produces metrics indicating how Data Prepper itself is running. These metrics allow Data Prepper administrators to monitor the health of Data Prepper and its pipelines. Now, Data Prepper administrators can add custom tags to these metrics. This can help them to better organize their Data Prepper metrics.\nWhat‚Äôs next\nThe Data Prepper roadmap is the best place to see what\nis coming next. The maintainers are working toward Data Prepper 2.0, which will include conditional routing of events and core peer forwarding for log aggregations.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/announcing-style-guide/",
    "title": "OpenSearch style guidelines are now available",
    "content": "The OpenSearch team is excited to announce the release of the style guidelines for our documentation and marketing content! These guidelines cover the style standards and terms to be observed when creating OpenSearch content. Our growing team of technical writers and editors is committed to providing complete and best-in-class documentation to the OpenSearch community, and releasing our style guidelines reflects this commitment.\nIncluded in the style guidelines is guidance on voice and tone, punctuation and capitalization, numbers and measurement, formatting and organization, special considerations for blog posts, inclusive content, and more. We have also provided a list of terms to be observed in OpenSearch content. These resources will evolve as we implement best practices and lessons learned from working with you, the community.\nAs part of our focus on producing exceptional content, we are developing quality assurance processes, resources, and initiatives to help us standardize our approach to creating bar-raising OpenSearch content. Our style guidelines represent one of the mechanisms our team will use to achieve this. Check out the style guidelines, and feel free to let us know what you think.\nOpenSearch values transparency, and we welcome community contributions to our style guidelines. To access our documentation, see the OpenSearch documentation home page. If you want to contribute to the OpenSearch documentation, see the CONTRIBUTING.md file, which covers how to open an issue or create a pull request on GitHub. We look forward to collaborating with you!",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-2-1-is-available-now/",
    "title": "OpenSearch 2.1 is available now!",
    "content": "OpenSearch 2.1 is now available for download! This release includes new features and significant enhancements aimed at boosting performance and expanding functionality for search, analytics, and observability use cases. OpenSearch 2.1 delivers a number of capabilities that have come up consistently in the OpenSearch community, including a dedicated node type for running machine learning (ML) workloads at scale, enhanced data protection capabilities, more flexible search options, and user interface upgrades.\nThis release builds on recently launched OpenSearch 2.0, which brought new features and performance upgrades with Lucene 9 support, expanded ML tools, new monitoring and notifications capabilities, and more.\nCheck out the release notes for a full view of what‚Äôs new in version 2.1, and read on for the highlights! Dedicated resources for ML workloads Since we launched the ML Commons plugin with OpenSearch 1.3, more and more users have been using OpenSearch to train and deploy large ML models on their data. These workloads, which use ML Commons, can demand a lot of processing resources, running the risk of negatively impacting the performance of critical tasks like search and ingestion when distributed across data nodes.\nWe heard from users that they‚Äôd like a way to isolate those ML tasks from their production indexes. Now, with ML Nodes, you can configure a dedicated node type to host your ML workloads and go as big as you want with your ML models without worrying about the performance implications for your cluster. Setting up a dedicated ML Node is simple‚Äîyou can configure the node to your specifications and add it to your cluster via the YAML file. Automate snapshots with Snapshot Management OpenSearch provides snapshots to enable you to back up and restore a cluster‚Äôs indexes and state. In previous versions, you would need to manage your snapshots manually or with an external management tool. Community feedback has indicated a lot of interest in an automated process that can be managed within the OpenSearch suite.\nWith 2.1, you can now use the Index Management (IM) plugin to automate and manage your snapshots through the Snapshot Management feature. You can create a time-based Snapshot Management policy to specify the time interval or schedule for creating snapshots for a group of indexes and define how many of the snapshots to retain and for how long. You also get a nice UI to manage all Snapshots and Snapshot policies easily via Dashboards. Other notable upgrades in the 2.1 release include multi-terms aggregation, user interface updates, and increased zoom levels. Multi-terms aggregation lets you create dynamic buckets from multiple term sources, with the ability to return a top number of results in a desired order. For example, you may want to support an analysis by searching the most recent 1,000 documents for the number of servers by location where the average CPU usage is greater than 90%, in descending order. Previously, this kind of search would only be available through OpenSearch Dashboards; with 2.1, multi-terms aggregation is part of OpenSearch core, enabling greater efficiency by distributing the search workload to all relevant OpenSearch nodes and then returning only the specified data to the dashboard. User interface updates include a redesigned and reorganized header and menus across OpenSearch Dashboards to conserve visual space, simplify navigation, and give you more flexibility to organize custom menus. Increased zoom levels offer OpenSearch Dashboards users deeper visualizations when viewing maps. You will be able to zoom in to maps at up to 14x on existing raster tiles, up from 10x. This upgrade is now available across all versions of OpenSearch Dashboards, and because changes are enabled on the backend from the maps server, no cluster upgrade is required.\nIn closing, we want to extend our heartfelt thanks to the OpenSearch community for your feedback on‚Äîand contributions to‚Äîfeatures that can make a big impact on OpenSearch builders. Stay tuned for more exciting developments on the roadmap from the 2.x release line, starting in August 2022 with OpenSearch 2.2. This release will include significant updates for security analytics, real-time observability tools, anomaly detection, memory management, and more. As always, we invite you to make your voice heard as part of the OpenSearch community through our community meetings, forum discussions, and GitHub repository.\nGet started\nYou can download OpenSearch 2.1 here! Make sure to look over the release notes and updated documentation as you dive in.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/year-one-the-opensearch-project/",
    "title": "Year One: The OpenSearch Project",
    "content": "I know its clich√©, but what a difference a year makes. We launched the OpenSearch project a year ago today to help ensure that the community of open-source Elasticsearch users had a path forward. We wanted to be sure users continue to have a secure, high-quality, fully open-source search and analytics suite with a rich roadmap of new and innovative functionality.\nWhat is the OpenSearch project?\nAll along, the mission of the OpenSearch project has been to promote the long-term success of a thriving, feature-rich, broadly adopted, community-driven open-source search and analytics software suite. We strive to make OpenSearch easy to use for as many people and organizations as possible. Whether you are an independent developer, an enterprise IT department, a software vendor, or a managed service provider, the ALv2 license grants you well-understood usage rights for OpenSearch. You can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services‚Äîbroad adoption benefits the entire community.\nWith OpenSearch, you‚Äôre able to easily ingest, secure, search, aggregate, view, and analyze data for a number of use cases, such as log analytics, application search, enterprise search, and more. Because of its incredible utility and wide range of features (full-text search, time series, geospatial, aggregations, etc.), I often compare it to a Swiss Army knife.\nAs a search engine, OpenSearch is robust enough to support all kinds of applications, including everything from business-critical e-commerce to massive-scale document search, full-text search, natural language capabilities, inverted index data structures for fast response times, broad language support, and built-in, highly tunable relevance scoring.\nFor streaming data, OpenSearch provides high-volume data ingest, near real-time processing at scale, and distributed infrastructure that can scale horizontally and vertically to support large workloads. This makes a great foundation for use cases like log analytics and observability, for which OpenSearch provides rich visualization tools with OpenSearch Dashboards that make it easy for your users to explore and find meaning in vast troves of data. And that‚Äôs just scratching the surface! I am surprised on an almost daily basis by the new and exciting ways OpenSearch users are leveraging these capabilities to provide new experiences for their users.\nWhat have we accomplished in the last year?\nLike a lot of technology leaders, I spend more time thinking about what‚Äôs ahead than dwelling on what‚Äôs been done. However, this milestone represnts a unique moment to pause and take stock of what the OpenSearch community has accomplished in such a short amount of time. And when I do, I am amazed and humbled. These are a few of the achievements I‚Äôd invite the community to celebrate:\n47M+ downloads of OpenSearch in 2022 33 OpenSearch partners, and more being added weekly\n6 production releases driven by community feedback 18 community projects supported\n303+ external contributions to the project in 2022\nI‚Äôm also particularly impressed with the cadence and scope of the project‚Äôs product releases. Since OpenSearch 1.0 launched 365 days ago, we‚Äôve followed that with 3 minor releases on the 1.x line, pulling in features like cross-cluster replication, bucket-level alerting, a new observability interface, shard-level indexing back-pressure, and several new anomaly detection features. We also added ML Commons, a dedicated plugin for machine learning (ML) applications, plus support for ML and other functions in Piped Processing Language (PPL).\nIn May, we released our next major version, OpenSearch 2.0, which upgraded the project to Lucene 9.1, introduced document-level alerting and a new Notifications plugin, delivered significant enhancements to ML Commons, and more. Just last week, OpenSearch 2.1 went live with new dedicated node types for ML workloads, automated Snapshot Management, and multi-terms aggregation functionality. We‚Äôve also added a number of clients so that the project now supports Java, High Level Python, Rust, Node.js, Low Level Python, Ruby, Golang, and PHP.\nWhat‚Äôs next for the OpenSearch Project?\nWhat I find perhaps most exciting is how the pace of innovation behind the project continues to accelerate. Our project roadmap is public, so anyone can follow along, contribute or provide feedback, and you‚Äôre able to see the comprehensive slate of high-impact features and enhancements we plan to release throughout the year on the 2.x line and into 2023 with the release of OpenSearch 3.0. This includes significant new capabilities we‚Äôll be rolling out next month with OpenSearch 2.2, like upgrades for security analytics, real-time observability tools, anomaly detection, memory management, and more.\nAn open-source project‚Äôs roadmap reflects its priorities, and in the case of OpenSearch, those priorities reflect the feedback registered by our growing community of users, contributors, and partners. A thorough review of the roadmap and the discussions across our forum reveals the biggest priorities for the project in 2022 and beyond: Cost, performance, and reliability: The project aims to deliver significant value to users; new functionalities that drive down costs and improve performance and reliability are always top of mind. Ease of use: The project team is prioritizing many ways to make the project easier to use For example, a simple drag-and-drop editor for creating visualizations is on the roadmap for the 2.2 release. Log analytics: As a key use case for OpenSearch, users can expect the project to add several features that make it easier to get more from your logs. An event explorer and PPL-based visualizations are just two examples of upcoming features. Extensibility: Making the code base more modular through extensions‚Äîand in so doing, making the project easier to maintain‚Äîis another top priority. This will simplify management of plugins as versions are decoupled and enable secure sandboxing, among many other benefits. ML: Users are doing more and more to incorporate ML into their workloads. This has been and continues to be a key priority, as shown with upcoming enhancements to anomaly detection, k-NN vectors, and more.\nHow can you get started with the OpenSearch project?\nI hope you‚Äôll join me in taking a moment to acknowledge the momentum the OpenSearch project has built over the course of its first year and to give thanks for the tireless work of the OpenSearch community has made it possible. And if you haven‚Äôt already done so, I invite you to join the effort to build the world-class, truly open-source search and analytics solution that users deserve, in whatever way interests you the most.\nWe‚Äôd love to see you add your voice to the project‚Äôs community meetings and forum discussions or find opportunities to add features, conduct tests, fix bugs, or improve documentation through our GitHub repository. You can also register to attend OpenSearchCon, our one-day conference for the people and organizations that are shaping the future of the OpenSearch project. We hope to see you at the conference or online contributing to the future of the open-source community!",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/whatsnew-document-level-monitors/",
    "title": "What‚Äôs new: Document-level monitors",
    "content": "In OpenSearch 2.0, OpenSearch released document-level monitors. With document-level monitors, alert creators can monitor documents as they are indexed in OpenSearch. If an alert is configured on a document-level monitor, the alert returns a reference to the document that triggered the alert. In this blog post, we will provide the following:\nA quick review of query‚Äîand bucket-level monitors\nAn introduction to document-level monitors\nQuick review\nFor those who want to get up to speed on the OpenSearch alerting offering, refer to the ‚ÄúIntroduction to OpenSearch Alerting‚Äù blog post. Before we jump into document-level monitors, let‚Äôs take a quick look at the monitors OpenSearch already offers:\nWith query-level monitors, you specify a query, and the monitor uses that query to review documents based on a timestamp of when they were indexed. If an alert is configured on a monitor, the results of the alert are returned in aggregate (e.g., one alert was triggered in the last review of newly indexed documents). This type of alert is used when you need an alert set up quickly. Since query-level monitors are so easy to setup, it is recommended to review query-level monitors regularly to see if they be combined or migrated to bucket-level to save on performance.\nWith bucket-level monitors, you can set up an aggregate of something (e.g., hosts) and run multiple queries against that aggregate (e.g., host CPU usage, host memory usage, hostname). Bucket-level monitors make it easier to manage what would take many query-level monitors to accomplish. Like query-level monitors, bucket-level monitors use a scheduled time to review what has been indexed as part of the monitor, and those results are provided in aggregate. Bucket-level monitors can be used to help with maintainability of alerting rules (e.g., rules for hosts, rules for IP addresses, rules for errors).\nIntroducing document-level monitors\nInstead of an aggregate report, what if you want to see which documents are triggering an alert? That‚Äôs where document-level monitors come in. Document-level monitors allow for multiple queries and will tell you which documents triggered an alert when an alert is configured. Knowing which documents triggered an alert means you can dive deep into an analysis, whether of a service outage or a security incident. Document-level monitors feature a new concept called a findings index. When a document-level monitor executes a query that matches a document in an index, a finding is created. A finding will store the document ID, index name, matching query, and timestamp indicating when it was found. OpenSearch provides a findings index(.opensearch-alerting-finding) that contains findings data for all document-level monitor queries. You can search the findings index with the Alerting API search operation. This means you can look at all of the findings at a later date for audit purposes. Additionally, query-level and bucket-level monitors can be used to monitor the findings index to alert on the data at a less granular level. With granularity comes great responsibility Document-level monitors, when configured with an alert, will alert on each document they find. That means someone will have to acknowledge each document that triggers an alert. Alerts should be written precisely so as to avoid the fatigue of acknowledging numerous alerts that do not require attention/action. If your use case doesn‚Äôt require document-level granularity or if you have performance concerns, query- or bucket-level alerts are better suited.\nSummary\nWith OpenSearch 2.0, document-level monitors were released, adding document-level traceability and a new findings index that make it easy to dive deep into the details of what caused an alert. For those who don‚Äôt need a granular view, OpenSearch offers query- and bucket-level monitors to provide an aggregate view. Do you see a use case for using document-level monitors in your organization not covered by this blog post? The team and community would benefit from hearing about it, so feel free to submit a blog proposal on Github.\nTo learn more about using document-level monitors in OpenSearch Dashboards, see Monitors. If you find an issue with document-level monitors, submit an issue in Github.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/bottlerocket-k8s-fluent-bit/",
    "title": "Using Fluent Bit and OpenSearch with Bottlerocket and Kubelet logs",
    "content": "It‚Äôs great to be writing about OpenSearch again!\nToday, I‚Äôm going to show you how to run OpenSearch with the OpenSearch Operator on Kubernetes using the Bottlerocket and then add Fluent Bit to collect logs from the nodes of the same Kubernetes cluster. Phew. If that seems like a lot, don‚Äôt worry‚Äîthis post will take it bit by bit. I‚Äôll assume you know what OpenSearch is, but what about the rest of that stuff? OpenSearch Operator for Kubernetes controls and abstracts the configuration of OpenSearch and OpenSearch Dashboards in the Kubernetes environment. Fluent Bit is a logging processor and forwarder that‚Äôll be used to take the raw logs and send them up to OpenSearch for processing. Bottlerocket is an open source, container-optimized Linux distribution used on the nodes of a Kubernetes cluster.\nBottlerocket is probably the most unfamiliar of these components, so it‚Äôs worth it to zoom in on what makes it different. Most Linux distros are general purpose - you can use them to run a variety of workloads on a bunch of different types of machines. Take Ubuntu: you can run it on your laptop or you can use it on a cloud instance. To be general purpose, these distros can‚Äôt make many assumptions so they have to have a wide variety of software to support the general purpose mission of the OS. There are many specially optimized Linux distros such as OpenWrt for routers or even one that is optimized to only run Doom. While the goals of these distros are vastly different, the philosophy is the same: remove what you don‚Äôt need and only add what you do. Bottlerocket‚Äôs goal is to be able to host containers and nothing more. Today, you can run Bottlerocket on EC2, VMware vSphere, and on bare metal with plans for more platforms and hardware support in the future.\nWhat‚Äôs actually needed for hosting containers is surprisingly minimal. In Bottlerocket, two notable differences are the lack of a shell and a read-only root file system. Instead of a shell, containerd is run right from systemd and actually there are two isolated instances of containerd - one for hosting your orchestrated containers and the other containerd for ‚Äúhost containers,‚Äù which are privileged containers used only for operational and administrative purposes. To configure and manipulate the node, you can use Bottlerocket‚Äôs API which is only available to privileged containers. This means your nodes come up and reboot faster because there is no slow shell, scripts, or interpreted software to instantiate. Lacking a shell also results in a smaller security surface area. This does present some challenges - how the heck do you configure the nodes for OpenSearch and install Fluent Bit on the node if there is no shell? Well, dear reader, charge forward and you will find out.\nGetting the nodes ready for running OpenSearch\nSetting up your Bottlerocket Nodes\nOne quirk of OpenSearch is that it has special requirements for the Linux setting of vm.max_map_count. It must be equal or greater to 262144. This even has to occur on the host machine running the container. Normally you would use something like sysctl either manually or set a value in /etc/sysctl.conf. Without a shell and with a read-only root file system, this might sound challenging, but turns out, it‚Äôs not so bad.\nWhat you need is provided by the Bottlerocket API instead of the shell or configuration scripts. This can be accomplished with the API and the apiclient on each node: apiclient set --json '{\"kernel\": {\"sysctl\": {\"vm.max_map_count\": \"262144\"} } }' Typically, you would run apiclient from the control container, but you‚Äôd need to manually visit every node in the cluster for this change: no one wants to do that! Kubernetes has the concept of DaemonSet that will run on every node in a cluster. However, an unprivileged container in Kubernetes wouldn‚Äôt have the ability to run this command. What you can do is setup a DaemonSet that has the correct securityContext and socket connection to the Bottlerocket API using this spec: ---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\nname: api-pod\nspec:\nselector:\nmatchLabels:\nname: all-nodes\ntemplate:\nmetadata:\nlabels:\nname: all-nodes\nspec:\nvolumes:\n- name: api-socket\nhostPath:\npath: /run/api.sock\n- name: apiclient\nhostPath:\npath: /usr/bin/apiclient\ncontainers:\n- name: api\nimage: debian:bullseye-slim\ncommand:\n- /bin/sh\n- \"-c\"\n- \"apiclient set --json '{\\\"kernel\\\": {\\\"sysctl\\\": {\\\"vm.max_map_count\\\": \\\"262144\\\" } } }'\"\nvolumeMounts:\n- mountPath: /run/api.sock\nname: api-socket\n- mountPath: /usr/bin/apiclient\nname: apiclient\nsecurityContext:\nprivileged: true If you saved this as api-pod-daemonset.yaml you apply this with kubectl apply -f./api-pod-daemonset.yaml. If you look at kubectl get pods, it will likely show these pods are constantly restarting - DaemonSet‚Äôs are designed to run continuously and we only need this to run for enough time to send the apiclient set once. Once you see that it‚Äôs run at least once, you can then run kubectl delete with the same spec file to remove the DaemonSet. Since the DaemonSet has altered the underlying node configuration, removing the DaemonSet won‚Äôt undo the changes.\nIf you‚Äôre using EKS and eksctl, you don‚Äôt even have to do this. In the config file you pass into eksctl create cluster you can add the bottlerocket map to nodeGroups like this:...\nnodeGroups:...\nbottlerocket:\nsettings:\nkernel:\nsysctl:\nvm.max_map_count: \"262144\" Either way, all your nodes in the group will be configured to have the correct vm.max_map_count value.\nRunning OpenSearch with the OpenSearch Operator\nThe recently developed Kubernetes operator for OpenSearch will work just fine on this cluster - the fact that Bottlerocket is the OS is transparent to the operator. The following steps are based off of the quick start guide from the OpenSearch operator repo:\nFirst, add and install the opensearch-operator using Helm: helm repo add opensearch-operator https://opster.github.io/opensearch-k8s-operator/ helm install opensearch-operator opensearch-operator/opensearch-operator Then create your config file for the operator, I called it opensearch-cluster.yaml: apiVersion: opensearch.opster.io/v1\nkind: OpenSearchCluster\nmetadata:\nname: opensearch-cluster\nnamespace: default\nspec:\ngeneral:\nserviceName: opensearch-cluster\nhttpPort: 9200\nversion: 1.3.1\ndashboards:\nenable: true\nversion: 1.3.1\nreplicas: 1\nresources:\nrequests:\nmemory: \"512Mi\"\ncpu: \"200m\"\nlimits:\nmemory: \"512Mi\"\ncpu: \"200m\"\nnodePools:\n- component: masters\nreplicas: 3\ndiskSize: \"5Gi\"\nNodeSelector:\nresources:\nrequests:\nmemory: \"2Gi\"\ncpu: \"500m\"\nlimits:\nmemory: \"2Gi\"\ncpu: \"500m\"\nroles:\n- \"data\"\n- \"master\" Then you apply the file using kubectl: kubectl apply -f opensearch-cluster.yaml This will create a bootstrap node, then 3 pods for master nodes of OpenSearch and 1 pod for OpenSearch Dashboards. If you run watch -n 2 kubectl get pods while it‚Äôs starting you can see the pods come online live. Once you see opensearch-cluster-masters-2 in the running state, then your cluster is ready to go. It should look something like this: NAME READY STATUS RESTARTS AGE\nopensearch-cluster-dashboards-545f67d7f4-wp426 1/1 Running 0 8m4s\nopensearch-cluster-masters-0 1/1 Running 0 8m4s\nopensearch-cluster-masters-1 1/1 Running 0 5m28s\nopensearch-cluster-masters-2 1/1 Running 0 3m21s\nopensearch-operator-controller-manager-597cbf7fd4-r8z59 2/2 Running 0 9m20s Now, depending on how you‚Äôre running, exposing, or intending to access your Kubernetes cluster, you may not have access to OpenSearch Dashboards. In my case, I added an ingress controller and configured the routing rules as follows:...\nrules:\n- http:\npaths:\n- path: /\npathType: Prefix\nbackend:\nservice:\nname: opensearch-cluster-dashboards\nport:\nnumber: 5601 If you‚Äôre using the ingress method, you can get the hostname by using kubectl get ingress/&lt;name of your ingress controller&gt;, then point your browser at that hostname.\nOnce you‚Äôve sorted your preferred way of accessing OpenSearch Dashboards, go ahead and test it out with the username admin and the password admin.\nAdding Fluent Bit\nUp to this point, you should have a working K8s cluster that uses Bottlerocket instead of a general purpose Linux distro and running in this cluster is OpenSearch and OpenSearch Dashboards. You could manually put some data into OpenSearch, but you probably want to setup something that automatically collects logs. For the purposes of this blog post, I‚Äôll show you how to point Fluent Bit at the node logs of the cluster itself.\nJust to be clear: this is a bit of a logging perpetual motion machine - you‚Äôve setup a lot of stuff just to monitor, well, itself. In a more real-world setup you would have other workloads in the Kubernetes cluster along side OpenSearch or even OpenSearch running in a different cluster all together. Many people also use a log aggregator like Fluentd or Logstash to act as an intermediary between OpenSearch and Fluent Bit. Like most things, there is no one right way to make it all work together.\nOpenSearch runs with authentication as standard and Fluent Bit will need to know those credentials, so you‚Äôre going to need to put your credentials in as a secret. You can do this with kubectl: kubectl create secret generic opensearchpass \\\n--from-literal=username=&lt;your username&gt; \\\n--from-literal=password=&lt;your password&gt; Now, since we‚Äôre looking at kubelet logs, we‚Äôre going to need an instance of Fluent Bit for every node. There are many ways to do this, but let‚Äôs use the Fluent Bit operator. I‚Äôve adapted the following quick start from Kubesphere‚Äôs walkthrough: apiVersion: fluentbit.fluent.io/v1alpha2\nkind: FluentBit\nmetadata:\nname: fluent-bit\nnamespace: default\nlabels:\napp.kubernetes.io/name: fluent-bit\nspec:\nimage: kubesphere/fluent-bit:v1.8.11\ncontainerLogRealPath: /var/log/containers\npositionDB:\nhostPath:\npath: /var/lib/fluent-bit/\nresources:\nrequests:\ncpu: 10m\nmemory: 25Mi\nlimits:\ncpu: 500m\nmemory: 200Mi\nfluentBitConfigName: fluent-bit-only-config\ntolerations:\n- operator: Exists\n---\napiVersion: fluentbit.fluent.io/v1alpha2\nkind: ClusterFluentBitConfig\nmetadata:\nname: fluent-bit-only-config\nlabels:\napp.kubernetes.io/name: fluent-bit\nspec:\nservice:\nparsersFile: parsers.conf\ninputSelector:\nmatchLabels:\nfluentbit.fluent.io/enabled: \"true\"\nfluentbit.fluent.io/mode: \"fluentbit-only\"\nfilterSelector:\nmatchLabels:\nfluentbit.fluent.io/enabled: \"true\"\nfluentbit.fluent.io/mode: \"fluentbit-only\"\noutputSelector:\nmatchLabels:\nfluentbit.fluent.io/enabled: \"true\"\nfluentbit.fluent.io/mode: \"fluentbit-only\"\n---\napiVersion: fluentbit.fluent.io/v1alpha2\nkind: ClusterInput\nmetadata:\nname: kubelet\nlabels:\nfluentbit.fluent.io/enabled: \"true\"\nfluentbit.fluent.io/mode: \"fluentbit-only\"\nspec:\nsystemd:\ntag: service.kubelet\npath: /var/log/journal\ndb: /fluent-bit/tail/kubelet.db\ndbSync: Normal\nsystemdFilter:\n- _SYSTEMD_UNIT=kubelet.service\n---\napiVersion: fluentbit.fluent.io/v1alpha2\nkind: ClusterFilter\nmetadata:\nname: systemd\nlabels:\nfluentbit.fluent.io/enabled: \"true\"\nfluentbit.fluent.io/mode: \"fluentbit-only\"\nspec:\nmatch: service.*\nfilters:\n- lua:\nscript:\nkey: systemd.lua\nname: fluent-bit-lua\ncall: add_time\ntimeAsTable: true\n---\napiVersion: v1\ndata:\nsystemd.lua: |\nfunction add_time(tag, timestamp, record)\nnew_record = {}\ntimeStr = os.date(\"!*t\", timestamp[\"sec\"])\nt = string.format(\"%4d-%02d-%02dT%02d:%02d:%02d.%sZ\", timeStr[\"year\"], timeStr[\"month\"], timeStr[\"day\"], timeStr[\"hour\"], timeStr[\"min\"], timeStr[\"sec\"], timestamp[\"nsec\"])\nkubernetes = {}\nkubernetes[\"pod_name\"] = record[\"_HOSTNAME\"]\nkubernetes[\"container_name\"] = record[\"SYSLOG_IDENTIFIER\"]\nkubernetes[\"namespace_name\"] = \"kube-system\"\nnew_record[\"time\"] = t\nnew_record[\"log\"] = record[\"MESSAGE\"]\nnew_record[\"kubernetes\"] = kubernetes\nreturn 1, timestamp, new_record\nend\nkind: ConfigMap\nmetadata:\nlabels:\napp.kubernetes.io/component: operator\napp.kubernetes.io/name: fluent-bit-lua\nname: fluent-bit-lua\nnamespace: default\n---\napiVersion: fluentbit.fluent.io/v1alpha2\nkind: ClusterOutput\nmetadata:\nname: es\nlabels:\nfluentbit.fluent.io/enabled: \"true\"\nfluentbit.fluent.io/mode: \"fluentbit-only\"\nspec:\nmatchRegex: (?:kube|service)\\.(.*)\nes:\nhost: opensearch-cluster-masters\nport: 9200\ngenerateID: true\nlogstashPrefix: fluent-log-fb-only\nlogstashFormat: true\ntimeKey: \"@timestamp\"\nhttpUser:\nvalueFrom:\nsecretKeyRef:\nname: opensearchpass\nkey: username\nhttpPassword:\nvalueFrom:\nsecretKeyRef:\nname: opensearchpass\nkey: password\ntls:\nverify: false A few key differences from the Kubesphere original:\nIn the FluentBit spec, containerLogRealPath needs to be added and set to /var/log/containers. The default configuration for containerLogRealPath is /containers which is read-only on Bottlerocket. If you fail to do this you‚Äôll get the error spec: failed to generate spec: failed to mkdir \"/containers\": mkdir /containers: read-only file system when the pods try to start.\nIn the ClusterOutput spec, I‚Äôve adapted the connection information to work with OpenSearch by adding the httpUser and httpPassword maps as well as changing the host to work with the cluster built earlier in this blog post.\nI‚Äôve also set the tls map to verify: false. This is due to the self-signed cert that comes standard with OpenSearch.\nOne other thing to note is that I‚Äôm using the es spec because OpenSearch doesn‚Äôt have a specific one yet. You can rely on the common lineage here and Fluent Bit isn‚Äôt doing any sort of compatibility check.\nI‚Äôve saved the file as fluent-bit-opensearch.yaml so it can be applied with kubectl like this: kubectl apply -f./fluent-bit-opensearch.yaml After that, looking at kubectl get pods you should see something that resembles this: NAME READY STATUS RESTARTS AGE\nfluent-bit-62bjg 1/1 Running 0 16m\nfluent-bit-fqwm4 1/1 Running 0 16m\nfluent-bit-n64pd 1/1 Running 0 16m\nfluent-bit-rtn7m 1/1 Running 0 16m\nfluent-bit-zb2kz 1/1 Running 0 16m\nfluent-operator-9646748f-h8xvw 1/1 Running 0 12m\nopensearch-cluster-dashboards-545f67d7f4-wp426 1/1 Running 0 55m\nopensearch-cluster-masters-0 1/1 Running 0 55m\nopensearch-cluster-masters-1 1/1 Running 0 52m\nopensearch-cluster-masters-2 1/1 Running 0 50m\nopensearch-operator-controller-manager-597cbf7fd4-r8z59 2/2 Running 0 56m Why are there five different fluent-bit-xxxx pods? In my case I‚Äôm running a cluster of five nodes, the operator knows the size of the cluster and has deployed one instance per node.\nBringing it all together\nOK, now that you have all the components, you can start seeing how this all comes together. Fire up OpenSearch Dashboards and head over to Dev Tools (Hamburger menu &gt; Management heading).\nOn the left side of the screen you can type your queries. First, run the following query to confirm that you‚Äôve go a Fluent Bit sourced index: GET /_cat/indices/fluent-log-*?h=index After clicking the play arrow, you should see the index name(s) created by the Fluent Bit example in the right-hand panel.\nNow let‚Äôs look at one of those indexes by running this query: GET /&lt;index name&gt;/_search\n{\n\"query\": {\n\"match_all\": {}\n}\n} This query will show the results on the right in JSON format. The log lines are located under hits.hits.\nWith this basic information you can build dashboards and visualizations, run queries, set up alerts, and even use anomaly detection to automatically find unusual blips in your kubelet logs.\nBottlerocket, Kubernetes, OpenSearch, and Fluent Bit are powerful when combined. Getting them all configured requires a few special tweaks, but it‚Äôs nothing you can‚Äôt handle with the right information. If you want to find out more, check out the Bottlerocket GitHub repo or ask a question on the Bottlerocket Discussion Forum.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/aggregate-by-multiple-terms/",
    "title": "Aggregate by multiple terms in OpenSearch",
    "content": "Multi-terms aggregation allows you to group and sort results from a query. While terms aggregation has existed in OpenSearch for some time, multi-terms aggregation allows for sorting by deeper levels. This is particularly useful in the observability space. Say, for example, you need to identify servers that have the most contention for CPU so you can either redistribute the load or scale up the server.\nYou can now do this with multi-terms aggregation, as shown by the demo below. This can be run from the Dev Tools section in OpenSearch Dashboards. If you need a test environment, you can spin up a single-node development environment using Docker Compose (make sure you have at least 4 GB of memory available in Preferences ‚Üí Resources).\nEnvironment setup\nFirst, we will set up a test index for storing our sample data. We can do this by issuing a PUT request with the index name we want to create‚Äîin this case, test_0001. The body in the example below contains the settings and mappings that we wish to use for each of the fields. PUT test_0001\n{\n\"settings\": {\n\"index\": {\n\"number_of_replicas\": 0\n}\n},\n\"mappings\": {\n\"properties\": {\n\"region\": {\n\"type\": \"keyword\"\n},\n\"host\": {\n\"type\": \"keyword\"\n},\n\"container\": {\n\"type\": \"keyword\"\n},\n\"cpu\": {\n\"type\": \"integer\"\n}\n}\n}\n} Next, we can use the below GET request on the _mapping endpoint to validate the mappings have been created properly for our test index. GET /test_0001/_mapping Finally, we will add some test data to practice with. Our data here has four fields: region, host, container, and CPU. PUT /test_0001/_bulk\n{ \"index\": { \"_index\": \"test_0001\", \"_id\": \"1\" } }\n{ \"region\": \"iad\", \"host\": \"h1\", \"container\": \"c1\", \"cpu\": 10}\n{ \"index\": { \"_index\": \"test_0001\", \"_id\": \"2\" } }\n{ \"region\": \"iad\", \"host\": \"h1\", \"container\": \"c2\", \"cpu\": 15}\n{ \"index\": { \"_index\": \"test_0001\", \"_id\": \"3\" } }\n{ \"region\": \"iad\", \"host\": \"h2\", \"container\": \"c1\", \"cpu\": 20}\n{ \"index\": { \"_index\": \"test_0001\", \"_id\": \"4\" } }\n{ \"region\": \"iad\", \"host\": \"h2\", \"container\": \"c2\", \"cpu\": 50}\n{ \"index\": { \"_index\": \"test_0001\", \"_id\": \"5\" } }\n{ \"region\": \"dub\", \"host\": \"h1\", \"container\": \"c1\", \"cpu\": 50}\n{ \"index\": { \"_index\": \"test_0001\", \"_id\": \"6\" } }\n{ \"region\": \"dub\", \"host\": \"h1\", \"container\": \"c2\", \"cpu\": 90}\n{ \"index\": { \"_index\": \"test_0001\", \"_id\": \"7\" } }\n{ \"region\": \"dub\", \"host\": \"h2\", \"container\": \"c1\", \"cpu\": 50}\n{ \"index\": { \"_index\": \"test_0001\", \"_id\": \"8\" } }\n{ \"region\": \"dub\", \"host\": \"h2\", \"container\": \"c2\", \"cpu\": 70} Using multi-terms aggregation\nNow for the real fun! For those who may not have worked with aggregations, ‚Äúhot‚Äù is an arbitrary name for this aggregation. Additionally, \"size\": 0 specifies that we do not want to return the documents that contributed to the query. The multi_terms field accepts keyword fields that are then used as buckets for aggregation. This is similar to how the SQL GROUP BY statement works, where the order in which the terms are specified determines the order in which they are grouped in the results.\nThe order section specifies an aggregation that should be used to order the buckets. The max-cpu aggregation finds the maximum value of the CPU metric for each bucket. This layering of sorting is what makes multi-terms aggregation so powerful. GET /test_0001/_search\n{\n\"size\": 0,\n\"aggs\": {\n\"hot\": {\n\"multi_terms\": {\n\"terms\": [{\n\"field\": \"region\"\n},{\n\"field\": \"host\"\n}],\n\"order\": {\"max-cpu\": \"desc\"}\n},\n\"aggs\": {\n\"max-cpu\": { \"max\": { \"field\": \"cpu\" } }\n}\n}\n}\n} Below we can see the results of our query. As a reminder, ‚Äúhits‚Äù is empty here because we specified ‚Äúsize‚Äù: 0 earlier, meaning we don‚Äôt want to return the documents that contributed to the aggregations. Now we can see clearly that the region dub has the servers with the highest CPU contention‚Äîin particular, host h1. {\n\"took\": 3,\n\"timed_out\": false,\n\"_shards\": {\n\"total\": 1,\n\"successful\": 1,\n\"skipped\": 0,\n\"failed\": 0\n},\n\"hits\": {\n\"total\": {\n\"value\": 8,\n\"relation\": \"eq\"\n},\n\"max_score\": null,\n\"hits\": []\n},\n\"aggregations\": {\n\"hot\": {\n\"doc_count_error_upper_bound\": 0,\n\"sum_other_doc_count\": 0,\n\"buckets\": [\n{\n\"key\": [\n\"dub\",\n\"h1\"],\n\"key_as_string\": \"dub|h1\",\n\"doc_count\": 2,\n\"max-cpu\": {\n\"value\": 90.0\n}\n},\n{\n\"key\": [\n\"dub\",\n\"h2\"],\n\"key_as_string\": \"dub|h2\",\n\"doc_count\": 2,\n\"max-cpu\": {\n\"value\": 70.0\n}\n},\n{\n\"key\": [\n\"iad\",\n\"h2\"],\n\"key_as_string\": \"iad|h2\",\n\"doc_count\": 2,\n\"max-cpu\": {\n\"value\": 50.0\n}\n},\n{\n\"key\": [\n\"iad\",\n\"h1\"],\n\"key_as_string\": \"iad|h1\",\n\"doc_count\": 2,\n\"max-cpu\": {\n\"value\": 15.0\n}\n}]\n}\n}\n} Wrapping it up\nWith this information we can now see which regions and hosts have the most contention for CPU resources. This is just one example of how multi-terms aggregation can be useful. To learn more be sure to check out the documentation page or read the original pull request. Additionally, if you find any issues or have questions, feel free to either create an issue on GitHub or ask a question on the forum!",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/content-compass/",
    "title": "Which Way is the Compass Pointing?",
    "content": "I‚Äôm still tagging, and much like in my last post, I‚Äôve been tagging forum threads in hopes of gaining insight into where to target content creation. What is most clear is the data showing that we definitely need more configuration, troubleshooting, and documentation material for OpenSearch and OpenSearch Dashboards in general (don‚Äôt forget to file an issue for whatever you think is missing!)\nA few lines of code and some persistence is all it took for this to become clear. I‚Äôm personally enjoying using OpenSearch Dashboards to visualize forum data in a number of ways. I thought I‚Äôd take you along on part of my journey and share what I think might be meaningful for everyone.\nIngestion\nI‚Äôm applying a business intelligence use case to OpenSearch. Rather than a constant stream of data coming from the various nodes of the infrastructure, I‚Äôm taking batches of data provided to me at regular intervals and processing them with a local instance of OpenSearch.\nThis could be called an ‚Äúad hoc‚Äù use of OpenSearch. Some may have called this use case silly, but I really don‚Äôt believe that to be true. OpenSearch makes a great analysis tool, even if its powerful search capabilities aren‚Äôt used. This is mostly because of the simplicity of being able to spin up an OpenSearch cluster with OpenSearch Dashboards using Docker.\nThe first challenge was indexing the data. I wrote a standalone Ruby app to take a CSV file that has headers on the first line, convert each line into a JSON object, and then index each object using the bulk API (thanks to the maintainers and contributors of the openseach-ruby gem!). I did not consider the task to be difficult, despite my limited programming ability.\nSoon I had an index filled with objects that represented all the data I needed for my monthly counts. Something wasn‚Äôt quite right though. A previous article I had written came to mind, and I was reminded of an important lesson about index mappings. Index mappings are important! I went into stack management to create an index pattern, and I noticed that it didn‚Äôt detect my date and time field as such. I won‚Äôt be able to visualize any aggregations with date or time filters, which I found to be unacceptable. Here‚Äôs the mapping I ended up with: {\n'mappings': {\n'properties': {\n'created_at': { \"type\": \"date\",\n\"format\": \"yyyy-MM-dd HH:mm:ss zzz\" },\n'tag': { \"type\": \"keyword\" },\n'category_id': { \"type\": \"keyword\" },\n'title': { \"type\": \"keyword\" }\n}\n}\n} In case you need to know how to create your own time format, the format field follows the conventions of the ‚Äú DateTimeFormatter ‚Äù class. Check out the section under ‚ÄúPatterns for Formatting and Parsing.‚Äù\nThis time I was able to create an index pattern, and I had some data to look at. Poof! You‚Äôre a compass! We‚Äôre all the compass. The tags and participation in the forum provide this data, which is, quite literally, the community‚Äôs own activity. Now that the data was loaded with the right mapping, the visualizations were easy. Let‚Äôs check a few out!\nMy favorite is the heat map. As a means of providing a compass, this is quite helpful. Each of the categories in the forum is on an axis, and each of the tags is on another. Absorbing this was easy. From start to finish, it took 10 minutes. I thought it would be neat to create a starting dashboard by combining the heat map with a data table readout of the actual topics in the forum. So, I made this data table ** with just a few mouse clicks. Combining the heat map and data table into a dashboard made something that was more useful than I expected. From this dashboard, I have a plain and easy way to filter topics by category and tag. This will be very useful for helping to target specific topics. No queries. Just drilling. What‚Äôs next?\nFrom here on out, I plan to take steps to deepen the dimensions of our data and will be implementing this compass as a feedback and guidance mechanism for content. It might be time to revisit the forum categories to see if a rearrangement there would deepen our view here.\nIn the meantime, please do continue to join our community meetings as well as get involved in the forums.",
    "keywords": [
      "intro"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-2-2-is-now-available/",
    "title": "OpenSearch 2.2 is now available!",
    "content": "OpenSearch 2.2 is ready to download! This release includes 23 new features and 12 enhancements to help you build and optimize your solutions for search, analytics, and observability workloads. Following are some highlights of the capabilities you can use to advance machine learning (ML) models, data visualizations, cluster resiliency, and more. As always, the release notes provide further details. Update: On August 12 the OpenSearch project published a security advisory for a vulnerability in the software‚Äôs security plugin. The vulnerability allows requests to access sensitive information when the customer has acted to restrict access that specific information. This issue has been fixed in OpenSearch 2.2.0. Users of 2.0.0 or 2.1.0 versions are encouraged to upgrade to version 2.2.0 to resolve the vulnerability.\nNew ML features and algorithms Logistic Regression: This release extends OpenSearch‚Äôs ML capabilities with the addition of a logistic regression algorithm to the ML Commons plugin. Often, logistic regression is used to model a binary outcome (something that can take two values, such as ‚Äúyes/no‚Äù or ‚Äútrue/false‚Äù) to solve a classification problem, such as predicting whether a new sample best fits a particular category. This supports a number of interesting use cases in areas such as natural language processing, recommendation systems, fraud detection, and more. Lucene HNSW Implementation: The 2.2 release benefits from the project‚Äôs collaboration with the Lucene community with the addition of Lucene‚Äôs implementation of the hierarchical navigable small worlds (HNSW) algorithm for approximate k-NN search. Now OpenSearch users have a choice between Lucene-based k-NN search, which is platform independent, and the C-based libraries Non-Metric Space Library (Nmslib) and Facebook AI Similarity Search (Faiss), which users find well-suited for high-performance, highly scalable workloads. RCFSummarize Algorithm: This release also introduces a new clustering algorithm, RCFSummarize, that enables users to cluster data into similar groups. The RCFSummarize clustering technique is different from traditional k-means algorithms in that it does not require you to specify a k value for the number of categories. Instead, it will adaptively find out the real k, making it easier to get insights from your data.\nOther updates in OpenSearch 2.2 include the following. Search by Relevance: With OpenSearch 2.2, users can now search their indexes for documents by the relevance of the input query using Structured Query Language (SQL) or Piped Processing Language (PPL). This update gives users another approach to generating relevance-based searches along with the option of using OpenSearch query domain-specific language (DSL). For a comprehensive view of the relevance-based functions and search parameters now available in the SQL plugin, check out the feature‚Äôs documentation. Awareness Attribute: In order to make sure OpenSearch clusters are well balanced and resilient to failures in a zone or rack, a new configuration option has been added. Enabling routing.allocation.balance.awareness will ensure that the number of replicas is, at maximum, a multiple of the two awareness attributes cluster.routing.allocation.awareness.attributes and cluster.routing.allocation.awareness.force.zone.values. See our documentation regarding cluster formation for more information. Custom GeoJSON Support in Region Maps: Until 2.2, region maps in OpenSearch Dashboards had support for a limited set of vector maps for visualization. This feature now provides the ability to upload your own custom region in GeoJSON format and use it as a vector map for visualizations in OpenSearch Dashboards. It also provides the option to draw your own geographic boundaries on a visualization. If you‚Äôre unfamiliar with GeoJSON, get your start here. Data Rollup Enhancements: For data analytics workloads, older time series data can support valuable insights; however, retaining complete datasets as they age can lead to increased storage costs. In 2.2, OpenSearch adds several rollup enhancements that let you roll up aggregated results from older data to dynamic target indexes with the use of a Mustache template. Along with this, you can also now run one search query across multiple target indexes. You can use these features to keep mining value from older time series data while reducing data granularity and lowering the cost burden. Feature Attribution on Anomaly Details Page: OpenSearch added feature attribution and expected value to its anomaly detection functionality in version 1.2. Today‚Äôs release introduces a new user experience that makes it easier to understand what data is driving anomalies. With this update, feature attribution and expected value are exposed on the anomaly detection details page, as seen in the example below. Get started\nOpenSearch 2.2 is ready for download here! Take a look at the latest documentation and release notes, and keep an eye on the OpenSearch Project roadmap for exciting features and enhancements coming in version 2.3!",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/dattell-compares/",
    "title": "Partner Highlight: Dattell blog post compares OpenSearch and Elasticsearch",
    "content": "OpenSearch partner and contributor Dattell has published a blog post that explores technical and business aspects of the OpenSearch project and Elasticsearch offerings. The post by Dattell co-founder Maria Hatfield, whose company offers consulting support and managed services for OpenSearch, Elasticsearch, and Kafka, offers an in-depth look at how these software tools stack up in areas like community, licensing, and security, as well as comparing feature sets, visualization tools, and documentation. Check out Dattell‚Äôs post to see their perspective.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Agent-Ingestion-Usage-In-OpenSearch-Survey-Results/",
    "title": "Agent Ingestion Usage in OpenSearch Survey Results",
    "content": "First, a huge thank you to all of you who responded to the survey. Understanding how you use agents in your ingestion pipelines helps us prioritize use cases that deliver the most value to the community.\nIn total, 67 individuals responded to the survey run in July 2022. Following are the results:\n~54% of participants said that they still use Beats in their client ingestion pipeline (down from ~66% in June 2021).\nOf the participants who said that they still use Beats:\n~52% are not planning to move off of Beats.\n~23% plan to move off in the next 12 months.\n~25% are waiting for a feature/solution before moving off.\n~46% of participants do not use Beats in their client ingestion pipeline.\n21% use Fluent Bit.\n14% use Fluentd.\n0% use Open Telemetry Collector.\n~64% chose Other, which consisted of custom-built solutions, Logstash, and other solutions.\nThe survey also asked the community which agents and modules are most popular in their environments. Below are the results from those who use Beats agents in their client ingestion environment. Agent/Module Number Who Use Filebeat w/ Logstash 19 Metricbeat w/ system 16 Filebeat w/ Apache 16 Winlogbeat w/ security 15 Auditbeat w/ auditd 11 Filebeat w/ *SQL (all SQL logs) 10 Filebeat w/ Netflow 10 Filebeat w/ Nginx 10 Auditbeat w/ file integrity 8 Auditbeat w/ system 8 Metricbeat w/ http 6 Metricbeat w/ Kafka 6 Metricbeat w/ *SQL (all SQL logs) 6 Filebeat w/ Cisco 6 Filebeat w/ Kafka 6 Heartbeat 6 Metricbeat w/ Nginx 5 Filebeat w/ IIS 5 Packetbeat 4 Metricbeat w/ IIS 3 Functionbeat 3 Journalbeat 2 Filebeat w/ HAProxy 1 Fortinet 1 Checkpoint 1 So what did we learn?\nThanks to the survey response, the community now has a better understanding of how Beats usage is trending. Last year, 66% of community members were using Beats, which dropped to 54% this year. If all things go as expected with planned migrations in the commuity, Beats usage will drop to 42% in 2023. For those who are still using Beats, the most popular agents are Filebeat, Metricbeat, Winlogbeat, and Auditbeat.\nFor the ~28% of the community who have no plan to stop using Beats into OpenSearch via Logstash using the OpenSearch Output plugin, users should be aware that Elastic Common Schema (ECS) compatibility mode is turned on by default in Logstash 8.0. If community members encounter ECS compatibility errors, they should disable ECS in their pipeline.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-plugin-zips-now-in-maven-repo/",
    "title": "OpenSearch plugin zips now in Maven repo",
    "content": "Starting with the release of OpenSearch 2.1.0, OpenSearch plugin zips are now signed and published to a central Apache Maven repo. Using the Release zips and Snapshot zips Maven Repo URLs, OpenSearch plugin zips can now be consumed as a dependency to build other plugins or fetched as standalone components for your OpenSearch cluster.\nMotivation\nBefore OpenSearch 2.1, plugin zips used as dependencies could not be downloaded dynamically during runtime because plugin zips were not a part of the version-controlled Maven system. The only mechanism for plugin downloads was each plugin‚Äôs respective Java jars through Maven coordinates. This system forced users who wanted more control over their OpenSearch plugin configuration to use a dependency plugin zip built on a developer desktop instead of a more reliable version-controlled plugin.\nFurthermore, to facilitate the plugin availability as a dependency, tests executed against the OpenSearch build process from zip were not accurate, as each local build zip had to find the plugin repo in order to ensure that plugin‚Äôs availability. These restrictions were challenging to our community because using plugin zips as separate isolated components via a cached mechanism proved to be impossible.\nBenefits of Maven\nWith Maven, plugin zips can now be retrieved by:\nDownloading each plugin directly using their respective Maven coordinates.\nUsing clickstream from the central Maven repo, which can be cached later to a local Maven repo.\nFetching the development SNAPSHOT version with the same Maven groupID as org.opensearch.plugin.\nUsing OpenSearch plugin zips through Maven offers the following benefits:\nPlugins zip in the central Maven repo are already signed with.asc,.md5,.sha1,.sha256, and.sha512 extensions.\nUsers are no longer required to to check in zips to any src/ files because zips can be fetched with the right groupID, artifactID, and version.\nTests and continuous integration (CI) workflows can directly run against zips from the Maven repo instead of requiring a manual download.\nMaven zip publication with Gradle\nOpenSearch publishes plugin zips using a custom Gradle plugin, opensearch.pluginzip. With OpenSearch 2.1, all OpenSearch Gradle-supported plugins create a new task, publishPluginZipPublicationToZipStagingRepository. The task performs all the heavy lifting for users by:\nIdentifying the distribution plugin zip.\nSetting the Maven coordinates.\nGenerating the POM file.\nUpdating with the user-generated POM fields.\nPublishing the zip to your Maven repo.\nYou can find more details about the inner workings of OpenSearch plugins in the opensearch-plugins repo. Figure 1: Workflow that ships generated plugin zips to Maven Figure 1: Workflow that ships generated plugin zips to Maven\nConsume plugin in zips\nYou can fetch plugin zips in three different ways. Using the Maven CLI Consume from the Central Maven repo: mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:get -DrepoUrl=https://repo1.maven.org/maven2 -Dartifact=org.opensearch.plugin:opensearch-job-scheduler:2.1.0.0:zip Consume from the Snapshot Maven repo: mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:get -DrepoUrl=https://aws.oss.sonatype.org/content/repositories/snapshots -Dartifact=org.opensearch.plugin:opensearch-job-scheduler:2.1.0.0-SNAPSHOT:zip Gradle Project: Using the build.gradle file dependencies {\nclasspath \"org.opensearch.plugin:opensearch-job-scheduler:2.1.0.0@zip\"\nclasspath \"org.opensearch.plugin:opensearch-knn:2.1.0.0@zip\"\n} Maven Project: Using the pom.xml file &lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.opensearch.plugin&lt;/groupId&gt;\n&lt;artifactId&gt;opensearch-job-scheduler&lt;/artifactId&gt;\n&lt;version&gt;2.1.0.0&lt;/version&gt;\n&lt;packaging&gt;zip&lt;/packaging&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/day-in-the-life-of-a-release-manager/",
    "title": "Day in the Life of a Release Manager",
    "content": "An OpenSearch version release can be a daunting prospect, especially if you‚Äôre the one managing it. There are ambiguously worded tasks that need to be completed, deadlines to be met. Now not only do you have to get your work done, but you also have to make sure that everyone else does too. Facing this challenge for the first time, I thought it would be useful to document the process so that any future release managers can avoid the mistakes I made.\nIt all starts with a gif. My manager asks in our team Slack if anyone would like to volunteer to be the release manager of our plugins. My mind immediately goes to the scene in the The Hunger Games where Katniss volunteers as tribute. I want to send the gif, but that means adding more to my plate. In addition to it being my first release, this one has big features, so I ask my colleagues if it will be manageable. They tell me that the work of a release manager starts after code freeze and that there will be no overlap between my responsibilities as an individual contributor and a release manager. The responsibilites would just be to complete a list of tasks for each of our plugins and report to the overall release manager. I am quickly convinced, and I hit send on the gif.\nA couple weeks go by with no updates until a couple days before feature freeze, I am given my first mission: Add all of the plugins to the 1.3 release manifest with 1.3 branches. My actual first mission: Figure out what a release manifest is.\nWhat is a release manifest? I turn to the results of a quick Google search. A release manifest contains the collection of versioned stuff that is being deployed, configuration settings, and approvals. What, how, where, and who approved it. This is similar to a shipping form listing out the boxes sent, value and contents of the goods, destination, and signatures from sender and receiver. I find the release manifest for OpenSearch within a repository called opensearch-build within a folder conveniently called manifests. I start reading all of the docs I can find related to this release and previous ones.\nAfter forking the repository, I check out a branch, add my team‚Äôs plugins to the manifest, and create a pull request (PR).\nTwo weeks before release, GitHub issues are created in each repository with a checklist for each (the actual checklist that was created for this version release is pictured below). I self-assign the issue and check off some of the completed tasks. In hindsight, it seems obvious that I would know that checking off a task does not mean that it is actually completed. But this only becomes clear to me when I wake up one morning to the continuous integration (CI) builds failing. Thanks to a teammate, the issue is quickly triaged: dashboards-visualizations‚Äô main branch has not been bumped to 1.3. My memory of bumping the version for three repositories conveniently expanded to include the fourth. Mortified, I glare at the fourth and fifth check marks of my checklist. Mistake #1: Checking off an item that wasn‚Äôt complete. As the code freeze date approaches, I hear other release managers talk about making sure branches are cut and manifests are updated. For the repositories I am managing, that means communicating with respective repo owners and tracking when they are ready to cut a 1.3 branch (Cutting a branch just means creating a new branch labeled 1.3, which marks the state of that release). Once pencils are down, I cut 1.3 branches and update the manifest to reference 1.3 instead of main.\nAs OpenSearch moves toward its release date, I start my first release task: completing documentation. This means updating the documentation-website repo. Along with documentation, release notes have to be gathered (the actual checklist that was created for this version release is pictured below). Now you may be wondering, as I was, what are release notes and where are they stored? Release notes are a categorized list of the commits that are going into the new release. They are most likely stored in a folder called release-notes (https://github.com/opensearch-project/observability/tree/main/release-notes) within the repository. Luckily, my team has a script that automatically drafts release notes based on PRs that are merged with labels. Unluckily, none of the observability PRs were labeled, which means manually categorizing PRs. I create new release note files for this release in every repository. Note to self: For future releases, put a mechanism in place to make sure PRs are labeled before they are merged for future releases. After merging the PRs adding release notes, I check that off all of the lists. Little do I know, I have just made mistake #1 again. That afternoon I get a message from the overall release manager that one of my repositories does not have release notes. I learn that even repositories with no changes made for that release still need to have release notes. Mistake #2: Not adding release notes for repositories with no changes. I quickly add a short release note and backport it to the 1.3 branch. To backport is to add commits to the main branch and then also push them to the tip of a previously cut branch. Most repositories have scripts to auto-backport PRs. I return to completing documentation. Soon after, the release launches and release tags are automatically created. My job as release manager abruptly comes to an end. My final mission: make sure any future, first-time release managers do not struggle and make the same mistakes I did.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/New-series-From-the-editors-desk/",
    "title": "From the editor's desk: OpenSearch and inclusion",
    "content": "As part of our commitment to providing complete and best-in-class documentation to the OpenSearch community, we want to ensure that you have visibility into how we approach creating bar-raising content. To that end, we will be publishing regular blog posts with tips, guidance, and best practices that will help contributors collaborate with us. Subjects may include technical writing, style, voice and tone, brand messaging, and other topics relevant to the production of our documentation. We hope you find this to be useful, and we would love to hear your thoughts on any of the subjects we discuss in this series. Please feel free to make your voice heard as part of the OpenSearch community through our community meetings, forum discussions, and GitHub repository.\nOpenSearch is for everyone, and we are inclusive\nWe‚Äôd like to begin this ongoing conversation with a subject that is foundational to who we are and how we talk to each other as a community: inclusion. As an open-source project, OpenSearch is for everyone, and we are inclusive. We value the diversity of backgrounds and perspectives in the OpenSearch community and welcome feedback from any contributor, regardless of their experience level.\nWhen developing OpenSearch documentation, we strive to create content that is inclusive and free of bias. We use inclusive language to connect with the diverse and global OpenSearch audience, and we are careful in our word choices. Inclusive and bias-free content improves clarity and accessibility of our content for all audiences, so we avoid ableist and sexist language and language that perpetuates racist structures or stereotypes. In practical terms, this means that we do not allow certain terms to appear in our content, and we avoid using others, depending on the context.\nInclusive terminology\nOur philosophy is that we positively impact users and our industry as we proactively reduce our use of terms that are problematic in some contexts. Instead, we use more technically precise language and terms that are inclusive of all audiences.\nThe following terms may be associated with unconscious racial bias, violence, or politically sensitive topics and should not appear in OpenSearch content, if possible. Note that many of these terms are still present but on a path to not being supported. Don‚Äôt use Use instead abort\nstop\nblack day\nblocked day\nblacklist\ndeny list\nexecute\nstart, run\nhang\nstop responding\nkill\nend, stop\nmaster\nprimary, main, leader\nmaster account\nmanagement account\nslave\nreplica, secondary, standby\nwhite day\nopen day\nwhitelist\nallow list The following terms may be problematic in some contexts. This doesn‚Äôt mean that you can‚Äôt use these terms‚Äîjust be mindful of their potential associations when using them, and avoid using them to refer to people. Avoid using Use instead blackout\nservice outage, blocked\ndemilitarized zone (DMZ)\nperimeter network, perimeter zone\ndisable\nturn off, deactivate, stop\nenable\nturn on, activate, start\ninvalid\nnot valid\nprimitive\nprimitive data type, primitive type\npurge\ndelete, clear, remove\nsegregate\nseparate, isolate\ntrigger\ninitiate, invoke, launch, start Version 2.0 replaced non-inclusive terminology (such as master, blacklist) throughout OpenSearch with inclusive terminology (such as cluster manager, allow list), but please let us know if you think we missed something so that we can address it in an expedient manner. Check out the associated GitHub issue for more information.\nAccessibility\nAccessibility involves designing and creating websites, user interfaces, and documentation so that people with disabilities can perceive, navigate, and interact with them. OpenSearch follows basic accessibility guidelines to help ensure that our documentation is available and useful for everyone. Following these principles also helps improve the general usability of content.\nWe follow these general accessibility guidelines in our documentation:\nLinks: Use link text that adequately describes the target page. For example, use the title of the target page instead of ‚Äúhere‚Äù or ‚Äúthis link.‚Äù In most cases, a formal cross-reference (the title of the page you‚Äôre linking to) is the preferred style because it provides context and helps readers understand where they‚Äôre going when they choose the link.\nImages:\nAdd introductory text that provides sufficient context for each image.\nAdd ALT text that describes the image for screen readers.\nProcedures: Not everyone uses a mouse, so use device-independent verbs; for example, use ‚Äúchoose‚Äù instead of ‚Äúclick.‚Äù\nLocation: When you‚Äôre describing the location of something else in your content, such as an image or another section, use words such as ‚Äúpreceding,‚Äù ‚Äúprevious,‚Äù or ‚Äúfollowing‚Äù instead of ‚Äúabove‚Äù and ‚Äúbelow.‚Äù\nWe aim to follow these guidelines consistently, but, again, please let us know if you think we missed something, and we‚Äôll work with you to make an appropriate change.\nJoin the conversation\nOpenSearch is committed to being a community where everyone can join and contribute, and we welcome community contributions to the OpenSearch style guidelines. To access our documentation, see the OpenSearch documentation home page. If you want to contribute to the OpenSearch documentation, see the CONTRIBUTING.md file, which covers how to open an issue or create a pull request on GitHub. We look forward to collaborating with you!",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/snapshot-operations/",
    "title": "Snapshot Operations in OpenSearch",
    "content": "In this post, we want to dive deep into snapshot operations in OpenSearch. Snapshots are backups of a cluster‚Äôs indexes and state. The state can include cluster settings, node information, index metadata, and shard allocation information. Snapshots are used to recover from failures, such as a red cluster, or to move data from one cluster to another without data loss. Remote searchable snapshots will enable users to restore a snapshot without downloading all the shards on the cluster nodes, requiring us to dive deep into the existing snapshot operations within OpenSearch.\nThis post will help you understand the fundamentals of snapshot operations. If you enjoy this topic and want to put your knowledge into practice, please consider contributing to remote searchable snapshots.\nWhat snapshot operations can I perform on my cluster?\nYou can perform the following snapshot operations on a cluster: Creating a snapshot: This operation creates a snapshot within a repository, backing up the cluster settings and index data.\nDeleting a snapshot: This operation deletes unwanted versions of a snapshot from a repository. Restoring a snapshot: This operation restores a snapshot within a repository in the event of a failure or cluster migration to bring the cluster back to the same state as when the snapshot was created.\nThe SnapshotsService is responsible for creating and deleting snapshots, whereas the RestoreService is responsible for restoring snapshots. We will cover these operations in detail in the following sections. A key thing to note is that snapshots are incremental in nature, meaning that they only store data that has changed since the last successful snapshot. Refer to Take and restore snapshots for more information.\nHow are snapshots stored?\nA snapshot is a backup taken from a running OpenSearch cluster and stored within a repository.\nWhat is a repository?\nA Repository acts as an interface on top of an underlying storage system and provides higher-level APIs for working with snapshots. The repository interface is part of the plugin feature of OpenSearch. Out of the box, OpenSearch comes with three repository implementations‚ÄîAmazon Simple Storage Service (Amazon S3), Azure Blob Storage, and Google Cloud Storage‚Äîthat can be optionally enabled in your installation. External developers can also provide their own implementation of a repository if none of the existing implementations fit their use case.\nThe abstract class BlobStoreRepository provides the base implementation of a repository, which implements all common snapshot management operations. The default shared file system repository is implemented by the FsRepository, whereas the repositories for other types of storage systems are provided as plugins.\nInternally, a BlobStoreRepository uses a BlobStore, a wrapper on top of a BlobContainer, that provides additional operations such as retrieving stats on the store. Further, the BlobContainer interface is an abstraction on top of the underlying storage systems (such as a shared file system and the other plugin-based implementations) for managing blob entries, where each blob entry is a named group of bytes. The concrete implementations of the BlobContainer interface define the CRUD operations on blob entries. For instance, FsRepository uses the FsBlobContainer as the file-system-based implementation of the BlobContainer. For other storage systems, specific repository plugins provide the underlying implementations, such as S3BlobContainer, GoogleCloudStorageBlobContainer, AzureBlobContainer, and HdfsBlobContainer.\nThe following diagram depicts the relationships between Repository, BlobStore, and BlobContainer. What is the structure of a repository?\nA repository can store multiple snapshots, which can contain a single index or multiple indexes. At the low level, the RepositoryData object holds the list of all snapshots as well as the mapping of the index name to the repository IndexId. For each shard i in a given index, its path in the blob store is root/indices/${index-snapshot-uuid}/${i}. The following diagram shows the directory structure of the blob store (For more information, see Javadocs). How does snapshot creation work?\nClients can create snapshots by performing a PUT operation on the _snapshot API (For more on the snapshot creation API, see Take snapshots). The SnapshotService.createSnapshot method is called to create a new snapshot. Only one snapshot creation process can be running at a given time.\nHow do the nodes coordinate for a snapshot operation?\nThe ClusterState object holds the information about the current cluster state.\nThere are two communication channels for snapshots between the cluster manager and all other nodes:\nThe cluster manager updates the ClusterState object by adding, removing, or altering the contents of its custom entry SnapshotsInProgress. All nodes consume the state of the SnapshotInProgress and start or terminate the relevant shard snapshot tasks accordingly.\nNodes executing shard snapshot tasks report either success or failure of their snapshot tasks by submitting an UpdateIndexShardSnapshotStatusRequest to the cluster manager node, which updates the snapshot‚Äôs entry in the ClusterState object accordingly.\nThe following image depicts the interaction between the cluster manager node, the data nodes, and the ClusterState object described above. Flow of events for snapshot creation\nThe following image depicts the flow of events for snapshot creation. The numbers in the image correspond to the steps within the flow outlined below. The SnapshotService on the cluster manager node determines the node allocation of primary shards for all indexes that are part of the snapshot request. It then creates a SnapshotsInProgress.Entry with the STARTED state. Each snapshot Entry has a map of ShardId to ShardSnapshotStatus to keep track of the nodes and the shards associated with the snapshot. Each shard‚Äôs status (enum ShardState) is set to one of the following: INIT ‚Äì This is the initial state for all shards that have a healthy primary node. WAITING ‚Äì Primary is initializing at this point. MISSING ‚Äì Primary for a shard is unassigned. SnapshotsInProgress is the container for the snapshot metadata that is part of the ClusterState object, hence this is updated by the cluster manager node using a ClusterStateUpdateTask.\nThe primary node receives a ClusterChangedEvent and then executes the snapshot process for the shard with an INIT state.\nThe primary nodes write the the shard‚Äôs data files to the snapshot‚Äôs repository.\nOnce it finishes, the node sends an UpdateIndexShardSnapshotStatusRequest to the cluster manager with a signal indicating the status of the snapshot process. The cluster manager then updates the state of the shard to one of the following: SUCCESS ‚Äì The snapshot of the shard was successful. FAILED ‚Äì Either the shard‚Äôs primary has been relocated after the entry was created or the snapshot process on the primary node failed.\nThe cluster manager node then updates the entry for the snapshot to SUCCESS if all the primary shards‚Äô snapshots were in a completed state ( SUCCESS, FAILED, or MISSING). It ends the snapshot process by writing all the metadata to the repository.\nFinally, the cluster manager node removes the SnapshotsInProgress.Entry from the ClusterState object, indicating the end of the snapshot creation process.\nBoth the cluster manager and the data nodes can read from and write to a blob store. All metadata related to a snapshot‚Äôs scope and health is written by the cluster manager node only. Data nodes can only write the blobs for shards they hold as primary. The nodes write the primary shard‚Äôs segment files to the repository as well as metadata about all the segment files that the repository stores for the shard.\nWhat happens on the primary data node for snapshot creation?\nIn the shard‚Äôs primary data node, the BlobStoreRepository.snapshotShard method is executed. The method performs the following steps:\nThe method starts by taking a Lucene IndexCommit object and retrieving all file names related to the particular commit.\nThen the method gets the BlobStoreIndexShardSnapshots object, which contains information about all the snapshots in the repository for the assigned primary shard on the current data node.\nAfter retrieving data from the IndexCommit and the repository, the method compares the files in IndexCommit and existing files in the blob store to determine the new segment files to write.\nThe method then proceeds to build a new BlobStoreIndexShardSnapshot that contains a list of all the files referenced by the snapshot as well as some metadata about the snapshot. For each segment file, the method writes the referenced file to the blob store with a unique UUID whose mapping to the real segment file is in the BlobStoreIndexShardSnapshot.\nThe method then writes the BlobStoreIndexShardSnapshot data that contains the details of all files in the snapshot.\nFinally, it writes the updated shard metadata BlobStoreIndexShardSnapshots.\nHow is the snapshot metadata updated within the repository?\nAfter all primaries have finished writing the necessary segment files to the blob store, the cluster manager node finalizes the snapshot by invoking Repository.finalizeSnapshot. This method executes the following actions in order:\nFirst, the method writes a blob containing the cluster metadata to the root of the blob store repository at /meta-${snapshot-uuid}.dat.\nThen the method writes the metadata for each index to a blob in that index‚Äôs directory at /indices/${index-snapshot-uuid}/meta-${snapshot-uuid}.dat.\nThe method then writes the SnapshotInfo blob for the given snapshot to the key /snap-${snapshot-uuid}.dat directly under the repository root.\nFinally, the method writes an updated RepositoryData blob containing the new snapshot.\nHow are snapshots deleted?\nDeletion of a snapshot involves either deleting it from the repository or terminating (if in progress) and subsequently deleting it from the repository. To delete it from the repository, clients can perform a DELETE operation on the _snapshot API.\nTerminating snapshots\nTerminating a snapshot starts by updating the state of the snapshot‚Äôs SnapshotsInProgress.Entry to ABORTED. Then, the following steps are performed:\nThe termination of a snapshot begins when the snapshot‚Äôs state changes to ABORTED in the ClusterState object by the cluster manager node.\nThis change in the ClusterState object is then picked up by the SnapshotShardsService on all nodes.\nThose nodes that are assigned a shard snapshot action terminate the process and notify the cluster manager. If the shard snapshot action was completed or in the FINALIZE state when the termination was registered by the SnapshotShardsService, then the shard‚Äôs state is reported to the cluster manager as SUCCESS. Otherwise, it is reported as FAILED.\nOnce all the shards‚Äô statuses are reported to the cluster manager, the SnapshotsService on the cluster manager finishes the snapshot process and updates the metadata in the repository.\nFinally, the SnapshotsService on the cluster manager removes the SnapshotsInProgress.Entry from the cluster state.\nThe following image depicts the interaction between the cluster manager, the repository, the data nodes, and the cluster state. The numbers in the image correspond to the steps described above. Deleting a snapshot\nSnapshot deletion is executed exclusively on the cluster manager node. To delete a snapshot, the following steps are performed:\nAssuming there are no entries in the ClusterState ‚Äôs SnapshotsInProgress, deleting a snapshot starts with the SnapshotsService creating an entry for deleting the snapshot in the ClusterState ‚Äôs SnapshotDeletionsInProgress.\nOnce the ClusterState contains the deletion entry in SnapshotDeletionsInProgress, the SnapshotsService invokes Repository.deleteSnapshots for the given snapshot.\nThe Repository then removes files associated with the snapshot from the repository store and updates its metadata to reflect the deletion of the snapshot.\nAfter the deletion of the snapshot‚Äôs data from the repository finishes, the SnapshotsService submits an update to ClusterState to remove the deletion‚Äôs entry in SnapshotDeletionsInProgress, which concludes the process of deleting the snapshot.\nThe following image depicts the interaction between the cluster manager, the repository, and the cluster state. The numbers in the image correspond to the steps described above. How is the snapshot metadata updated within the repository?\nIn the cluster manager node, BlobStoreRepository.deleteSnapshots is executed.\nThis method executes the following actions in order:\nFirst, the method fetches the current RepositoryData from the latest index-N blob in the repository data.\nThen, for each index referenced by the snapshot, the method performs the following:\nDeletes the snapshot‚Äôs IndexMetadata at /indices/{index-snapshot-uuid}/meta-{snapshot-uuid}.\nIterates through all shard directories /indices/{index-snapshot-uuid}/{i} and performs the following:\nRemoves the BlobStoreIndexShardSnapshot blob at /indices/{index-snapshot-uuid}/{i}/snap-{snapshot-uud}.dat.\nLists all blobs in the shard path /indices/{index-snapshot-uuid} and builds a new BlobStoreIndexShardSnapshots from the remaining BlobStoreIndexShardSnapshot blobs in the shard. Afterwards, writes it to the next shard generation blob at /indices/{index-snapshot-uuid}/{i}/index-{uuid} (The shard‚Äôs generation is retrieved from the map of shard generations in the RepositoryData in the root index-{N} blob of the repository).\nCollects all segment blobs (identified by the data blob prefix __) in the shard directory that are not referenced by the new BlobStoreIndexShardSnapshots that was written in the previous step and the previous index-{uuid} blob so that it can be deleted at the end of the snapshot deletion process.\nThe method then writes an updated RepositoryData blob where the deleted snapshot is removed and the repository generations that changed for the shards affected by the deletion are updated.\nThen the method deletes the global metadata blob meta-{snapshot-uuid}.dat stored directly under the repository root for the snapshot as well as the SnapshotInfo blob at /snap-{snapshot-uuid}.dat.\nFinally, the method deletes all the blobs with no references, which were collected when updating the shard directories, and removes any index folders or blobs under the repository root that are not referenced by the new RepositoryData written in the previous step.\nWhat happens when I restore a snapshot?\nThe RestoreService.restoreSnapshot method is used for restoring a snapshot from the repository. This method is executed when the clients call the _restore API under a particular snapshot to be restored. The following steps describe the snapshot restore operation:\nThe service ensures the existence of the snapshot within the repository. It further ensures the ability to restore the snapshot by performing version checks against the snapshot metadata.\nIf the above checks succeed, the service reads additional information about the snapshot and the related metadata for the indexes within the snapshot. It also filters the requested indexes and performs any rename operation based on the request properties.\nThe next steps use the cluster state update task ( submitStateUpdateTask) to perform further restore operations.\nFor each index from Step 2, the service performs the following steps:\nIt checks to ensure the snapshot being restored is not currently undergoing a delete operation. Otherwise, it fails with ConcurrentSnapshotExecutionException. RestoreService ensures that the index is currently either closed or does not exist, in which case it is restored as a new index or throws the SnapshotRestoreException.\nThen, the RestoreService validates that the number of primary shards and replica shards for the index can be served by the current cluster setup.\nFinally, the service adds the new routing and metadata entries once the above pre-checks have succeeded for the particular index.\nAfter creating all the necessary structures for routing, metadata, and the shards, the RestoreService creates a RestoreInProgress.Entry, which is added to the cluster state to keep track of the snapshot restore.\nThis entry also hosts the individual shard states, which can be one of the following: INIT STARTED SUCCESS FAILED The shards are recovered using the IndexShard.restoreFromRespository, where each shard goes through the above states to be restored from the repository to the assigned node. The RestoreInProgress.Entry has a corresponding overall state that is calculated using the individual shard states.\nEventually, the request is routed to AllocationService, which takes care of allocating the unassigned shards and starting the shard recovery process.\nDuring the shard recovery process, an observer instance of RestoreInProgressUpdater is utilized to keep track of the shard states, which helps update the overall progress of snapshot restore. The observer instance contains the logic used to update the shard restore status within the RestoreInProgress.Entry based on the current shard recovery status.\nFinally, once the restore operations for all of the shards are complete, the overall status of RestoreInProgress.Entry is updated to reflect a SUCCESS or FAILURE status. After this update, RestoreService calls cleanupRestoreState, which removes the RestoreInProgress.Entry from the cluster state, concluding the restore process.\nHow can I contribute?\nWe are currently working on searchable snapshots, which involves working directly with the snapshot restore process and the repository interfaces. If you‚Äôd like to contribute, the following issues would be a great place to start:\nhttps://github.com/opensearch-project/OpenSearch/issues/2578\nhttps://github.com/opensearch-project/OpenSearch/issues/3895\nhttps://github.com/opensearch-project/OpenSearch/issues/2919",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/sql-search-relevance-selective-aggregation-and-window-functions-in-OpenSearch/",
    "title": "SQL search relevance, selective aggregation, and window functions in OpenSearch",
    "content": "According to a review by IEEE Spectrum in 2022, SQL is the sixth most popular programming language. IEEE came to this conclusion by pulling and weighting data across GitHub, Google, Stack Overflow, Twitter, and IEEE Xplore. Did you know that OpenSearch offers a way to query OpenSearch using SQL? In recent releases, the SQL plugin included support for search relevance, selective aggregation, and window functions. You can use the REST API or use the OpenSearch Query Workbench to query OpenSearch using the SQL plugin. Today, we will use the Query Workbench inside the OpenSearch Playground environment to walk through search relevance, selective aggregation, and window functions.\nWe‚Äôll use sample data that everyone can access through the playground environment or the demo Docker install. First, add ‚ÄúSample web logs‚Äù if they aren‚Äôt already added. Next, open the Query Workbench. Now let‚Äôs dive in. Let‚Äôs say that you were a security researcher and wanted to keep track of your system activity. You can use search relevance, selective aggregation, and window functions to monitor this activity.\nSearch relevance\nSearch relevance is a powerful tool that allows you to search through data, such as logs, to find specific documents. In the example below you can use count to determine how many logs fit your selected criteria.\nSearch through logs by client IP addresses and error counts for GET requests resulting in a 503 server error: SELECT clientip, COUNT(*) AS cnt\nFROM opensearch_dashboards_sample_data_logs\nWHERE MATCH(message, \"GET 503\", operator=\"AND\")\nGROUP BY clientip\nORDER BY cnt DESC Additionally, the highlight function returns text with matched terms highlighted: SELECT clientip, HIGHLIGHT(message)\nFROM opensearch_dashboards_sample_data_logs\nWHERE MATCH_PHRASE(message, \"Linux x86_64\") Selective aggregation\nSelective aggregation is useful when you want to understand a total of something across an entire dataset. The SQL plugin will also allow you to filter with certain criteria. Say you want to keep track of large file transfers in your system in order to react to someone moving sensitive data outside of your place of work. In the example below, selective aggregation is used to determine how many HTTP requests were submitted where the bytes count was higher than 10,000: SELECT\nCOUNT(*) AS totalReq,\nCOUNT(*) FILTER(WHERE bytes &gt; 10000) AS totalLargeReq\nFROM opensearch_dashboards_sample_data_logs Window functions Aggregate window functions provide users an easy way to calculate aggregate results over a custom window of time. For example, if you wanted to understand how many web calls occurred each day in order to identify abnormal traffic patterns, you could use the following statement: SELECT\ndate,\nSUM(dailyCnt) OVER(ORDER BY date) AS cumulative\nFROM (\nSELECT\nDATE_FORMAT(timestamp, '%Y/%m/%d') AS date,\nCOUNT(*) AS dailyCnt\nFROM opensearch_dashboards_sample_data_logs\nGROUP BY date) AS tmp\nORDER BY date But what if you wanted to monitor the top 10 websites visited by employees of your company? You could use a classic top-K elements per group problem. The following statement queries the top 10 most visited URLs for each day by using the ranking window function: SELECT date, url, cnt\nFROM (\nSELECT\ndate, url, cnt,\nRANK() OVER(PARTITION BY date ORDER BY cnt DESC) AS rnk\nFROM (\nSELECT\nDATE_FORMAT(timestamp, '%Y/%m/%d') AS date,\nurl AS url,\nCOUNT(*) AS cnt\nFROM opensearch_dashboards_sample_data_logs\nGROUP BY date, url) AS a) AS r\nWHERE rnk &lt;= 10\nORDER BY date, rnk How are you using OpenSearch‚Äôs SQL in your environment? Do you have use cases that have worked for you? We would love to hear about it! As always, if you don‚Äôt see functionality you would like or want to get involved, please visit the SQL Github repository to file an issue.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-2-3-is-ready-for-download/",
    "title": "OpenSearch 2.3.0 is ready for download!",
    "content": "OpenSearch 2.3.0 is here with new capabilities for you to explore! For this release, we prioritized three new features that OpenSearch users have been asking for and that offer significant advances in performance, data durability, and usability. We‚Äôre including them as experimental features so that you have the option to deploy them as you wish or stick with the default approach to these tasks. We hope you will put these features to work and share your feedback on how they perform in your environment, what added functionality you‚Äôd like to see, and any opportunities for improvement. Read on for a look at each new feature and where you can share your impressions.\nSegment replication\nSegment replication offers a new approach to how OpenSearch replicates data, with performance improvements on high-ingestion workloads. Currently, OpenSearch uses a document replication strategy, which indexes the primary shard and each replica in parallel whenever documents are added to, removed from, or updated within an index. With document replication, each transaction needs to be rerun on each replica shard. With segment replication, you can opt to copy Lucene segment files from the primary shard to its replicas instead of having replicas rerun the operation. Because Lucene uses a write-once segmented architecture, only new segment files need to be copied; the existing ones will never change. This approach offers improved indexing throughput and lower resource utilization at the expense of increased network utilization and refresh times.\nTo learn how to enable this feature, see the documentation. You‚Äôll have the option to have some indexes within a cluster that use document replication and others that use segment replication. You can learn more about how this feature is designed, see some preliminary performance metrics, and provide your feedback on this GitHub issue.\nRemote-backed storage\nWith the introduction of segment replication, OpenSearch can now enable remote-backed storage as a new way to protect your clusters against data loss. With OpenSearch 2.3.0, you have the option to automatically back up all transactions on an index to durable remote storage with your choice of cloud storage services. Previously, OpenSearch users could only mitigate hardware failures by backing up with snapshots or by adding replica copies of indexed data. These approaches have limitations: With snapshots, it‚Äôs possible to lose any data that was indexed since the last snapshot was taken, while adding and maintaining replicas can consume significant storage and processing resources.\nNow this experimental feature lets you deploy remote-backed storage on a per index basis for your OpenSearch clusters using Amazon Simple Storage Service (Amazon S3), Azure Blob Storage, Google Cloud Storage, or Oracle Cloud Infrastructure (OCI) Object Storage. You can choose from a few different approaches to activating remote-backed storage, as detailed in the feature documentation.\nWe expect OpenSearch users will be excited about the increased data durability afforded by cloud-based backup and restore. We look forward to your feedback on how this feature works with your clusters, which can be provided here on GitHub.\nDrag-and-drop visualization\nOpenSearch Dashboards provides a feature-rich set of tools to help you visualize and explore your data. Now we‚Äôre making it faster and more intuitive for you to generate visualizations from your data and tweak them on the fly. With this release, you can create visualizations with a drag-and-drop interface within your visualization canvas. You can change visualization types, index patterns, and data fields quickly and see suggestions for other visualizations based on the data features you‚Äôve selected.\nDrag-and-drop visualization is an experimental feature for the 2.3.0 release, and we‚Äôre keen to capture your feedback here. Give it a try and let us know what else you want to see in your data visualizations. For more information and how to apply it to your data, check out the feature‚Äôs documentation. Explore OpenSearch 2.3.0\nOpenSearch 2.3.0 is ready for download here! This release also includes several updates and fixes to OpenSearch and OpenSearch Dashboards. Please see the release notes for a comprehensive view of the latest updates, check out the latest documentation, and download version 2.3.0 to keep your OpenSearch tools up to date!",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/security-issue-response/",
    "title": "Documenting our security issue response process",
    "content": "Today we are taking the next step in our open-source journey by updating our security policy to include a process for how we respond to security issues. In proper open-source fashion, we are creating this as a pull request, and we are inviting everyone to take part in the discussion.\nBesides making the security issue response process transparent, this policy has a significant addition: the creation of a pre-disclosure list.\nHaving a pre-disclosure list (a group of individuals and companies that will be informed of a security issue before it becomes public) is a risk balancing act. If a security issue is pre-disclosed to too broad of an audience, we risk it becoming public ahead of time. On the other hand, we might be missing out on collaborations that would expedite the creation and testing of a fix by not having a mechanism for involving some individuals and companies early, in addition to effectively creating zero-days for all the issues we disclose, leaving everyone scrambling to apply the fixes.\nThis is just the first version of this process, and we most likely won‚Äôt get it exactly right the first time. But with everyone‚Äôs help, we will keep iterating on it and advancing the security of our open-source project.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/whatsnew-custom-geo-json/",
    "title": "What‚Äôs new: Custom GeoJSON",
    "content": "In OpenSearch 2.2, we released custom GeoJSON support for region map visualizations in OpenSearch Dashboards. Custom GeoJSON enables customers to map their own GeoJSON file. Custom GeoJSON builds on the GeoShape features included in earlier releases. In this blog post, we will provide the following:\nA quick overview of the GeoJSON format standard defined in RFC7946.\nAn overview of how to create a custom GeoJSON region map.\nUntil the OpenSearch 2.2 release, region maps in OpenSearch Dashboards had support for a limited set of vector maps for visualization. Starting with this release, you can create custom GeoJSON region maps and upload your own custom region map for visualization in OpenSearch Dashboards. This enables you to comply with local government regulations in accordance with their geographic map boundary claims. Usually, the maps for disputed boundaries are not available outside the region. A quick overview of the GeoJSON standard The GeoJSON format encodes data structures in various geometric shapes, such as a polygon for multiple coordinates or a single point that consists of longitude and latitude coordinates. Shapes such as line strings or polygons represent map areas of multiple coordinates. The following example shows a single-point coordinate map location in the Barents Sea. {\n\"type\": \"Feature\",\n\"geometry\": {\n\"type\": \"Point\",\n\"coordinates\": [74, 40.71]\n},\n\"properties\": {\n\"name\": \"Barents sea location\"\n}\n} With OpenSearch, you can use the standard geographic shapes defined in the GeoJSON standard: point, linestring, polygon, multipoint, multilinestring, and multipolygon. You can specify the geographic shape that you want to use with the corresponding geospatial field types that OpenSearch provides for that shape. To learn more about the geospatial shape field types that OpenSearch supports, see Geoshape field type in the OpenSearch documentation.\nTo learn more about the GeoJSON format, see geojson.org. The GeoJSON website also provides information about the GeoJSON format specification RFC 7946.\nWhy would I need a custom vector map with GeoJSON?\nYou might want to define a specific locale‚Äôs coordinates in a polygon shape that are not provided by one of the standard GeoJSON files. For example, you can create your own custom vector map with a GeoJSON file, such as a US county or ZIP Code.\nHow to create custom vector maps with GeoJSON\nThe process for creating a custom GeoJSON file for a region map visualization includes four tasks, described below. For in-depth details on each task, see Region map visualizations in the OpenSearch documentation. Prerequisite: You need to install two plugins to use custom GeoJSON files: the OpenSearch Dashboards dashboards-maps frontend plugin and the OpenSearch geospatial backend plugin.\nThe following four steps represent individual task procedures for creating a custom vector map with GeoJSON:\nPrepare a GeoJSON file that contains geospatial data for your new region, such as coordinates, and iso designations (for example, ‚Äúiso2‚Äù: ‚ÄúUS‚Äù). To see an example GeoJSON file, see Example GeoJSON in the OpenSearch documentation.\nUpload a JSON file with the geospatial data for the region you want to map in OpenSearch Dashboards.\nSet the Dashboards visualization layer settings option to Custom vector map.\nView your custom region map in OpenSearch Dashboards. For example, the following image shows an example polygon coordinate shape in a region map for Los Angeles county. Summary\nWith OpenSearch 2.2, users can create their own maps using GeoJSON format and upload them to OpenSearch Dashboards.\nIf you have suggestions for the team and community, we‚Äôd love to hear them! Please feel free to submit a blog proposal using the blog post:Issue template on GitHub.\nTo learn more about using custom region map visualizations with GeoJSON in OpenSearch Dashboards, see Region map visualizations in the OpenSearch documentation. If you encounter an issue with this feature, please let us know by creating a GitHub issue.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-supported-by-tracetest/",
    "title": "Using OpenSearch as the traces storage for Tracetest",
    "content": "I am excited to announce that Tracetest now supports OpenSearch! If you already use OpenSearch Trace Analytics, you can start writing tests based on your telemetry using Tracetest without having to change anything in your application. In this article, we will explain how you can start building trace-based tests right now.\nWhat is Tracetest?\nTracetest is an open-source project that allows you to use your telemetry data to assert the behavior of your application. It fetches the telemetry generated by your application during a test execution and uses it to run assertions against it. With this functionality, you can verify if the latencies of your system are within your thresholds, validate your application flow and much more. It is also quite useful for writing end-to-end tests when you don‚Äôt have a complete understanding of how the application interacts with other services. You can use the trace generated by a test to do exploration work while writing your tests.\nInstalling Tracetest with OpenSearch\nThe Tracetest team is working diligently to make installation as seamless as possible. Currently, you can use CLI to install the server to run on either Docker Compose or Kubernetes. Visit the installation guide to install both the CLI and server.\nBefore proceeding, make sure you have an OpenSearch instance running and have a Data Prepper instance pointing to it. Data Prepper is an OpenSearch component capable of converting OpenTelemetry format into JSON that can be indexed by OpenSearch. Tracetest has an example application. See how Data Prepper is configured.\nFor simplicity‚Äôs sake in this article, you will learn how to setup Tracetest using Docker Compose. Execute the following steps to configure your Tracetest instance to connect to your OpenSearch instance:\nRun tracetest version to verify your CLI version.\nCheck if your CLI version is greater or equal to 0.7.2.\nCreate a folder called tracetest-demo. mkdir ~/tracetest-demo Open the folder on your terminal. cd ~/tracetest-demo Run tracetest server install and follow the steps:\nSelect Using Docker Compose.\nAllow the CLI to install any dependency that might be missing from your system.\nProject‚Äôs docker-compose file? Press Enter Do you want me to create an empty docker-compose file? Yes Do you have a supported tracing backend you want to use? (Jaeger, Tempo, OpenSearch, SignalFX) Yes Select OpenSearch Set the addresses to match your nodes‚Äô addresses\nConsider that the collector is running inside a Docker container, so you cannot use localhost to reference other containers running on your machine. Use either host.docker.internal (Mac) or 172.17.0.1 (Linux) to reference your local machine.\nSet the index used to store your traces.\nSet your data-prepper endpoint.\nConsider that the collector is running inside a Docker container, so you cannot use localhost to reference other containers running on your machine. Use either host.docker.internal (Mac) or 172.17.0.1 (Linux) to reference your local machine.\nDo you have an OpenTelemetry Collector? No Do you want me to set up one? Yes Do you want to enable the demo app? Yes Tracetest output directory Press Enter Run the docker compose command that the CLI just printed on your terminal: docker compose -f docker-compose.yaml -f tracetest/docker-compose.yaml up -d Now you should have Tracetest running and connected to your OpenSearch instance.\nCreating Your First Test\nYou should be able to access Tracetest by opening http://localhost:8080 on your browser and see Tracetest‚Äôs home screen: Click on Create Test, select HTTP Request and then click on Next. Click on Choose Example, select Pokemon - Add, and then click on Next. To review the request that is used to trigger the test, you don‚Äôt have to change anything. Just click on Create.\nNow you can see the result of the test execution. You should be seeing the response from the Pokemon demo: Click on Trace to see the trace generated by the Pokemon demo when the test was executed. This trace was retrieved from your OpenSearch instance! And, if you click on Test, you will see all assertions for this test. However, as you just created the test, there aren‚Äôt any assertions yet. You can add one to see how it works. Click on the POST /pokemon span and then click on Add Test Spec. This will generate the query to select that specific span in the trace. Then you can add assertions based on attributes from that span. Next, test executions will run those assertions against the new trace automatically. To show its capabilities, add two assertions to that span.\nSelect http.status_code Equals 201 to assert that the status code of the request will be 201 everytime this test is run.\nSelect tracetest.span.duration Less than 100ms and to assert that the request will take less than 100ms to run in future tests. Click on Save Test Spec and then on Publish.\nNow it‚Äôs possible to run the test. Tracetest will execute all assertions you just created using the new trace. Click on Run Test and wait for the test to finish executing. Tracetest is not only a UI tool. You also can run that test from your CI environment. Click on the gear icon at the top of the page and click on Test Definition. Copy the test definition and save into a file. Then open your terminal, and type tracetest test run -d your-file.yaml -w and wait for the result. Conclusion\nWith this new integration, any team using OpenSearch as your OpenTelemetry trace data store will be able to use Tracetest to help you test your application. We are super excited to work with the OpenSearch community to release the integration and to continue to enhance this feature as we develop Tracetest. If you have any issues or questions about Tracetest with OpenSearch, reach out the team on the Discord channel or open an issue on the git repository.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-playground/",
    "title": "Launch Highlight: OpenSearch Playground",
    "content": "We are excited to announce the live demo environment of OpenSearch and OpenSearch Dashboards. OpenSearch Playground provides a central location for existing and evaluating users to explore features in OpenSearch and OpenSearch Dashboards without installing or downloading anything. You can access OpenSearch Playground at playground.opensearch.org.\nWhat can you do in OpenSearch Playground environment?\nThe first version of OpenSearch Playground provides you with anonymous, read-only access, allowing you to try new features and explore preconfigured sample data. You can interact with sample dashboards, data visualizations, and data sources without installing and configuring OpenSearch or OpenSearch Dashboards in your own environment. You also can query demo data and evaluate plugin features, such as anomaly detection and observability, without installing or configuring in your instance. Example: Anomaly detection dashboard Example: Observability dashboard How does OpenSearch Playground work?\nAt a high level, OpenSearch Playground is a deployment of OpenSearch and OpenSearch Dashboards hosted in AWS EKS (Elastic Kubernetes Service), deployed by OpenSearch-helm-charts, and made publicly accessible at playground.opensearch.org. We use fluent bit to ingest OpenSearch and OpenSearch Dashboards logs and use the Alerting plugin to monitor the site‚Äôs heartbeat and trigger an alert if the site goes down. We update OpenSearch Playground with each new release of OpenSearch. High-level architecture diagram We plan to provide a more specific design breakdown in a subsequent blog to highlight how the team built OpenSearch Playground. In the meantime, more details can be found on the GitHub proposal.\nWhat‚Äôs next?\nWhile this launch of OpenSearch Playground completes a short-term goal to build a demo site providing real-time user experience with read-only permissions, we aren‚Äôt stopping here. The following are longer-termer goals we are working on:\nUser specific sessions: We want to add user authentication via GitHub login to support more exploration and experimentation for individual users. This will enable more capabilities for evaluating users such as persistent editing and saving to allow users to explore the entire user journey.\nImproved landing experience: We want to add a landing experience that guides users through new features, specific use cases, and ongoing experimental features. This will provide new users a more guided demonstration experience.\nMore curated demos, data, and use cases: We want to add additional demo data, dashboards, visualization, and curated experiences to better demonstrate the range of use cases supported by the OpenSearch Project.\nPartner highlights: We want to provide direct partner highlights and links within OpenSearch Playground similar to the Partners page on OpenSearch.org. Providing this information further highlights community and partner projects to evaluating users and community members.\nWe have created a public backlog for OpenSearch Playground on GitHub. If you‚Äôre interested in this topic and want to put your knowledge to further practice, consider contributing to the dashboards-anywhere repo!\nWhat requests do you have?\nWe‚Äôre happy to add more features to the environment. Just let us know!",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/hacktoberfest-2022/",
    "title": "Hacktoberfest 2022",
    "content": "Happy Hacktoberfest everyone! Wait‚Ä¶you don‚Äôt know what Hacktoberfest is? Well my friend, Hacktoberfest is an event that happens every October to promote contributions to open-source projects. It is a way to celebrate the work of open-source maintainers and contributors and make it easier for new developers to get involved with open-source projects.\nFor more information you can check out GitHub and DigitalOcean‚Äôs Hacktoberfest page. They have some pretty neat incentives for beginning your open-source journey! In addition to what they‚Äôve got going on, we are offering incentives of our own for contributing that we will talk about a little later.\nWhat does it mean to ‚Äúcontribute‚Äù to open-source projects?\nWell, contributing can mean a lot of different things. We need people using an open-source project to report any bugs that they find so that the project can run smoother. Once those bugs are reported, we need the community‚Äôs help to fix them. To get people using the project, we also need good documentation, ‚Äúhow-to‚Äù blogs, and even videos. Even more so, we need help answering questions on the project‚Äôs forum. Many people have questions‚Äîyou may know the answer!\nTo sum it up, here are a few of the ways that people can contribute to an open-source project:\nCode\nBug fixes\nTests\nFeatures\nCode examples\nNon-code\nDocumentation\nFeature requests\nBug reports\nBlogs\nTutorials\nAnswers on the forum\nThat sounds nice, but how do I know what to contribute?\nI am so glad you asked! For contributions, I recommend you start small. OpenSearch has a list of issues marked ‚Äúgood first issue‚Äù that are a great place to start. Check out the query below to see some of our good first issues.\nOnce you have found an issue you would like to work on, you should check out their CONTRIBUTING.md file. It will take you through the process of contributing to that particular repo. Also, if you are contributing to a particular issue, it is helpful to comment on it to let others know you are working on it.\nGenerally, the process for contributing involves forking the repository, making your edit (don‚Äôt forget to sign your commits using -s in your commit message: git commit -s -m \"&lt;your commit message&gt;\"), and then submitting a pull request (PR). This guide goes into much more detail: First Contribution Guide.\nIf you have questions along the way, you can always @ mention one of the maintainers on the issue. You can find the maintainers in the MAINTAINERS.md file for that repo. Please be patient as the maintainers may take several days to reply depending on their capacity. If more than a few days goes by without a response, feel free to reach out to another maintainer.\nNeat! Now what about those incentives you were mentioning earlier?\nOkay, okay, here we go: In addition to your PRs counting towards your completion of Hacktoberfest, we are running the following leader board! This is to thank all of our fantastic contributors for the work that they do in supporting our open-source project.\nFor the top 5 points scorers on the leader board we have a swag pack that will include a water bottle, sweatshirt, and a sticker pack. All other contributors will be eligible for a free OpenSearch sticker! To get points for your contribution you must open an issue on GitHub and it must be tagged Hacktoberfest with the exception of the forum answers. These issues must be submitted before October 31, 2022 23:59 UTC +0. They must also be marked hacktoberfest-approved by the maintainers.\nTo get credit for off platform blogs submit a link to your blog in an issue on the opensearch-project/project-website repo. Ask for it to be tagged Hacktoberfest and then if it is acceptable as determined by the maintainers it will be tagged hacktoberfest-approved and will count to your credit. We are looking forward to all your contributions! Happy Hacktoberfest and make sure to follow our Twitter, LinkedIn, and YouTube pages for live streams and other Hacktoberfest related content! Also, if you have ANY questions feel free to open a thread in our Hacktoberfest 2022 topic on the forum. Happy Hacking!",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/one-million-enitities-in-one-minute/",
    "title": "Improving Anomaly Detection: One million entities in one minute",
    "content": "‚ÄúWhen you can measure what you are speaking about, and express it in numbers, you know something about it.‚Äù William Thomson, co-formulator of Thermodynamics\nWe continually strive to improve the existing OpenSearch features through harnessing the capabilities of OpenSearch itself. One such feature is the Anomaly Detection (AD) plugin, which automatically detects anomalies in your OpenSearch data.\nBecause OpenSearch is used to index high volumes of data in a distributed fashion, we knew it was essential to design the AD feature to have minimal impact on application workloads. OpenSearch 1.0.1 did not scale beyond 360K entities. Since OpenSearch 1.2.4, it has been possible to track one million entities with a data arrival rate of 10 minutes using 36 data nodes.\nWhile the increase to one million entities was great, most monitoring solutions generate data at a far higher rate. If you want to react quickly to emergent scenarios within your cluster, that 10-minute interval is insufficient. In order for AD to be truly useful, our goal was simple: Shorten the interval to one minute for one million entities, without changing the model output or increasing the number of nodes.\nThe task of improving AD since its release has mirrored OpenSearch itself‚Äî-distributed, dynamic, and not easy to categorize in a linear manner. This post describes the non-linear task of improving AD for all supported OpenSearch versions using the built-in features of OpenSearch. These features include the streamlined access of documents (asynchronous mode, index sorting), changes to system resource management (distributing CPU load, increasing memory), and innovations in the AD model (lower memory, faster throughput).\nFirst steps\nAt first, it was unclear how to model data characteristics that would measure bottlenecks in a 39-node (36 data nodes, r5.2xlarge Graviton instances) system. Inspired by Thomson, we began testing synthetic periodic functions with injected noise and some anomalies. A rough estimate of model inference time (including time for multiple invocations scoring, supporting explainability features, and updates) was about two-minutes of compute in a 10-minute interval. Clearly, reducing the interval to one-minute would require newer ideas, and we would have to focus on the model.\nRCF 2.0 compared with 3.0\nOne idea was to initiate a Random Cut Forest (RCF) 3.0 model that made scoring and conditional computing of all the explanatory information available through a single API call based on a predictor-corrector architecture reminiscent of a streaming Kalman filter.\nRCF was originally designed to flag observations that were too difficult to explain given past data. With RCF 3.0, the algorithm now extends seamlessly to support forecasting and many other capabilities of streaming random forests.\nEach model had a dynamic cache internal to the model. Memory consumption corresponded to storing an unused model, whereas the throughput corresponded to a model in use. That difference is germane when each node is processing 1,000,000/36‚âà27,750 models; the 3x overhead from a fully enlarged model in the running threads would only be dwarfed if we were running 27,750/3‚âà9,250 threads.\nWe did not have 9,250 threads. To reduce the number of running threads, we used a dynamic trade-off, switching efficiently between the dynamic and static memory, instead of the static memory-throughput trade-off used from RCF 2.0 used in OpenSearch 1.2.4. When we did, the cache was turned on at the invocation, and all the explanatory computation was performed and discarded afterward.\nSince the memory-throughput trade-off was dynamic, we also explored alternate tree representations. In aggregate, we were handling 30 million decision trees. Therefore, any per-tree improvements we could make to size mattered. The newer RCF 3.0 model was at least 2x faster and 30 percent smaller when stored in memory compared to the corresponding usage of RCF 2.0 in OpenSearch 1.2.4.\nBut this still fell short of our one-minute goal. We would need to change more than the algorithm‚Äôs memory throughput and also make tweaks to our CPU usage, especially for OpenSearch 2.0 and above.\nCPU usage\nIn OpenSearch 2.0.1, RCF 3.0 took 30 percent more time when compared with the 2-minute per-interval execution time of OpenSearch 1.2.4. In OpenSearch 1.2.4, the JVM usage was smaller by 30 percent, just as we had expected from smaller models. Also, CPU spikes decreased by 30 percent, even though total execution was larger. To leverage OpenSearch better, we investigated comparable Graviton (r6g.2xlarge) instances and found that CPU spikes were the same as in our baseline (1.2.4) and that the execution time was 20 percent faster. We also looked at using 9 nodes of r6g.8xlarge or r5.8xlarge (both of which have the same vCPUs and RAM in their initial configurations) and found that in both cases, the CPU spikes were 4x smaller.\nStill, execution time in OpenSearch 2.0.1 was 1.5x slower than in our baseline. To dig deeper, we looked at the CPU spikes of the original c5.4xlarge nodes in OpenSearch 2.0.1 and found the underlying issue. CPU usage stayed around 1 percent, with hourly spikes of up to 65 percent. The spikes were likely caused by the usual culprits: internal hourly maintenance jobs on the cluster, saving hundreds of thousands of model checkpoints and then clearing unused models, and performing bookkeeping for internal states.\nFor OpenSearch 2.2, we chose to even out the resource usage across a large maintenance window. We also changed the pagination of the coordinating nodes from sync to async mode so that the coordinating nodes did not need to wait for responses before fetching the next page.\nSetting up the detector\nTo set up the detector itself, we used the same category order in our configuration as in our OpenSearch index fields. For example, if the category fields for which we wish to search for anomalies are host and process, and we know that we set indexes to look for the host field before process field, we reflect that order in the category_field: {\n\"name\": \"detect_gc_time\",\n\"description\": \"detect gc processing time anomaly\",\n\"time_field\": \"@timestamp\",\n\"indices\": [\n\"host-cloudwatch\"],\n\"category_field\": [\"host\", \"process\"],... The detector becomes more efficient when we set it up by using the same field order in our documents and in the sort.field. Therefore, the following example request reflects the same category order: host &gt; process &gt; @timestamp. The body also allocates two nodes for the detector in order to balance shard size. request_body = {\n\"settings\":{\n\"index\": {\n\"sort.field\": [ \"host\", \"process\", \"@timestamp\"],\n\"sort.order\": [ \"asc\", \"asc\", \"desc\"],\n\"routing.allocation.total_shards_per_node\": \"2\"\n}\n},...\n} When set up with the same category and sort order, the CPU spikes when using OpenSearch 2.0 were below 25%. Finally, we achieved continuous anomaly detection of one million entities at a one-minute interval.\nTesting for anomalies in the AD plugin\nIt was serendipitous that the measurements taken from improving the plugin helped us uncover more about the anomalies within the plugin itself. The reading shown in the following image was taken from the plugin‚Äôs JVM. Interestingly, our explorations of the 9 r6g.8xlarge Graviton instances, each with a 128 GB (50 percent) heap, resulted in the measurements seen in the following readings. Notice the lower CPU spikes when compared with our measurements of the same instances from OpenSearch 2.0. See it for yourself\nThe improvements outlined in this post are available in OpenSearch 2.2 or greater.\nIf you want to run this experiment for yourself, remember the following:\nUse the sort.order of the index for which you are detecting anomalies.\nUse the following cluster settings to ensure that 50 percent of dynamic memory is available to the AD plugin, page_size is 10K, and max_entities_per_query is 1 million. PUT /_cluster/settings\n{\n\"persistent\": {\n\"plugins.anomaly_detection.model_max_size_percent\": \"0.5\",\n\"plugins.anomaly_detection.page_size\": \"10000\",\n\"plugins.anomaly_detection.max_entities_per_query\": \"1000000\"\n}\n} But what if I don‚Äôt use one million entities\nFrom OpenSearch 1.2.4 to OpenSearch 2.2 or greater, many incremental improvements were made to AD, in particular to historical analysis and other downstream log analytics tasks. However, ‚Äúcold start,‚Äù the gap between loading data and seeing results, has been a known challenge in AD since the beginning. Despite this challenge, the cold start gap has decreased from release to release as the AD model has improved.\nBecause of these improvements over time, the performance of AD is not hampered in more modest cluster settings. To prove this, we performed experiments comparing the recommended testing framework for index/query workloads on a 5-node (2 data node, all c5.4xlarge) cluster both without AD and with AD.\nWhen adding AD, we set AD memory usage to the default 10 percent. We performed each experiment twice (for verification), with two warmup iterations and five test iterations. The final result is the average of these two experiments.\nBefore each experiment, we restarted the whole cluster, cleared the OpenSearch cache, and removed all existing AD detectors, when applicable.\nThe following table compares the index latency, CPU and memory usage, GC time and cold start average time with and without AD under the indexing workload. Version query latency P50 (ms) query latency P90 (ms) CPU usage P90 (%) Memory usage P90 (%) GC time young (ms) cold start avg (ms) No AD ¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n2.3\n60.958\n67.33\n73\n52.2\n56,992\n¬†\n2.2\n62.649\n69.636\n68\n52\n60,831\n¬†\n2.1\n61.693\n67.782\n63.333\n51\n59,736\n¬†\n2.0.1\n67.682\n71.785\n69\n48\n59,151\n¬†\n1.3.3\n84.618\n90.82\n52\n44\n65,240\n¬†\n1.2.4\n77.54\n85.376\n61\n45\n40,103\n¬†\n1.1.0\n87.33\n93.147\n56.625\n45.5\n36,206\n¬†\n1.0.1\n77.224\n82.556\n62\n47\n37,899\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬† With AD ¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n2.3\n97.319\n106.2\n68.367\n53\n63,722\n7.93\n2.2\n106.1\n114.4\n69.5\n52\n63,293\n7.85\n2.1\n108.6\n117.7\n66.5\n47\n64,242\n8.4\n2.0.1\n111.8\n120.2\n67\n53\n67,764\n107.81\n1.3.3\n92.933\n98.349\n72\n42\n104,510\n78.87\n1.2.4\n92.954\n98.853\n65.6\n56\n67,202\n132.11\n1.1.0\n83.545\n90.319\n67\n55\n61,147\n985.08\n1.0.1\n103.6\n112\n67\n48.2\n39,707\n600000 The following table compares the index latency, CPU and memory usage, GC time and cold start average time with and without AD under the indexing workload. Version index latency P50 (ms) index latency P90 (ms) CPU usage P90 (%) Memory usage P90 (%) GC time young (ms) cold start avg (ms) No AD ¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n2.3\n510.6\n717.8\n64\n50\n13,881\n¬†\n2.2\n510.9\n709\n61\n50\n13,994\n¬†\n2.1\n515.5\n723.9\n77\n51.333\n14,227\n¬†\n2.0.1\n534.6\n755\n77\n51.333\n14,386\n¬†\n1.3.3\n548.6\n776.4\n72\n49\n30,752\n¬†\n1.2.4\n604.3\n840\n64\n56\n16,767\n¬†\n1.1.0\n579.3\n821.6\n69\n51\n16,286\n¬†\n1.0.1\n563.7\n795.6\n68.375\n55\n16,730\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n¬† With AD ¬†\n¬†\n¬†\n¬†\n¬†\n¬†\n2.3\n565.5\n808.5\n65\n56\n17,487\n9.4361\n2.2\n530.4\n731.4\n63\n52\n16,126\n59.67\n2.1\n487.2\n679.7\n71.5\n47\n15,341\n6.9428\n2.0.1\n481.3\n680.5\n72\n53\n21,098\n71.2371\n1.3.3\n636.9\n894\n85.333\n45\n44,675\n48.7008\n1.2.4\n618.7\n870\n79\n56\n30,081\n47.8312\n1.1\n622\n860\n66.5\n54.5\n40,208\n577.8122\n1.0.1\n583.4\n824.7\n57\n54.667\n17,642\n600000 Conclusion\nWhile the benchmark data produced in our experiment could be viewed as less than perfect or perhaps more illustrative of a thought experiment in physics, the improvements were real.\nWe do expect that explainability, the ability to understand the impact of a model of your nodes, outstrips the demands of inference within analytics. After all, the number of invocations within a model often takes more computational power than accounting for the data running inside. It is simpler to provide explainability based directly on the algorithm being used than to retrofit interpretations. Could streaming algorithms provide a process for invoking multiple API invocations and therefore improve explainability inside the AD model?\nIf that question or other questions related to machine learning interest you, we would love to hear from you about your experience.\nTo discuss topics with other OpenSearch users, start a conversation at forum.opensearch.org.\nTo request improvements to the AD plugin, create an issue in the Anomaly Detection plugin repository.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/snapshot-management/",
    "title": "Snapshot Management (SM): A new way to automate snapshots",
    "content": "In version 2.1, the OpenSearch project introduced Snapshot Management (SM)‚Äîa way to automatically take snapshots of your cluster. While previously you had to rely on external management tools like Curator, now you can automatically back up your index data and cluster state through OpenSearch. Just set a schedule to take snapshots every hour or every Sunday at midnight, and sit back while the snapshots are created.\nYou can even get notifications of the snapshot activities. Plus, if you have large amounts of data coming in, you can manage the lifecycle of the snapshots you store. You can specify a retention policy to keep snapshots from the last 30 days or to keep only the 100 most recent snapshots. In short, SM frees you of menial tasks so you can be more productive.\nSnapshots 101\nThink of taking snapshots as taking pictures of your cluster.\nWhat is a snapshot\nA snapshot is a point-in-time backup of your cluster. You can use it for disaster recovery, migrating data to a new cluster, or preserving a point-in-time view of your data.\nWhat do snapshots store?\nA snapshot preserves two things: the cluster data and the cluster state. The cluster data includes OpenSearch index data, such as settings and mappings. The cluster state includes persistent cluster settings and index templates. Preserving the cluster state in a snapshot is optional and can be controlled with the include_global_state parameter.\nWhy do you need to take snapshots?\nLet‚Äôs go through the two most common scenarios in which you would need to take snapshots. Scenario 1: Red Alert! ‚Äì Your cluster goes red because nodes went down, causing data loss. In this case, you can restore the red indexes from the most recent snapshot. Scenario 2: Migration ‚Äì You need to migrate from one cluster to another. In this case, you can restore the indexes on your new cluster from a snapshot. Scenario 3: Restore to a point-in-time view ‚Äì You need to revert back to a point-in-time view. You can restore your cluster from a snapshot to go back to a previous state. Scenario 4: Save costs ‚Äì You need to save historical data for compliance, but you primarily use only last seven days of data. You can save all previous data in a snapshot and delete it from your cluster to save cost.\nSnapshots are incremental\nSnapshots only store the changes since the last successful snapshot. The first time you take a snapshot, it contains all the data in the index, so the operation is resource intensive. Each snapshot you take after that contains only the incremental changes in the data, so the operation becomes lightweight.\nIf you want to learn more about snapshots, take a look at the snapshot documentation. Now let‚Äôs dive into SM.\nSM in action\nThink of SM as having your index periodically take selfies.\nHow to set up SM\nYou can set up SM either using the REST API or through OpenSearch Dashboards. Here is the sequence that you should follow.\nFirst, create a repository to hold your snapshots.\nThen, create an SM policy. The SM policy describes the following:\nThe indexes for which to take snapshots.\nHow often and when to take snapshots (for example, every hour or every Sunday at midnight).\nWhich snapshots to retain (for example, retain snapshots taken within the last 30 days or retain the last 100 snapshots). This is optional. If you don‚Äôt specify a retention schedule, OpenSearch will keep all snapshots.\nHow you want to be notified of snapshot events and what snapshot events interest you (creation, deletion, or failure). This is optional.\nYou need to set the schedule for your automatic snapshots using a cron expression. Let‚Äôs dissect the expression 0 8 * * *, which we‚Äôll use in the example below. 0\n8\n*\n*\n*\nminute\nhour\nday of month\nmonth\nday of year This cron expression means you want to take snapshots at 8:00 AM every day.\nHere‚Äôs a sample SM policy that puts together all of this information: POST _plugins/_sm/policies/daily-policy { \"description\": \"Daily snapshot policy\", \"creation\": { \"schedule\": { \"cron\": { \"expression\": \"0 8 * * *\", \"timezone\": \"UTC\" } }, \"time_limit\": \"1h\" }, \"deletion\": { \"schedule\": { \"cron\": { \"expression\": \"0 1 * * *\", \"timezone\": \"America/Los_Angeles\" } }, \"condition\": { \"max_age\": \"7d\", \"max_count\": 21, \"min_count\": 7 }, \"time_limit\": \"1h\" }, \"snapshot_config\": { \"date_format\": \"yyyy-MM-dd-HH:mm\", \"timezone\": \"America/Los_Angeles\", \"indices\": \"*\", \"repository\": \"s3-repo\", \"ignore_unavailable\": \"true\", \"include_global_state\": \"false\", \"partial\": \"true\", \"metadata\": { \"any_key\": \"any_value\" } }, \"notification\": { \"channel\": { \"id\": \"NC3OpoEBzEoHMX183R3f\" }, \"conditions\": { \"creation\": true, \"deletion\": false, \"failure\": false, \"time_limit_exceeded\": false } } } You can also set up your SM policy through OpenSearch Dashboards as follows: After you create an SM policy, you may start it using the SM Start API and stop it using the SM Stop API. To view the status of the snapshot operations, you can use the SM Explain API.\nWhat if something goes wrong?\nSnapshots are retried three times. Even if you don‚Äôt set up notifications of failure in the SM policy, you can view the failure message using the SM Explain API or on the policy details page in OpenSearch Dashboards.\nHow does SM work under the hood?\nSM uses a state machine that has two workflows: creation and deletion. These workflows are executed sequentially, so SM does not start a new operation until the previous operation finishes.\nThe following image shows the creation workflow of the SM state machine. The creation workflow starts in the CREATION_START state and continuously checks whether the conditions in the creation schedule are met.\nWhen the conditions are met, the creation workflow switches to the CREATION_CONDITION_MET state and continues to the CREATING state.\nThe CREATING state calls the Create Snapshot API asynchronously and then waits for snapshot creation to end in the CREATION_FINISHED state.\nWhen snapshot creation ends, the creation workflow goes back to the CREATION_START state, and the cycle continues.\nThe deletion workflow follows the same pattern, going through the DELETION_START, DELETION_CONDITION_MET, DELETING, and DELETION_FINISHED states.\nSM depends on the Job Scheduler plugin to schedule and periodically run a job for each SM policy. The job is lightweight, so the burden of SM depends on how often you take snapshots and the overhead of taking a snapshot itself.\nWrapping it up\nSM automates taking snapshots of your cluster and provides useful features like notifications. To learn more about SM, check out the SM documentation section. For more technical details, read the SM meta issue.\nIf you‚Äôre interested in snapshots, consider contributing to the next improvement we‚Äôre working on: searchable snapshots.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Announcing-Data-Prepper-2.0.0/",
    "title": "Announcing Data Prepper 2.0.0",
    "content": "The Data Prepper maintainers are proud to announce the release of Data Prepper 2.0. This release makes Data Prepper easier to use and helps you improve your observability stack based on feedback from our users. Data Prepper 2.0 retains\ncompatibility with all current versions of OpenSearch.\nHere are some of the major changes and enhancements made for Data Prepper 2.0.\nConditional routing\nData Prepper 2.0 supports conditional routing to help pipeline authors send different logs to specific OpenSearch clusters.\nOne common use case for conditional routing is reducing the volume of data going to some clusters.\nWhen you want info logs that produce large volumes of data to go to a cluster, to index with more frequent rollovers, or to\nadd deletions to clear out large volumes of data, you can now configure pipelines to route the data with your chosen action.\nSimply choose a name appropriate for the domain and a Data Prepper expression.\nThen for any sink that should only have some data coming through, define one or more routes to apply. Data Prepper will evaluate\nthese expressions for each event to determine which sinks to route these events. Any sink that has no routes defined will accept all events.\nFor example, consider an application log that includes log data. A typical Java application log might look like the following. 2022-10-10T10:10:10,421 [main] INFO org.example.Application - Saving 10 records to SQL table \"orders\" The text that reads INFO indicates that this is an INFO-level log. Data Prepper pipeline authors can now route logs with this level to only certain OpenSearch clusters.\nThe following example pipeline takes application logs from the http source. This source\naccepts log data from external sources such as Fluent Bit.\nThe pipeline then uses the grok processor to split the log line into multiple fields. The grok processor adds a\nfield named loglevel to the event. Pipeline authors can use that field in routes.\nThis pipeline contains two OpenSearch sinks. The first sink will only receive logs with a log level of WARN or ERROR.\nData Prepper will route all events to the second sink. application-log-pipeline:\nworkers: 4\ndelay: \"50\"\nsource:\nhttp:\nprocessor:\n- grok:\nmatch:\nlog: [ \"%{NOTSPACE:time} %{NOTSPACE:thread} %{NOTSPACE:loglevel} %{NOTSPACE:class} - %{GREEDYDATA:message}\"]\nroute:\n- warn_and_above: '/loglevel == \"WARN\" or /loglevel == \"ERROR\"'\nsink:\n- opensearch:\nroutes:\n- warn_and_above\nhosts: [\"https://opensearch:9200\"]\ninsecure: true\nusername: \"admin\"\npassword: \"admin\"\nindex: warn-and-above-logs\n- opensearch:\nhosts: [\"https://opensearch:9200\"]\ninsecure: true\nusername: \"admin\"\npassword: \"admin\"\nindex: all-logs There are many other use cases that conditional routing can support. If there are other conditional expressions\nyou‚Äôd like to see support for, please create an issue in GitHub.\nPeer forwarder\nData Prepper 2.0 introduces peer forwarding as a core feature.\nPrevious to Data Prepper 2.0, performing stateful trace aggregations required using the peer forwarder processor plugin.\nBut this plugin only worked for traces and would send data back to the source. Also, log aggregations only worked on a single node.\nWith peer forwarding as a core feature, pipeline authors can perform stateful\naggregations on multiple Data Prepper nodes. When performing stateful aggregations, Data Prepper uses a hash ring to determine\nwhich nodes are responsible for processing different events based on the values of certain fields. Peer forwarder\nroutes events to the node responsible for processing them. That node then holds the state necessary for performing the aggregation.\nTo use peer forwarding, configure how Data Prepper discovers other nodes and the security for connections in your data-prepper-config.yaml file.\nIn the following example, Data Prepper discovers other peers by using a DNS query on the my-data-prepper-cluster.production domain.\nWhen using peer forwarder with DNS, the DNS record should be an A record with a list of IP addresses for peers. The example also uses a custom certificate and private key.\nFor host verification, it checks the fingerprint of the certificate. Lastly, it configures each server to authenticate requests using\nMutual TLS (mTLS) to prevent data tampering. peer_forwarder:\ndiscovery_mode: dns\ndomain_name: \"my-data-prepper-cluster.production\"\nssl_certificate_file: /usr/share/data-prepper/config/my-certificate.crt\nssl_key_file: /usr/share/data-prepper/config/my-certificate.key\nssl_fingerprint_verification_only: true\nauthentication:\nmutual_tls: Directory structure\nBefore the release of Data Prepper 2.0, we distributed Data Prepper as a single executable JAR file. While convenient,\nthis made it difficult for us to include custom plugins.\nWe now distribute Data Prepper 2.0 in a bundled directory structure. This structure features a shell script to launch\nData Prepper and dedicated subdirectories for JAR files, configurations, pipelines, logs, and more. data-prepper-2.0.0/\nbin/\ndata-prepper # Shell script to run Data Prepper\nconfig/\ndata-prepper-config.yaml # The Data Prepper configuration file\nlog4j.properties # Logging configuration\npipelines/ # New directory for pipelines\ntrace-analytics.yaml\nlog-ingest.yaml\nlib/\ndata-prepper-core.jar... any other jar files\nlogs/ You now can launch Data Prepper by running bin/data-prepper; there is no need for additional command line arguments or Java system\nproperty definitions. Instead, the application loads configurations from the config/ subdirectory.\nData Prepper 2.0 reads pipeline configurations from the pipelines/ subdirectory. You can now define pipelines across\nmultiple YAML files in the subdirectory, where each file contains the definition for one or more pipelines. The directory\nalso helps keep pipeline definition distinct and, therefore, more compact and focused.\nJSON and CSV parsing\nMany of our users have incoming data with embedded JSON or CSV fields. To help in these use cases, Data Prepper 2.0\nsupports parsing JSON and CSV.\nFor example, when one large object includes a serialized JSON string, you can use the parse_json processor to extract\nthe fields from the JSON string into your event.\nData Prepper can now import CSV or TSV formatted files from Amazon Simple Storage Service (Amazon S3) sources.\nThis is useful for systems like Amazon CloudFront,\nwhich write their access logs as TSV files. Now you can parse these logs using Data Prepper.\nAdditionally, if your events have\nCSV or TSV fields, Data Prepper 2.0 now contains a csv processor that can create fields from your incoming CSV data.\nOther improvements\nData Prepper 2.0 includes a number of other improvements. We want to highlight a few of them.\nThe OpenSearch sink now supports create actions for OpenSearch when writing documents. Pipeline authors can configure their pipelines to only create new documents and not update existing ones.\nThe HTTP source now supports loading TLS/SSL credentials from either Amazon S3 or AWS Certificate Manager (ACM). Pipeline authors can now configure them for their log ingestion use cases. Before Data Prepper 2.0, only the OTel Trace Source supported these options.\nData Prepper now requires Java 11 or higher. The Docker image deploys with JDK 17.\nPlease see our release notes for a complete list.\nTry Data Prepper 2.0\nData Prepper 2.0 is available for download now! The maintainers encourage you to\nread the latest documentation and try out the new features.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Adopting-inclusive-language-across-OpenSearch/",
    "title": "Adopting inclusive language across OpenSearch",
    "content": "We are removing exclusionary language from the OpenSearch Project, including from APIs, documentation, and source code. Specifically, we are replacing the non-inclusive terms ‚Äúmaster‚Äù and ‚Äúblacklist/whitelist‚Äù with inclusive language throughout the OpenSearch Project. Deprecated terminology Alternative terminology master\ncluster manager\nblacklist\ndeny list\nwhitelist\nallow list Why are we making this change?\nInclusive language helps to avoid excluding people based on gender, sexual orientation, age, race, ability, etc. By using language that avoids prejudice, we aim to create a more equitable community.\nUsing inclusive language helps to build an environment that encourages diversity and ensures that all community members feel welcome, respected, and safe.\nHow does this impact users?\nMany users will see no impact, as the indexing and search APIs have few changes. However, all users should audit their usage of OpenSearch 2.x to prepare for and make the necessary updates ahead of removing exclusionary language in OpenSearch 3.0.\nOpenSearch users\nOpenSearch displays warnings for all uses of deprecated REST APIs and settings. The general process for updating is to find all uses of a deprecated API or cluster setting and replace them with the new alternative. There is a simple one-to-one replacement option for the exclusionary terms in all cases. For example, the parameter master_timeout can be replaced directly with cluster_manager_timeout. All deprecations and their replacements in the REST APIs and settings are available in OpenSearch 2.0, so users can complete the update today.\nHere are some common REST APIs and settings to update when upgrading to version 3.0:\nThe node role master -&gt; cluster_manager The REST API endpoint GET _cat/master -&gt; GET _cat/cluster_manager The cluster setting cluster.initial_master_nodes -&gt; cluster.initial_cluster_manager_nodes The OpenSearch documentation will list other REST APIs and settings affected by the change in terms upon the release of 3.0.\nPlugin developers\nThe OpenSearch server offers extensibility through a plugin API that allows external developers to add features and functionality by implementing custom Java-based plugins. For the most part, plugin developers can follow the same general process of finding uses of deprecated Java APIs in OpenSearch 2.x and replacing them with the new alternative. Unfortunately, there has never been a strictly enforced boundary between a plugin extension point in the OpenSearch server and a logically internal but public Java class. This means the surface area of potentially impacted APIs is quite large, and given some limitations in extending Java classes, we can‚Äôt account for all instances by using a two-step deprecate-and-replace-with-alternative approach. For example, an existing method that has an exclusionary term only in the return type cannot be overridden to provide a non-exclusionary alternative side by side. In these cases, plugin developers will encounter compiler errors when upgrading to OpenSearch 3.0. Fortunately, the fix in all cases is a simple replacement.\nFor example, the following line will create a compiler error: import org.opensearch.action.support.master.AcknowledgedResponse This type of error can be fixed by changing the line to the following: import org.opensearch.action.support.clustermanager.AcknowledgedResponse Here‚Äôs a list of other APIs you should update when upgrading to OpenSearch 3.0: org.opensearch.action.support.master.AcknowledgedRequest -&gt; org.opensearch.action.support.clustermanager.AcknowledgedRequest org.opensearch.action.support.master.AcknowledgedRequestBuilder -&gt; org.opensearch.action.support.clustermanager.AcknowledgedRequestBuilder org.opensearch.action.support.master.AcknowledgedResponse -&gt; org.opensearch.action.support.clustermanager.AcknowledgedResponse Map&lt;ScriptContext&lt;?&gt;, List&lt;Whitelist&gt;&gt; getContextWhitelists() -&gt; Map&lt;ScriptContext&lt;?&gt;, List&lt;Allowlist&gt;&gt; getContextAllowlists() org.opensearch.painless.spi.Whitelist -&gt; org.opensearch.painless.spi.Allowlist org.opensearch.painless.spi.WhitelistLoader -&gt; org.opensearch.painless.spi.AllowlistLoader How can you get involved?\nCheck out our blog post From the editor‚Äôs desk: OpenSearch and inclusion to learn more about our guidelines for inclusive language and how you can help drive change across our community, documentation, forums, and projects. You can also learn more about the Inclusive Naming Initiative, which works closely with companies and the open-source community to remove harmful, exclusionary language.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/asking-the-right-people-the-right-questions/",
    "title": "Are you asking the right people the right questions?",
    "content": "To me, it has always been especially discouraging when I have an issue blocking my way and I need some help, and it‚Äôs even more discouraging when I can‚Äôt find that help. There‚Äôs also no better feeling than getting some help from a living, breathing human being and finding out that I‚Äôm not alone in my struggle. Unfortunately (or perhaps fortunately), we don‚Äôt all think alike, and what seems like enough information for one of us may not be for another. So let‚Äôs help each other by talking about where your questions about OpenSearch can be answered and how to make those question clear.\nWhat exactly do I want to know?\nBefore you decide where your question might be best addressed, it can be helpful to consider the nature of the question itself. What exactly is it that you want to know?\nAsk yourself, ‚ÄúWhat is it that I really need?‚Äù Help with a configuration file?\nHelp making use of an existing feature?\nHelp working past error messages you don‚Äôt understand?\nAsk yourself, ‚ÄúWhat kind of answer do I want?‚Äù A purely informational response?\nAn example configuration snippet?\nA code example?\nDon‚Äôt forget to ‚Äúshow your work‚Äù (also known as ‚Äúlogs, or it didn‚Äôt happen‚Äù).\nIf you need help with a config file, provide the one you‚Äôre currently using formatted in a code block.\nIf you‚Äôre receiving error messages, please share them.\nIf you have a screen shot, please provide that as well.\nWith that out of the way‚Ä¶\nWhere do I go?\nDepending on your issue and what kind of help you want, your destiny lies in one or more of three places: the OpenSearch GitHub repositories, documentation, or forum. The one you need depends on the details of your issue.\nPlease try to search existing material first in all three places. I‚Äôll make sure to keep reminding you.\nDocumentation\nIf you‚Äôre not sure what something does, how it works, or how it is supposed to be configured, please visit our documentation first. There you will find descriptions and examples.\nThis is one way in which you can be of help to us ( yes, you!) by creating an issue on our documentation website repository. We take documentation contributions from anyone. Feel free to create an issue or submit a PR!\nGitHub\nGitHub is the best outlet for feature requests and bug reports.\nIf you want OpenSearch or OpenSearch Dashboards to do something that it currently does not do, create a feature request ( OpenSearch / OpenSearch Dashboards). Share with the community as descriptively as possible what your cool new feature would do, what it might look like, what you‚Äôd use it for, etc. Search existing material first. If someone has already created a feature request for your idea, give it a thumbs up on GitHub to let the community know it‚Äôs something you‚Äôre interested in. Share any details you think are important about how the feature gets made. You have a stake in the build as much as anyone.\nThink you have a bug? If you‚Äôre certain you‚Äôve configured a feature correctly (perhaps have someone from the forums check your config), and you‚Äôre still seeing confusing behavior, this might be the time to file a bug report ( OpenSearch / OpenSearch Dashboards). Please search for existing material (in this case, bug reports in GitHub) first. Someone might already be working on the same problem you‚Äôre having. Give it a thumbs up if the bug applies to you. Someone might even offer a workaround until an official fix can be implemented.\nIf you‚Äôre just not sure, don‚Äôt worry. There‚Äôs one more option to try.\nThe forum Our community is quite diverse and filled with experts of all kinds. If you‚Äôre not sure about something, are getting an error that you don‚Äôt understand, or would just like feedback, visit our forum and set up an account, if you haven‚Äôt already. The forum is yet another way that you can become part of the community. Not only will you develop expertise by answering questions, but you‚Äôll also develop the community‚Äôs collective expertise by sharing yours. Please search for existing forum topics first to make sure you‚Äôre not duplicating questions. It‚Äôs possible someone has asked the same question and has marked an answer as the solution.\nShare\nWe‚Äôre a very diverse crowd with a ton of knowledge to share, so let‚Äôs share it with each other! Share your logs. Share your configs. Share your thought process. Share your code and code examples. Ask others for code and code examples. Sharing will build our community and collective expertise.\nAs always, we encourage you to participate in the OpenSearch community. Answering questions on the forum is a great place to start. Issues with the ‚Äú Good First Issue‚Äù label on GitHub are another great place to start. Also check out David Tippett‚Äôs live stream on Contributing To Open Source. One extra special bonus‚Äînow that it‚Äôs Hacktoberfest, your participation may just earn you a swag pack that will include a water bottle, a sweatshirt, and a sticker pack!",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/aws-knn-algorithms-workload-optimizations/",
    "title": "Partner Highlight: AWS explores different k-NN algorithms and workload optimizations",
    "content": "OpenSearch partner and contributor AWS recently published a blog that focuses on the different algorithms and techniques used to perform approximate k-NN search at scales of more than a billion data points within OpenSearch. In the blog, Othmane Hamzaoui, AI/ML Specialist Solutions Architect with AWS, and Jack Mazanec, Software Dev Engineer and OpenSearch committer, explain algorithms including Hierarchical Navigable Small Worlds (HNSW), Inverted File System(IVF), and Product Quantization (PQ), and share the techniques and the memory footprint to support each of these algorithms, with examples. The post also includes the metrics and trade-offs between recall and performance, so you can choose the right algorithm for your k-NN workload at scale. Explore the AWS blog and be sure to check out the benchmarking codebase for yourself in GitHub.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/public-jenkins/",
    "title": "OpenSearch automated build system is now live",
    "content": "The OpenSearch Project deployed automated build, test, and release infrastructure that supports the end-to-end lifecycle of public distributions with the launch of OpenSearch 2.3.0. This includes the migration of OpenSearch Gradle check workflows that are run on every PR merge to the public infrastructure. Community members can now view logs by accessing the build infrastructure to understand the current state of their workflows and monitor runs as needed.\nThis build system act as a continuous integration and continuous deployment (CI/CD) system that triggers the generation of distribution builds along with automated execution of integration, backward compatibility, and performance tests for every build. The enhanced process also automatically publishes the artifacts to target distribution channels, including Docker, Amazon Simple Storage Service (Amazon S3), Amazon CloudFront, and Maven. Plugin developers can onboard their plugin artifacts into OpenSearch distributions by adding the component to the distribution input manifest file. The onboarding plugin will need to provide information such as the GitHub repository URL, the branch that needs to be built, the platforms the plugin supports, and the checks to run on the component, such as a version check and dependency check. See example for information on how to add the onboarding plugin to the manifest. The end-to-end build process uses the information in the file to build, assemble, test, and publish the artifacts to the corresponding distribution targets.\nThe build system is deployed using infrastructure as code (IaC) that uses AWS Cloud Development Kit (AWS CDK). The build infrastructure is completely reproducible and consumes the opensearch-ci repository as its source. The readme.md file in the opensearch-ci repository provides the instructions for reproducing this infrastructure. External contributors can contribute to the build ecosystem by creating a pull request in this GitHub repository for any future enhancements, bug fixes, or new features.\nMotivation\nBefore now, setting up the test and build infrastructure was a tedious process that required a lot of manual intervention and development effort in order to make changes and updates. Also, the infrastructure used for building and releasing the artifacts was not reproducible, and teams were finding it difficult to track the status of builds due to limited visibility into the build process. This created discrepancies among the development teams in terms of building, testing, and debugging the artifact builds during every release.\nThe manual processes also required development teams to create, own, and maintain their own automation build process, mimicking the manual build process. The new system automates all the manual steps involved in creating, managing, maintaining, and monitoring the DevOps stack for OpenSearch distribution builds in order to improve reproducibility and developer and release velocity.\nOpenSearch CI/CD infrastructure design\nFigure 1 depicts a high-level overview of the CI system architecture. Figure 1: Jenkins Infrastructure Overview\nOverview of the Jenkins workflow(s)\nAll the workflows are managed through source-controlled code located in the GitHub repository: Distribution-build-opensearch: The all-in-one pipeline used to clone, build, assemble, and store OpenSearch and its related plugin artifacts with one click. It currently supports Linux tarball and Linux RPM artifacts. We are in the process of adding MacOS tarball and Windows zip. Distribution-build-opensearch-dashboards: The all-in-one pipeline used to clone, build, assemble, and store OpenSearch Dashboards and its related plugin artifacts with one click. It currently supports LINUX tarball and Linux RPM artifacts. We are in the process of adding MacOS tarball and Windows zip. Check-for-build: This workflow will periodically check out the latest changes/commits in opensearch-project repositories and trigger the above distribution-build workflows. Integ-test: The integration tests for OpenSearch plugins are initiated and run here. Integ-test-opensearch-dashboards: The integration tests for OpenSearch Dashboards plugins are initiated and run here. Distribution-promote-artifacts: The one-click workflow used to pull, sign, and push OpenSearch and OpenSearch Dashboards artifacts from staging to production, including Linux tarball and Linux RPM. Docker-build: The workflow used to build both CI runner Docker images and Docker images for OpenSearch, OpenSearch Dashboards, and others. Docker-copy: The workflow used to copy Docker images between different repositories and the foundation of the Docker-promotion workflow. Docker-promotion: The workflow used to promote Docker images from the staging registry to both production Docker Hub and Amazon Elastic Container Registry (Amazon ECR). Whitesource-scan: The workflow used to trigger CVE scanning on a daily basis using the whitesource/Mend toolchain. Gradle-check: The heavyweight Gradle check workflow that serves the PR checks for the OpenSearch repository.\nBenefits\nManually running the build, test, and release steps is a time-consuming process that adds a lot of manual overhead and human error in running, monitoring, and reporting the outcome of each build. This is not sustainable or scalable for frequent builds (that is, hourly, nightly builds). The enhanced end-to-end distribution generation system helps us to overcome these gaps by automating the steps required to build, test, and publish the distribution artifact on a regular cadence. The automation process not only surfaces issues, gaps, and blockers on time but also helps us move faster by producing builds at a faster pace. This reduces the resource congestion, as we have limited manual overhead in generating distribution builds. This process allows us to release the major, minor, and patch versions quickly, frequently, and on time.\nNow that the build, test, and release infrastructure is public, the community can access, view, and contribute to the infrastructure as well as workflows that build the OpenSearch and OpenSearch Dashboards distributions. As a user with the anonymous role, you can view all the available jobs, logs, packaged artifacts, and Gradle checks related to OpenSearch and OpenSearch Dashboards workflows.\nWrapping it up\nThe primary goal of this project is to improve the reproducibility and transparency of the OpenSearch build infrastructure. The build, release, and test infrastructure used by the OpenSearch Project is easily reproduced by following the steps documented in the readme.md file, which uses IaC to reproduce the infrastructure. Contributors can now contribute to this infrastructure by submitting their code in a pull request to the opensearch-ci repository. The community can also contribute to the build, release, and test workflows deployed on this infrastructure by submitting their code in a pull request to the opensearch-build repository. To learn more, check out the Getting Started page. Additionally, if you find any issues or have questions, feel free to either create an issue on GitHub or ask a question on the forum!",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/ltr-with-opensearch-and-metarank/",
    "title": "Learn-to-Rank with OpenSearch and Metarank",
    "content": "Metarank is an open-source secondary ranker that can perform advanced search results by reordering the results with a LambdaMART learning to rank (LTR) model. In this post, we‚Äôll discuss why and when an LTR approach to ranking may be helpful and how Metarank implements LTR on top of OpenSearch.\nRanking in Lucene and OpenSearch\nOpenSearch, being a close relative to the Apache Lucene project, uses a traditional approach to search results ordering:\nIn the retrieval stage, Lucene builds a result set of all the documents matching the query from the inverted index within each shard.\nIn the per-shard scoring stage, a BM25 scoring function scores each document and orders them accordingly.\nIn the collection stage, each per-shard list of scored documents merges together in a final sequence of top-N matching documents.\nBM25 is a strong baseline for text relevance, but it only takes into account frequencies of terms in the query and some basic statistics on the collection of documents in the index. A more intuitive overview of the logical parts of BM25 formula is shown in the following diagram, taken from Lectures 17-18 of the Text Technologies for Data Science (TTSDS) course at the University of Edinburgh. The BM25 scoring function can typically be applied to all documents in the result set, so running the function should not become a CPU bottleneck on large sets.\nBut in common use cases like e-commerce, term frequencies and index statistics are not the only factors affecting relevance. For example, using implicit visitor behavior and item metadata may improve relevance, but these metadata factors should be included in the scoring function.\nOpenSearch provides a custom scoring function through the script_score query: GET /_search\n{\n\"query\": {\n\"script_score\": {\n\"query\": {\n\"match\": { \"message\": \"socks\" }\n},\n\"script\": {\n\"source\": \"doc['click_count'].value * _score\"\n}\n}\n}\n} In the example above, the function multiplies the BM25 score from the stored column and the per-document number of clicks, click_count, this document receives. With this approach, you can:\nUse item metadata from stored fields in the ranking, for example, item price.\nPut external ranking factors into the scoring function, like having a separate set of weights for mobile and desktop visitors.\nBut the script_score approach also has a set of important drawbacks:\nBecause the scoring function is invoked on each matching document, you cannot perform any complex computations inside the document.\nBecause no common scoring formula exists, you have to invent your own scoring function.\nBecause scoring formulas typically have a set of constants and weights, you have to choose the best values for parameters.\nFeature values used in the ranking formula should be either stored in the index, which requires frequent full reindexing, or served outside the search, which requires a custom satellite Feature Store.\nSecondary re-ranking\nHowever, another approach to ranking exists: secondary/multi-level re-ranking. Secondary ranking splits the retrieval and multiple re-ranking phases into independent steps:\nThe first-level index retrieves a full set of results without any extra ranking at all.\nThe second-level fast rankers like BM25 perform an initial scoring to reduce the number of matching documents to a top-N most relevant document, focusing on recall metrics.\nFinally, the third-level slow ranker reorders only the top-N candidates into the final ranking, focusing on a precision metric.\nThis approach is widespread in the industry. For a good overview, watch a talk Berlin Buzzwords 2019: Michael Sokolov &amp; Mike McCandless‚ÄìE-Commerce search at scale on Apache Lucene on how multi-stage ranking is implemented on amazon.com. Multi-level ranking also comes with a set of tradeoffs: It operates only on top-N candidates, so if your first-level ranker under-scored a document that wasn‚Äôt included in the top-N set, it may be missing from the final ranking.\nYou might have been forced to build a multi-stage system in house because, before now, there were no open-source solutions for this multi-level ranking.\nBut because final re-ranking happens outside your search application and happens only on a top-N subset of documents, you can implement complex LTR algorithms like LambdaMART.\nMetarank Metarank is an open-source secondary re-ranker that:\nImplements LambdaMART on top of the embedded feature engineering pipeline. A couple of other ranking model implementations, like BPR and BERT4Rec, are on the roadmap.\nContains a YAML DSL query that defines ranking factors such as rates, counters, windows, and UA/Referer/GeoIP parsers.\nAs a secondary re-ranker, it‚Äôs agnostic to the way you perform the candidate retrieval. It should be integrated with your app, not the search engine.\nMetarank‚Äôs ranking predictions are based on past historical click-through events. These events are analyzed, aggregated into a set of implicit judgments, and later used to train the machine learning (ML) model. Finally, in the real-time inference stage, the model tries to predict the best ranking based on past visitor behavior. Preparing historical events\nMetarank uses historical click-through events as an ML training dataset, looking at parameters such as:\nWhat visitor or item-specific information was updated, like item price, tags, and CRM visitor profile. { \"event\": \"item\", \"id\": \"81f46c34-a4bb-469c-8708-f8127cd67d27\", \"item\": \"product1\", \"timestamp\": \"1599391467000\", \"fields\": [ { \"name\": \"title\", \"value\": \"Nice jeans\" }, { \"name\": \"price\", \"value\": 25.0 }, { \"name\": \"color\", \"value\": [ \"blue\", \"black\"]}, { \"name\": \"availability\", \"value\": true }] } Impression: How and when a visitor accessed an item listing. For example, impression measures the relevancy of search results, collections, or recommendation widgets. { \"event\": \"ranking\", \"id\": \"81f46c34-a4bb-469c-8708-f8127cd67d27\", \"timestamp\": \"1599391467000\", \"user\": \"user1\", \"session\": \"session1\", \"fields\": [ { \"name\": \"query\", \"value\": \"socks\" }], \"items\": [ { \"id\": \"item3\", \"relevancy\": 2.0 }, { \"id\": \"item1\", \"relevancy\": 1.0 }, { \"id\": \"item2\", \"relevancy\": 0.5 }] } Interaction: How a visitor interacted with an item from the list, for example, click, add-to-cart, or a mouse hover. { \"event\": \"interaction\", \"id\": \"0f4c0036-04fb-4409-b2c6-7163a59f6b7d\", \"impression\": \"81f46c34-a4bb-469c-8708-f8127cd67d27\", \"timestamp\": \"1599391467000\", \"user\": \"user1\", \"session\": \"session1\", \"type\": \"purchase\", \"item\": \"item1\", \"fields\": [ { \"name\": \"count\", \"value\": 1 }, { \"name\": \"shipping\", \"value\": \"DHL\" }], } Mapping events to ranking factors\nMetarank has a rich DSL, which can help you map feedback events to the actual numerical ranking factors. Apart from some simple cases, like directly using item metadata numerical fields, Metarank can also handle complex ML features.\nYou can perform a one-hot/label encoding of low-cardinality string fields: - name: genre type: string scope: item source: item.genre values: - comedy - drama - action Extract and one-hot encode a mobile/desktop/tablet category from the User-Agent field: - name: platform type: ua field: platform source: ranking.ua Perform a sliding window count of interaction events for a particular item: - name: item_click_count type: window_count interaction: click scope: item bucket_size: 24h // make a counter for each 24h rolling window windows: [ 7, 14, 30, 60] // on each refresh, aggregate to 1-2-4-8 week counts refresh: 1h Or see rates for each item: - name: CTR type: rate top: click // divide number of clicks bottom: impression // to number of examine events scope: item bucket: 24h // aggregate over 24-hour buckets periods: [ 7, 14, 30, 60] // sum buckets for multiple time ranges There are many more feature extractors available, so check the Metarank docs for other examples and a more detailed quickstart guide.\nThese feature extractors map events into numerical features and form a set of implicit judgments, which are later used for ML model training. Sending requests\nTo integrate LTR between search applications, OpenSearch and Metarank require a couple of additions to your current setup:\nIt‚Äôs time to start collecting visitor feedback events if you‚Äôre not yet doing so. You can either roll your own or use existing open-source telemetry collectors like Snowplow Analytics.\nMetarank uses Redis as a state store. The Redis node sizing usually depends on your ML feature setup and number of users. You can check the Metarank RAM usage benchmark for ballpark estimations for your use case.\nAfter receiving a response with top-N candidates from OpenSearch, you need to send an extra request to Metarank to perform re-ranking. An example re-ranking request is the same as the ranking event above: { \"event\": \"ranking\", \"id\": \"id1\", \"items\": [ { \"id\": \"72998\" }, { \"id\": \"67197\" }, { \"id\": \"77561\" }, { \"id\": \"68358\" }, { \"id\": \"72378\" }, { \"id\": \"85131\" }, { \"id\": \"94864\" }, { \"id\": \"68791\" }, { \"id\": \"109487\" }, { \"id\": \"59315\" }, { \"id\": \"120466\" }, { \"id\": \"90405\" }, { \"id\": \"117529\" }, { \"id\": \"130490\" }, { \"id\": \"92420\" }, { \"id\": \"122882\" }, { \"id\": \"113345\" }, { \"id\": \"2571\" }, { \"id\": \"122900\" }, { \"id\": \"88744\" }, { \"id\": \"95875\" }, { \"id\": \"60069\" }, { \"id\": \"2021\" }, { \"id\": \"135567\" }, { \"id\": \"122902\" }, { \"id\": \"104243\" }, { \"id\": \"112852\" }, { \"id\": \"102880\" }, { \"id\": \"96610\" }, { \"id\": \"741\" }, { \"id\": \"166528\" }, { \"id\": \"164179\" }, { \"id\": \"71057\" }, { \"id\": \"3527\" }, { \"id\": \"6365\" }, { \"id\": \"6934\" }, { \"id\": \"114935\" }, { \"id\": \"8810\" }, { \"id\": \"173291\" }, { \"id\": \"1580\" }, { \"id\": \"1917\" }, { \"id\": \"135569\" }, { \"id\": \"106920\" }, { \"id\": \"1240\" }, { \"id\": \"85056\" }, { \"id\": \"780\" }, { \"id\": \"1527\" }, { \"id\": \"5459\" }, { \"id\": \"8644\" }, { \"id\": \"60684\" }, { \"id\": \"7254\" }, { \"id\": \"44191\" }, { \"id\": \"97752\" }, { \"id\": \"2628\" }, { \"id\": \"541\" }, { \"id\": \"106002\" }, { \"id\": \"2012\" }, { \"id\": \"79357\" }, { \"id\": \"6283\" }, { \"id\": \"113741\" }, { \"id\": \"27660\" }, { \"id\": \"34048\" }, { \"id\": \"1882\" }, { \"id\": \"1748\" }, { \"id\": \"34319\" }, { \"id\": \"1097\" }, { \"id\": \"115713\" }, { \"id\": \"2916\" }], \"user\": \"alice\", \"session\": \"alice1\", \"timestamp\": 1661345221008 } Metarank will respond with the same set of requested items, but in another ordering: { \"items\": [ { \"item\": \"1580\", \"score\": 3.345952033996582 }, { \"item\": \"5459\", \"score\": 2.873959541320801 }, { \"item\": \"8644\", \"score\": 2.500633478164673 }, { \"item\": \"56174\", \"score\": 2.2979140281677246 }, { \"item\": \"2571\", \"score\": 2.0133864879608154 }, { \"item\": \"1270\", \"score\": 1.807900071144104 }, { \"item\": \"109487\", \"score\": 1.7143194675445557 }, { \"item\": \"589\", \"score\": 1.706472396850586 }, { \"item\": \"780\", \"score\": 1.7030035257339478 }, { \"item\": \"1527\", \"score\": 1.6445566415786743 }, { \"item\": \"60069\", \"score\": 1.6372750997543335 }, { \"item\": \"1917\", \"score\": 1.6299139261245728 }] } Re-ranking latency\nThe main drawback of secondary re-ranking is extra latency: Apart from the initial search request, you need to perform an extra network call to the ranker. Metarank is no exception, and latency depends on the following things: State encoding format: Metarank can store aggregated user and item state in JSON format in Redis, but it takes more memory and is much slower to encode-decode than a custom binary format. Network latency between Metarank and Redis: Metarank pulls all features for all re-ranked items in a single large batch request. There are no multiple network calls and only a constant overhead. Request size: The more items you ask to re-rank, the more data needs to be loaded. Number of features used: The more per-item features that are defined in the config, the more data that is loaded during the request processing.\nBased on a RankLens dataset, you can build a latency benchmark to measure a typical request processing time, dependent on the request size. So while planning your installation, expect Metarank to have an extra 20‚Äì30ms latency budget.\nOpenSearch Remote Ranker Plugin RFC\nOpenSearch Project maintainers are currently discussing a new approach for a better and more flexible re-ranking framework: [RFC] OpenSearch Remote Ranker Plugin (semantic).\nThe main idea is that the OpenSearch Project will: Define the common ranking API and handle the re-ranking process internally; there will be no need to send multiple queries from the application to different backends. Implement this common API across multiple rankers, which can be hot-swappable and does not require modifying application code while reimplementing.\nTo quote the original RFC:\nThe OpenSearch Semantic Ranker is a plugin that will re-rank search results at search time by calling an external service with semantic search capabilities for improved accuracy and relevance. This plugin will make it easier for OpenSearch users to quickly and easily connect with a service of their choice to improve search results in their applications.\nThe plugin will modify the OpenSearch query flow and do the following: Get top-N BM25-scored document results from each shard of the OpenSearch index. Preprocess document results and prepare them to be sent to an external ‚Äúre-ranking‚Äù service. Call the external service that uses semantic search models to re-rank the results. Collect per-shard results into the final top-N ranking. Metarank developers are already collaborating with the OpenSearch team on this proposal and plan to support the Remote Ranker Plugin API when it is released.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/OpenSearch-Project-voice-and-tone/",
    "title": "From the editor's desk: OpenSearch Project voice and tone",
    "content": "In the first installment of our From the editor‚Äôs desk series, we focused on the OpenSearch Project and inclusion, a subject that is foundational to who we are and how we talk to each other as a community. For the second installment, we‚Äôd like to focus on another subject related to community interaction: voice and tone.\nIn response to feedback received during OpenSearchCon 2022, we have updated our guidance on voice and tone. We have also added new guidance on OpenSearch Project naming conventions and brand personality traits. Check out this new and updated guidance in the OpenSearch Project style guidelines.\nHere is a summary of what has changed.\nNaming conventions\nThe following naming conventions should be observed in OpenSearch Project content:\nCapitalize both words when referring to the OpenSearch Project. The OpenSearch Project has three products: OpenSearch, OpenSearch Dashboards, and Data Prepper. OpenSearch is the name for the distributed search and analytics engine used by Amazon OpenSearch Service.\nAmazon OpenSearch Service is a managed service that makes it easy to deploy, operate, and scale OpenSearch. Use the full name Amazon OpenSearch Service on first appearance. The abbreviated service name, OpenSearch Service, can be used for subsequent appearances.\nOpenSearch Dashboards is the UI for OpenSearch. On first appearance, use the full name OpenSearch Dashboards. Dashboards can be used for subsequent appearances.\nRefer to OpenSearch Project customers as users, and refer to the larger group of users as the community or the OpenSearch community.\nVoice and tone\nVoice is the point of view or style of a writer. Voice can refer to active or passive but may also refer to verb tense (past, present, future, and so on). Tone is the emotional undercurrent (such as calm or angry) of the voice. We strive to speak to the community with a consistent voice and tone, as if a single writer writes all content. Writing with a common voice also helps to establish the OpenSearch Project identity and brand.\nVoice\nThe voice of the OpenSearch Project is people oriented and focused on empowering the user directly. We use language that emphasizes what the user can do with OpenSearch rather than what tasks OpenSearch can perform.\nWhenever possible, use the active voice instead of the passive voice. The passive form is typically wordier and can often cause writers to obscure the details of the action. For example, change the agentless passive it is recommended to the more direct we recommend.\nRefer to the reader as you (second person), and refer to the OpenSearch Project as we (first person). If there are multiple authors for a blog post, you can use we to refer to the authors as individuals.\nFor procedures or instructions, ensure that action is taken by the user (‚ÄúThen you can stop the container‚Ä¶‚Äù) rather than the writer (‚ÄúWe also have to stop the container‚Ä¶‚Äù). Reserve the first-person plural for speaking as the OpenSearch Project, with recommendations, warnings, or explanations.\nIn general, use the present tense. Use the future tense only when an event happens later than, not immediately after, the action under discussion.\nTone\nThe tone of the OpenSearch Project is conversational, welcoming, engaging, and open. The overall tone is knowledgeable but humble, informal but authoritative, informative but not dry, and friendly without being overly familiar.\nWe talk to readers in their own words, never assuming that they understand how OpenSearch works. We use precise technical terms where appropriate, but we avoid technical jargon and insider lingo. We speak to readers in simple, plain, everyday language.\nAvoid excessive words, such as please. Be courteous but not wordy. Extra detail can often be moved elsewhere. Use humor with caution because it is subjective, can be easily misunderstood, and can potentially alienate your audience.\nBrand personality traits Personality trait Description Guidance Clear and precise The OpenSearch Project understands that our community works, develops, and builds in roles and organizations that require precise thinking and thorough documentation. We strive to use precise language‚Äîto clearly say what we mean without leaving ideas open to interpretation, to support our assertions with facts and figures, and to provide credible and current (third-party) references where called for. We communicate in plain, direct language that is easily understood. Complex concepts are introduced in a concise, unambiguous way that does not assume knowledge on the part of the reader. High-level content is supported by links to more in-depth or technical content that users can engage with at their convenience.\n- Write with clarity and choose words carefully. Think about the audience and how they might interpret your assertions. - Be specific. Avoid estimates or general claims when exact data can be provided. - Support claims with data. If something is ‚Äúfaster‚Äù or ‚Äúmore accurate,‚Äù say how much. - When citing third-party references, include direct links. Transparent and open As an open-source project, we exchange information with the community in an accessible and transparent manner. We publish our product plans in the open on GitHub, share relevant and timely information related to the project through our forum and/or our blog, and engage in open dialogues related to product and feature development in the public sphere. Anyone can view our roadmap, raise a question or an issue, or participate in our community meetings.\n- Tell a complete story. If you‚Äôre walking the reader through a solution or sharing news, don‚Äôt skip important information. - Be forthcoming. Communicate time-sensitive news and information in a thorough and timely manner. - If there‚Äôs something the reader needs to know, say it up front. Don‚Äôt ‚Äúbury the lede.‚Äù Collaborative and supportive We‚Äôre part of a community that is here to help. We aim to be resourceful on behalf of the community and encourage others to do the same. To facilitate an open exchange of ideas, we provide forums through which the community can ask and answer one another‚Äôs questions.\n- Use conversational language that welcomes and engages the audience. Have a dialogue. - Invite discussion and feedback. We have several mechanisms for open discussion, including requests for comment (RFCs), a community forum, and community meetings. Trustworthy and personable We stay grounded in the facts and the data. We do not overstate what our products are capable of. We demonstrate our knowledge in a humble but authoritative way and reliably deliver what we promise. We provide mechanisms and support that allow the audience to explore our products for themselves, demonstrating that our actions consistently match our words. We speak to the community in a friendly, welcoming, judgment-free way so that our audience perceives us as being approachable. Our content is people oriented and focused on empowering the user directly.\n- Claims and assertions should be grounded in facts and data and supported accordingly. - Do not exaggerate or overstate. Let the facts and results speak for themselves. - Encourage the audience to explore our products for themselves. Offer guidance to help them do so. - Write directly and conversationally. Have a dialogue with your audience. Imagine writing as if you‚Äôre speaking directly to the person for whom you‚Äôre creating content. - Write from the community, for the community. Anyone creating or consuming content about OpenSearch is a member of the same group, with shared interest in learning about and building better search and analytics solutions. - Use judgment-free language. Words like simple, easy, and just create a skill judgment that may not apply to everyone in the OpenSearch community. Inclusive and accessible As an open-source project, The OpenSearch Project is for everyone, and we are inclusive. We value the diversity of backgrounds and perspectives in the OpenSearch community and welcome feedback from any contributor, regardless of their experience level. We design and create content so that people with disabilities can perceive, navigate, and interact with it. This ensures that our documentation is available and useful for everyone and helps improve the general usability of content. We understand our community is international and our writing takes that into account. We use plain language that avoids idioms and metaphors that may not be clear to the broader community.\n- Use inclusive language to connect with the diverse and global OpenSearch Project audience.- Be careful with our word choices. - Avoid sensitive terms. - Don‚Äôt use offensive terms. - Don‚Äôt use ableist or sexist language or language that perpetuates racist structures or stereotypes. - Links: Use link text that adequately describes the target page. For example, use the title of the target page instead of ‚Äúhere‚Äù or ‚Äúthis link.‚Äù In most cases, a formal cross-reference (the title of the page you‚Äôre linking to) is the preferred style because it provides context and helps readers understand where they‚Äôre going when they choose the link. - Images: ¬†¬†- Add introductory text that provides sufficient context for each image. ¬†¬†- Add ALT text that describes the image for screen readers. - Procedures: Not everyone uses a mouse, so use device-independent verbs; for example, use ‚Äúchoose‚Äù instead of ‚Äúclick.‚Äù - Location: When you‚Äôre describing the location of something else in your content, such as an image or another section, use words such as ‚Äúpreceding,‚Äù ‚Äúprevious,‚Äù or ‚Äúfollowing‚Äù instead of ‚Äúabove‚Äù and ‚Äúbelow.‚Äù Join the conversation\nAs always, the OpenSearch Project team is committed to promoting a community where everyone can join and contribute, and we welcome community contributions to the OpenSearch style guidelines. To access our documentation, see the OpenSearch documentation home page. If you want to contribute to the OpenSearch documentation, see the CONTRIBUTING.md file, which covers how to open an issue or create a pull request on GitHub.\nThank you for contributing to the OpenSearch Project and helping us to raise the bar on quality!",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Adding-New-Distributions-to-OpenSearch-Project/",
    "title": "Adding new distributions to the OpenSearch Project",
    "content": "Starting with OpenSearch version 1.3.2 for 1.x, and 2.0.0 for 2.x, OpenSearch has expanded its artifacts into multiple distributions, including TAR, Docker, and RPM. Looking back, it has been a challenging and exciting journey to get here.\nWith TAR as the initial distribution, we were able to wrap the archive into a container as part of the Docker release and then expand into multiple architectures, including x64 and arm64. Development of other distributions was delayed because the fundamental code base involved much hard coding to extend it.\nOpenSearch continually improves. The community has asked for an RPM package, as it is an easy way for LINUX users to deploy, run, and test OpenSearch and OpenSearch Dashboards clusters without complex manual setup. We see the demands and benefits of this new distribution and have prioritized its implementation. This effort led to a two-month-long project involving overhauling code and carrying out modularization activities and testing to advance the RPM distribution.\nAs the lead engineer on the RPM project, I would like to share the steps, processes, and results for adding new distribution(s) with the community. I hope you find this information useful in contributing to the code to add new distributions in the future.\nGetting started\nThe opensearch-build repository (repo) is the centerpiece of artifact generation for the OpenSearch Project. Most automation and pipelines run the code of this repo. To answer where to start the implementation of a new distribution ( RPM for the purpose in this blog), we need to understand the workflow in the opensearch-build repo.\nThe OpenSearch Project‚Äôs final release artifacts (non-SNAPSHOT) have two formats: with plugins (normal format) and without plugins (min format, only for TAR). This naturally separates OpenSearch into two different parts: the OpenSearch engine and the related plugins. The engine is based on the main OpenSearch repository. The related plugins are in their own repositories, such as machine learning plugin, index management plugin, sql plugin, and many more.\nThree stages are involved in generating OpenSearch artifacts: Build, Assemble, and Test.\nIn the build stage, you first separately compile the source code for the OpenSearch engine and related plugins. In this stage, the OpenSearch engine and related plugins will run their corresponding build scripts and generate artifacts based on the platform and distribution format you passed through. You will receive a min artifact of the engine and a zip format of each plugin. The plugins zips then will be installed onto the OpenSearch engine and repackaged into the final products in the assemble stage. These products then will be pushed to the object storage staging bucket and, once completed, will be validated (the test stage) and then promoted to the object storage production bucket for release. Example: Directory structure/tree list of opensearch-build repository src directory opensearch-build/src\n‚îú‚îÄ‚îÄ assemble_workflow\n‚îú‚îÄ‚îÄ build_workflow\n‚îú‚îÄ‚îÄ checkout_workflow\n‚îú‚îÄ‚îÄ ci_workflow\n‚îú‚îÄ‚îÄ git\n‚îú‚îÄ‚îÄ jenkins\n‚îú‚îÄ‚îÄ manifests\n‚îú‚îÄ‚îÄ manifests_workflow\n‚îú‚îÄ‚îÄ paths\n‚îú‚îÄ‚îÄ run_assemble.py\n‚îú‚îÄ‚îÄ run_build.py\n‚îú‚îÄ‚îÄ run_bwc_test.py\n‚îú‚îÄ‚îÄ run_checkout.py\n‚îú‚îÄ‚îÄ run_ci.py\n‚îú‚îÄ‚îÄ run_integ_test.py\n‚îú‚îÄ‚îÄ run_manifests.py\n‚îú‚îÄ‚îÄ run_perf_test.py\n‚îú‚îÄ‚îÄ run_sign.py\n‚îú‚îÄ‚îÄ sign_workflow\n‚îú‚îÄ‚îÄ system\n‚îî‚îÄ‚îÄ test_workflow The corresponding directories, build_workflow, assemble_workflow, and test_workflow, contain the code for the build, assemble, test stages, respectively. In this blog post, we focus on the build and assemble stages because they are crucial to implementing a new OpenSearch distribution.\nWhile we are using Jenkins to execute the opensearch-build repo code for automated distribution builds, this blog post doesn‚Äôt go into depth on that topic; See OpenSearch automated build system is now live for more information. In this blog post, we focus on running the build and assemble code in a local environment with the necessary changes. Example: Simplified flow to generate artifacts on Jenkins As shown in the figure above, after contributors commit their code or merge pull requests (PRs) in the opensearch-project repo, the pipelines on Public Jenkins will be triggered. From there, build_workflow, assemble_workflow, and test_workflow will be run sequentially, and then the final products will be promoted to the object storage production bucket and made available publicly at artifacts.opensearch.org.\nHow to run build_workflow\nA common question asked during the build_workflow is ‚ÄúHow do we run it?‚Äù. To answer that question, it‚Äôs necessary to understand what available parameters we have access to in order to control the output. Older version of opensearch-build/src/build_workflow/README.md before we add RPM The following parameters are available in `build.sh`.\n| Name | Description |\n|-------------------------|-------------------------------------------------|\n| -s, --snapshot | Build a snapshot instead of a release artifact. |\n| -a, --architecture | Specify architecture to build. |\n| -p, --platform | Specify platform to build. |\n| --component [name...] | Rebuild a subset of components by name. |\n| --keep | Do not delete the temporary working directory. |\n| -l, --lock | Generate a stable reference manifest. |\n| -v, --verbose | Show more verbose output. | The figure above shows the parameters available in build_workflow before RPM is implemented. There is no --distribution parameter, as the --platform parameter is used to hardcode the distribution type. Initially, it was assumed that if platform=linux, then distribution=tar.\nLet‚Äôs now look at the commands in build_workflow. First, clone the opensearch-build repo and move to its root directory, and then use the following command to build artifacts for the 1.3.2 version tarball as an example. Note that we specify the component to be OpenSearch only for the purpose of this blog post. All the components in input manifest file opensearch-1.3.2.yml will be compiled by default, unless you specify otherwise. # More information on the commands can be found in README.md of each workflow directory./build.sh manifests/1.3.2/opensearch-1.3.2.yml --component OpenSearch -p &lt; &gt; -a &lt; &gt;...... Next, we need to add a new parameter to handle the different distributions. | -d, --distribution | Specify distribution to build, default is `tar`. | When running./build.sh, it runs./run.sh internally to check dependencies on python/pip, which then runs src/run_build.py. Note that src/run_build.py is the entry point to build_workflow and starts the building process, including handling the user-defined parameters (see run_build.py). from build_workflow.build_args import BuildArgs...... args = BuildArgs () Next, you add the following code in src/build_workflow/build_args.py using the argparse module. # tar and zip already existed prior to our change\n# we will keep them so that olded functions are not broken SUPPORTED_DISTRIBUTIONS = [ \"tar\", \"zip\", \"rpm\"]...... parser. add_argument ( \"-d\", \"--distribution\", type = str, choices = self. SUPPORTED_DISTRIBUTIONS, help = \"Distribution to build.\", default = None, dest = \"distribution\") Other files will need updates to support this new parameter. Examples include src/build_workflow/build_target.py and src/run_build.py.\nWhen running the build_workflow, you use an input manifest file (for this blog post, it‚Äôs manifest/1.3.2/opensearch-1.3.2.yml). This file consists of the components you want to build (the OpenSearch engine and related plugins). To learn more about the manifest files, read the code in src/manifests for more information. Two other manifest files (both named manifest.yml) are generated after build_workflow and assemble_workflow to record the results. The two manifest.yml files are referred to as ‚Äúbuild manifest‚Äù and ‚Äúbundle manifest‚Äù. Sometimes, ‚Äúbundle manifest‚Äù is also referred to as ‚Äúdist manifest‚Äù.\nTo add the distribution key-value pair in the build manifest so that the assemble_workflow can use the build manifest to assemble the actual artifact in the given distribution format, you update src/build_workflow/build_recorder.py and src/manifests/build_manifest.py.\nIn build_recorder.py, you define the default value distribution as tar: self. data [ \"build\"][ \"distribution\"] = target. distribution if target. distribution else \"tar\" In build_manifest.py, distribution is None so that the older manifest.yml files from previous releases without the distribution key-value pair are compatible with the new code: self. distribution: str = data. get ( 'distribution', None) After the above steps are complete, you are ready to craft the build scripts. First, you will focus on src/build_workflow/builder_from_source.py. This file will call a subprocess that runs a shell script returned by the ScriptFinder function. The distribution value is None if related key is not found in input manifest, and you add a filter around the list of commands so that -d parameter is not called if distribution is None. build_script = ScriptFinder. find_build_script ( self. target. name, self. component. name, self. git_repo. working_directory) build_command = \" \". join ( filter ( None, [ \"bash\", build_script, f \"-v { self. target. version } \", f \"-p { self. target. platform } \", f \"-a { self. target. architecture } \", f \"-d { self. target. distribution } \" if self. target. distribution else None, f \"-s { str ( self. target. snapshot). lower () } \", f \"-o { self. output_path } \",])) The logic of ScriptFinder.find_build_script is in src/paths/script_finder.py, which would allocate a build script named build.sh (this script has the same name as the one in the root directory of the opensearch-build repository but contains very different content and is used for component-specific build activity). This script can be found in one of the following paths:\nThe opensearch-build repository scripts/components/&lt;Component Name&gt;/build.sh The component‚Äôs own repository root path build.sh The component‚Äôs own repository scripts/build.sh For the OpenSearch engine, because a scripts/components/OpenSearch/build.sh script exists in the opensearch-build repo, that script takes precedence and is returned by the ScriptFinder function. You now have confirmation that builder_from_source.py sends all the parameters from opensearch-build/build.sh (root build.sh) to scripts/components/OpenSearch/build.sh (component build.sh).\nFor scripts/components/OpenSearch/build.sh, you add distribution as part of the variables and change how the corresponding Gradle task is called. In the OpenSearch engine, you use Gradle to compile the Java code into an artifact and component build.sh uses the platform value to decide the Gradle task. You now add additional conditions to set the format for the final artifacts: PLATFORM, DISTRIBUTION, and ARCHITECTURE. case $PLATFORM - $DISTRIBUTION - $ARCHITECTURE in linux-tar-x64) PACKAGE = \"tar\" EXT = \"tar.gz\" TYPE = \"archives\" TARGET = \" $PLATFORM - $PACKAGE \" QUALIFIER = \" $PLATFORM -x64\";;......\nlinux-rpm-x64) PACKAGE = \"rpm\" EXT = \"rpm\" TYPE = \"packages\" TARGET = \"rpm\" QUALIFIER = \"x86_64\";;......./gradlew:distribution: $TYPE: $TARGET:assemble -Dbuild.snapshot = $SNAPSHOT The following command is called to run the proper Gradle task to build the OpenSearch engine RPM distribution on a LINUX host with x64 architecture../gradlew:distribution:packages:rpm:assemble -Dbuild.snapshot = $SNAPSHOT Once execution completes, you will find the following directory structure in the RPM directory for the opensearch-build repo root directory. If you want to learn more about implementing the RPM directory, see this GitHub PR. opensearch-build/rpm\n‚îî‚îÄ‚îÄ builds\n‚îî‚îÄ‚îÄ opensearch\n‚îú‚îÄ‚îÄ core-plugins\n‚îú‚îÄ‚îÄ dist\n‚îÇ ‚îî‚îÄ‚îÄ opensearch-min-1.3.2.x86_64.rpm\n‚îú‚îÄ‚îÄ manifest.yml\n‚îî‚îÄ‚îÄ plugins...... Congratulations! You have successfully implemented the build of RPM.\nThe Python code can call the component build.sh and pass the distribution value so that the proper Gradle task is selected. After compilation, you get an OpenSearch engine min RPM as the artifact and a build manifest in builds/opensearch/dist/manifest.yml to pass on in assemble_workflow. If you specify more components in --component or leave it empty by default, you should see other plugins being compiled similarly in the builds/opensearch/plugins directory. For plugins in the OpenSearch engine repo, they are in the core-plugins directory because they are part of the OpenSearch engine code base.\nNow that you have the necessary artifacts to produce final products, let‚Äôs move on to next stage: assemble_workflow.\nHow to run assemble_workflow\nIn build_workflow, you learned that OpenSearch artifacts are generated by calling the root build.sh and component build.sh scripts as they interact with Gradle tasks (OpenSearch Dashboards is similar but interacts with Yarn). To combine or bundle the artifacts into a final product, you need to add more functions in assemble_workflow.\nYou can use the command below to start the assemble process, as information in build_workflow is saved in build manifest.yml and we can pass the YAML file as the input. # More parameters can be found in the README./assemble.sh rpm/builds/opensearch/manifest.yml Similar to build_workflow, several file changes are required to support the new distribution parameter in assemble_workflow, such as src/manifests/bundle_manifest.py. self. distribution: str = data. get ( 'distribution', None) The distribution value from build manifest.yml is retrieved by src/assemble_workflow/bundle_recorder.py so that you can use it in the assemble_workflow. self. distribution = build. distribution Next, you need to understand the file structure. Unlike build_workflow, where everything is build related, assemble_workflow is separated into bundle and dist. ‚ÄúBundle‚Äù controls the general functions of assemble to create a bundle of the OpenSearch engine and related plugins. ‚ÄúDist‚Äù contains specific business logic for different types of distribution to repackage. For more information about the relationship between bundle and dist, see Assemble Artifacts Based on Distribution Not File Extension.\nBefore you implement the RPM distribution, you need to clean up the existing code to support older distributions such as Tar and Zip. To do this, remove the from_path function in src/assemble_workflow/dist.py, as it hardcodes the distribution type based on the file extension. You need to remove all of these old logic - @classmethod\n- def from_path(cls, name: str, path: str, min_path: str) -&gt; 'Dist':\n- ext = os.path.splitext(path)[1]\n- if ext == \".gz\":\n- return DistTar(name, path, min_path)\n- elif ext == \".zip\":\n- return DistZip(name, path, min_path)\n- else:\n- raise ValueError(f'Invalid min \"dist\" extension in input artifacts: {ext} ({path}).') You now have created a new file, src/assemble_workflow/dists.py, that acts as a class selector for the different distributions. from assemble_workflow.dist import Dist, DistTar, DistZip class Dists: DISTRIBUTIONS_MAP = { \"tar\": DistTar, \"zip\": DistZip, } @classmethod def create_dist ( cls, name: str, path: str, min_path: str, distribution: str) -&gt; Dist: if distribution is None: logging. info ( \"Distribution not specified, default to tar\") distribution = 'tar' return cls. DISTRIBUTIONS_MAP [ distribution]( name, path, min_path) DistTar and DistZip are dist classes. They contain the specific business logic required for the distribution to be properly assembled for creating the bundled final product. Based on the distribution value parsed from build manifest.yml, you can return the corresponding class.\nTake a look at class DistTar as an example before you implement a similar DistRpm. class DistTar ( Dist): def __extract__ ( self, dest: str) -&gt; None: with tarfile. open ( self. path, \"r:gz\") as tar: tar. extractall ( dest) def __build__ ( self, name: str, dest: str) -&gt; None: with tarfile. open ( name, \"w:gz\") as tar: tar. add ( self. archive_path, arcname = os. path. basename ( self. archive_path)) The DistTar class extends Dist class with two @abstractmethod to be implemented: extract and build. Extract takes the OpenSearch engine tarball artifact from the build stage, extracts the content so you can install plugins, and adjusts the configuration. Build (not to be confused with build_workflow) wraps the extracted content after the extract process and regenerates a new tarball as the final bundled product. Extract and build can be treated as tar and untar or zip and unzip, respectively.\nAfter extracting the content, additional refinements can be made in src/run_assemble.py, which was called by assemble.sh in the opensearch-build repo root directory. with Bundles. create ( build_manifest, artifacts_dir, bundle_recorder, args. keep) as bundle: bundle. install_min () bundle. install_plugins () logging. info ( f \"Installed plugins: { bundle. installed_plugins } \") The flow runs as assemble.sh calls run_assemble.py, which calls Bundles.create. It then returns the class bundle_opensearch.py, which creates an instance of BundleOpenSearch that extends Bundle, and then Bundle internally calls Dists.create, which returns DistTar and calls extract. min_dist = Dists. create_dist ( min_bundle. name, min_dist_path, min_path, self. build. distribution) logging. info ( f \"Extracting dist into { self. tmp_dir. name }.\") min_dist. extract ( self. tmp_dir. name) After extraction, you return to run_assemble.py to continue installation. bundle. install_min () bundle. install_plugins () If no plugin is installed, OpenSearch and OpenSearch Dashboards are referred to as min, meaning they are the minimal product. The two calls, bundle.install_min() and bundle.install_plugins(), act similarly to the build() call in builder_from_source, run another script install.sh internally to complete the installation, such as src/scripts/component/OpenSearch/install.sh for bundle.install_min() and src/scripts/default/install.sh for bundle.install_plugins(). Typically, the plugin install.sh is no-op, as OpenSearch and OpenSearch Dashboards use different installation methods. You override the call with the following function in bundle_opensearch.py: def install_plugin ( self, plugin: BuildComponent) -&gt; None: tmp_path = self. _copy_component ( plugin, \"plugins\") cli_path = os. path. join ( self. min_dist. archive_path, \"bin\", self. install_plugin_script) self. _execute ( f \" { cli_path } install --batch file: { tmp_path } \") super (). install_plugin ( plugin) You now have completed installation, so you return to run_assemble.py to call the following remaining functions: bundle_recorder. write_manifest ( bundle. min_dist. archive_path) bundle. package ( output_dir) bundle_recorder. write_manifest ( output_dir) When you check out the business logic for bundle.package, it internally calls build from DistTar, which creates a tarball based on the extracted content.\nNow that you know how assemble_workflow runs, let‚Äôs implement a new section for RPM. Like TAR, you will add the new section DistRpm in src/assemble_workflow/dist.py. from assemble_workflow.bundle_rpm import BundleRpm...... class DistRpm ( Dist): def __extract__ ( self, dest: str) -&gt; None: BundleRpm ( self. filename, self. path, self. min_path). extract ( dest) def __build__ ( self, name: str, dest: str) -&gt; None: BundleRpm ( self. filename, self. path, self. min_path). build ( name, dest, self. archive_path) Similar changes will be added to src/assemble_workflow/dists.py. You then will implement RPM -specific bundle code in src/assemble_workflow/bundle_rpm.py. This file is categorized by two @abstractmethod: extract and build.\nExtract:\nGiven the min version of OpenSearch.rpm, run rpm2cpio to convert.rpm to.cpio format.\nUse the cpio -imdv command to extract the.rpm raw files.\nPrepare necessary settings before bundle.install_min() bundle.install_plugins() runs.\nBuild:\nAfter installation, organize the directory structure to prepare for the final build.\nUse rpmbuild to generate the final.rpm product for release.\nMove the final.rpm product to the corresponding directory ( opensearch-build/rpm/dist/opensearch/).\nThe current directory structure, or tree, after assemble_workflow is similar to the tree for build_workflow. The difference is that this tree has a new dist folder with the new artifacts. opensearch-build/rpm\n‚îú‚îÄ‚îÄ builds\n‚îÇ ‚îî‚îÄ‚îÄ opensearch\n‚îÇ ‚îú‚îÄ‚îÄ core-plugins\n‚îÇ ‚îú‚îÄ‚îÄ dist\n‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ opensearch-min-1.3.2.x86_64.rpm\n‚îÇ ‚îú‚îÄ‚îÄ manifest.yml\n‚îÇ ‚îî‚îÄ‚îÄ plugins\n‚îî‚îÄ‚îÄ dist\n‚îî‚îÄ‚îÄ opensearch\n‚îú‚îÄ‚îÄ manifest.yml\n‚îú‚îÄ‚îÄ opensearch-1.3.2-linux-x64.rpm...... Congratulations! You have implemented the RPM distribution creation process within the opensearch-build code base.\nFor more information on rpmbuild,.spec files,.service files, and signing, see opensearch-build/scripts/pkg/ on GitHub.\nConclusion\nWe continually improve by adding new distributions to the OpenSearch Project. With the addition of RPM, we have set up a new structure, standard operating procedures, and an entry point to encourage the community to contribute code more easily. For example, we are working with Graylog on DEB distribution implementation, using similar steps as RPM; the draft PR is in review. We encourage you to engage with us on improving the process for adding new distributions to OpenSearch.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/thudtest/",
    "title": "The thud test: Measuring OpenSearch Documentation",
    "content": "When I first started my career as a technical writer about 20 years ago, success was measured by the volume of content produced. Back then, everything was printed and bound. When you dropped a stack of paper on a desk or the ground, it would make an audible thud. The theory went that the louder the thud, the more content you had produced, and the happier your customers were.\nFast-forward about 10 years, and it was all about simplification: Only provide the minimum amount of content that your customers need to accomplish their tasks. So we set about removing information and only documenting what was absolutely necessary.\nIn the years that followed, technical writers continued to face tough choices. We know that we have to write for customers first. We know that writing simply is hard. We know that we need to insist on the highest standards. We know that words are expensive to translate and resource intensive to change as the user interface changes.\nAnd we know that product managers still want to use the thud test to measure documentation output. So how do we strike the right balance between quantity and simplicity? The answers aren‚Äôt always straightforward, and we make judgment calls every day in a never-ending pursuit of high quality.\nWhere did my OpenSearch journey begin?\nEarlier this year, I became the Documentation Manager for OpenSearch, a recently launched product that had 2 writers and estimated documentation coverage of 30%. My dilemma was how to build the content while growing a technical writing team and balancing all the theories of the last 20 years of technical writing.\nIn addition, for the first time I found myself managing documentation for an open-source project, where anyone can contribute directly to the content. I‚Äôve always loved the idea of community-sourced content, and now it was a staple in my day-to-day work. These added contributors represent another strong voice in the pool of opinions that affect the choices a technical writer makes.\nThe OpenSearch documentation team and I began our journey by working backwards from what our customers were asking for: more content. As the technical writing team has grown to 8 writers and we‚Äôve learned more about how OpenSearch works, we‚Äôve been able to make considerable progress in closing the gap. But not only are we creating a lot of content‚Äîproducing a thud‚Äîwe are also ensuring that our quality goals are met.\nFortunately, an advantage of open source writing is that quality is inherently built in to the process. For example, there are several levels of approvals before content is published, from both internal and external OpenSearch contributors. We also rely on analytics to help us make informed decisions about which documentation needs the most attention. So where are we now?\nWe are making headway in our pursuit. In the past nine months, we‚Äôve enhanced our documentation to include the addition of or enhancement to the following topics: OpenSearch documentation home page Getting started with OpenSearch Dashboards\nNew Dashboards visualization types\nData Prepper 2.0\nOpenSearch core REST APIs\nAlerting API - new document-level monitors, and per-document monitors for Dashboards\nNotifications - new Notifications plugin and Notifications API\nSegment replication\nAnomaly Detection\nSecurity\nField types\nOpenSearch CLI\nOpenSearch Kubernetes Operator\nInstallation instructions for Tarball, RPM, and Docker\nPerformance Analyzer\nIndex Management\nSearch and query - new Optimizing text for searches\nMachine Learning (ML) Commons\nSecurity overview.Net, Ruby, and Go clients\nSnapshot management\nCustom GeoJSON - new region map visualizations\nRollup enhancements\nSQL and PPL section refactoring and additions\nSQL Aggregate functions\nWhat‚Äôs next for OpenSearch documentation?\nWe are not stopping at creating a thud in our content. In addition to providing full content coverage, we are making usability enhancements to the documentation website as well. We are restructuring the navigation pane to make it easier and more accessible for our users, we are making improvements to our metadata tagging (taxonomy) for optimizing search, and we are designing a more friendly UI that makes it easier to access what our users need with fewer clicks. We will also implement improved analytics in order to have another level of feedback to drive our content decisions. We sincerely thank you for your patience as we build the OpenSearch documentation and appreciate any input you may have. Please see our contributing guidelines if you are interested in contributing to the OpenSearch documentation, and check out our documentation site periodically to learn more about OpenSearch.\nAs always, the OpenSearch documentation team looks forward to your feedback and contributions!",
    "keywords": [
      "feature"
    ],
    "type": "News"
  },
  {
    "url": "/blog/OpenSearch-Partner-Highlight-Using-Cloaked-Search/",
    "title": "OpenSearch partner highlight: Using Cloaked Search to protect your data",
    "content": "Most companies will tell you that the data they hold is well protected. They‚Äôll say it‚Äôs encrypted ‚Äúat rest and\nin transit,‚Äù which means they use HTTPS and disk-level or database-level encryption. Unfortunately, this isn‚Äôt\nthe protection most people think it is.\nMost encryption claims are smoke and mirrors\nWhat passes for data protection these days is often wildly overblown on the ‚Äúprotection‚Äù front.\nHTTPS protects the data for a brief moment in time by warding off eavesdroppers and interlopers, but it doesn‚Äôt change the availability of the data. For example, this blog is served over HTTPS, and you are\nprobably not authenticated.\n‚ÄúAt-rest encryption‚Äù is really only useful when someone steals a hard drive. When used by an always-on server,\nthe value drops to near zero. That‚Äôs because the encryption key is only needed when the system starts up, after\nwhich the protection is transparent to anybody accessing the server. Application-layer encryption: A better approach\nWhen you encrypt the data before it is sent to the storage layer, an attacker scraping a database gets a bunch\nof garbage bytes that aren‚Äôt useful to them without the relevant keys. This is known as application-layer\nencryption (ALE) because it happens at the layer above the data store instead of below it.\nBy separating the data and keys, companies can achieve meaningful defense-in-depth, where the everyday problems\nof cloud misconfigurations, application vulnerabilities, and network breaches don‚Äôt necessarily result in a\ndata breach. This makes gaining access to the needed keys an entirely different and more difficult problem for\nan attacker.\nIf this approach were used more often, we wouldn‚Äôt constantly hear stories about attackers who get into a\ncorporate network, move laterally within the network to a database server, and then scrape all of the data\ndown for sale on the darknet. The companies who are implementing this correctly are the ones who aren‚Äôt in\nthe news.\nWhy encrypt data in OpenSearch?\nSearch services are often an underprotected backdoor way to gain access to sensitive data. They frequently hold\nmany different types of data that should be protected: personal information like names and addresses; log data\nthat includes web form submissions; communications records like emails, customer messages, and chats;\ninsider-trading fodder like roadmap info, financial projections, legal documents; and a variety of other\ndata that should be defended against unauthorized access.\nA search service is akin to an attractive nuisance. Just as a swimming pool without a fence will attract\nneighborhood kids, search data without encryption will attract hackers and employees.\nThe employee side is what we call the ‚Äúcurious insider‚Äù problem, where trusted users ignore policy and bypass or\nabuse application-level access controls in order to peek at data they shouldn‚Äôt. In most companies, employees in\nroles such as database administration, ops, customer support, and engineering can sneak peeks at sensitive data.\nThese trusted folks who scratch the itch of curiosity can get a company in just as much trouble as a hacker. Access to sensitive data should always be controlled. Backdoor access should be eliminated whenever possible.\nThat includes controlling access by admins. With ALE, a company can build in these\ncontrols for all users and administrators and can fortify legitimate access with unbypassable audit trails.\nProtecting search data with Cloaked Search\nAt IronCore Labs, we provide tools and technologies that help companies protect their data with\nALE while keeping that data usable. One way we do that is by enabling search over\nencrypted data. Cloaked Search by IronCore Labs is a drop-in proxy that\nsits in front of OpenSearch and automatically encrypts the sensitive data before it gets to the OpenSearch\nservice. It‚Äôs ALE for search. A valid key is then required in order to search the data. If you\ndon‚Äôt have a valid key, the data in OpenSearch is useless. Features and functionality\nCloaked Search is easy to deploy, simple to configure, and immediately adds protection to the private data\nheld in your search service. It comes packaged as a Docker container that deploys on your infrastructure.\nYour application connects to it, and it then connects to the OpenSearch service.\nNo plugins or other modifications needed to be added to OpenSearch, so it works as well with Amazon‚Äôs service as with\nyour own cluster. And it can be configured such that even Amazon, if you host with them, can‚Äôt access your\nsensitive data.\nCloaked Search protects text fields and supports the most popular types of queries. It handles Boolean searches\n(‚Äúand/or‚Äù qualifiers) with ‚Äúmust‚Äù and ‚Äúmust not‚Äù restrictions, prefix searches, suffix searches, phrase searches,\nand phonetic searches. The results are similar to what you would get without the encryption. Cloaked Search supports\nweighting of fields and other rank tuning, though you can‚Äôt do everything. For example, Cloaked Search doesn‚Äôt\nsupport regular expression searches on encrypted fields. But for most users, it drops in and just works. For those who are holding personal, private, sensitive, or regulated data in their OpenSearch service, Cloaked\nSearch reduces the inherent risks. This makes it easier to comply with data privacy laws, data security requirements\nset by statutes, and contractual obligations. If you offer software as a service, it also helps you earn the\ntrust of your customers. And for customers who have multiple tenants within OpenSearch, Cloaked Search seamlessly\nintegrates with SaaS Shield. If you‚Äôre looking for ALE encryption for the rest of your data, from queues to databases to files\non disk, IronCore Labs can help you protect that, too, with developer-proof,\ncrypto-agile tools that handle key orchestration and make it easy for you to be private and secure by design.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-2-4-is-available-today/",
    "title": "OpenSearch 2.4.0 is available today!",
    "content": "OpenSearch 2.4.0 brings Windows support, security analytics, new geospatial features, and a variety of upgrades for search, analytics, and observability use cases\nThe final minor release of 2022 extends the OpenSearch Project toolkit with more than 25 features and enhancements across a range of use cases like geospatial, search, security analytics, observability, and more. Support for Windows environments and other firsts for the project are now ready to put to work in your environment. Read on for a rundown of the new functionality you can build on with OpenSearch 2.4, and check out the downloads page to get started!\nApply semantic search and sharpen results with experimental tools\nFor the first time, OpenSearch users can perform automated semantic search on their documents, enhancing search results with additional meaning and context provided by machine learning (ML) models. A new collection of features, experimental for version 2.4.0, lets you combine traditional BM25 lexical search with deep-learning-powered semantic search, offering new ways to tune your search queries for increased relevance, recall, and precision. Enabling this functionality is a new plugin that allows you to upload your ML models, vectorize your documents and your queries, and search your documents using k-NN.\nOpenSearch 2.4.0 also supports building language models tuned to your document collection as an experimental functionality. Fine-tuning semantic search models for a particular set of documents is important for the best results but has sometimes been difficult because it has generally required large volumes of training data. With this release, you can now use a pretrained query generator model that creates query-response pairs using your own documents.\nYou‚Äôll need to enable this functionality in your YAML configuration to access these tools. As experimental features, the semantic search tools in this release are recommended for use outside of production environments. We‚Äôre excited for you to explore these capabilities with your document stores and hope to see any and all feedback about semantic search here.\nAdditionally, a new plugin that lets you compare different ranking techniques, including semantic search, side by side in the UI is included as an experimental tool. For example, you can compare the results of a BM25 search against the results from your ML model so that you can see the two rankings and tune your results accordingly. Search Comparison: The search comparison tool shows two sets of results for the image search ‚Äúwild west.‚Äù The search on the left uses BM25; at right, a search using a custom language model. BM25 ignores the context of the phrase and matches ‚Äúwild west‚Äù with ‚Äúwest‚Äù virginia state university, ‚Äúwild‚Äô animal, and ‚Äúwest‚Äù highland terrier. On the right, even though the captions do not contain ‚Äúwild‚Äù or ‚Äúwest,‚Äù the terms ‚Äúcowboy,‚Äù ‚Äúhorse,‚Äù and ‚Äúrodeo‚Äù form the basis for a better match to the query. Integrate your own ML models The 2.4.0 release introduces the model serving framework, an experimental feature that unlocks the ability for users to upload their own ML models to OpenSearch. In this version, the framework supports integrating text-embedding models to power semantic search. This release is a first step toward making it easier to integrate and operationalize ML on OpenSearch to power a variety of use cases, like visual and audio search, natural language processing (NLP)-powered analytics, and beyond. We encourage anyone using OpenSearch‚Äôs ML functionality to explore this new capability and let us know what else you‚Äôd like to see on this feature‚Äôs roadmap here.\nQuery your data from a specific point in time\nAnother new feature for 2.4.0 is Point in Time (PIT) search. Previously, there was no way to run queries against the same dataset and produce the same results; as the underlying data changed, the results of the search would change as well, making it challenging to compare and analyze query results. Now you can specify a PIT for which you want to run different queries against the same fixed dataset for consistent results. This functionality is accessible via APIs in version 2.4.0 and is planned to be enabled in the UI in a future version. Explore a single pane of glass for analytics workloads The OpenSearch community has asked for a ‚Äúsingle pane of glass‚Äù through which to visualize data across different clusters in an environment. OpenSearch Dashboards takes a significant step toward meeting that challenge with the release of version 2.4.0. For the first time, OpenSearch Dashboards supports multiple data sources, allowing users to manage connections to different OpenSearch endpoints and build visualizations and dashboards with data from different OpenSearch clusters. Available as an experimental feature, this is the first of many capabilities on the project roadmap intended to provide a unified view of your data; we encourage you to try this feature outside of a production environment and share your feedback here to inform future development.\nTo enhance security administration, we‚Äôve also added a feature that lets you configure different authentication types using the security plugin. Previously, users could set up a single authentication methodology (such as Basic, OpenID Connect, or SAML) for OpenSearch Dashboards; users have requested the option of configuring different ways to authenticate logins, including SSO. Now users can take advantage of this functionality with an integrated login/logout UI.\nAdd metrics analysis and automated log patterns to your observability stack\nWith this release, you can now visualize metrics data from the open-source Prometheus monitoring solution as well as from log data aggregated within OpenSearch, unlocking new observability capabilities that allow you to analyze and correlate data across logs and traces as well as metrics. Previously, the OpenSearch observability plugin only allowed you to ingest and visualize logs and traces from your monitored environments. With this feature, you can observe your digital assets with more granularity, gain deeper insight into the health of your infrastructure, and better inform root cause analysis.\nYou can now analyze your metrics data and correlate it with other signals to quickly track down the root cause of an issue or get a better understanding of the relationship between your infrastructure and service usage patterns. For example, you can aggregate log data to extract user throughput metrics and correlate those metrics with the infrastructure data from an external metrics store like Prometheus to see how your memory usage or compute usage scales in relation to the rise in throughput.\nAlso enhancing the observability solution set is the addition of log patterns to OpenSearch Dashboards. This feature auto-generates patterns from log data and makes them available for visualization and correlation. Automatically analyzing large volumes of data to detect and surface unique patterns, this feature is designed to shorten the time it takes to identify and respond to issues and make it easier to baseline and monitor patterns over time. You can use these patterns to detect outliers in your log data or identify noisy log data that you can filter away from ingestion. In 2.4.0, this feature creates log patterns based on the regex profiles of raw event data; look for additional ML-driven pattern generation in future releases.\nDeploy directly on Windows\nWe‚Äôre delighted to share that the 2.4.0 release fulfills a long-standing request from the community to provide Windows x64 distributions for OpenSearch and OpenSearch Dashboards. That‚Äôs right; for the first time, you can deploy the OpenSearch suite directly in your Windows environment! Available in ZIP format, this distribution includes support for all OpenSearch plugins except for the performance analyzer and shares common UIs with other distributions. You can download the Windows artifacts here and refer to the documentation to help you get started. Identify potential threats with security analytics As users continue to extend OpenSearch to new use cases, the ability to analyze security event logs to detect and investigate potential threats has been a popular request. OpenSearch introduces a new security analytics plugin for OpenSearch and OpenSearch Dashboards, experimental in 2.4.0, with an array of tools to help you identify attack signatures, create alerts from security findings, and visualize threat patterns using dashboards. With 2,000+ prepackaged open-source Sigma rules and support for multiple log sources, including Windows, Netflow, DNS, AWS CloudTrail, and more, security analytics provides you a range of options to help you protect your data and business-critical infrastructure.\nEnhance cluster resiliency\nThis release also brings a number of enhancements to OpenSearch usability and new features designed to improve cluster reliability and resiliency. Cluster manager task throttling lets you set limits on the volume of tasks submitted by data nodes to the cluster manager node. By throttling the incoming tasks submitted to the cluster manager, you can avoid scenarios in which spikes in the number of pending tasks can affect the cluster manager‚Äôs performance, which can impact cluster availability. Search backpressure supports enhanced resiliency by protecting OpenSearch clusters against traffic surges and other issues that can overwhelm cluster resources and lead to degraded performance or failures. This feature monitors the resources required for individual search requests and can route requests to different nodes based on resource availability. This can help keep nodes and clusters stable during periods of high demand.\nA new weighted zonal search request routing policy takes advantage of OpenSearch‚Äôs distributed processing framework to allow you to configure policies to route search traffic away from zones that have degraded performance or failures. Restore snapshots functionality lets you restore your data from existing snapshots through the OpenSearch Dashboards UI. In the 2.1.0 release, the OpenSearch Project introduced snapshot management as part of the Index Management (IM) plugin, enabling automated snapshots to back up user data from OpenSearch indexes. Previously, restoring your snapshots required you to use the restore API.\nSimplify storage management and recovery with searchable snapshots\nAnother new capability designed to help you manage your snapshots is the searchable snapshots feature. Now you can search indexes that are stored as snapshots within remote repositories without the need to download all of the index data to disk ahead of time, allowing you to save time and conserve storage capacity. This experimental feature represents phase two of the project‚Äôs storage roadmap, which aims to improve the performance, scalability, and cost of OpenSearch as part of the project‚Äôs high-level vision for storage. To share your feedback on this feature, please open a GitHub issue.\nNew functionality for geospatial data\nOpenSearch 2.4.0 also expands OpenSearch Dashboards‚Äô geospatial tools with two new data field types: The xy point field and xy shape field let you index and search geographic points represented as Cartesian coordinates or as Cartesian shapes, respectively. These field types join the latitude-longitude data field type previously offered for geographic data. Another new feature, GeoHex grid aggregation, lets you use datasets that use the Hexagonal Hierarchical Geospatial Indexing System (H3) for indexing and visualization with OpenSearch Dashboards coordinate maps. Get started with OpenSearch 2.4.0 You can download the latest version of OpenSearch here, and you can explore OpenSearch Dashboards live on the playground. You can learn more about all the capabilities we‚Äôve discussed here and much more in the release notes and documentation, as well as the documentation website release notes. And check back here regularly for future blog posts as we dive deeper into the new features included in OpenSearch 2.4.0.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/multiple-data-source/",
    "title": "Launch Highlight: Multiple Data Sources",
    "content": "OpenSearch Dashboards‚Äô current architecture works only with a single OpenSearch cluster, and to view data in different OpenSearch clusters and to navigate between Dashboards endpoints, users must have a complete OpenSearch with Dashboards stack setup. To solve this challenge, we are excited to announce for release 2.4 the Multiple Data Sources feature. This experimental feature supports multiple data source connections in OpenSearch Dashboards.\nBenefits\nThe Multiple Data Sources feature provides users the following benefits: Cost Savings: Users with hundreds of OpenSearch or compatible clusters (such as enterprise customers) don‚Äôt need to create or maintain an equal number of Dashboards servers. Performance: OpenSearch Dashboards runs a health check against OpenSearch, and in a shared-node architecture (that is, OpenSearch and Dashboards run on the same node), CPU and system memory usage can reach 10%, which can cause CPU, memory, and network overhead on OpenSearch clusters. Multiple Data Sources can reduce the number of total running Dashboards instances and trigger a health check only for Dashboards‚Äô dedicated metadata storage. Usability: Administrators who manage users‚Äô access to sensitive data no longer need to manage explicit user credentials across several clusters or configure, install, provision, and administer multiple OpenSearch Dashboards. Users also don‚Äôt have to navigate across several OpenSearch Dashboards applications to analyze data, combine data, and build reports. Flexibility: Users can add, connect, and remove any compatible data sources.\nHow can you use the Multiple Data Sources experimental feature?\nMultiple Data Sources enables Dashboards to query data from multiple compatible OpenSearch endpoints:\nAdd or remove data sources that are compatible with OpenSearch DSL, such as OpenSearch domains, and customer-managed OpenSearch clusters.\nCreate visualizations comparing time-series data from different index patterns and combine that data in a single dashboard. How does the Multiple Data Sources feature work? High-level architecture diagram The Multiple Data Sources feature introduces a new saved object, data-source, that provides information about a data source connection, such as endpoint or authentication information. Users can dynamically add the data source in Dashboards via the UI or API.\nWhen using the Multiple Data Sources feature, the data source‚Äôs associated index pattern will have a reference field like the following: The following pseudocode shows the current supported data source attributes: DataSourceAttributes extends SavedObjectAttributes {\ntitle,\ndescription,\nendpoint,\nauth: {\ntype,\ncredentials,\n}\n} To enable other Dashboards components to interact with a user data source cluster, this feature forked and repurposed the existing opensearch_service to create a data source connection. When Dashboards needs to access data sources, it will fetch the data source information from its metadata storage by data source ID and then send the request to the corresponding data source endpoint.\nThe following example code shows how to use the dataSource client: client: OpenSearchClient = await context. dataSource. opensearch. getClient ( dataSourceId); //Support for legacy client apiCaller: LegacyAPICaller = context. dataSource. opensearch. legacy. getClient ( dataSourceId). callAPI; In this first development phase, several Dashboards plugins are integrated with the data source feature (for example, data, index pattern management, discover, and visualize) to enable users to specify a data source when using their functions. High-level sequence diagram A comprehensive design breakdown will be added to the Dashboards repository.\nWhat‚Äôs next?\nThe OpenSearch Project team continues to enhance the Multiple Data Sources feature. Here are a few enhancements the team is working on:\nDashboards plugin support: The Multiple Data Sources feature currently works with index-pattern-based visualizations. The team is working on support with other plugins and data source capability within other plugins.\nOpenSearch client‚Äôs support for more authN approaches, such as AWS Sigv4 and JWT.\nImproved version compatibility between Dashboards and data sources.\nGet started with the Multiple Data Sources feature\nFor more information on setting up and exploring this feature, see the OpenSearch documentation. You can try out this feature in your local environment or the OpenSearch playground. To leave feedback, visit the OpenSearch Forum. If you‚Äôre interested in contributing to this experimental feature, consider contributing to the OpenSearch Dashboards repository.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Multiple-Authentication/",
    "title": "Launch highlight: Multiple authentication options for OpenSearch Dashboards",
    "content": "We‚Äôre excited to announce support for concurrent multiple authentication methods in OpenSearch Dashboards. This enhancement to the Dashboards security plugin provides a more unified and user-friendly login experience to Dashboards users, who are now able to choose a preferred option from a login UI that integrates basic authentication (username and password) and multiple single sign-on (SSO) providers for OpenID Connect (OIDC) and SAML.\nBenefits\nPreviously, Dashboards limited users to a single login authentication option. Users could either use basic authentication (username and password) or SSO with a single external SAML/OIDC identity provider (IdP). Starting in OpenSearch 2.4.0, the Dashboards security admin can enable multiple authentication options. The following are some of the benefits of enabling this feature:\nSimultaneous login with basic authentication (username and password) and SSO\nAn integrated login/logout experience regardless of your authentication methodology\nFlexibility for Dashboards security admins to customize the login page options\nSupport for identity redundancy across multiple IdPs to ensure high availability\nSimplified troubleshooting of external IdP-related issues\nUse cases\nCustomers using dedicated third-party enterprise-based IdPs to manage Dashboards user identities can now use OpenSearch as an internal IdP for admin, security, and privileged accounts and a single enterprise-based external IdP to maintain authentication information for general Dashboards user accounts. Larger enterprises that have redundancy needs can enable multiple enterprise-based IdPs, for example, an enterprise OIDC IdP and an enterprise SAML IdP.\nAdditionally, you can now add multiple external IdPs to meet different needs. For example, OpenSearch can serve as an internal IdP to maintain authentication information for admin and security accounts, an enterprise-based IdP for privileged and analyst accounts, and a social-based IdP for limited read-only accounts that can only view specific data.\nIf you have any additional use cases you think this feature can meet, we would love to hear about them! Please visit the OpenSearch Forum to leave feedbacks for us.\nHow can you use this feature?\nOpenSearch Dashboards admins can enable single or multiple authentication options on demand and customize the integrated login UI by editing opensearch_dashboards.yml. For the essential settings required to set up multiple authentication, see Multiple Authentication Options. Once these options are configured, the login UI will be automatically updated. For example, the login UI below shows both basic authentication and SSO enabled. How does this feature work?\nWhen Dashboards bootstraps and loads security settings from opensearch-dashboards.yml, the Dashboards client can detect whether Single-Authentication Mode or Multiple-Authentication Mode is enabled by evaluating the Multiple Authentication feature flag and authentication type setting:\nIf Single-Authentication Mode is detected, Dashboards registers a single authentication handler with the client based on the authentication type defined by the opensearch_security.auth.type setting. When Dashboards users log in, all requests flow to the dedicated IdP to complete identification and authentication.\nIf Multiple-Authentication Mode is detected, Dashboards evaluates all authentication handlers required for the multiple authentication types defined by the opensearch_security.auth.type setting and forms a compound authentication handler. Dashboards then registers this compound authentication handler with the client. The compound authentication handler is able to redirect the authentication request to various IdP endpoints for authentication interchangeably based on the authentication type defined within the login request.\nThe following image shows a high-level diagram of the multiple authentication options for OpenSearch Dashboards. Getting started\nFor more information on setting up and exploring this feature, see the OpenSearch documentation. To leave feedback, visit the OpenSearch Forum. If you‚Äôre interested in contributing to this feature, please check the OpenSearch Dashboards repository for more details.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/searchium-improve-search-performance/",
    "title": "Partner Highlight: Improving search performance for a better user experience",
    "content": "In search, speed is key to delivering a good user experience. In a recent blog post, Noam Schwartz of OpenSearch Project partner Searchium.ai explores five ways to improve search speed and cluster performance for OpenSearch. Check out ‚Äú Bolster OpenSearch performance with 5 simple steps ‚Äù for practical advice on managing your indices, optimizing cache utilization, selecting the right refresh interval, and more.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/point-in-time/",
    "title": "Launch highlight: Paginate with Point in Time",
    "content": "New for version 2.4, Point in Time (PIT) is the preferred pagination method for OpenSearch. While OpenSearch still supports other ways of paginating results, PIT search provides superior capabilities and performance because it is not bound to a query and supports consistent pagination going both forward and backward.\nOpenSearch pagination methods\nThere are four ways to paginate results in OpenSearch:\nThe from and size parameters\nThe scroll search operation\nThe search_after parameter\nPIT with search_after What makes a good pagination method?\nSo, what are the desired characteristics of a good pagination method? It depends, of course, on your application. If you don‚Äôt need the ability to skip pages, you may be perfectly fine with the most basic scroll search. However, in general, here are the qualities of a good pagination method: Moving forward and backward: In addition to moving forward in search results, the user may want to go back to the page before the current page. Skipping pages: The user may want to skip to a page out of order. Consistency: The search results must stay consistent, even with live index updates. If the user is on Page 1 of the results, selects Page 2, and then goes back to Page 1, Page 1 stays the same despite documents being indexed or deleted in the meantime. Deep pagination: The search must stay efficient even when the user wants to view results starting with Page 1,000.\nPagination methods compared\nHere is how OpenSearch pagination methods compare to each other. Pagination method Can move forward and backward? Consistent pagination? Efficient for deep pagination? Results not bound to a particular query? from and size parameters ‚úî\n-\n-\n- Scroll search -\n‚úî\n-\n- search_after parameter ‚úî\n-\n-\n- PIT with search_after ‚úî\n‚úî\n‚úî\n‚úî And the winner is‚Ä¶\nAs you can see, PIT with search_after is a clear winner because it checks all the boxes. Not only can it move both forward and backward in search results and provide consistent pagination while documents are being indexed and deleted, but it is also efficient for deep pagination. Plus, the results are frozen in time and not bound to a particular query, so you can run different queries against the same result dataset.\nHow PIT works\nWhen you create a PIT for a set of indexes, OpenSearch takes the corresponding segments of the indexes‚Äô shards and freezes them in time, creating contexts (pointers to the data) that you can use to access and query those shards. When you use a query with a PIT ID, it searches the segments that are frozen in time. Because a PIT is query agnostic, you can use any query to search the data in a PIT. PIT search allows for consistent pagination because even though the index continues to ingest and delete documents, the PIT does not reflect those changes and the dataset remains constant. Alternatively, if you use a normal query without a PIT ID, it searches live segments.\nWhat‚Äôs the catch?\nSo far we‚Äôve seen that PIT search is superior to other pagination methods. But what are the drawbacks? First, for a PIT, OpenSearch has to keep the segments even though they might have been merged and are not needed for the live dataset. This leads to an increased heap usage. Second, there is currently no built-in resiliency in a PIT, so if your node goes down, all PIT segments are lost.\nHow to use PIT search\nThe example in this section uses the shakespeare index.\nSet up a sample index\nAssuming you are not running the security plugin, you can set up the shakespeare index as follows:\nDownload the mapping file: wget http://media.sundog-soft.com/es7/shakes-mapping.json Index the mapping file: curl -H \"Content-Type: application/json\" -XPUT http://localhost:9200/shakespeare ( https://localhost:9200/shakespeare) --data-binary \"@shakes-mapping.json\" Download the shakespeare dataset: wget http://media.sundog-soft.com/es7/shakespeare_7.0.json Index the data into OpenSearch: curl -H \"Content-Type: application/json\" -XPUT http://localhost:9200/shakespeare/_bulk ( https://localhost:9200/shakespeare/_bulk) --data-binary \"@shakespeare_7.0.json\" Use the PIT functionality\nFollow these steps to use the PIT functionality. Step 1: Create a PIT The following request creates a PIT that will be kept for 1 hour: POST shakespeare/_search/point_in_time?keep_alive= 1 h The response contains a PIT ID: { \"pit_id\": \"o8L8QAELc2hha2VzcGVhcmUWck5NRDE3SFNUZ3lRZmJEMW1COWRidwAWd21kTFZRWlZSQ2k2YmRDeWh4U2w3ZwAAAAAAAAAACBZ0aFdURzJZbVJIYVlxczBsbkZ4emVnARZyTk1EMTdIU1RneVFmYkQxbUI5ZGJ3AAA=\", \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"creation_time\": 1669082665118 } Step 2: Use the PIT ID for your search The following request searches for all documents in the play ‚ÄúHamlet,‚Äù sorted by speech number and then ID, and returns the first 20 results: GET /_search { \"size\": 20, \"query\": { \"match\": { \"play_name\": \"Hamlet\" } }, \"pit\": { \"id\": \"o8L8QAELc2hha2VzcGVhcmUWck5NRDE3SFNUZ3lRZmJEMW1COWRidwAWd21kTFZRWlZSQ2k2YmRDeWh4U2w3ZwAAAAAAAAAACBZ0aFdURzJZbVJIYVlxczBsbkZ4emVnARZyTk1EMTdIU1RneVFmYkQxbUI5ZGJ3AAA=\", \"keep_alive\": \"100m\" }, \"sort\": [ { \"speech_number\": \"asc\" }, { \"_id\": \"asc\" }] } Note that there is no need to specify the index because the PIT is already created against the shakespeare index. The optional keep_alive parameter that is passed in the search prolongs the PIT keep-alive period by 100 minutes.\nThe response contains the first 20 results. Here is the last result; you have to note its sort values to get the next batch of results: { \"_index\": \"shakespeare\", \"_id\": \"32652\", \"_score\": null, \"_source\": { \"type\": \"line\", \"line_id\": 32653, \"play_name\": \"Hamlet\", \"speech_number\": 1, \"line_number\": \"1.2.19\", \"speaker\": \"KING CLAUDIUS\", \"text_entry\": \"Or thinking by our late dear brothers death\" }, \"sort\": [ 1, \"32652\"] } To get the next 20 results, use the search_after parameter and specify the sort values of the last result in the first batch: GET /_search { \"size\": 20, \"query\": { \"match\": { \"play_name\": \"Hamlet\" } }, \"pit\": { \"id\": \"o8L8QAELc2hha2VzcGVhcmUWck5NRDE3SFNUZ3lRZmJEMW1COWRidwAWd21kTFZRWlZSQ2k2YmRDeWh4U2w3ZwAAAAAAAAAACBZ0aFdURzJZbVJIYVlxczBsbkZ4emVnARZyTk1EMTdIU1RneVFmYkQxbUI5ZGJ3AAA=\" }, \"sort\": [ { \"speech_number\": \"asc\" }, { \"_id\": \"asc\" }], \"search_after\": [ 1, \"32652\"] } To list all PITs, use the following request: GET /_search/point_in_time/_all When you‚Äôre done, you can delete the PIT: DELETE /_search/point_in_time { \"pit_id\": \"o8L8QAELc2hha2VzcGVhcmUWck5NRDE3SFNUZ3lRZmJEMW1COWRidwAWd21kTFZRWlZSQ2k2YmRDeWh4U2w3ZwAAAAAAAAAACBZ0aFdURzJZbVJIYVlxczBsbkZ4emVnARZyTk1EMTdIU1RneVFmYkQxbUI5ZGJ3AAA=\" } You can also get information about the PIT‚Äôs segments using the CAT PIT Segments API.\nTo learn more about all PIT APIs, see Point in Time API.\nWhat‚Äôs next?\nFor more information about PIT search, see the PIT documentation section.\nNext, we‚Äôre planning to release a PIT frontend that you can use in OpenSearch Dashboards. To track the frontend progress, see the PIT meta issue.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/OpenSearchCon-2022-Videos/",
    "title": "OpenSearchCon Videos Are Now Available",
    "content": "OpenSearchCon 2022 is now available on our YouTube channel! View demos and talks about observability, log analytics, speeding up your search, and much more! Featuring Eli Fisher of the OpenSearch Project, Maria Hatfield of Dattell, Laysa Uchoa of PyLadies Munich and Aiven Oy, and Jonah Kowall of Logz.io <!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> The Art of the Possible with OpenSearch Anomaly Detection and UEBA Lennart Koopmann, Founder &amp; CTO, Graylog Get an overview of how OpenSearch components can be used to successfully detect anomalies for critical cybersecurity challenges (brute force attacks, data exfiltration, etc.) that are indicators of potential cyber threats for organizations of all sizes and in all industries.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> Getting Started with the OpenSearch Core Codebase Nicholas Knize, OpenSearch Maintainer, Lucene Committer, and PMC Member Interested in contributing to OpenSearch? Curious how to get started? After this review of the OpenSearch Core, we hope you will walk away with a better understanding of the architecture and implementation so you can begin contributing today.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> Creating the OpenSearch Kubernetes Operator Ziv Segal, CEO &amp; Co-Founder, Opster The OpenSearch Kubernetes Operator was the product of a unique, international collaboration by the open-source software community. We cover everything you need to know about the operator, including installation, hardware requirements and environment compatibility, and best practices. You‚Äôll also learn about features such as out-of-the-box security, auto-scaling, upgrading sessions, blue-green deployments, and more.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> Hundreds of Terabytes of Data Daniel Doubrovkine, Principal Engineer, OpenSearch Prabhakar Sithanandam, Principal Engineer, OpenSearch In 2020 Pinterest was ingesting 1.7 TB of data daily, growing to 3 TB that year. Since then, data volumes haven‚Äôt just grown exponentially, they have exploded. Hundreds of TB per day is no longer some crazy number in 2022‚Äîyou can draw a curve from there into the future. The big question now is not whether OpenSearch can support a few TB of data per day but, rather, what does OpenSearch need to look like to support hundreds of TB of data, and how soon? In this talk, we present and explore some disruptive ideas for the next decade of OpenSearch.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> How Organizations Can Get the Most Out of Their OpenSearch Contributions Jeff Zemerick, Search Relevance Consultant, OpenSource Connections OpenSearch being available under the Apache License provides exciting possibilities for contributing to the future of the project‚Äîbut how do you get the most benefit from your contributions? We describe common pitfalls and best practices that will help you maximize your contributions to the OpenSearch Project so you‚Äôre able to get beyond just committing on code.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> How to Do Microservice Observability with OpenSearch Rafael Gumiero, Sr. Analytics Specialist Solutions Architect, Amazon Web Services Prashant Agrawal, Search Specialist Solutions Architect, Amazon OpenSearch Service Learn how to instrument, collect, correlate, and analyze traces along with log data from user frontends to service backends. We demo these concepts using OpenSearch, Open Telemetry, Fluent Bit, Data Prepper, and OpenSearch Kubernetes Operator.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> Deploying OpenSearch Solutions on Hybrid Multi-Cloud Environments Mehdi Bendriss, Senior Software Engineer, Canonical This session showcases a cloud-vendor-agnostic alternative to deploying and operating OpenSearch in a hybrid multi-cloud scenario. An open-source solution, the Charmed Operators, is used to achieve this. The end goal is to empower developers to use and operate OpenSearch solutions in production as well as to lower the adoption barrier.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> Using Data Prepper for Observability Ingest David Venable, Senior Software Engineer, Amazon Web Services Rajiv Taori, Principal Product Manager, Technical, Amazon Web Services Data Prepper is an open-source data collector for observability data as well as a standalone application that runs as a last-mile ingestor. Teams can create data pipelines in Data Prepper that receive events from their application hosts and ingest them into OpenSearch. Data Prepper provides stateful processing of trace data and log events to aggregate data before ingesting it into OpenSearch. It also provides a suite of processors to transform events before sending them to OpenSearch. In this session, we not only introduce you to Data Prepper, but we also walk you through some of the use cases it supports and provide guidance on how to use it for ingesting log and trace data into OpenSearch.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> Speed Up Your Search: Concurrency and Merge Policies to the Rescue Andriy Redko, Staff Software Developer, Aiven Oy Obtaining fast query times when dealing with large amounts of data in OpenSearch can be a challenge, and, unfortunately, there‚Äôs not a single solution that will work for all use cases. This talk explores two somewhat orthogonal, but closely related, techniques to address the problem available to the community as of OpenSearch 2.1.0.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> Rediscover Your Data with a New Multimodal Search Capability James Sharpnack, Senior Applied Scientist, AWS Xingjian Shi, Senior Applied Scientist, Amazon AI In modern search, our databases can be any modality‚Äîtext, images, or tabular. In this session, we explore an approach to adding multimodal search capabilities to OpenSearch through AutoGluon, Amazon‚Äôs open-source AutoML software. Through multimodal embeddings, we‚Äôre able to perform semantic search and retrieve data based on its contextual meaning. We discuss how, with these embeddings, we can detect anomalous data and assess the similarities between datasets to improve our models.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> OpenSearch Project Roadmap 2022 and Beyond Charlotte Henkle, Senior Software Development Manager, OpenSearch The OpenSearch Project has grown tremendously over the past year. In this session, we take a look ahead at the OpenSearch roadmap and share more with you about what we have planned for the next release, the far-off foggy future, and everything in between.\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> Get the latest updates on OpenSearchCon 2023 through our forum or social channels. The project‚Äôs latest release, OpenSearch 2.4.0, is available for download, along with documentation to guide you.\nFollow the project! Forum Twitter LinkedIn YouTube MeetUp",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/aws-sigv4-support-for-clients/",
    "title": "AWS SigV4 support for OpenSearch clients",
    "content": "OpenSearch clients now support the ability to sign requests using AWS Signature V4. This has been a community request for a while, and we‚Äôre happy to announce that we have completed work across all clients, in collaboration with external contributors. Signing requests using native clients has been an essential requirement for accessing the Amazon OpenSearch Service on AWS using fine grained access controls. Having native SigV4 support in clients avoids the need to use cURL requests and other workarounds.\nSetting up the managed service to use fine-grained access control\nBe sure to update your access control type to an AWS Identity and Access Management (IAM) role/user. Do not use the master user role. In the following image, the IAM role allows access to the specific OpenSearch domain that is selected: Alternatively, you can set a domain-level access policy without using fine-grained access. Ensure that the IAM role you use has read/write access to the domain. Creating a client connection using SigV4 signing\nBefore you begin, ensure that you have AWS credentials set up on your machine. AWS credentials can be stored in ~/.aws/credentials or set as AWS_ environment variables, and contain an access key, a secret key and an optional session token, that allow you to authenticate with AWS resources using IAM.\nCreating a client connection in Java import java.io.IOException; import org.opensearch.client.opensearch.OpenSearchClient; import org.opensearch.client.opensearch.core.InfoResponse; import org.opensearch.client.transport.aws.AwsSdk2Transport; import org.opensearch.client.transport.aws.AwsSdk2TransportOptions; import software.amazon.awssdk.http.SdkHttpClient; import software.amazon.awssdk.http.apache.ApacheHttpClient; import software.amazon.awssdk.regions.Region; public static void main ( final String [] args) throws IOException { SdkHttpClient httpClient = ApacheHttpClient. builder (). build (); try { OpenSearchClient client = new OpenSearchClient ( new AwsSdk2Transport ( httpClient, \"search-xxx.region.es.amazonaws.com\", Region. US_WEST_2, AwsSdk2TransportOptions. builder (). build ())); InfoResponse info = client. info (); System. out. println ( info. version (). distribution () + \": \" + info. version (). number ()); } finally { httpClient. close (); } } Creating a client connection in Python\nThe Python client requires you to have boto3 installed. Make sure to update the connection_class to use RequestsHttpConnection. from urllib.parse import urlparse from boto3 import Session from opensearchpy import AWSV4SignerAuth, OpenSearch, RequestsHttpConnection url = urlparse ( \"https://search-xxx.region.es.amazonaws.com\") region = 'us-east-1' credentials = Session (). get_credentials () auth = AWSV4SignerAuth ( credentials, region) client = OpenSearch ( hosts = [{ 'host': url. netloc, 'port': url. port or 443 }], http_auth = auth, use_ssl = True, verify_certs = True, connection_class = RequestsHttpConnection) info = client. info () print ( f \" { info [ 'version'][ 'distribution'] }: { info [ 'version'][ 'number'] } \") Creating a client connection in JavaScript\nThe JavaScript client requires you to have aws-sdk installed. Depending on which version of the SDK you are using, initialize the client appropriately as shown in the following code segments.\nUsing AWS V2 SDK const AWS = require ( ' aws-sdk '); const { Client } = require ( ' @opensearch-project/opensearch '); const { AwsSigv4Signer } = require ( ' @opensearch-project/opensearch/aws '); const client = new Client ({... AwsSigv4Signer ({ region: ' us-east-1 ', getCredentials: () =&gt; new Promise (( resolve, reject) =&gt; { AWS. config. getCredentials (( err, credentials) =&gt; { if ( err) { reject ( err); } else { resolve ( credentials); } }); }), }), node: \" https://search-xxx.region.es.amazonaws.com \" }); Using AWS V3 SDK const { defaultProvider } = require ( \" @aws-sdk/credential-provider-node \"); const { Client } = require ( ' @opensearch-project/opensearch '); const { AwsSigv4Signer } = require ( ' @opensearch-project/opensearch/aws '); async function main () { const client = new Client ({... AwsSigv4Signer ({ region: \" us-east-1 \", getCredentials: () =&gt; { const credentialsProvider = defaultProvider (); return credentialsProvider (); }, }), node: \" https://search-xxx.region.es.amazonaws.com \" }); var info = await client. info (); var version = info. body. version console. log ( version. distribution + \": \" + version. number); } main (); Creating a client connection in Ruby\nThe opensearch-aws-sigv4 gem provides the OpenSearch::Aws::Sigv4Client class, which has all the features of OpenSearch::Client. The only difference between these two clients is that OpenSearch::Aws::Sigv4Client requires an instance of Aws::Sigv4::Signer during instantiation to authenticate with AWS. require 'opensearch-aws-sigv4' require 'aws-sigv4' signer = Aws:: Sigv4:: Signer. new ( service: 'es', region: 'us-east-1', access_key_id: '...', secret_access_key: '...', session_token: '...') client = OpenSearch:: Aws:: Sigv4Client. new ({ host: \"https://search-xxx.region.es.amazonaws.com\", log: false }, signer) info = client. info puts info [ 'version'][ 'distribution'] + ': ' + info [ 'version'][ 'number'] Creating a client connection in.NET\nAll required request signing is handled by the AwsSigV4HttpConnection implementation. By default, AwsSigV4HttpConnection uses the.NET AWS SDK‚Äôs default credentials provider to acquire credentials from the environment. However, you may opt to pass in your own credentials provider, for example, to assume a role. Refer to the OpenSearch.Net User Guide for complete instructions. using OpenSearch.Client; using OpenSearch.Net.Auth.AwsSigV4; namespace Application { class Program { static void Main ( string [] args) { var endpoint = new Uri ( \"https://search-xxx.region.es.amazonaws.com\"); var connection = new AwsSigV4HttpConnection (); var config = new ConnectionSettings ( endpoint, connection); var client = new OpenSearchClient ( config); Console. WriteLine ( $\" { client. RootNodeInfo (). Version. Distribution }: { client. RootNodeInfo (). Version. Number } \"); } } } Creating a client connection in Rust\nRequest signing is configured using the Credentials::AwsSigV4 enum variant or its helper conversion from an AWS SDK configuration. See aws-config for other AWS credentials provider implementations, for example, to assume a role. #[tokio::main] async fn main () -&gt; Result &lt; (), Box &lt; dyn std:: error:: Error &gt;&gt; { use std::{ convert:: TryInto, env, thread, time }; use serde_json:: Value; use opensearch::{ http:: transport::{ SingleNodeConnectionPool, TransportBuilder }, OpenSearch, }; use url:: Url; let url = Url:: parse ( \"https://search-xxx.region.es.amazonaws.com\"); let conn_pool = SingleNodeConnectionPool:: new ( url?); let aws_config = aws_config:: load_from_env ().await.clone (); let transport = TransportBuilder:: new ( conn_pool).auth ( aws_config.clone ().try_into ()?).build ()?; let client = OpenSearch:: new ( transport); let info: Value = client.info ().send ().await?.json ().await?; println! ( \"{}: {}\", info [ \"version\"][ \"distribution\"].as_str ().unwrap (), info [ \"version\"][ \"number\"].as_str ().unwrap ()); Ok (()) } Creating a client connection in PHP\nThe PHP client uses the setSigV4CredentialProvider attribute to assume credentials from the the local credential store. Use the setSigV4Region attribute to set the AWS Region. &lt;?php require_once __DIR__. '/vendor/autoload.php'; $client = ( new \\ OpenSearch\\ClientBuilder ()) -&gt; setHosts ([ \"https://search-xxx.region.es.amazonaws.com\"]) -&gt; setSigV4Region ( \"us-east-1\") -&gt; setSigV4CredentialProvider ( true) -&gt; build (); $info = $client -&gt; info (); echo \" { $info [ 'version'][ 'distribution'] }: { $info [ 'version'][ 'number'] } \\n \"; Creating a client connection in Go package main import ( \"context\" \"encoding/json\" \"fmt\" \"log\" \"strings\" \"github.com/aws/aws-sdk-go-v2/config\" \"github.com/opensearch-project/opensearch-go/v2\" requestsigner \"github.com/opensearch-project/opensearch-go/v2/signer/awsv2\") func main () { ctx:= context. Background () cfg, _:= config. LoadDefaultConfig ( ctx) signer, _:= requestsigner. NewSigner ( cfg) endpoint:= \"https://search-xxx.region.es.amazonaws.com\" client, _:= opensearch. NewClient ( opensearch. Config { Addresses: [] string { endpoint }, Signer: signer, }) if info, err:= client. Info (); err!= nil { log. Fatal ( \"info\", err) } else { var r map [ string] interface {} json. NewDecoder ( info. Body). Decode ( &amp; r) version:= r [ \"version\"]. ( map [ string] interface {}) fmt. Printf ( \"%s: %s \\n \", version [ \"distribution\"], version [ \"number\"]) } } Use with Amazon OpenSearch Serverless (preview)\nRefer to this article for information about how to use clients with Amazon OpenSearch Serverless.\nSummary\nYou can now sign your requests natively using the client APIs instead of workarounds. We‚Äôre continuing to work on improving the capabilities of SigV4 in clients with scenarios like asynchronous connections, compressed requests, and connection pooling support, and we welcome your pull requests and feedback in the form of issues on GitHub.",
    "keywords": [
      "feature",
      null,
      ""
    ],
    "type": "News"
  },
  {
    "url": "/blog/fluentbit20-and-opensearch/",
    "title": "Fluent Bit 2.0 and OpenSearch",
    "content": "Earlier this year at KubeCon North America, the Fluent team announced Fluent Bit v2.0. The Fluent Bit project is an open-source Apache 2.0 project that helps users collect, process, and enrich observability data (logs, metrics, and traces) from a variety of sources and send it to downstream analytics engines, such as OpenSearch. In this blog post, we will discuss some of the features of Fluent Bit v2.0 and how you can use them to get started with OpenSearch.\nOverview of Fluent Bit v2.0\nThe latest version of Fluent Bit includes commonly requested features from users within the larger Fluent community, including:\nFull support for OpenTelemetry standards (logs, metrics, and traces)‚Äîboth input and output.\nEnhanced support for Prometheus metrics (Node Exporter, Prometheus Scraper, and Prometheus Remote Write).\nFlexibility with Golang input plugin support and new WebAssembly plugins.\nGreater debugging (tap) and monitoring capabilities (storage metrics).\nFull TLS support for ingestion of syslog and other network traffic.\nConfiguration support for the YAML format.\nDynamic index support for OpenSearch, as of 2.0.6. (This is one of my favorites!)\nDynamic index support for OpenSearch\nOne of the most requested features has been the ability to send data that is part of an incoming data stream to OpenSearch. For example, if you‚Äôre reading data from Kubernetes logs, you may want to follow an indexing strategy where each index is named after a Kubernetes namespace. With dynamic index support through Record Accessor, you can now set the index to pull values from the incoming message stream. Record Accessor is a syntax used to pull values from nested fields to use as part of the index. To learn more about this feature, visit the following documentation link. As an example, we will start with a simple configuration where we are collecting CPU metrics, enriching the logs with our Linux system‚Äôs hostname, and then setting the index to be equal to the hostname. [INPUT]\nName cpu\n[FILTER]\nName modify\nMatch *\nRecord host $hostname\n[OUTPUT]\nName opensearch\nType\n{INSERT REMAINING CONFIGURATION} After running this configuration, you can see that a create request is sent to OpenSearch and that the index is based on the hostname. Next steps\nIn this blog post, we covered the new features of Fluent Bit v2.0 and how you can make use of dynamic index support to send data to OpenSearch. There are many integrations with the Fluent Bit v2 release that you can make use of with OpenSearch. Some include OpenTelemery data to OpenSearch Data Prepper, ingesting more secure network-based sources, and using the enhanced self-metrics monitoring for Fluent Bit to ensure all of your data flows to OpenSearch.\nIf you have any questions or are interested in joining the Fluent Community, join us on our Fluent Slack channel or in the GitHub repository.",
    "keywords": [
      "releases",
      null,
      ""
    ],
    "type": "News"
  },
  {
    "url": "/blog/similar-document-search/",
    "title": "Similar document search with OpenSearch",
    "content": "This blog is available in Japanese here: OpenSearch „Çí‰ΩøÁî®„Åó„ÅüÈ°û‰ººÊñáÊõ∏Ê§úÁ¥¢ When users need to find multiple results in your search data, such as recommendations for similar articles on a news website or recommendations for related products on an e-commerce site, you can take advantage of s imilar document search inside of OpenSearch. When you give OpenSearch a document as a query, similar document search retrieves contents similar to the query document.\nThere are two main approaches to retrieving similar documents:\nRetrieval based on the similarity of the contents to the document query\nRetrieval based on the user‚Äôs query history\nThis blog post focuses on the first approach and explains how to implement the approach using OpenSearch.\nSimilar document search in OpenSearch\n1. More like this query\nThe traditional way to achieve similar document search is to use a More Like This (MLT) query. MLT is a search based on term frequency, which has the assumption that similar documents contain the same words. While MLT is an intuitive mechanism, it lacks the flexibility of considering context or different words with similar meanings.\nMLT uses term frequency-inverse document frequency (tf-idf) to extract document features:\nTerm frequency (tf) is the number of times a term appears within a document.\nInverse document frequency (idf) is the measure of how rare the term is across all the documents.\ntf-idf is the product of tf and idf. That product is used to reflect how important a word is to a document in a collection of documents. MLT analyzes an input document and selects the top k terms with the highest tf-idf to form a query. When a user enters a query, MLT uses the query to return the top similar documents.\n2. k-NN\nYou can also use the k-nearest neighbors (k-NN) algorithm for similar document search. In k-NN, vectors represent documents. When given a query vector, k-NN returns k documents with the highest similarity. To create vectors from documents, you can use a machine learning (ML) model to position documents with similar meanings close to each other. Therefore, similar document search using k-NN allows for more advanced search than MLT because vectors can contain context and not just the same words. However, search results highly depend on the model, and it is quite difficult to understand why certain search results appears where others don‚Äôt.\nOpenSearch supports k-NN search, which can use exact k-NN search or approximate k-NN for an efficient search. In order to use k-NN, you must convert the text of a field to be searched into a vector through an ML model before indexing and searching in OpenSearch. In OpenSearch 2.4, you can use the Neural Search plugin (experimental), which allows you to upload your ML models, vectorize documents and queries, and search documents using k-NN. With the Neural Search plugin, you don‚Äôt need to vectorize documents yourself when indexing and searching. The following figure shows an overview of the Neural Search plugin. Note that Amazon OpenSearch Service, the AWS managed OpenSearch service, currently only supports versions up to 2.3, which don‚Äôt support the model-serving framework or the Neural Search plugin.\nExamples of MLT and k-NN in OpenSearch\nIn the following examples, we use OpenSearch 2.4 to demonstrate a similar document search using both an MLT query and k-NN through the Neural Search plugin, and we compare the search results.\nBuilding an OpenSearch environment\nOpenSearch has several installation options, such as Docker, Tarball, and RPM. We use Docker for this demonstration.\nBecause this is just a demonstration, prepare a docker-compose.yml that uses a single node and disables security plugins: version: \" 3\" services: opensearch-node: image: opensearchproject/opensearch:latest container_name: opensearch-node environment: - discovery.type=single-node - \" DISABLE_INSTALL_DEMO_CONFIG=true\" - \" DISABLE_SECURITY_PLUGIN=true\" ulimits: memlock: soft: -1 hard: -1 ports: - 9200:9200 opensearch-dashboards: image: opensearchproject/opensearch-dashboards:latest container_name: opensearch-dashboards ports: - 5601:5601 expose: - \" 5601\" environment: - \" DISABLE_SECURITY_DASHBOARDS_PLUGIN=true\" - \" OPENSEARCH_HOSTS=http://opensearch-node:9200\" Next, navigate to the directory of your docker-compose.yml and enter the following command to run OpenSearch and OpenSearch Dashboards: docker compose up -d Go to http://localhost:5601. You should be able to access OpenSearch Dashboards as shown in the following figure. Dataset\nIn this example, we use The Multilingual Amazon Reviews Corpus dataset, an open-source dataset of Amazon review data. The following is a sample of the corpus, and we search for similar documents in the ‚Äúreview_body‚Äù field: { \"review_id\": \"en_0802237\", \"product_id\": \"product_en_0417539\", \"reviewer_id\": \"reviewer_en_0649304\", \"stars\": \"3\", \"review_body\": \"I love this product so much i bought it twice! But their customer service is TERRIBLE. I received the second glassware broken and did not receive a response for one week and STILL have not heard from anyone to receive my refund. I received it on time, but am not happy at the moment.\", \"review_title\": \"Would recommend this product, but not the seller if something goes wrong.\", \"language\": \"en\", \"product_category\": \"kitchen\" } Search with MLT\nYou can enter a search using an MLT query with either Dev Tools in OpenSearch Dashboards or any OpenSearch client.\nTo start, create an index named amazon-review-index. MLT uses the statistics of the analyzed text, and you can speed up the search by storing term statistics at index time. Therefore, we set ‚Äúterm_vector‚Äù: ‚Äúyes‚Äù for the review_body field: PUT /amazon-review-index\n{\n\"mappings\": {\n\"properties\": {\n\"review_id\": { \"type\": \"keyword\" },\n\"product_id\": { \"type\": \"keyword\" },\n\"reviewer_id\": { \"type\": \"keyword\" },\n\"stars\": { \"type\": \"integer\" },\n\"review_body\": { \"type\": \"text\", \"term_vector\": \"yes\" },\n\"review_title\": { \"type\": \"text\" },\n\"language\": { \"type\": \"keyword\" },\n\"product_category\": { \"type\": \"keyword\" }\n}\n}\n} After creating the index, we load the review data into OpenSearch. There are various ways to load data into OpenSearch, such as curl, OpenSearch language clients, or a logging tool like Data Prepper. In this example, we provide code for uploading data using opensearch-py, a Python client for OpenSearch: import json from opensearchpy import OpenSearch def payload_constructor ( data): payload_string = '' for datum in data: action = { 'index': { '_id': datum [ 'review_id']}} action_string = json. dumps ( action) + ' \\n ' payload_string += action_string this_line = json. dumps ( datum) + ' \\n ' payload_string += this_line return payload_string index_name = 'amazon-review-index' batch_size = 1000 client = OpenSearch ( hosts = [{ 'host': 'localhost', 'port': 9200 }], http_compress = True,) with open ( '../json/train/dataset_en_train.json') as f: lines = f. readlines () for start in range ( 0, len ( lines), batch_size): data = [] for line in lines [ start: start + batch_size]: data. append ( json. loads ( line)) response = client. bulk ( body = payload_constructor ( data), index = index_name) After loading data, we can now perform a search with an MLT query.\nThere are two ways to provide a query text:\nSpecify the text in the MLT query. However, search by text can increase latency since the search must perform analysis on the text.\nSpecify an ID of the document that already exists in your index. This method is faster if you want to find documents similar to your indexed documents.\nIn the following query, we use the review data listed in the dataset section. Because sentences in this example dataset are not long, there is a possibility min_term_freq default value 2 will exclude important terms. So, we set min_term_freq to 1, the minimum term frequency for which the input document ignores. GET amazon-review-index/_search?size=5\n{\n\"query\": {\n\"more_like_this\": {\n\"fields\": [\"review_body\"],\n\"like\": {\n\"_id\": \"en_0802237\"\n},\n\"min_term_freq\": 1\n}\n},\n\"fields\": [\"review_body\", \"stars\"],\n\"_source\": false\n} Upon query completion, we receive the following results. The query text contains, ‚ÄúI love this product so much i bought it twice! But their customer service is TERRIBLE. I received the second glassware broken and did not receive a response for one week and STILL have not heard from anyone to receive my refund. I received it on time, but am not happy at the moment.‚Äù As you can tell from the text, a dissatisfied customer left this review. Similar document search returns results that are similar to the text query, including words such as ‚Äúresponse‚Äù, ‚Äúcustomer‚Äù, ‚Äúservice‚Äù, and ‚Äúterrible‚Äù: { \"took\": 53, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 779, \"relation\": \"eq\" }, \"max_score\": 35.644646, \"hits\": [ { \"_index\": \"amazon-review-index\", \"_id\": \"en_0398542\", \"_score\": 35.644646, \"fields\": { \"stars\": [ 1], \"review_body\": [ \"Used it twice and the plastic clip that holds the strap snapped off. Useless now. Haven‚Äôt received a response from customer service. 5 months later, still no response. Terrible.\"] } }, { \"_index\": \"amazon-review-index\", \"_id\": \"en_0157395\", \"_score\": 27.64468, \"fields\": { \"stars\": [ 1], \"review_body\": [ \"Lost pressure in first month. Never received any response from their customer service. Total waste of money and time. Bought a different brand that works great, Do not buy this product.\"] } }, { \"_index\": \"amazon-review-index\", \"_id\": \"en_0439049\", \"_score\": 26.461647, \"fields\": { \"stars\": [ 1], \"review_body\": [ \"The product arrived damage, opened with a damaged soaking box. The seller was contacted and there was no response and no refund offered. The customer service is terrible. I do not recommend this seller.\"] } }, { \"_index\": \"amazon-review-index\", \"_id\": \"en_0021763\", \"_score\": 26.441845, \"fields\": { \"stars\": [ 1], \"review_body\": [ \"DO NOT ORDER FROM THEM!! I placed my order on February 12th and STILL have not received my item. To make matters worse I emailed the seller with my issue on March 10th and haven't so much as even gotten a response back. Terrible customer service and they just TOOK my money.\"] } }, { \"_index\": \"amazon-review-index\", \"_id\": \"en_0582638\", \"_score\": 26.065836, \"fields\": { \"stars\": [ 1], \"review_body\": [ \"Never received it! Was supposed to receive March 6. Now it‚Äôs the end of March. I‚Äôve tried to contact the company twice with no response. Poor customer service! Don‚Äôt buy! Never received item and never got my money back! If I could give zero stars I would!!!! Amazon please intervene!\"] } }] } } Search with k-NN\nIn this example, we demonstrate similar document search with k-NN using the Neural Search plugin, following the model-serving framework and neural-search plugin documentation. For the feature extraction ML model, we use the sentence-transformer model from Hugging Face.\nFirst, upload the ML model: POST /_plugins/_ml/models/_upload\n{\n\"name\": \"all-MiniLM-L6-v2\",\n\"version\": \"1.0.0\",\n\"description\": \"test model\",\n\"model_format\": \"TORCH_SCRIPT\",\n\"model_config\": {\n\"model_type\": \"bert\",\n\"embedding_dimension\": 384,\n\"framework_type\": \"sentence_transformers\"\n},\n\"url\": \"https://github.com/opensearch-project/ml-commons/raw/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip?raw=true\"\n} OpenSearch returns the following response after the upload: { \"task_id\": \"NHBlGYUBej1j0hjelDel\", \"status\": \"CREATED\" } To check the status of the model upload, run the following API. Pass the task_id after /_plugins/_ml/tasks/ from the previous response: GET /_plugins/_ml/tasks/&lt;task_id&gt; If the state is COMPLETED, as shown in the following response, the model upload is complete: { \"model_id\": \"NXBlGYUBej1j0hjelTc0\", \"task_type\": \"UPLOAD_MODEL\", \"function_name\": \"TEXT_EMBEDDING\", \"state\": \"COMPLETED\", \"worker_node\": \"hGSG_GzpSGePCkmCmY3cvg\", \"create_time\": 1671168365569, \"last_update_time\": 1671168376567, \"is_async\": true } Next, load the uploaded model in to memory. Pass the model_id from the previous response: POST /_plugins/_ml/models/&lt;model_id&gt;/_load Using the task_id in the load API response, run the _ml/tasks API. If the state is COMPLETED, the model has been successfully uploaded: { \"model_id\": \"NXBlGYUBej1j0hjelTc0\", \"task_type\": \"LOAD_MODEL\", \"function_name\": \"TEXT_EMBEDDING\", \"state\": \"COMPLETED\", \"worker_node\": \"hGSG_GzpSGePCkmCmY3cvg\", \"create_time\": 1671238820338, \"last_update_time\": 1671238820447, \"is_async\": true } Next, create a data ingestion pipeline using the uploaded model.\nFor model_id, specify the model ID from the previous response.\nFor field_map, enter the text field you want to embed in your chosen vector field. In this example, we map the text field named review_body to the vector field named review_embedding: PUT _ingest/pipeline/nlp-pipeline\n{\n\"description\": \"An example neural search pipeline\",\n\"processors\": [\n{\n\"text_embedding\": {\n\"model_id\": \"&lt;model_id&gt;\",\n\"field_map\": {\n\"review_body\": \"review_embedding\"\n}\n}\n}]\n} After creating the pipeline, create an index named amazon-review-index-nlp. For default_pipeline, enter the name of the pipeline you just created, and create a field named review_embedding and set the information about k-NN as follows. PUT /amazon-review-index-nlp\n{\n\"settings\": {\n\"index.knn\": true,\n\"default_pipeline\": \"nlp-pipeline\"\n},\n\"mappings\": {\n\"properties\": {\n\"review_embedding\": {\n\"type\": \"knn_vector\",\n\"dimension\": 384,\n\"method\": {\n\"name\": \"hnsw\",\n\"space_type\": \"l2\",\n\"engine\": \"nmslib\",\n\"parameters\": {\n\"ef_construction\": 128,\n\"m\": 24\n}\n}\n},\n\"review_id\": { \"type\": \"keyword\" },\n\"product_id\": { \"type\": \"keyword\" },\n\"reviewer_id\": { \"type\": \"keyword\" },\n\"stars\": { \"type\": \"integer\" },\n\"review_body\": { \"type\": \"text\" },\n\"review_title\": { \"type\": \"text\" },\n\"language\": { \"type\": \"keyword\" },\n\"product_category\": { \"type\": \"keyword\" }\n}\n}\n} After creating the index, we load data into the index. Data can load in the same way as in an MLT search. However, while the k-NN search contains a new field named review_embedding, you do not have to pass data to this field because the ML pipeline automatically loads data.\nAfter uploading the data, you can search with k-NN. We use the same text as in the MLT example: GET amazon-review-index-nlp/_search?size=5\n{\n\"query\": {\n\"neural\": {\n\"review_embedding\": {\n\"query_text\": \"I love this product so much i bought it twice! But their customer service is TERRIBLE. I received the second glassware broken and did not receive a response for one week and STILL have not heard from anyone to receive my refund. I received it on time, but am not happy at the moment., review_title: Would recommend this product, but not the seller if something goes wrong.\",\n\"model_id\": &lt;model_id&gt;,\n\"k\": 10\n}\n}\n},\n\"fields\": [\"review_body\", \"stars\"],\n\"_source\": false\n} The search results are as follows. Again, the top results are reviews that express dissatisfaction with customer service and delivery, but, unlike MLT search, the results do not include words that are common to the query. For example, the query includes the word ‚Äúglassware‚Äù, but the MLT search did not return any words related to ‚Äúglassware‚Äù. However, the k-NN search includes the word ‚Äúglass‚Äù in the response. We can also see that words such as ‚Äúship‚Äù, which were not in the query, are also in the response.\nThis is just one example, but it demonstrates how a k-NN search will return results that better understand the meaning of the input text: { \"took\": 76, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 40, \"relation\": \"eq\" }, \"max_score\": 0.61881816, \"hits\": [ { \"_index\": \"amazon-review-index-nlp\", \"_id\": \"n3B3GYUBej1j0hjek1ug\", \"_score\": 0.61881816, \"fields\": { \"stars\": [ 1], \"review_body\": [ \"\"\"I guess I should have trusted the other reviews, as my arrived and was broken in its box. So now I need to return it for a refund? And ship broken glass..which I'm not really comfortable with üò´\"\"\"] } }, { \"_index\": \"amazon-review-index-nlp\", \"_id\": \"aXB3GYUBej1j0hjeV1N0\", \"_score\": 0.60459375, \"fields\": { \"stars\": [ 1], \"review_body\": [ \"Bought this almost a week ago and it broke on me. Definitely don‚Äôt recommend getting this product. We also contacted the buyer and heard nothing back.\"] } }, { \"_index\": \"amazon-review-index-nlp\", \"_id\": \"e3B3GYUBej1j0hjeGE7v\", \"_score\": 0.5942412, \"fields\": { \"stars\": [ 1], \"review_body\": [ \"Very very very horrible customer service and product was never seen. Ordered for a christmas gift, then found out they had 8 week shipping, no sooner. When I emailed them, they responded with a very unapologetic response or solution. So yeah I never recieved this product and I would advise no one to order from them!\"] } }, { \"_index\": \"amazon-review-index-nlp\", \"_id\": \"u3B2GYUBej1j0hjetkCU\", \"_score\": 0.5831761, \"fields\": { \"stars\": [ 1], \"review_body\": [ \"Items arrived completely smashed in the box. It was full of broken glass and amazon would not give me a refund without sending the broken pieces back. I did not feel comfortable mailing a broken box full of glass and said no, so will receive no refund.\"] } }, { \"_index\": \"amazon-review-index-nlp\", \"_id\": \"SHB3GYUBej1j0hje22Xg\", \"_score\": 0.58303237, \"fields\": { \"stars\": [ 1], \"review_body\": [ \"I bought this for a gift and when opened, the glass was broken and no usable. Very disappointed and embarrassed when my friend opened it up. this was not cheap either, so it should have been packaged better.\"] } }] } } Wrapping it up\nIn this blog post, we introduced and compared two methods of similar document search: MLT and k-NN. Note that the results will vary depending on factors such as parameter tuning, synonyms, and custom dictionary settings. In order to get more suitable search results, it is also possible to combine a match query with k-NN search or MLT with k-NN search, as in the example request for neural search.\nIf you have any feedback, feel free to comment on [Feedback] Neural Search plugin or [Feedback] Machine Learning Model Serving Framework.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/aws-kendra-intelligent-ranking/",
    "title": "Partner Highlight: AWS releases Kendra Intelligent Ranking for self-managed OpenSearch",
    "content": "OpenSearch partner and contributor AWS recently released Kendra Intelligent Ranking, a new plugin for self-managed OpenSearch deployments that lets users tap into Amazon Kendra‚Äôs semantic search capabilities to increase the relevance of search results. With the new plugin, users of open-source OpenSearch version 2.4.0 or later can semantically rerank their search results using Amazon Kendra, which uses machine learning to infer the meaning behind a search query, and compare those results to the default OpenSearch ranker to assess improvements. To learn more, check out the AWS What‚Äôs New announcement or find out how to get started in this post on the AWS Machine Learning blog.",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Introducing-Identity/",
    "title": "Introducing identity and access control for OpenSearch",
    "content": "The existing OpenSearch 1 access control features included in the Security plugin let administrators apply access control to indexes and cluster actions so that users have the right permissions 2 and the cluster is protected from unwanted activity. However, the current access control features do have certain limitations that can make it difficult to use them with other plugins.\nAs the core OpenSearch Project begins its shift away from a plugin 3 model to a platform model that utilizes extensions 4, those extensions, its legacy plugins, and the administrators who manage them will need mechanisms for controlling access that are more granular and able to manage a broader range of scenarios where effective access control is critical. We are developing a new suite of features that are designed to provide comprehensive identity and access control to the OpenSearch ecosystem.\nThe main objectives are to:\nProvide mechanisms for OpenSearch and its plugins and extensions that can check permissions before attempting an action.\nRestrict plugins and extensions from performing actions unless they have been granted access.\nIntroduce functionality that allows background tasks to run with the same access controls as interactive user requests.\nAdd new security boundaries inside OpenSearch that create conditions for a better defense-in-depth posture.\nDetailed objectives\nLet‚Äôs look at some of the ways we plan to meet these objectives.\nPermission checks\nTo work effectively, application developers need to know what users can and cannot do in OpenSearch based on the permissions assigned to them. Likewise, administrators responsible for setting up users and the permissions assigned to users require a reliable way to make sure the permissions are configured correctly. Given the degree of complexity that these configurations can reach in large systems, there needs to be a way to verify which users have what permissions. At this time, a mechanism that would allow administrators and developers to check these mappings doesn‚Äôt exist.\nOur approach to closing this gap is to provide a robust set of APIs that will allow these checks on permissions. Furthermore, these APIs should have the capacity to work not only in core OpenSearch but across all plugins and extensions as well.\nPlugin and extension restrictions\nJust as downloading an app to your smartphone includes restrictions intended to prevent surprises and risk, downloading a plugin to OpenSearch should be just as safe and predictable a process. To create this kind of experience for all actions involving plugins and extensions, we intend to introduce certain restrictions that will allow administrators to handle tasks confidently and avoid having to second-guess whether an action might involve any risk.\nTo ensure that these kinds of limitations are enforced, we plan to leverage the same structures used to grant users access and map permissions for all plugin and extension management activities. As a result, we will reduce the complexity and risk involved in managing these tasks.\nBackground task permissions\nThere are many tasks that run in the background of an OpenSearch cluster, such as deletion of old indexes and generation of monitoring data and reports. When these tasks run, each should have only the minimum number of permissions needed to protect against task errors and any unintended results and their consequences.\nAssociating identity to tasks will ensure that permissions applied to these tasks are well defined and effective at preventing errors and unexpected results.\nSecurity isolation\nFollowing the principle of least privilege, using a minimum number of permissions to run tasks is key to preventing execution errors or software flaws from impacting the stability of an OpenSearch cluster. We will launch features that can protect all of the separate elements in OpenSearch by isolating the potential impact a problem in one element may have on another element. This will reduce the spread of errors and keep them from affecting other areas of OpenSearch.\nGet involved\nThese features present a significant departure from the current security model by making it easier for OpenSearch developers to access and build assets and for administrators to carry out tasks in a stable environment. Over the last several months, we‚Äôve been busy defining new features and building tools to prepare for the integration of identity and access control into core OpenSearch. We‚Äôve struck out on this path in an effort to make OpenSearch security features more efficient, more reliable, and simply easier and more enjoyable to work with.\nAnd since we operate in the open-source community, we‚Äôd like to learn about your ideas and benefit from your contributions as we make progress.\nWatch for further blog posts on specific identity and access control features, and join us for community meetings. Furthermore, you can stay informed of development by visiting the following resources in the OpenSearch repository: OpenSearch events Support for native authentication and authorization in OpenSearch Feature branch for Identity Current issues for Identity If you have any feedback, feel free to comment on [Feedback] Identity and access control for OpenSearch.\nWe look forward to your participation!\nTerms in this blog post\nOpenSearch ‚Äì A community-driven, open-source (Apache 2.0‚Äìlicensed) search engine.¬† &#8617; Permissions ‚Äì Rights and privileges granted to a user that allow the user to perform specified tasks within a software application or platform. Permissions are typically managed by administrators responsible for overseeing who can access different parts of the platform.¬† &#8617; Plugin ‚Äì A piece of software designed to customize a larger software application. Plugins do not modify the core functionality of the larger application; they only add to the core functionality.¬† &#8617; Extension ‚Äì A software program designed to enhance and extend the functionality of a larger software application. Extensions involve some integration with the core of the larger application to meet the aims of the extra functionality they introduce.¬† &#8617;",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-2-5-is-live/",
    "title": "OpenSearch 2.5 is live!",
    "content": "OpenSearch 2.5.0 is ready for download! This year‚Äôs first release focuses on investments in core functionality, including new and improved ways to administer your clusters as well as tools for accessing, managing, and visualizing your data. Many of these features are building blocks, introducing functionality that the project can continue to expand upon throughout 2023 and beyond. Also included are updates to existing tools, such as the ability to analyze traces using data stored in the Jaeger schema and the general availability of Security Analytics. As you put this release to work, we hope you‚Äôll share your feedback and help shape what these features can offer the community. As always, you can explore OpenSearch Dashboards without downloading software on the Playground.\nSimplify cluster operations with index management UI enhancements\nPreviously, users relied on REST APIs or YML configurations for basic administrative operations and interventions. This release takes the first step toward a unified administration panel in OpenSearch Dashboards with the launch of several index management UI enhancements. The new interface provides a more user-friendly way to run common indexing and data stream operations. Now you can perform create, read, update, and delete (CRUD) and mapping for indexes, index templates, and aliases through the UI as well as open, close, reindex, shrink, and split indexes. The UI runs index status and data validation before submitting requests and lets you compare changes with previously saved settings before making updates. We believe this will boost usability for cluster administrators, particularly those who may be new to the tools or less familiar with REST and YML configurations. In the future, you can expect the project to add a lot more UI functionality as we move toward a unified administration panel that addresses cluster configuration, monitoring, management, and more. We appreciate your feedback on areas to prioritize as the UI is expanded.\nAnalyze Jaeger trace data with OpenSearch Dashboards\nNow OpenSearch Dashboards users can analyze trace data collected by the widely adopted open-source Jaeger tools. OpenSearch Dashboards Observability now allows you to analyze traces using Jaeger data stored in OpenSearch, with the option to select Data Prepper or Jaeger as the source of your trace data within the same UI. If you currently store your Jaeger trace data in OpenSearch, you can now use Trace Analytics to analyze error rates and latencies. You can also filter traces and examine the span details of a trace to pinpoint any service issues. To learn more, see Analyze Jaeger trace data in the OpenSearch documentation. Build layered maps with multiple sources\nIn previous versions of OpenSearch Dashboards, maps were limited to one data source comprising a single layer from which the map could be built. Now you can build multi-layered maps using raw data from multiple sources and interact with layered maps to gain new insights. This functionality lets you combine data from different indexes into a single visualization, add and remove layers to spot correlations, view different data at different zoom levels, and more. This, in turn, will help map builders and other analysts uncover new ways to ask and answer questions about their geospatial data using these tools. Deploy directly to Debian environments\nSome users of Debian-based Linux distributions like Ubuntu have been asking for a Debian distribution of OpenSearch. As of this release, those users can now deploy OpenSearch and OpenSearch Dashboards directly to their Debian servers with the project‚Äôs first Debian distribution. You‚Äôll find step-by-step installation instructions in the OpenSearch documentation. We would especially like to acknowledge the OpenSearch community, including project partner Graylog, for their significant contributions to building this distribution.\nDeploy remote-backed storage on a per-index basis\nThis release builds on the experimental remote-backed storage capabilities introduced in OpenSearch 2.3 with request-level durability for remote-backed indexes. This feature lets you deploy remote-backed storage on a per-index basis using Amazon Simple Storage Service (Amazon S3), Azure Blob Storage, Google Cloud Storage, or Oracle Cloud Infrastructure (OCI) Object Storage. We expect users will want to explore the increased data durability afforded by cloud-based backup and restore and welcome your feedback on how this works with your clusters. As a reminder, experimental features are recommended for use outside of production environments.\nEnhancements to existing features\nThis release also includes a number of enhancements to existing features. For a comprehensive list of updates, check out the release notes.\nIntroduced as an experimental feature in the 2.4.0 release, Security Analytics for OpenSearch and OpenSearch Dashboards is now generally available and ready for production workloads. With more than 2,000 prepackaged Sigma security rules and support for multiple log sources, including Windows, Netflow, DNS, AWS CloudTrail, and more, Security Analytics offers a range of tools to help you monitor and detect potential security threats before they can disrupt your operations.\nOpenSearch 2.4.0 introduced the model-serving framework, an experimental feature that lets users upload their own text-embedding machine learning (ML) models. OpenSearch 2.5.0 allows users to serve ML models on ML nodes that can take advantage of CUDA-compatible GPUs. This offers potential price-performance benefits and may reduce the inference latency of deep learning models that power semantic search queries.\nOpenSearch users use index rollups to compress older time-series data into summarized indexes, reducing data granularity and offering potential improvements in storage costs and performance. This release adds query string search queries to the range of query types you can use to search your rollup indexes and find the data you‚Äôre looking for.\nPreviously, OpenSearch only showed administrators a health assessment for a cluster as a whole. This release gives users the option to view the health of their cluster at the awareness attribute level when shard allocation awareness is configured.\nGetting started\nYou can download OpenSearch 2.5.0 here, and you can also explore OpenSearch Dashboards live on the Playground. To find out more about the release, see the release notes and the documentation release notes. To learn more about OpenSearch, see the OpenSearch documentation.\nWe look forward to seeing your thoughts on this release in the community forum!",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-project-2022-recap-and-whats-next/",
    "title": "OpenSearch Project 2022 recap and what's next",
    "content": "What an amazing year for the OpenSearch community! As we head into 2023, we wanted to take a look back at what we were able to accomplish in 2022 as well as a look forward at what the next year will bring.\nWhile the numbers aren‚Äôt everything, they are a big part of our story. There were over 100 million downloads of OpenSearch, OpenSearch Dashboards, and our client libraries, and there were 8,760 pull requests created by 496 contributors. This community has really come together to make some amazing things happen. Here‚Äôs what several of our partners have to say about OpenSearch:\n‚ÄúIn 2022 we saw OpenSearch establish itself as a viable, [Apache 2.0‚Äìlicensed] open-source solution for search and analytics. After proof of concepts and careful monitoring of the progress of OpenSearch, we think customers will be ready to migrate to OpenSearch for production applications in 2023 and beyond.‚Äù ‚ÄîPhil Lewis, CTO, PureInsights ‚ÄúOpenSearch‚Äôs value propositions are a win/win combination of open-source license, many new features that were previously hidden by paid subscription, and an amazing community supporting it. As experts in open-source software, Aiven is able to take the cognitive load off of our users and customers so they can focus on growing their businesses.‚Äù ‚ÄîHannu Valtonen, Chief Product Officer and Co-founder, Aiven ‚ÄúOur adoption and implementation of OpenSearch was smooth and straightforward. We quickly took advantage of new capabilities to enhance our Graylog platform.‚Äù ‚ÄîRobert Rhea, CTO, Graylog What‚Äôs next?\nIn support of our mission to be the search and analytics suite for builders, we have a lot of great things planned for 2023!\nDecoupling\nSince we started the OpenSearch Project, we have been working to decouple many of its components. The first step was to split the original repository, consisting of plugins, documentation, and build pipelines, into distinct repos. This physical separation of code moves us closer to logically separating the components.\nThe extensibility project will help to decouple plugins in three ways. First, we will provide a standard API for the new extensions that will enable them to be released independently of OpenSearch and OpenSearch Dashboards (for example, extension version 2.3.1 could work with OpenSearch 2.4.2). Second, we will have extensions load in a sandboxed environment. This will allow them to be loaded without needing to restart the OpenSearch server. Additionally, it will add a layer of security to plugins, as they will no longer share the same runtime environment as OpenSearch. Third, we will create a catalog so that plugins can be easily found and installed.\nWe are also working on decoupling OpenSearch Dashboards from OpenSearch. One part of this work will allow users to connect to multiple OpenSearch clusters even if they are running different versions of OpenSearch. The second part will allow for OpenSearch Dashboards to connect to data stores other than OpenSearch. In the 2.4 release, we enabled users to connect to Prometheus so that they can visualize metrics on their dashboards.\nPerformance and cost savings\nOne area we are focusing on in order to enable increased performance is replication. The segment replication feature will allow users to perform replication on a per-segment rather than a per-document basis. This will reduce the amount of processing needed on the replicas, as shards are just copied after processing is complete. Smaller node sizes can be used with this replication strategy. The tradeoff is that this strategy requires more network bandwidth and adds a slight delay to replication latency.\nOur second area of focus is remote data stores. We will be working to support remote data stores along with local data. In this setup, a remote store will be used for storing translogs and segments for the replica nodes. Overall, this will help to reduce storage costs, scale compute/storage separately, and increase durability by supporting restore points and continuous backup. The end goal is to allow users to spin up a cluster with just remote storage.\nIntegrations\nIn the world of search, it is common to use a secondary re-ranker to improve the relevance of a set of results. Because of this, we are working on a re-ranking plugin that will allow users to integrate natively with external re-rankers. An example of an external ranker is Metarank for personalization ranking. If you have a ranking service you would like to see integrated, suggest it on GitHub.\nAdditionally, we are working on client libraries to help with integration into many common data lakes. Apache released its Flink connector for OpenSearch with help from Andriy Redko, OpenSearch maintainer. We are also working on a connector for Hadoop for fans of MapReduce. Both of these were frequently requested by the community. If you have something you would like to see integrated into OpenSearch, propose it here.\nMore tooling\nTo make cluster management simpler, there are several tools planned for 2023. To kick things off, we are implementing several index management UI enhancements in OpenSearch Dashboards. We are providing an interface for some common cluster management operations (force merge, shard reroute, snapshot, and more), and the UI will also provide intelligent recommendations regarding sizing, security, and more. This interface can then be extended to include components like Data Prepper, providing a central location for full stack administration.\nWe are also working to simplify upgrades and migrations. This work is still in the early stages, so if you have ideas or challenges you would like to share, feel free to provide some feedback on an issue in the OpenSearch-Migrations repo.\nFor search, we have recently added a search comparison tool. This tool allows users to apply two different search strategies on the same query and see the results side by side. It‚Äôs currently a simple tool, but it has a lot of potential for search relevance use cases. Check it out here and let us know what additional functionality you would find useful!\nWrapping it up\nWe plan to spend 2023 making your experience with OpenSearch an even better one. Whether you are building an observability platform, search backend, or security analytics suite, we want OpenSearch to work for you. Decoupling components will help us to release features faster and allow Dashboards to visualize data from several data stores. With segment replication and remote data stores, you‚Äôll be able to reduce the cost of your clusters and increase your overall performance, and new integrations will allow you to build with the tools you already have deployed. If there is anything else that would improve your OpenSearch experience, let us know about it by opening a thread on our forum. We look forward to discussing your ideas and collaborating with you to make 2023 another great year for the OpenSearch community!",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/opensearch-newsletter-vol1-issue1/",
    "title": "OpenSearch Project Newsletter - Volume 1, Issue 1",
    "content": "The OpenSearch Project is pleased to present our first newsletter. Distributed monthly, the newsletter is a great way to learn more about the latest OpenSearch news, relevant content from the community, and upcoming events that you may be interested in. What‚Äôs New? OpenSearch 2.5 is live! OpenSearch 2.5.0 is ready for download! This year‚Äôs first release focuses on investments in core functionality, including new and improved ways to administer your clusters as well as tools for accessing, managing, and visualizing your data. OpenSearch Project 2022 recap and what‚Äôs next What an amazing year for the OpenSearch community! As we head into 2023, we wanted to take a look back at what we were able to accomplish in 2022 as well as a look forward at what the next year will bring. Introducing identity and access control for OpenSearch We are developing a new suite of features that are designed to provide comprehensive identity and access control to the OpenSearch ecosystem.\nBlogs from the Community Using OpenSearch in Fedora Linux This article will discuss how you can use OpenSearch in Fedora Linux. Get smarter search results with the Amazon Kendra Intelligent Ranking and OpenSearch plugin If you haven‚Äôt migrated to Amazon Kendra and would like to improve the quality of search results, you can use Amazon Kendra Intelligent Ranking for self-managed OpenSearch on your existing search solution. How to Index OpenSearch In this post, Dattell defines both components of an Index and outline how to create, add to, delete, and reindex Indices in OpenSearch. It will also touch on querying, but querying OpenSearch is covered in more depth in our article How to Query OpenSearch. OpenSearch LDAP Authentication &amp; Active Directory In this article, we will explore how to use Active Directory (AD) via Lightweight Directory Access Protocol (LDAP). How to Install OpenSearch on Debian 11 In this tutorial, you‚Äôll deploy OpenSearch - an open-source search, analytics, and visualization suite - to the Debian 11 server. This process includes downloading the OpenSearch package and installing it manually on your Debian system. You‚Äôll also set up secure SSL/TLS certificates for OpenSearch and secure the deployment with authentication and authorization.\nWebinar: How to reduce costs and operate large scale Elasticsearch &amp; OpenSearch clusters Upcoming Events FOSDEM23, February 4-5, 2023 Developer Advocate, David Tippett, will be presenting, ‚ÄúPython Logging Like Your Job Depends on It‚Äù. It is a fast track to understanding logging in Python that ends in a demo of how to log into OpenSearch with Python.\nIf you‚Äôd like to meetup at FOSDEM or chat about the event we can all use this forum thread! Hope to see everyone there. OpenSearch Community Meeting - February 14th, 2023 Join us for our online community meeting. Meeting topics and passcode are listed in the collaborative agenda on the forum. Feel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forum thread - we welcome community presentations. FOSS Backstage, March 13-14, 2023 If you are attending FOSS Backstage, reach out to community manager, Kris Freedain or our product marketing manager, James McIntyre. They would be happy to setup some time to chat about community building and open source.\nSubscribe to our MeetUp group to join the latest meetings, local user groups.\nFollow us on Twitter and LinkedIn to stay up-to-date with the latest #OpenSearch news.\nVisit our forum to ask questions and interact with the OpenSearch community.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/OpenSearch-Flink-Connector/",
    "title": "Apache Flink Connector for OpenSearch",
    "content": "Apache Flink¬Æ and OpenSearch¬Æ are widely known and successful open source projects. Even if you have never used them, it is likely you have heard or read about them in search or streaming data transformation use cases. And there are reasons for that: although they target different tech markets, they perform their job very well.\nThis post will help you understand how the Apache Flink Connector for OpenSearch bridges both projects by enabling you to push the outcome of data transformations directly to an OpenSearch index. What is Flink?\nFrom the official project website:\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Apache Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\nApache Flink started from a fork of Stratosphere‚Äôs distributed execution engine and became an Apache Incubator project in March 2014. In December 2014, Apache Flink was accepted as an Apache top-level project.\nFlink is both a framework, providing the basics to create apps in Java, Python, and Scala, and a distributed processing engine that can scale both vertically and horizontally.\nIt is used to create data pipelines over both unbounded (for example, streaming) and bounded (batch) datasets, offering various levels of abstractions, depending on the computation needs.\nData is sourced and sinked through dedicated pluggable connectors, similarly to Apache Kafka. The Apache Flink connector ecosystem is continually evolving, with new technologies being added regularly.\nAll of these principles make Apache Flink a go-to open source data processing solution for a wide variety of industries and use cases.\nWhy did Aiven choose Apache Flink over other options?\nAiven applies dogfooding: Aiven runs on Aiven, meaning that we operate on the same data infrastructure we offer to our clients. Our standard approach is to evaluate open-source solutions for our internal needs, gain experience, make them robust, and then, once we‚Äôre confident, offer them to our clients.\nWe took the same approach with Flink, which was chosen because it‚Äôs open source (Aiven loves open source, and we have a dedicated OSPO team) and because it has a large, varied, and supportive community.\nOn the technical side, Flink has all the qualifications to become the de facto standard in data processing because it:\nUnifies unbounded and bounded data pipelines, making the transition from batch to streaming a matter of just redefining data sources/sinks.\nDecouples compute from data storage, meaning that it‚Äôs possible to change the backend without needing to re-architect the transformation layer.\nScales both vertically and horizontally to accommodate large workloads.\nOffers a rich SQL interface that covers all our data transformations needs.\nOffers a variety of language SDKs at various abstraction levels.\nWorks natively with other open-source tools like PostgreSQL¬Æ, Apache Kafka¬Æ, and now OpenSearch.\nHow does Aiven use Flink?\nThe spread of Apache Flink‚Äôs SQL functions means that Flink can be used to define a vast variety of data pipelines. Apart from the traditional analytics or filtering workloads, Aiven is using or planning to use Flink for two main use cases: Streaming joins: Aiven‚Äôs data is streamed through Apache Kafka. Flink jobs help integrate data from different sources on the fly, applying lookups and checking data validation and therefore reducing the load from the data warehouse. Sessionization: Calculating ‚Äúuser sessions‚Äù in the data warehouse is very resource intensive. Flink‚Äôs watermarking feature allows us to perform sessionization with a simple SQL query.\nWhat is the best way to get started with Flink? Apache Flink‚Äôs documentation is awesome because it covers both the theory behind the tool and the tool‚Äôs practical usage in great detail.\nTo start defining the first data pipelines, it might be helpful to use the highest level of abstraction in Flink, represented by its SQL layer. You can experiment with a Docker version of Flink, which offers bare-bones capabilities, or explore a slicker experience on Aiven for Apache Flink.\nIf you‚Äôre looking for a practical example, check out how to build a real-time alerting solution with Apache Flink and a few SQL statements.\nWhat does the Apache Flink Connector for OpenSearch do?\nThe Apache Flink Connector for OpenSearch allows writing from Apache Flink into an OpenSearch index (sink side). It does not support reading from the index (source side). The connector is a recent addition to the long list of connectors supported by Apache Flink and is available starting with release 1.16.\nThere are two API flavors that the Apache Flink Connector for OpenSearch supports: the DataStream API and the Table API. The Table API is the most convenient way to start off with OpenSearch, because it relies on SQL, which is familiar to many users. Follow the official instructions to download the SQL flavor of the connector and deploy it in your Apache Flink cluster.\nThe Apache Flink Connector for OpenSearch in action\nTo set up an OpenSearch cluster, follow the OpenSearch installation instructions. The fastest way to get an OpenSearch cluster running locally is by spawning a Docker container: docker run -d \\\n-p 9200:9200 \\\n-p 9600:9600 \\\n-e \"discovery.type=single-node\" \\\nopensearchproject/opensearch:2.5.0 The latest OpenSearch version is 2.5.0; however, you can use the Apache Flink Connector for OpenSearch with any 1.x or 2.x OpenSearch version.\nPush data to OpenSearch with the Apache Flink SQL Client\nWith the OpenSearch cluster up and running, you can use Apache Flink‚Äôs SQL Client to create a table backed by OpenSearch. The following SQL statement creates an Apache Flink table definition ( myUserTable) pointing to an OpenSearch index named users: CREATE TABLE myUserTable (\nuser_id STRING,\nuser_name STRING,\nuv BIGINT,\npv BIGINT,\nPRIMARY KEY (user_id) NOT ENFORCED) WITH (\n'connector' = 'opensearch',\n'hosts' = 'https://localhost:9200',\n'username' = 'admin',\n'password' = 'admin',\n'allow-insecure' = 'true',\n'index' = 'users'); The 'connector' = 'opensearch' parameter defines the type of connector. The hosts, username, and password define the target OpenSearch endpoint and authentication credentials.\nBy default, OpenSearch distributions come with security turned on, requiring communication over HTTPS and mandatory username/password authentication. The allow-insecure connector option allows connections to clusters that use self-signed certificates.\nThe index parameter defines the target OpenSearch index.\nYou may be wondering what happens if we insert some data into the table. Let‚Äôs see.\nThe following SQL statement inserts some data into the myUserTable table: INSERT INTO myUserTable VALUES ('u1', 'admin', 100, 200) The data should appear in the target OpenSearch users index. You can use curl to search the index with the following command: curl -ki -u admin:admin https://localhost:9200/users/_search?pretty The result should contain the values listed in the preceding SQL statement in the _source field: {...,\n\"hits\": {...,\n\"hits\": [\n{\n\"_index\": \"users\",\n\"_id\": \"u1\",\n\"_score\": 1.0,\n\"_source\": {\n\"user_id\": \"u1\",\n\"user_name\": \"admin\",\n\"uv\": 100,\n\"pv\": 200\n}\n}]\n}\n} Push data to OpenSearch with the Apache Flink DataStream API\nThe Apache Flink DataStream API allows you to write Apache Flink data pipelines in Java and Scala and therefore allows the use of the Apache Flink Connector for OpenSearch.\nRefer to the OpenSearch SQL Connector instructions for information about the necessary dependencies for the build tool of your choice. The following example mimics the data push to the target OpenSearch users index in the previous SQL-based code: final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", 8081);\nfinal Collection&lt;Tuple4&lt;String, String, Long, Long&gt;&gt; users = new ArrayList&lt;&gt;();\nusers.add(Tuple4.of(\"u1\", \"admin\", 100L, 200L));\nfinal DataStream&lt;Tuple4&lt;String, String, Long, Long&gt;&gt; source = env.fromCollection(users);\nfinal OpensearchSink&lt;Tuple4&lt;String, String, Long, Long&gt;&gt; sink =\nnew OpensearchSinkBuilder&lt;Tuple4&lt;String, String, Long, Long&gt;&gt;().setHosts(new HttpHost(\"localhost\", 9200, \"https\")).setEmitter( (element, ctx, indexer) -&gt; {\nindexer.add(\nRequests.indexRequest().index(\"users\").id(element.f0).source(Map.ofEntries(\nMap.entry(\"user_id\", element.f0),\nMap.entry(\"user_name\", element.f1),\nMap.entry(\"uv\", element.f2),\nMap.entry(\"pv\", element.f3))));\n}).setConnectionUsername(\"admin\").setConnectionPassword(\"admin\").setAllowInsecure(true).setBulkFlushMaxActions(1).build();\nsource.sinkTo(sink);\nenv.execute(\"OpenSearch end to end sink test example\"); Like in the previous example, the data should appear in the OpenSearch users index.\nHow can I contribute?\nBecause the Apache Flink Connector for OpenSearch is hosted by the Apache Software Foundation, anyone can contribute to it.\nThe process is very simple:\nCreate a JIRA issue.\nCreate a pull request in the Apache Flink Connector for OpenSearch repository.\nOptionally, once the pull request is merged, ask for a connector release by following the release process.\nApache Flink has a large community, and the project is being actively developed. Even though reviewing and merging changes may take some time, it should not discourage anyone from contributing. One of the interesting new features being developed is the implementation of a new generation of sinks based on this proposal proposal. The related pull request is already open, so please feel free to check it out and contribute!\nIf you want to get involved or just stay informed of what is happening in the Apache Flink community, please consider subscribing to the mailing list or joining the official Slack channel.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/semantic-search-solutions/",
    "title": "Building a semantic search engine in OpenSearch",
    "content": "Semantic search helps search engines understand queries. Unlike traditional search, which takes into account only keywords, semantic search also considers their meaning in the search context. Thus, a semantic search engine based on a deep neural network (DNN) has the ability to answer natural language queries in a human-like manner. In this post, you will learn about semantic search and the ways you can implement it in OpenSearch. If you‚Äôre new to semantic search, continue reading to learn about its fundamental concepts. For information about building pretrained and custom semantic search solutions in OpenSearch, skip to the Semantic search solutions section.\nSemantic search vs. keyword search\nImagine a dataset of public images with captions. What should the query Wild West on such a dataset return? A human would most likely expect it to return images of cowboys, broncos, and rodeos. While these results have no textual overlap with either wild or west, they are certainly relevant. However, a keyword-based retrieval system such as BM25 will not return images of cowboys precisely because there is no text overlap. Clearly, there are aspects of relevance that cannot be measured by keyword similarity. Humans understand relevance in a far broader sense, which involves semantics, contextual awareness, and general world knowledge. Using semantic search, we would like to build systems that can do the same.\nTo compare keyword search to semantic search, consider the following image, which shows search results for Wild West produced by BM25 (left) and a DNN (right). Notice how on the left, keyword search surfaces West Virginia University and wild animal. On the right, neither caption contains the words wild or west, yet other terms in the caption form the basis for a closer match.\nSearch in embedding space\nA DNN sees everything as a vector, whether images, videos, or sentences. Any operation that a DNN performs, such as image generation, image classification, or web search, can be represented as some operation on vectors. These vectors live in a very high-dimensional space (on the order of 1,000 dimensions), and the precise position and orientation of a vector defines a vector embedding. A neural network creates, or ‚Äúlearns‚Äù, vector embeddings so that it maps similar objects close to each other and dissimilar ones farther apart. In the following image, you can see that the words Wild West and Broncos correspond to closer vectors, both of which are far apart from the vector for Basketball, as expected. In the context of web search, a neural network creates vector embeddings for every document in the database. At search time, the network creates a vector for the query and finds all the document vectors that are closest to the query vector by using an approximate nearest neighbor search, such as k-NN. Because the vectors of similar texts are mapped close to each other, a nearest neighbor search is equivalent to a search for similar documents.\nThe search quality crucially depends on the architecture and size of the neural network, because a large neural network learns more expressive embeddings. An example of such a large neural network is a transformer.\nTransformers\nA transformer is a state-of-the-art neural network that performs well on a variety of tasks. It is trained on lots of training data that includes millions of books, Wikipedia pages, and webpages. This training improves a transformer‚Äôs performance on tasks that require world knowledge and natural language understanding. For instance, the transformer learns that the term cowboy tends to appear near the term Wild West in many text documents; consequently, it maps the corresponding vectors close to each other. When such a transformer is further fine-tuned ‚Äîtrained on data that consists of (query, relevant passage) pairs‚Äîit learns to rank relevant passages higher in search results. Several such fine-tuned transformer architectures are publicly available, and you can use them off-the-shelf for web search.\nHowever, a transformer that is trained on a particular kind of data has limited performance on data domains outside of the one it was trained on‚Äîthis is a common issue with machine learning algorithms. One solution is to train a transformer on data from as many domains as possible. In principle, we can train a large enough network on vast amounts of data and expect it to ‚Äúlearn‚Äù everything. But the real problem is that more often than not, we do not have access to data from different domains. For instance, most organizations do not have public access to (query, relevant passage) pairs in the fields of medicine, finance, or e-commerce. We provide a solution to this problem using the technique of synthetic query generation.\nIn the rest of this post, you‚Äôll learn how to build a semantic search solution in OpenSearch.\nSemantic search solutions in OpenSearch\nThere are two types of models you can use to build a semantic search engine in OpenSearch: Pretrained: A ready-to-go model that you can download from a public repository Tuned: A custom model that uses synthetic query generation\nBoth options allow you to build a powerful search engine that is straightforward to implement. However, both come with tradeoffs: A pretrained model is easier to use, while a tuned model is more powerful.\nOption 1: Semantic search with a pretrained model\nA readily available pretrained model requires minimal setup and saves you time and effort of training. Follow these steps to build a pretrained solution in OpenSearch: Choose a model. We recommend the TAS-B model that is publicly available on HuggingFace. This model maps every document to a 768-dimensional vector. It has been trained on the MS Marco dataset and demonstrates impressive performance on datasets that belong to domains outside of MS Marco [ BEIR 2021]. Make sure the model is in a format suitable for high-performance environments. You can download the TAS-B model and some other popular models that are already in the correct format. If you are using another model, download the model and then follow the instructions in the Demo Notebook to trace Sentence Transformers model to convert it into a format suitable for high-performance environments, such as TorchScript or ONNX. Upload the model to an OpenSearch cluster. Use the model-serving framework to upload the model to an OpenSearch cluster, where it will create a vector index for the documents in the dataset. Search using this model. Create a k-NN search at query time with the OpenSearch Neural Search plugin. To learn how to create a k-NN search, follow the steps in the Similar document search with OpenSearch blog post.\nFor those interested in semantic search performance, we will be releasing detailed benchmarking documentation in an upcoming blog post. This documentation will include benchmarking data such as query latency, ingestion latency, and query throughput.\nOption 2: Semantic search with a tuned model\nTo build a custom solution, you ideally should have a dataset that consists of (query, relevant passage) pairs from the chosen domain in order to train a model so that it performs well on that domain. In the context of OpenSearch, it is common to have passages but not queries. The synthetic query generation technique circumvents this problem by automatically generating artificial queries. For example, given the passage on the left in the following image, the synthetic query generator automatically generates a question similar to the one shown on the right. A medium-sized transformer model such as TAS-B can then be trained on several such (query, passage) pairs. Using this technique, we trained and released a large machine learning model that can create queries based on passages. Performance We conducted several tests to measure the performance of our technique. We generated synthetic queries for nine different challenge datasets and trained the TAS-B model on the dataset that contained the generated queries. When combined with BM25, our solution provides a 15% boost in search relevancy (measured in terms of nDCG@10) compared with the search relevancy provided by BM25 alone. We found that the number of synthetic queries generated per passage drastically affects search performance, with more queries leading to better performance. Additionally, we found that the size of the synthetic query generator model also affects downstream performance: Larger models lead to better synthetic queries, which in turn lead to better TAS-B models. Try it To try the synthetic query generator model, follow the end-to-end guide in the Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch. After you run this notebook, it will create a custom TAS-B model tuned to your corpus. You can then upload the model to the OpenSearch cluster and use it at query time by following the steps in the Similar document search with OpenSearch blog post.\nNext steps\nIf you have any comments or suggestions regarding semantic search, we welcome your feedback on the OpenSearch forum.\nWe‚Äôll be releasing blog posts about benchmarking studies, an end-to-end guide to setting up a custom solution, and posts about new models and better neural search algorithms in the coming months.\nReferences\nAttention is all you need, Vaswani et al. https://doi.org/10.48550/arxiv.1706.03762\nLanguage Models are Unsupervised Multitask Learners, Radford et al. https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf\nSentence-BERT: Sentence Embeddings using Siamese BERT-Networks, Reimers et al. https://www.sbert.net/index.html\nEfficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling, Hofst√§tter et al. https://doi.org/10.48550/arXiv.2104.06967\nSynthetic QA Corpora Generation with Roundtrip Consistency, Alberti et al. https://aclanthology.org/P19-1620/\nEmbedding-based Zero-shot Retrieval through Query Generation, Liang et al. https://doi.org/10.48550/arXiv.2009.10270",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/whatsnew-reporting-cli/",
    "title": "What‚Äôs new: OpenSearch Reporting CLI",
    "content": "The OpenSearch Project is happy to announce the release of the Reporting CLI, making it convenient to generate reports externally without logging in to OpenSearch Dashboards. This gives you the flexibility to programmatically generate reports and connect them to your preferred messaging system. You can now schedule and transport reports efficiently through email.\nIn this post we‚Äôll show you a few examples of how to leverage the capabilities of the Reporting CLI, such as:\nGenerating a PDF from an eCommerce dashboard that shows data visualizations and scheduling the report to run at a specific time using cron.\nIncorporating report generation into your workflow with email or notifications.\nGenerating a CSV report from tabular data in the Dashboards Discover application.\nWhat is the Reporting CLI?\nThe Reporting CLI is a package you can install separately from the Reporting plugin that allows you to specify an OpenSearch Dashboards Discover URL and generate a report in PDF or PNG format. You can download reports you generate locally or send reports programmatically to downstream messaging systems. You can also leverage the capabilities of the Reporting CLI to email CSV reports, but that requires the Reporting plugin.\nWhat can you do with the Reporting CLI?\nThe Reporting CLI can save you time by automating report generation and distribution across messaging systems and provides access to the report data through several authentication options.\nIf you want more flexibility in how you can distribute reports throughout your organization through multiple messaging systems, such as Slack or SES email systems, you can conveniently schedule and deliver those Dashboard Discover reports with the Reporting CLI.\nIf you want to use an AWS service event to trigger a report, you can configure a trigger using AWS Lambda.\nExample: Generating a PDF from a dashboard\nYou can generate an eCommerce revenue dashboard PDF report that shows all of the data visualizations and schedule the report to run at a specific day and time with the cron utility, as shown in the following image. Example: Using advanced options to add the Reporting CLI to your workflow\nThe Reporting CLI allows engineers and builders to configure and connect their generated reports to messaging systems by specifying advanced CLI options. Out of the box, the Reporting CLI will download the report locally, but the following options will allow you to send the report to someone: Email ‚Äì Send the report to stakeholders using Amazon Simple Email Service (Amazon SES) or Oracle Cloud Infrastructure (OCI) Email Delivery. Schedule a report using notifications ‚Äì Use Amazon Simple Notification Service (Amazon SNS) or Oracle Notification Service (ONS) to trigger a report and send a message to Slack, PagerDuty, or ServiceNow.\nExample: Generating a CSV report from tabular data\nIf you have tabular data in a Discover view, you can generate a CSV report to capture all the data and send it to stakeholders, as shown in the following image. This is the one instance that requires you to log in to OpenSearch Dashboards and use the Reporting plugin. Summary\nWe encourage you to try out the Reporting CLI and let us know what you think. How do you incorporate the Reporting CLI into your workflows? We would love to hear from you on the OpenSearch community forum. If you encounter any challenges with this feature, please let us know by creating a GitHub issue.\nTo get started with the Reporting CLI, see Creating reports with the Reporting CLI in the OpenSearch documentation.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/OpenSearch-Partner-Highlight-Searchium-text-vs-vector/",
    "title": "Partner Highlight: Using OpenSearch to set up a hybrid system that uses text and vector search",
    "content": "On their own, text databases and vector search tools can add a lot of value to commercial workloads. A recent blog post by Noam Schwartz of OpenSearch Project Partner, Searchium.ai, explains how these tools can deliver significant impact using a hybrid approach. In the blog, ‚ÄúText search vs. Vector search: Better together?,‚Äù Schwartz explores how combining text and vector search using OpenSearch and Sentence Transformers can help organizations improve their search results. Schwartz also presents an approach to reduced latency and improved operating costs by applying OpenSearch‚Äôs Associative Processing Unit (APU).",
    "keywords": [
      "partners"
    ],
    "type": "News"
  },
  {
    "url": "/blog/introducing-opensearch-2-6/",
    "title": "Introducing OpenSearch 2.6",
    "content": "OpenSearch 2.6.0 is now available, with a new data schema built to OpenTelemetry standards that unlocks an array of future capabilities for analytics and observability use cases. This release also delivers upgrades for index management, improves threat detection for security analytics workloads, and adds functionality for visualization tools, machine learning (ML) models, and more. Read on for an overview of the latest additions to the project and visit our downloads page to get started with the new distributions. You can always explore OpenSearch Dashboards on the Playground, with no need to download.\nUnlock data sources with an OpenSearch simple schema\nAs OpenSearch continues to add functionality to power analytics and observability use cases, users and community partners have identified an opportunity to take a standardized approach to accessing metrics, traces, and unstructured data, such as logs, from different sources. To help address this, OpenSearch 2.6.0 introduces Simple Schema for Observability, providing a common, unified data schema for OpenSearch.\nThe schema supports a structured definition for major analytics and observability signals, including logs, traces, and metrics, conforming to OpenTelemetry standards. In version 2.6.0, the schema will showcase, as a proof of concept, out-of-the-box observability dashboards based on integrations with community contributors. The proof of concept incorporates Calyptia‚Äôs Fluent Bit tools using NGINX log data, with dashboard content provided by WorldTechIT, offering the community a way to explore the schema, provide feedback, and suggest types of dashboards for future development. You can explore this proof of concept today on the OpenSearch Playground, or you can use this tutorial to get started.\nWith the new schema, the community can build rich functionality, such as predefined dashboards and configurations, based on common standards and formats across the data pipeline. This is a key step toward enabling a range of capabilities for ingesting, extracting, and aggregating telemetry data and driving discoveries from monitored systems. We invite the community‚Äôs feedback in this request for comments.\nSimplify cluster administration with index management tools\nOpenSearch 2.6.0 adds functionality that simplifies cluster administration. Building on the index management UI introduced in the 2.5.0 release, this update allows you to create, view, and manage data streams directly from the UI. OpenSearch admins also gain the ability to perform manual rollover operations for indexes or data streams‚Äîas well as force merge indexes or streams‚Äîfrom the UI, helping you manage and maintain your OpenSearch clusters more efficiently. Augment threat detection with Security Analytics Security Analytics tools also received upgrades for threat detection in this release. When you create threat detectors in OpenSearch 2.6.0, you can now use multiple indexes or index patterns to build the detector, rather than just a single source. Five new log types are available for threat detection, bringing the total to 13. These new log types include Google Workspace logs, GitHub actions, Microsoft 365 logs, Okta events, and Microsoft Azure logs. In addition, many detector types now include out-of-the-box dashboards designed to visualize the logs they are monitoring. With one click, you can now view surrounding documents from the time at which a security finding was generated. This can help you identify broader patterns in your security logs.\nView location and status of ML models\nVersion 2.6.0 includes a new ML model health dashboard as an experimental feature, allowing you to view the location and status of ML models within a cluster. Look for further development of the ML Commons UI in future releases to help simplify administration of semantic search deployments and other ML workloads.\nAdd maps to OpenSearch Dashboards\nThe project continues to deliver enhancements to help users input and visualize geographic data. With this release, you have the ability to add maps to dashboard panels within OpenSearch Dashboards. Previously, maps could only be created and displayed inside the Maps plugin; now you can access maps for visualization and analysis without leaving the Dashboards environment. Generate reports directly from dashboards\nOpenSearch 2.6.0 supports the OpenSearch Reporting CLI, launched this month separately from the project‚Äôs release cycle. The new CLI offers an out-of-the-box way to generate and download reports directly from OpenSearch Dashboards programmatically. Now you can use the Reporting CLI to create reports in PDF, PNG, or CSV format and distribute them as a file to downstream messaging systems. Check out the documentation to get started.\nAutomatically protect against traffic surges\nOpenSearch uses search backpressure to identify resource-intensive search requests and cancel them when traffic to the node exceeds resource limits. With this release, OpenSearch can now cancel queries at the coordinator node level and therefore provide more efficient protection against traffic surges that result from a small number of resource-intensive queries.\nAuthenticate data sources with SigV4\nWhile the ability to add multiple data sources from OpenSearch Dashboards remains an experimental feature in 2.6.0, this release adds support for AWS Signature Version 4 (AWS SigV4) as a request authentication method for connecting to data source domains, such as Amazon OpenSearch Service domains with AWS Identity and Access Management (IAM) authentication enabled.\nGetting started\nYou can download the latest version of OpenSearch here, and you can check out OpenSearch Dashboards live on the Playground. For more information about this release, see the release notes and the documentation release notes. We welcome your feedback on this release in the community forum!",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Announcing-Data-Prepper-2.1.0/",
    "title": "Announcing Data Prepper 2.1.0",
    "content": "Data Prepper 2.1.0 is now available for download! This release\nadds several new features to Data Prepper. Additionally, the maintainers have improved Data Prepper‚Äôs\nstability and performance. Many of the new features\ncame from community contributions in the form of GitHub issues and pull requests.\nGenerated metrics\nBy creating metrics for logs and traces that pass through Data Prepper, the performance and scalability of OpenSearch\n(data store, search engine, and visualization) can be significantly enhanced. The ability to generate metrics enables the\nsummarization of events over a specific time period, making it easier to visualize and set alerts, as opposed to utilizing\nraw data for visualization and alerting.\nData Prepper 2.1 supports generating OpenTelemetry-format metrics from all incoming events. The Data Prepper aggregate\nprocessor has two new actions that support the generation of metrics for logs and traces:\nThe histogram action generates a histogram of the data field being aggregated. The histogram data includes total\ncount, min, max, and sum, in addition to the histogram buckets and bucket-level counts.\nThe count action generates a count of the data being aggregated.\nYou can combine both of these actions with the aggregate processor‚Äôs conditional aggregation option in order to generate more meaningful metrics.\nFor example, you can obtain the number of traces with an error ( status_code equal to 2) using the count aggregate action along\nwith the aggregate_when option. You can also generate a histogram of trace latencies using another aggregate processor in\na parallel sub-pipeline.\nThe pipeline configuration for the preceding example is similar to the following: trace-error-metrics-pipeline:\nsource:\npipeline:\nname: \"span-pipeline\"\nprocessor:\n- aggregate:\nidentification_keys: [\"serviceName\", \"traceId\"]\naction:\ncount:\ngroup_duration: \"20s\"\naggregate_when: \"/status_code == 2\"\nsink:\n- opensearch\nhosts: [\"https://opensearch:9200\"]\ninsecure: true\nusername: \"admin\"\npassword: \"admin\"\nindex: trace-error-metrics\ntrace-high-latency-metrics-pipeline:\nsource:\npipeline:\nname: \"span-pipeline\"\nprocessor:\n- aggregate:\nidentification_keys: [\"serviceName\", \"traceId\"]\naction:\nhistogram:\nkey: \"durationInNaos\"\nrecord_minmax: true\nunits: \"nanoseconds\"\nbuckets: [1000000000, 1500000000, 2000000000]\ngroup_duration: \"20s\"\naggregate_when: \"/durationInNanos &gt; 1000000000\"\nsink:\n- opensearch\nhosts: [\"https://opensearch:9200\"]\ninsecure: true\nusername: \"admin\"\npassword: \"admin\"\nindex: trace-high-latency-metrics Anomaly detection\nData Prepper 2.1 introduces anomaly detection as an independent processor that can be placed anywhere in the pipeline to\ndetect anomalies using artificial intelligence/machine learning (AI/ML) techniques. For now, the processor only supports the random cut forest algorithm for detecting anomalies.\nSupport for other AI/ML algorithms may be added in the future. The processor can be configured to use any numerical (integer\nor floating-point data type) field in the input events to detect anomalies.\nAnomaly detection in Data Prepper provides a more scalable and efficient way to detect anomalies in logs and traces\nbecause the ML algorithm is applied on the server side.\nFor example, consider an aggregate processor that generates a histogram of trace latencies ( durationInNanos). If you place an anomaly detector processor after the aggregate processor, it can identify abnormally high latencies.\nThe following is an example of such a configuration: trace-metric-anomaly-detector-pipeline:\nsource:\npipeline:\nname: \"trace-high-latency-metrics-pipeline\"\nprocessor:\n- anomaly_detector:\nkeys: [\"max\"]\nmode:\nrandom_cut_forest:\nsink:\n- opensearch\nhosts: [\"https://opensearch:9200\"]\ninsecure: true\nusername: \"admin\"\npassword: \"admin\"\nindex: high-latency-traces-with-anomalies Sampling and rate limiting\nData Prepper 2.1 supports sampling and rate limiting to limit the number of events that are sent to a sink.\nThese features can be used to reduce the load on the OpenSearch cluster when storing ordinary logs and metrics.\nFor example, you can limit the number of success HTTP logs or repetitive requests, such as from a health check.\nBoth sampling and rate limiting are available as configurable actions in the aggregate processor.\nTo use sampling, specify the sampling percentage\nTo use rate limiting, set the events per second option\nFor example, the following percent sampling configuration sends 60% of events to OpenSearch: trace-normal-pipeline:\nsource:\npipeline:\nname: \"span-pipeline\"\nprocessor:\n- aggregate:\nidentification_keys: [\"serviceName\"]\naction:\npercent_sampler:\npercent: 60\ngroup_duration: \"30s\"\naggregate_when: \"/status_code!= 2 and /durationInNanos &lt;= 1000000000\"\nsink:\n- opensearch\nhosts: [\"https://opensearch:9200\"]\ninsecure: true\nusername: \"admin\"\npassword: \"admin\"\nindex: sampled-traces You can configure a rate-limiting action to limit the number of events by specifying the number of events per second.\nYou can also use the when_exceeds option to specify what should happen when the number of events exceeds the specified limit.\nThe when_exceeds option can take the following values: block: Blocks the current pipeline thread until events are allowed (this is the default value) drop: Drop any excess events\nSetting when_exceeds to block is useful when there is a temporary burst in the number of events and you don‚Äôt want to lose any of them.\nThe following rate-limiting configuration sends 10 events per second to OpenSearch and uses the drop option: trace-normal-pipeline:\nsource:\npipeline:\nname: \"span-pipeline\"\nprocessor:\n- aggregate:\nidentification_keys: [\"serviceName\"]\naction:\nrate_limiter:\nevents_per_second: 10\nwhen_exceeds: \"drop\"\ngroup_duration: \"30s\"\naggregate_when: \"/status_code!= 2 and /durationInNanos &lt;= 1000000000\"\nsink:\n- opensearch\nhosts: [\"https://opensearch:9200\"]\ninsecure: true\nusername: \"admin\"\npassword: \"admin\"\nindex: sampled-traces OpenTelemetry logs\nOne of Data Prepper‚Äôs goals is to support open standards. Data Prepper now supports the OpenTelemetry log format. Previously, Data Prepper supported log data through the HTTP Source plugin,\nwhich works well with tools such as Fluent Bit. If you would like to deploy the OpenTelemetry Collector on your applications\nand not require any other sidecars, you can now send logs to Data Prepper, which will route them to OpenSearch.\nOpenSearch sink improvements\nRunning Data Prepper as an ingestion pipeline in front of OpenSearch is an important capability. This\nrelease adds new enhancements to Data Prepper‚Äôs OpenSearch sink to make it even more useful.\nData Prepper now supports routing traces and logs to different indexes dynamically. To achieve this, you can define an\nindex name using a format string, which can include properties from different events. With this capability, Data Prepper\ncan support an arbitrary number of indexes using a single sink. For example, if you have logs with different\napplication identifiers, you can route events to an index specific to each application.\nData Prepper also now allows you to route documents to specific OpenSearch shards using the existing OpenSearch routing parameter.\nThe new routing_field property on the OpenSearch sink will use properties from events to specify how to\nroute them within OpenSearch. Most users prefer allowing OpenSearch to generate document IDs and choosing the shard\nfrom that. However, Data Prepper now allows you to specify these values explicitly.\nGoing hand in hand with the routing field, Data Prepper now lets you specify a complex field to\nuse when specifying a document ID. Previously, you could only copy a value to a field in the root of\nthe event in order to use it as a document ID.\nType conversion\nData Prepper now provides a new convert entry as part of its collection of mutate processors.\nThis feature can convert a value from one type to another. This can be particularly useful in conditional expressions when using conditional routing.\nFor example, you can use the following processor in a pipeline to convert the value of the status key to an integer:...\nprocessor:\n- convert_entry_type:\nkey: \"status\"\ntype: \"integer\"... Other improvements\nIn addition to the new features already described, Data Prepper 2.1.0 introduces several other important improvements:\nInstead of failing, Data Prepper pipelines with OpenSearch sinks now wait for OpenSearch to start during pipeline initialization if the sink is not available.\nOpenSearch sinks now allow you to load index mapping and Index State Management (ISM) template files from an Amazon Simple Storage Service (Amazon S3) bucket.\nData Prepper expressions now support the null value keyword in conditions with equality operators.\nTo help with pipeline stability, Data Prepper now provides a circuit breaker on Java heap usage. The circuit breaker will cause the sources to stop accepting new data that could possibly cause the process to crash.\nSee the release notes for a full list of changes.\nGetting started\nYou can download Data Prepper or install a Docker container from the OpenSearch Download &amp; Get Started page. The maintainers encourage\nall users to update to Data Prepper 2.1.0 on order to gain the improved stability, additional administrative options, and new features.\nAdditionally, we have already started working on Data Prepper 2.2. See the roadmap to learn more.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/admin-panels-for-index-operations/",
    "title": "Index Management UI enhancements",
    "content": "Managing indexes, aliases, and templates in OpenSearch Dashboards can be difficult using only the API because there are restrictions and relation-binding tasks, such as adding aliases to indexes, simulating an index template by its name, and more. We are pleased to announce that these cluster operations have been significantly simplified with the Index Management UI enhancements in OpenSearch Dashboards v2.5.\nVisual editor for index mappings with nested tree editor\nBuilding index mappings in pure JSON because it has several nested layers and properties. To simplify this task, we now provide a visual editor with editable nested properties, as shown in the following image. You can add properties by choosing the operation buttons and then see what the mappings will look like by switching to the JSON editor. Simulate an index by index name\nIt is challenging to predict the appearance of the index when taking into account existing templates. To resolve this issue, OpenSearch will attempt to match the index name with the templates whenever you change the index name, as shown in the image below. OpenSearch will merge what users manually input with what the matching template contains, making it a ‚Äúwhat you see is what you get‚Äù index. Edit settings by JSON editor and diff mode\nThe new Index Management UI offers a visual editor and a JSON editor, which you can use if the visual editor does not support all available fields. Additionally, we provided the editor with a diff mode, as shown in the following image, so that you can compare the existing index with the index in the old version and see the changes, preventing errors. One click to manage index aliases\nIt is straightforward to find out what aliases an index contains through the API, but it is difficult to determine how many indexes an alias points to through the API. The Aliases page provides you with these results by grouping indexes by alias. With the Index Management UI, it will be easier to attach or detach indexes from an alias, and the alias actions will be automatically generated, eliminating the need for you to build alias actions to add or remove indexes behind an alias and manually enter the indexes, which can lead to mistakes. Simplify the reindex operation flow\nYou can select the source and destination indexes, aliases, or data streams from the dropdown menu, as shown in the following image. Moreover, there is now a destination index creation flow that allows you to import settings and mappings directly from the source. Shrink index operation\nThe Shrink index operation is used to reduce the number of primary shards in an existing index and create a new index.\nTo shrink an index, go to the Indices page, select an index, and choose the Shrink action in the Actions menu. This will take you to the shrink index page, as shown in the following image. Only one index can be shrunk at once, and the shrink operation does not support data stream indexes. If multiple indexes are selected or if a data stream backing an index is selected in the Indices page, the Shrink option is disabled.\nBefore shrinking an index, ensure that the index is not in one of the following states:\nThe index‚Äôs health status is red.\nThe index has only one primary shard.\nIf the index is in either of these states, error messages will be displayed on the Shrink index page, and the Shrink button will be disabled, as shown in the following images. The Shrink index operation requires certain prerequisites to be met. If the source index does not meet the specified conditions, error messages will be displayed on the Shrink index page.\nThe source index must block write operations, which means that you need to set the index.blocks.write setting in the source index to true. If the source index is not set to block write operations, you can choose the Block write operations button to set the index.blocks.write setting to true, as shown in the following image. The source index must also be open. If the source index is closed, the shrink operation will fail. You can choose the Open button to open the index, as shown in the following image. This may take additional time to complete, and the index will be in the red health status while opening. A copy of every shard in the source index must reside on the same node. You can move a copy of every shard to one node manually or use the shard allocation filter to do so. However, this is not required when the cluster has only one data node or if the replica count of the source index is equal to the number of data nodes minus one. In these cases, a copy of every shard in the source index resides on the same node. If a copy of every shard in the source index does not reside on the same node, you can update the index.routing.allocation.require._name setting of the source index to move shards to one node automatically.\nYou can update this setting on the source index‚Äôs detail page, as shown in the following image. Or you can submit the following request with Dev Tools: PUT test-1/_settings\n{\n\"index.routing.allocation.require._name\":\"node1\"\n} After ensuring that all prerequisites are met, you can complete the input form on the Shrink index page. Specify a name, the primary shard count, and the replica count for the new shrunken index, as shown in the following image. You can also specify new aliases or select existing aliases to attach to the new shrunken index. All of the specified settings will also be shown in the JSON editor under the Advanced settings section. You can also specify additional index settings for the new shrunken index in the JSON editor. For example, you can set both index.routing.allocation.require._name and index.blocks.write to null, as shown in the following image. This will clear the allocation requirement and the index write block, which are copied from the source index. Once you have specified all of the settings for the new shrunken index, choose the Shrink button to trigger the shrink operation. You will see a notification indicating that the shrink operation has started successfully, as shown in the following image. After the shrink operation is complete, which may take anywhere from minutes to hours, a new notification will be displayed stating that the source index has been successfully shrunk, as shown in the following image. Open and close index operations\nYou can select multiple indexes, except for the backing indexes of a data stream, to open or close.\nIf you no longer need to read or search old indexes but do not want to delete them, you can use the Close operation to close indexes. This will maintain the data by occupying a small amount of overhead on the cluster. Additionally, when you want to add a new analyzer to an existing index, you must close the index, define the analyzer, and then open the index. A closed index is blocked for read and write operations, so you must enter the word ‚Äúclose‚Äù to confirm your action, as shown in the following image. You can select multiple indexes to open, even if some indexes are already open, as shown in the following image. Split index\nWhen you choose to split an index, the status of that index is checked. If the index is not able to be split, actions that you can take to prepare the index for splitting are provided. A list of shard numbers that the index can be split into is also provided so that you do not need to calculate the shard numbers manually.\nYou can then specify the number of replicas and associate the new index with an existing index or use a new alias. You can also use the JSON editor to specify any additional index settings, as shown in the following images. Next steps\nIf you want to try the new index management features, you can experiment with them on the OpenSearch playground. We will also be releasing more Index Management UI features for data streams, metrics monitoring, and more. If you have any feedback or suggestions, leave a message on the OpenSearch forum.\nReferences OpenSearch 2.5 is live! blog post",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/NGINX-Dashboards-with-Fluent-Bit/",
    "title": "Simple NGINX dashboards with Fluent Bit and OpenSearch",
    "content": "Fluent Bit is a graduated sub-project under the Cloud Native Computing Foundation (CNCF) Fluentd project umbrella. Fluent Bit integrates with hundreds of common tools such as Kafka, Syslog, Loki, and, of course, OpenSearch.\nWhile users can currently send their logs to OpenSearch, there are no set formats or schemas for the logs, which can make sharing dashboards and alerts cumbersome. In this blog post, we talk about using Fluent Bit, a new simple schema, and OpenSearch with NGINX as a new workflow that simplifies the sharing of dashboards and alerts.\nSimple Schema for Observability\nOpenSearch 2.6 introduced a standardization for conforming to a common and unified observability schema: Simple Schema for Observability.\nObservability is a collection of plugins and applications that let you visualize data-driven events by using PPL/SQL/DQL to explore and query data stored in OpenSearch. With the schema in place, Observability tools can ingest, automatically extract, and aggregate data and create custom dashboards, making it easier to understand the system at a higher level.\nSimple Schema for Observability is based on the way that Amazon Elastic Container Service ( Amazon ECS) organizes logs and on information provided by OpenTelemetry, including metadata.\nConnecting Simple Schema for Observability and Fluent Bit\nWhen raw logs are ingested by Fluent Bit, they are automatically converted into MessagePack, a binary representation of JSON. Fluent Bit‚Äôs automatic conversion makes parsing and modifying messages in transit simpler.\nIn addition, Fluent Bit comes with out-of-the-box parsers for common applications such as NGINX, Apache Web Logs, Kubernetes, and more. These parsers give structure to a log file, though they do not have the depth that other log schemas and formats do.\nFluent Bit can perform more advanced transformations by using a filter written with a programming language called Lua. Lua filters give users extreme flexibility in how they transform their data, including modification, addition, enrichment via API, calculation, or even redaction. To connect Fluent Bit and the new Simple Schema for Observability, we opted for a Lua script that you can paste into your configuration file to run on top of NGINX-ingested logs.\nThe following is an example fluent-bit.conf file for virtual machines (VMs) or standalone deployments: [ INPUT]\nname tail read_from_head true exit_on_eof true path /tmp/data.log\nparser nginx [ Filter]\nName lua\nMatch * code function cb_filter ( a,b,c) local d ={} local e = os.date ( \"!%Y-%m-%dT%H:%M:%S.000Z\") d[ \"observerTime\"]= e; d[ \"body\"]= c.remote.. \" \"..c.host.. \" \"..c.user.. \" [\"..os.date ( \"%d/%b/%Y:%H:%M:%S %z\").. \"] \\\" \"..c.method.. \" \"..c.path.. \" HTTP/1.1 \\\" \"..c.code.. \" \"..c.size.. \" \\\" \"..c.referer.. \" \\\" \\\" \"..c.agent.. \" \\\" \" d[ \"trace_id\"]= \"102981ABCD2901\" d[ \"span_id\"]= \"abcdef1010\" d[ \"attributes\"]={} d[ \"attributes\"][ \"data_stream\"]={} d[ \"attributes\"][ \"data_stream\"][ \"dataset\"]= \"nginx.access\" d[ \"attributes\"][ \"data_stream\"][ \"namespace\"]= \"production\" d[ \"attributes\"][ \"data_stream\"][ \"type\"]= \"logs\" d[ \"event\"]={} d[ \"event\"][ \"category\"]={ \"web\" } d[ \"event\"][ \"name\"]= \"access\" d[ \"event\"][ \"domain\"]= \"nginx.access\" d[ \"event\"][ \"kind\"]= \"event\" d[ \"event\"][ \"result\"]= \"success\" d[ \"event\"][ \"type\"]={ \"access\" } d[ \"http\"]={} d[ \"http\"][ \"request\"]={} d[ \"http\"][ \"request\"][ \"method\"]= c.method; d[ \"http\"][ \"response\"]={} d[ \"http\"][ \"response\"][ \"bytes\"]= tonumber ( c.size) d[ \"http\"][ \"response\"][ \"status_code\"]= c.code; d[ \"http\"][ \"flavor\"]= \"1.1\" d[ \"http\"][ \"url\"]= c.path; d[ \"communication\"]={} d[ \"communication\"][ \"source\"]={} d[ \"communication\"][ \"source\"][ \"address\"]= \"127.0.0.1\" d[ \"communication\"][ \"source\"][ \"ip\"]= c.remote; return 1,b,d end\ncall cb_filter [ OUTPUT]\nformat json\nname stdout\nmatch * Importing the dashboard in OpenSearch\nFor examples of using an NGINX dashboard with Fluent Bit, see the following resources:\nReference the text in this readme file for example preloaded data.\nReference the text in this readme file to view an example live NGINX &gt; Fluent Bit &gt; OpenSearch workflow.\nThe following OpenSearch Playground demo uses a preloaded NGINX &gt; Fluent Bit &gt; OpenSearch Simple Schema log data stream.\nSummary and next steps\nIn this blog post, we provided an overview of the new OpenSearch Simple Schema for Observability, showed how to take advantage of it using Fluent Bit, and modified and imported an NGINX dashboard contributed by WorldTechIT to demonstrate a full agent-to-dashboard flow. To dive deeper and see how it all works for yourself, download the Docker Compose file and leave us feedback in GitHub Discussions.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/q1-survey-results/",
    "title": "OpenSearch Project Q1 community survey results",
    "content": "Thank you to the OpenSearch community members who responded to our Q1 community survey! Our first quarterly survey focused on understanding who our users are, what they use OpenSearch for, and establishing a baseline for how they feel about the current OpenSearch user experience. We are thrilled to report a net promoter score (NPS) of 57.8 and a UX health score of 77.2, with 97 community members taking part in the survey.\nWe‚Äôve recently launched a research program in order to get a holistic view of the OpenSearch user experience. The program is designed to evaluate a user‚Äôs journey comprehensively using various research methodologies, including a quarterly survey. In addition to quarterly surveys, we also use ‚Äúdeep dives,‚Äù which are qualitative research studies that include conversations with users to understand their use cases, requirements, and OpenSearch experience. The quarterly surveys will ask community members to provide feedback on new features, existing OpenSearch experiences, and more. This will help us prioritize improvements to best serve the community. After our Q2 survey is released this month, you will be able to access it at OpenSearch.org, as shown in the following image. Here are some key insights from the Q1 survey:\nWe obtained our first NPS and UX health scores and will use these as a baseline against which to compare future results.\nPreviously, we were unsure about the split between search and analytics use cases and were curious whether there was an overlap in the way the community was using OpenSearch. Around 1/3 (one-third) of the Q1 survey sample indicated that they use OpenSearch as both a search and analytics solution.\nUsers identified with four roles with four roles in OpenSearch: Infrastructure Setup ‚Äì Responsible for deploying OpenSearch, for allocation of resources, responsible for allocation of initial resources, and any action that impacts the billing and usage of resources OpenSearch Admin ‚Äì Responsible for asset and user management Dashboard Creator or Data Producer ‚Äì Responsible for preparing data or dashboards for their own use or end user consumption Dashboard or Data Consumer ‚Äì Responsible for consuming data or dashboards\n48.8%, or almost half of the community members who took the survey, self-identified as being in an administrative role, and almost half of the sample self-identified with all 4 roles.\n34.1% self-identified as being in an infrastructure setup role.\nBased on these insights, we concluded that close to half of our users have complex needs that span across all four roles, from infrastructure setup to administrative to producer to consumer. Our offerings, therefore, should focus on providing consistent end-to-end workflows and transitions that enable users to perform tasks seamlessly. This finding is also a good indication of the sample who accessed the survey on OpenSearch.org, that is, respondents were primarily in infrastructure setup or admin roles. In future quarters we intend to expand recruitment for the quarterly survey to the OpenSearch Dashboards interface as well. This will help us obtain feedback from Dashboards users, who are typically both producers and consumers.\nIn Q2 we want to gain a deeper understanding of ways to improve the OpenSearch user experience. We want to understand what blockers our users face when setting up OpenSearch for themselves or others. We also want to understand what blockers users encounter when dashboards are set up, shared, and consumed.\nCheck out OpenSearch.org or the OpenSearch forum this month and take part in the Q2 community survey! If you are interested in participating in a deep-dive study, you‚Äôll have the opportunity at the end of the survey to provide information on how we can reach you. As always, we welcome all community members to attend our biweekly community meetings, where information on ongoing research studies is shared regularly.\nWe thank you for reading and for supporting OpenSearch!",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/newsletter/",
    "title": "OpenSearch Project Newsletter - Volume 1, Issue 2",
    "content": "The OpenSearch Project is pleased to present our newsletter. Distributed monthly, the newsletter is a great way to learn more about the latest OpenSearch news, relevant content from the community, and upcoming events that you may be interested in. What‚Äôs New? OpenSearch 2.6 is live! OpenSearch 2.6.0 is now available, with a new data schema built to OpenTelemetry standards that unlocks an array of future capabilities for analytics and observability use cases. This release also delivers upgrades for index management, improves threat detection for security analytics workloads, and adds functionality for visualization tools, machine learning (ML) models, and more. OpenSearch Forum Leaderboard is now live! The OpenSearch forum now has a leaderboard! Let‚Äôs give a big thanks to our community manager, Kris Freedain! Points are awarded for engaging with the community, such as visiting, liking, and posting. Data Prepper 2.1.0 is live! Data Prepper 2.1.0 is now available for download! This release adds several new features to Data Prepper. Additionally, the maintainers have improved Data Prepper‚Äôs stability and performance. Many of the new features came from community contributions in the form of GitHub issues and pull requests.\nBlogs from the Community Text search vs. Vector search: Better together? Learn how to use OpenSearch to set up a hybrid search system so you can benefit from both text and vector search advantages. Top 8 Open-Source Observability &amp; Testing Tools Adnan Rahiƒá, Senior Developer at Tracetest.io, has compiled a list of powerful opensource tools that can help you do both observability and testing. Tracetest.io is a partner is of the OpenSearch Project. AWS: Why We Support Sustainable Open Source This article by David Nalley, Head of Open Source Strategy at AWS, explains the ‚Äúwhy‚Äù behind AWS open source contributions and support for sustainable open source software. How to handle serverful resources when using ephemeral environments When your serverless architecture relies on serverful resources such as RDS or OpenSearch, it can be a challenge to use ephemeral environments. Take a look into this issue by reading this article by Yan, AWS Serverless Hero. OpenSearch Community Q1 Survey Results. OpenSearch receives high marks in its quarterly Q1 Community Survey, reporting a net promoter score of 57.8 and a UX health score of 77.2. Read our latest blog with full survey results and insights.\nRequest For Comments Search Pipeline RFC ML Model Health Dashboard RFC Standard Search Logging Metrics and Data Reuse For Relevance Upcoming Events FOSS Backstage, March 13-14, 2023 If you are attending FOSS Backstage, reach out to community manager, Kris Freedain or our product marketing manager, James McIntyre. They would be happy to setup some time to chat about community building and open source. DevRelCon Yokohama March 10th-11th, 2023 OpenSearch Developer Advocate, David Tippett will hit the stage at DevRelCon Yokohama with his presentation, ‚Äò‚ÄúAdvocating in the Open‚Äù on March 11th. KubeCon + CloudNativeCon Europe April 18th-21st, 2023 The Cloud Native Computing Foundation‚Äôs flagship conference gathers adopters and technologists from leading open source and cloud native communities in Amsterdam, The Netherlands from 18-21 April, 2023. Please reach out to community manager, Kris Freedain, or Developer Advocate, Nate Boot for more details on how to meet up with the OpenSearch team.\nSubscribe to our MeetUp group to join the latest meetings, local user groups.\nFollow us on Twitter and LinkedIn to stay up-to-date with the latest #OpenSearch news.\nVisit our forum to ask questions and interact with the OpenSearch community.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/multilayer-maps/",
    "title": "Getting started with multilayer maps in OpenSearch",
    "content": "OpenSearch 2.5 introduced new multilayer maps for visualizing geographical data. Displaying multiple layers on a map lets you show data from different sources at different zoom levels. Let‚Äôs say you‚Äôre building a real estate application. Suppose you have one index with the number of houses available at the county level, another index with the number of houses available at the city level, and a third index with detailed information about individual houses. Initially, you may want to show the user a map with county boundaries and data. As the user zooms in, you‚Äôll show city boundaries and data. As the user zooms in even more, you‚Äôll display each house with information like the price, number of bedrooms, and so on. With the new multilayer maps, you can build various data layers from different data sources to accomplish this task.\nMultilayer maps let you:\nAdd multiple layers to visualize your data.\nView or analyze geographical data that comes from different data sources.\nView different data at different zoom levels.\nVisualize additional data with tooltips.\nContinuously refresh data for a real-time dataset.\nAdditionally, in OpenSearch 2.6, you can add multilayer maps to dashboard panels within OpenSearch Dashboards, which makes it easier to analyze your geospatial data in the context of other visualizations.\nGetting started\nTo get started, try out the new maps on the OpenSearch Playground, which is currently on version 2.6 and includes all the latest features.\nTo explore a prebuilt example map, perform the following steps:\nOn the top menu, go to OpenSearch Plugins &gt; Maps.\nSelect [Flights] Flights Status on Maps Destination Location.\nYou‚Äôll see the following example map that is based on the Sample flight data dataset. Let‚Äôs get familiar with the map‚Äôs components:\nThe search bar (1) lets you search or filter the data that is displayed.\nThe time filter (2) lets you customize the date range for displayed data.\nThe Layers panel (3) shows the layers configured for this map. The example map consists of a basemap called Default map (4) and three additional document layers: Cancelled Flights, Delayed Flights, and Flights On Time (5).\nAll about that basemap\nThe basemap ( Default map in the preceding image) serves as a canvas for the data. In this case, the basemap is the default map provided by OpenSearch (see the attribution in the lower-right corner of the map). The OpenSearch basemap is a vector tile map and therefore offers faster loading times and smoother zooming than a raster tile map. It supports zoom levels 0‚Äì22.\nYou can add more than one basemap or change the basemap to your own or a third-party map by adding a custom map layer. To add a custom basemap, in the Layers panel select Add layer and then select Custom map. For detailed instructions on configuring a custom map, see Adding a custom map.\nDressing your map in data layers\nThe map in the preceding image has three additional layers: Cancelled Flights, Delayed Flights, and Flights on Time. All three are document layers. To examine each layer, select the layer name from the Layers panel on the upper right of the map.\nFor example, if you select the Cancelled Flights layer, you‚Äôll see the layer‚Äôs configuration, as shown in the following image. From the settings panel, explore the following tabs:\nThe Data tab provides information about the data settings for the layer, for example:\nThe layer is based on the opensearch_dashboards_sample_data_flights data source.\nThe number of documents in the layer is limited to 1,000.\nThe layer displays the DestLocation field with the following filter applied: { \"query\": { \"match_phrase\": { \"Cancelled\": true } } } To apply a different filter, select Add filter and then edit the filter in the UI or select Edit as Query DSL.\nThe Style tab indicates how the layer is displayed on the map, for example:\nThe data points for the layer are displayed in red.\nThe Settings tab contains other settings, for example:\nThe layer is visible at zoom levels 4‚Äì22.\nThe layer‚Äôs opacity is 70%.\nIt‚Äôs an illusion: The disappearing dots\nYou might have one question at this point: ‚ÄúWhere are the red dots that signify cancelled flights?‚Äù\nIn fact, in the example map all the dots are green. Are no flights cancelled? The answer: You have to look closer.\nThe Cancelled Flights layer is visible only at zoom levels 4‚Äì22, so you‚Äôll have to zoom in to see the cancelled flight data points, as shown in the following image. Another question that you might have is, ‚ÄúWhy are there only three document layers, but the dots on the map come in more than three colors?‚Äù\nThe answer to this question lies in the opacity and the order of the layers.\nIf you examine the colors for all three document layers by selecting each one in turn, you will map the layers to the following colors: Cancelled Flights: Red Delayed Flights: Orange Flights On Time: Green\nHowever, all layers are set to 70% opacity so that if one data point coincides with another, you‚Äôll see a nice blend of the preceding colors.\nMoreover, try reordering the layers by dragging them by the handlebar (two horizontal lines) next to the layer names. You will see the data points change color, as shown in the following image. To view only the Cancelled Flights layer, in the Layers panel, hide the Delayed Flights and Flights On Time by selecting the crossed-out eye icon next to the layer name. Now you see only the red dots that correspond to the Cancelled Flights layer, as shown in the following image. Tooltips are appreciated\nThe red data points alone don‚Äôt convey much information about your dataset. In this case, a tooltip can be worth a thousand words. The Cancelled Flights layer has tooltips configured, so if you hover over a city, you will see the information for the first cancelled flight. For example, hovering over Washington, D.C., lets you see that it had four cancelled flights. To view the information for all cancelled flights, select the tooltip again and use the arrows in the tooltip to paginate through the flights.\nTo add, remove, or change fields that are shown in the tooltip, in the Cancelled Flights pane, select the Data tab. Then add or delete tooltip fields in Tooltip fields, as shown in the following image. If multiple layers coincide in one city, the tooltip displays the information for all layers. If you reapply the Delayed Flights and Flights On Time layers, hover over Washington, D.C., once more, and select the tooltip, you‚Äôll see that the tooltip now contains 13 flights. You can select the layer you‚Äôre interested in from the dropdown list in the tooltip. Each layer type may have different fields in the tooltip. For example, the Flights On Time layer is the only layer that displays the FlightTimeMin field. To see all layers, select All layers, as shown in the following image. A dashing dashboard map\nIn OpenSearch Dashboards version 2.6, you can not only create a map but also add it to a dashboard. The caveat? You have to start from a dashboard, either new or existing.\nTo add the example map to a dashboard, perform the following steps, as shown in the following image. On the top menu, go to OpenSearch Dashboards &gt; Dashboard.\nSelect [Flights] Global Flight Dashboard.\nIn the upper-right corner, select Edit.\nFrom the top menu, select Add.\nIn the Types dropdown list, select Maps.\nSelect [Flights] Flights Status on Maps Destination Location.\nAfter closing the Add panels flyout, you‚Äôll see the example map on the bottom of the dashboard, as shown in the following image. What‚Äôs next?\nAfter trying out the new maps on the OpenSearch Playground, make sure to download OpenSearch 2.6 locally to create maps from your dataset. For detailed information about map features, see Using maps in the OpenSearch documentation.\nIn the next OpenSearch version release, we‚Äôre planning to add the ability to draw boundaries on a map and a new type of layer ( cluster layer) that will help you aggregate data on geopoints and geoshapes. For a full list of features planned for the upcoming release, see the OpenSearch Project Roadmap. If you have any feedback or suggestions, we‚Äôd love to hear from you on the OpenSearch community forum. To learn how you can contribute, see the dashboards-maps repo and the geospatial repo.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Expanding-k-NN-with-Lucene-aNN/",
    "title": "Expanding k-NN with Lucene approximate nearest neighbor search",
    "content": "OpenSearch pioneered k-nearest neighbor (k-NN) within search engines in 2019, and developers have adopted it enthusiastically on sets of millions or even billions of vectors. OpenSearch continues to innovate in the area of k-NN support. OpenSearch 2.2 added the Lucene 9.0 implementation of k-NN, and OpenSearch 2.4 added adaptive filtering.\nOpenSearch supports both exact k-NN and approximate k-NN (ANN). Approximate k-NN, based on the HNSW algorithm, is implemented in OpenSearch by faiss, nmslib, and Lucene. Approximate k-NN can produce results in tens of milliseconds, even for collections of hundreds of millions of vectors, orders of magnitude faster than exact k-NN.\nAdvantages of Lucene 9.0 k-NN\nAt the end of 2021, Lucene 9.0 added support for dense vector indexes and approximate k-NN search. It uses a new codec format for the indexes and takes advantage of the HNSW algorithm.\nHNSW uses a hierarchical set of proximity graphs in multiple layers to improve performance when searching large datasets. This helps to overcome the scaling problems that usually occur when searching high-dimensional datasets.\nThe Lucene codec encodes and decodes numeric vector fields. It creates two separate segment files: one for the vectors and one for the HNSW graph structure, which serves as a sort of index. This allows the vectors to exist outside Java‚Äôs heap memory, reducing the memory load.\nThe Lucene library is written in Java, like the rest of OpenSearch, so the system is platform independent and easier to build.\nWhen to use the Lucene library\nEach of the three approximate k-NN engines has its advantages. The Lucene functionality doesn‚Äôt displace faiss or nmslib but simply provides more options and thus more control over the results.\nFor datasets of up to a few million vectors, the Lucene engine has better latency and recall than the other two. Its indexes are also the smallest. Benchmarks show that the Lucene 9.2 solution is comparable to HNSW implementations based on nmslib, although there are some tradeoffs. In particular, Lucene 9.2 does not support very high recall. But with comparable recall values, the Lucene 9.2 solution consumes fewer resources and has better query latency.\nAnother functionality available in the Lucene library is efficient filtering of k-NN results. Until OpenSearch 2.4, only post-filtering was available, which can be inefficient and inaccurate. Starting with version 2.4, OpenSearch supports adaptive filtering, choosing pre- or post-filtering in order to provide the best speed and accuracy, thanks to improvements in Lucene.\nSummary\nOpenSearch provides a variety of options for implementing exact and approximate k-NN search. You may wish to experiment with more than one solution, tuning the various parameters to optimize result quality, resource usage, and performance for your application.\nFor more information about the approximate k-NN capabilities of OpenSearch, see Approximate k-NN search in the OpenSearch technical documentation.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/semantic-science-benchmarks/",
    "title": "The ABCs of semantic search in OpenSearch: Architectures, benchmarks, and combination strategies",
    "content": "In an earlier blog post, we described different ways of building a semantic search engine in OpenSearch. In this post, we‚Äôll dive further into the science behind it. We‚Äôll discuss the benefits of combining keyword-based search with neural search, the architecture and model options, and benchmarking tests and results:\nIn Section 1, we provide an overview of our proposed solutions and a summary of the main results.\nIn Section 2, we outline the steps needed to create a solution and fine-tune it for your own document corpus.\nIn Section 3 and Section 4, we discuss the effects of different combination strategies and normalization protocols on search relevance.\nIn Section 5, we present the conclusions of our experiments.\nIn the Appendix, we provide more information about the test datasets used for benchmarking.\nSection 1: Overview\nA search engine should work well for both keyword and natural language searches.\nBM25 excels at providing relevant search results when a query contains keywords. Keyword-based searches are extremely fast and robust but have natural drawbacks because keywords do not encapsulate natural language.\nLarge neural networks, such as transformers, perform better when a query requires natural language understanding (for example, using synonyms). However, even the largest transformer models show performance degradation on data that does not belong to their train-data distribution.\nBoth search methods have complementary strengths, so it is natural to investigate a solution that combines them. In addition, most document corpora contain documents that require keyword matching as well as semantic understanding. For example, an e-commerce shoe dataset should be searchable using highly specific keywords, such as ‚Äú12 US Men‚Äù, and natural language phrases, such as ‚Äúfashionable comfortable running shoes‚Äù.\nA metric and test datasets for benchmarking\nTo benchmark solutions, you must select a metric for measuring search relevance and the test datasets.\nWe chose nDCG@10, a widely used information retrieval metric, for measuring search relevance.\nFor the test datasets, we selected 10 different datasets covering a wide variety of domains, query lengths, document lengths, and corpus sizes. Each test dataset contains a list of queries, documents, and relevancy judgments. The relevancy judgments are annotated by human experts (see the Appendix for more information) and are usually represented by binary labels ( 1 for relevant and 0 for irrelevant). Nine of these datasets belong to the BEIR challenge‚Äîa popular collection of test datasets used to benchmark search engines. The tenth dataset is the Amazon ESCI challenge dataset for product search. These 10 datasets cover multiple domains, including e-commerce, COVID-19 statistics, personal finance, and quantum physics.\nThe zero-shot regime\nBecause most search systems are deployed on datasets outside of their training data, we needed to benchmark our solutions on data that they have never encountered before, that is, in a zero-shot regime. Our models were trained exclusively on data distributions that do not overlap with the data distributions of the 10 test datasets. Note that this is different from training models using a train-dev-test split. There, the model is trained on the train split and evaluated on the held-out dev and test splits. In this experiment, the test dataset comes from a different distribution than the training set.\nResults summary\nIn an earlier blog post, we proposed two semantic search solutions: a pretrained transformer + BM25 and a fine-tuned transformer + BM25. In the following sections, we discuss the details of combining transformers with BM25. The following table summarizes the nDCG@10 benchmarking results on the 10 test datasets for the pretrained and fine-tuned transformer (TAS-B) when combined with BM25. For the definition of the combination strategies (harmonic mean, arithmetic mean, and geometric mean), see Section 3. ¬† BM25 Pretrained transformer + BM25 (harmonic) Fine-tuned transformer + BM25 (arithmetic) Fine-tuned transformer + BM25 (geometric) NFCorpus\n0.343\n0.346 0.369 0.367\nTrec-Covid\n0.688\n0.731\n0.752 0.79 ArguAna\n0.472\n0.482 0.527 0.526\nFiQA\n0.254\n0.281 0.364 0.350\nScifact\n0.691\n0.673 0.728 0.727\nDBPedia\n0.313 0.395 0.373\n0.392\nQuora\n0.789\n0.847 0.874 0.872\nScidocs\n0.165\n0.173 0.184 0.184 CQADupStack\n0.325\n0.333\n0.3673 0.377 Amazon ESCI\n0.081\n0.088 0.091 0.091 Average performance against BM25 N/A 6.42% 14.14% 14.93% In Section 2 we discuss the details of obtaining a fine-tuned transformer. If you‚Äôre interested in the result details, skip to Section 3 and Section 4.\nSection 2: Obtaining a fine-tuned transformer\nTo understand the fine-tuned solution, we first need to explore the pretrained solution along with its strengths and limitations. Our pretrained solution consists of a state-of-the-art neural retriever model combined with BM25. We experimentally compared different ways of combining the neural retriever model with BM25 to produce the best results.\nA neural retriever model first creates a vector index of all the documents in the corpus and then at runtime conducts a search using a k-NN query. The model has been trained to map relevant documents close to each other and irrelevant documents farther apart by reading the labeled (query, passage) pairs. Recall that in a zero-shot regime, training data is different from the test datasets. TAS-B is a popular state-of-the-art model that is trained on the MS Marco dataset; it has been shown to have non-trivial zero-shot performance [2]. It uses the DistilBert checkpoint and has 66 million parameters and an embedding dimension of 768.\nThere are other models, such as MPNet, that show equivalent or better performance. These models are trained on a lot of data, which includes some of the test datasets, so it is difficult to benchmark them in terms of zero-shot performance.\nNote that one of the reasons we work in a zero-shot regime is that we often do not have access to supervised data from the domain of choice. To be precise, we have passages from the domain of choice but do not have access to queries or (query, relevant passage) pairs. If such data existed, the ideal solution would have been to use it to fine-tune a transformer model. A fine-tuned transformer would certainly perform better than a pretrained transformer [2].\nHowever, in the absence of domain-specific data, we can leverage the power of large language models (LLMs) to create artificial queries when given a passage. In the rest of this section, we discuss generating synthetic queries and using them to obtain a model fine-tuned to your corpus. As shown in the preceding table, fine-tuned models perform better than pretrained models.\nCreating a fine-tuned model consists of three steps:\nObtain an LLM for query generation.\nUse the query generator model to create synthetic queries given a corpus.\nTrain a small model (such as TAS-B) on the synthetic corpus.\nThe Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch automatically performs all of these steps for the corpus of your choice. Over time, we plan to release newer, more powerful LLMs for query generation that will produce better synthetic queries and lead to improved downstream search performance. 2.1. Obtaining a query generator model\nThere are many publicly available LLMs that can be used for free-form text generation. However, there are very few models that are trained to generate queries. To the best of our knowledge, there is no public GPT-style (that is, decoder only) LLM for query generation.\nWe fine-tuned and released the 1.5B GPT2-XL model that you can download for synthetic query generation. This model is automatically downloaded by the demo notebook. The model is fine-tuned using the MS Marco and Natural Questions (NQ) datasets. These datasets are famous, high-quality datasets in the field of information retrieval. They consist of human-generated queries and corresponding passages or documents that answer each query. For every (query, passage) pair, we created the following training sample: &lt;startoftext&gt; passage &lt;QRY&gt; query &lt;endoftext&gt; In the preceding expression, the special tokens enclosed in angle brackets denote the start of text, start of query, and end of text, respectively. The loss function is the autoregressive cross-entropy loss, that is, the model tries to predict the next word given previous words. We used the AdamW optimizer with a learning rate of 2e-5 and a linear schedule with 5,500 warmup steps. Training was performed on 8 Tesla V100 GPUs with a batch size of 24 for 4 epochs.\nWe only used those samples from NQ that have long answers and do not contain tables, producing approximately 110K (passage, query) pairs with a median passage length of 104 words. For MS Marco, we used the train and validation splits to obtain 559K (passage, query) pairs with a median passage length of 52 words.\n2.2. Creating a synthetic corpus\nAfter downloading the model, the demo notebook uses it to generate synthetic data.\nWe recommend running the notebook on a GPU-powered machine; we used a p3.x16large EC2 instance that has 8 16 GB GPUs.\nThe total time required to generate 16 queries per document for 1M documents is about 48 hours. The generation time scales linearly with the number of documents and can be improved by using larger batch sizes.\nDuring the generation phase we sampled tokens with a default temperature of 1, top-p value of 0.95, and top-k value of 50. From the model‚Äôs perspective, tokens are the atomic elements of a sentence. They are similar to words, except a word can be split into multiple tokens. Splitting words into tokens ensures that the vocabulary size does not grow very large. For instance, if a language consists of only four words‚Äî‚Äúinternet‚Äù, ‚Äúinternational‚Äù, ‚Äúnet‚Äù, and ‚Äúnational‚Äù‚Äîwe can save space by splitting these words into three tokens: ‚Äúinter‚Äù, ‚Äúnational‚Äù, and ‚Äúnet‚Äù.\nFor more information about these hyperparameters and how they affect text generation, see this Hugging Face blog post. Intuitively, higher temperature, top-k, or top-p values yield more diverse samples. We specified the maximum length of the generated query as 25 tokens. Additionally, we set the repetition_penalty to 1.2, thus incentivizing the model to create queries with no repeating tokens. Once the queries are generated, the notebook automatically applies a query filtering model to remove toxic queries using the publicly available Detoxify package.\n2.3. Fine-tuning TAS-B on the synthetic corpus\nThe synthetic corpus created in the previous step is used to fine-tune a pretrained small language model for search. The demo notebook downloads the pretrained TAS-B model and performs the remaining steps automatically. The model is trained to maximize the dot product between relevant queries and passages while at the same time minimizing the dot product between queries and irrelevant passages. This is known in the literature as contrastive learning. We implemented contrastive learning using in-batch negatives and a symmetric loss. The loss is defined for a given batch \\(B\\), where a batch is a subset of query-passage pairs. Let \\(p\\) be a vector representation of a passage and \\(q\\) be a vector representation of a query. Let \\(Q\\) be a collection of queries in a batch \\(B\\), such that \\(Q = \\{q_1‚Äã,q_2‚Äã,\\dots,q_{‚à£B‚à£‚Äã}\\}\\). Further, let \\(P\\) be a collection of passages, such that \\(P = \\{p_1‚Äã,p_2,\\dots,p_{‚à£B‚à£‚Äã}\\}\\). The loss \\(\\mathcal L\\) for a batch \\(B\\) is given by\n\\[\\begin{align}\n\\mathcal L = C(Q, P) + C(P‚Äã, Q) \\tag{1} \\label{1},\n\\end{align}\\]\nwhere\n\\[\\begin{align}\nC(Q‚Äã, P) = ‚àí\\sum_{i=1}^{|B|}‚Äãlog \\left(\\frac{sim(q_i‚Äã,p_i‚Äã)}{sim(q_i‚Äã,p_i‚Äã) + \\sum_{j \\neq i}^{|B|}sim(q_i‚Äã,p_j‚Äã)}‚Äã\\right) \\tag{2} \\label{2}\n\\end{align}\\]\nand\n\\[\\begin{align}\nsim(q‚Äã, p) = \\exp(q^T \\cdot p). \\tag{3}\n\\end{align}\\]\nThe total loss is the sum of the losses across all batches.\nNote that many models are trained using the dot product similarity (not cosine similarity), so the query and passage vectors are not necessarily normalized. For a given batch, the loss consists of the two terms \\(C(Q,P)\\) and \\(C(P,Q)\\). In equation \\((\\ref{2})\\), \\(C(Q,P)\\) is not symmetric in \\(Q\\) and \\(P\\) because the sums over \\(i\\) and \\(j\\) do not commute. Combining the two terms in equation \\((\\ref{1})\\) makes the loss symmetric.\nThe loss is minimized as the argument of the logarithm in equation \\((\\ref{2})\\) approaches 1. In other words, minimizing \\(C(Q,P)\\) is equivalent to maximizing \\(sim(q_i‚Äã,p_i‚Äã)\\) and minimizing \\(sim(q_i‚Äã,p_j‚Äã)\\) for \\(i\\neq‚Äãj\\). Here we are assuming that if the data is shuffled randomly, the queries \\(q_i\\)‚Äã and passages \\(p_j\\)‚Äã within a batch \\(B\\) are unrelated to each other for \\(i\\neq‚Äãj\\). To achieve this goal, the model will learn to map the relevant query and passage pairs \\((q_i‚Äã,p_i‚Äã)\\) close to each other and irrelevant pairs \\((q_i‚Äã,p_j‚Äã)\\) farther apart. This technique, known as in-batch negative sampling, is widely used for training dense retrievers [7].\nThe model is trained using the AdamW optimizer for 10 epochs with a learning rate of 2e-5 and a scheduler that uses a linear schedule with 10K warmup steps. Larger batch sizes lead to more in-batch negatives, which in turn lead to better models. On the other hand, larger batch sizes also may cause GPU out-of-memory issues and should be selected based on GPU memory. We also found that increasing the number of synthetic queries per passage produces better fine-tuned models. However, having a large number of synthetic queries comes with the cost of longer generation and training times, to the point of diminishing returns. On average, we created 24 queries per document in our experiments.\nSection 3: Combination methods\nWe combined transformers with BM25 using three main methods: arithmetic mean, geometric mean, and harmonic mean. For each of these combination methods, we retrieved the top 9,999 documents for BM25 and the top 250 documents for neural query. Each set of scores was normalized by the L2 norm. To be precise, given a list of scores \\(b=[b_1‚Äã, b_2‚Äã, \\dots]\\), we normalized them using the following formula:\n\\[\\begin{align}\n\\tilde{b_i}‚Äã=\\frac{b_i}{‚Äã{\\lVert b \\rVert}_2}.\n\\end{align}\\]\nGiven a list of scores \\(b\\) for BM25 and \\(n\\) for neural search, we can calculate their combined score \\(s\\) as follows:\n\\[\\begin{align}\ns_i‚Äã=\\left\\{\n\\begin{array}{ll}\n\\frac{\\tilde{b_i}+\\tilde{n_i}}{2}, &amp; arithmetic\\ mean \\\\\n\\sqrt{\\tilde{b_i}\\tilde{n_i}}, &amp; geometric\\ mean \\\\\n\\frac{2\\tilde{b_i}\\tilde{n_i}}{\\tilde{b_i}+\\tilde{n_i}}, &amp; harmonic\\ mean. \\\\\n\\end{array}\n\\right.\n\\end{align}\\]\nThe fine-tuned models have been trained for 10 epochs on the synthetic queries created by the query generator. For smaller datasets, such as NFCorpus, ArguAna, and FiQA, we created 32 queries per passage, while for larger datasets we created fewer queries per passage. In particular, we created 26 queries per passage for CQADupStack and 16 for Amazon ESCI.\nThe following table contains the results of combining these scores on the 10 test datasets. ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† BM25 TAS-B TAS-B with L2 norm (arithmetic mean) TAS-B with L2 norm (harmonic mean) TAS-B with L2 norm (geometric mean) Fine-tuned Fine-tuned with L2 norm (arithmetic mean) Fine-tuned with L2 norm (harmonic mean) Fine-tuned with L2 norm (geometric mean) NFCorpus\n0.343\n0.319\n0.346\n0.35\n0.348\n0.301\n0.37\n0.365 0.367 Trec-Covid\n0.688\n0.481\n0.732\n0.731\n0.735\n0.577\n0.752\n0.788 0.79 ArguAna\n0.473\n0.427\n0.485\n0.482\n0.484\n0.492 0.527 0.511\n0.526\nFiQA\n0.254\n0.3\n0.289\n0.281\n0.282\n0.314 0.364 0.326\n0.35\nScifact\n0.691\n0.643\n0.686\n0.691\n0.687\n0.623 0.728 0.722\n0.728\nDBPedia\n0.32\n0.384\n0.341 0.395 0.359\n0.342\n0.373\n0.392\n0.392\nQuora\n0.789\n0.835\n0.836\n0.847\n0.841\n0.855 0.874 0.87\n0.872\nScidocs\n0.165\n0.149\n0.17\n0.17\n0.17\n0.154 0.184 0.181 0.184 CQADupStack\n0.325\n0.314\n0.343\n0.337\n0.34\n0.357\n0.367\n0.352 0.377 Amazon ESCI\n0.081\n0.071\n0.085\n0.088\n0.087\n0.074 0.091 0.09 0.091 Average % change vs. BM25 N/A\n-3.52\n4.89\n6.7\n5.49\n-0.08\n14.14\n12.37 14.91 Overall, a fine-tuned model with an arithmetic or geometric combination provides a boost of almost 15% in terms of nDCG@10 over traditional keyword search (BM25).\nWe found that harmonic combination works best for the pretrained TAS-B model, while arithmetic and geometric combinations work best for the fine-tuned custom model. Note that for a given query, there could be documents that are only present in the dense results and not in the BM25 results. In such cases, we assume that the BM25 score for those documents is zero. Conversely, if there are documents that are only present in the BM25 results, we assume that the neural query score for those documents is zero.\nSection 4: Normalization and other combination methods\nBM25 and neural scores use different scales, so there is no unique strategy for normalizing both of them. In this section, we empirically demonstrate the effects of normalization on nDCG@10 to build intuition for the most useful and robust strategy.\n4.1. No normalization compared to L2 normalization\nWe compared the effects of applying min-max normalization against not applying any normalization at all, as shown in the following table. ¬†¬†¬†¬† BM25 TAS-B harmonic with norm TAS-B harmonic without norm Fine-tuned arithmetic with norm Fine-tuned arithmetic without norm NFCorpus\n0.343\n0.35\n0.345 0.37 0.353\nTrec-Covid\n0.688\n0.731\n0.73 0.752 0.666\nArguAna\n0.472\n0.482\n0.483 0.527 0.519\nFiQA\n0.254\n0.281\n0.274 0.364 0.326\nScifact\n0.691\n0.691\n0.681 0.728 0.665\nDBPedia\n0.32 0.395 0.338\n0.373\n0.363\nQuora\n0.789\n0.847\n0.82 0.874 0.864\nScidocs\n0.165\n0.17\n0.168 0.184 0.165\nCQADupStack\n0.325\n0.337\n0.333 0.367 0.331\nAmazon ESCI\n0.081\n0.088\n0.083 0.091 0.079 Average % change vs. BM25 N/A\n6.72\n3.17 14.16 5.66 We found that in all cases, normalizing the scores before combining them leads to better results, possibly because BM25 and dense models usually have scores that belong to different scales. Not normalizing the scores may lead to BM25 overwhelming dense models or vice versa during the combination phase. Both BM25 and dense models are impressive retrievers and have complementary strengths. Normalizing the scores calibrates the two retrievers and thus leads to better results.\n4.2. Comparing normalization strategies\nWe investigated different normalization strategies. In addition to L2, we tried sklearn‚Äôs minmax scaler, which normalizes the scores as\n\\[\\begin{align}\n\\tilde{b_i} = \\frac{{b_i}-min(b)}{max(b) - min(b)}.\n\\end{align}\\]\nThe results are presented in the following table. ¬† BM25 TAS-B harmonic with min-max norm TAS-B harmonic with L2 norm Fine-tuned arithmetic with min-max norm Fine-tuned arithmetic with L2 norm NFCorpus\n0.343\n0.357\n0.35\n0.365 0.37 Trec-Covid\n0.688\n0.737\n0.731 0.727 0.727 FiQA\n0.254\n0.32\n0.281\n0.359 0.364 ArguAna\n0.472\n0.476\n0.482 0.531 0.527\nScifact\n0.691\n0.688\n0.691\n0.722 0.728 Scidocs\n0.165\n0.168\n0.17\n0.182 0.184 Quora\n0.789\n0.861\n0.847 0.877 0.874\nAmazon ESCI\n0.081\n0.087\n0.088 0.091 0.091 Average % change vs. BM25 N/A\n6.99\n5.01\n13.03 13.56 For the pretrained harmonic combination model, on average, the min-max norm works better than L2. However, for the fine-tuned arithmetic combination model, we did not find a conclusive difference between the two normalization strategies.\n4.3. Combining scores differently\nWe experimented with different score combination methods. One such method is linear combination, where scores are calculated as\n\\[\\begin{align}\n{s_i} = \\tilde{b_i} + f \\cdot \\tilde{n_i},\n\\end{align}\\]\nwhere \\(f\\) is a float that ranges from 0.1 to 1,024 in powers of 2 and \\(b_i\\) ‚Äãand \\(n_i\\) ‚Äãare the min-max normalized BM25 and neural scores, respectively. We found that for the fine-tuned models, \\(f\\) = 1 works best. Note that this is identical to the arithmetic combination. For the pretrained models, we found that \\(f\\) = 8 works better. For more information, see the linear combination experiment results.\n4.4. Other comparisons\nWe explored using cosine similarity instead of dot product for the neural retriever, but it led to worse results. This is probably because TAS-B was trained using dot product similarity instead of cosine similarity.\nAmong other neural models, we tried the ANCE model, but it lead to substantially worse results than TAS-B. We did not benchmark the MiniLM or MPNet models because both were tuned on the 1B sentence pair data and thus could not be evaluated in a zero-shot regime. Nevertheless, we evaluated them after fine-tuning on synthetic data using private test datasets and found the performance to be almost as good as TAS-B.\nSection 5: Strengths and limitations\nThere are two ways of incorporating transformers in search: as cross-encoders and neural retrievers. Cross-encoders work in series with keyword search and can be thought of as rerankers. Once BM25 fetches the top \\(N\\) results for a given query, a cross-encoder reranks these \\(N\\) results given the query \\(q\\). This approach generally produces better results than using neural retrievers alone. However, it is computationally expensive (high latency) because the transformer needs to compute \\(N\\) different scores, one for each (query, document) pair. The cross-encoder is also limited by the result quality of the BM25 retrieval.\nIn contrast, neural retrievers have to perform only one computation that creates a vector for the query. The actual retrieval is accomplished by finding the top \\(N\\) nearest neighbors of this query vector. This is a very fast operation that is implemented using k-NN. Note that neural retrievers do not rely on keyword results but rather are used in combination with keyword search. Indeed, neural retrievers combined with BM25 yield better results than cross-encoders, as shown by our experiments. It is also worth noting that cross-encoders work in the reranker paradigm and rely on first-stage retrieval. They greatly benefit from combining BM25 and dense retrievers, discussed earlier in this post, for their first-stage retrieval.\nIn this blog post, we have included several experiments that can help build intuition about how and when to combine BM25 with neural retrievers. It is important to remember that because every dataset is different, there is a chance that the configurations used here are not optimal for your dataset. Nevertheless, we believe that there are some global conclusions that apply to most datasets:\nNeural retrievers with BM25 work better than neural retrievers or BM25 alone.\nNeural retrievers with BM25 deliver the same (or better) results as cross-encoders at a fraction of the cost and latency.\nIf a dataset contains a lot of keyword usage, BM25 works much better than neural retrievers. An example of such a dataset is one containing factory part numbers.\nIf a dataset contains a lot of natural language, neural retrievers work much better than BM25. An example is data from a community forum.\nFor datasets that contain both natural language and keywords, a combination of BM25 and neural retrievers works better. An example of such a dataset is one containing data for a clothing website that describes products using both natural language (product description) and numbers (product length, size, or weight).\nThe optimal combination method depends on the dataset. In general, we have found that harmonic mean performs best for pretrained models, while arithmetic mean and geometric mean perform best for fine-tuned models.\nMost small transformer models, such as TAS-B, have a context length of 512 tokens (about 350 words), and they ignore all words after that limit. If a document is long and the first few hundred words are not representative of its content, it is useful to split the document into multiple sections. Note that the index size will increase accordingly because each section corresponds to its own vector.\nAppendix\nIn this appendix, we provide further details of the test datasets used for benchmarking. Our primary data sources were the BEIR challenge and the Amazon ESCI datasets.\nThe BEIR dataset\nThe BEIR challenge dataset was introduced in a 2021 paper presented at NeurIPS. It consists of 18 test datasets that cover several domains, from personal advice on Yahoo Answers to Stack Exchange questions about quantum physics. The datasets also come in different evaluation formats, for example, fact checking, question answering, and news retrieval. We used nine of the 19 datasets from the BEIR challenge. We did not use the MS Marco and NQ datasets because the query generator was trained on this data, so it is not zero shot. We did not benchmark on datasets that are not publicly available without registration (BioASQ, Signal-1M, Trec-News, and Robust04). In the future, we plan to benchmark large (more than 5M documents each) datasets, such as fever, climate-fever, and HotpotQA.\nThe Amazon ESCI dataset Amazon ESCI is a shopping query dataset from Amazon. It contains difficult search queries and was released with the goal of fostering research in the area of semantic matching of queries and products. We restricted the data to queries and documents in English. We focused on the task of query-product ranking: given a user-specified query and a list of matched products, the goal is to rank the products by relevance. We used Task 1 (Query-Product Ranking) with product_locale = US and set the following relevancy ratings: E = 100, S = 10, C = 1, and I = 0. Note that the relevancy ratings in the Amazon ESCI paper by Reddy et al. are E = 1, S = 0.1, C = 0.01, and I = 0. Consequently, we cannot directly compare our nDCG scores with the ones mentioned in the Amazon paper. However, the percentage improvement between different models (such as the one shown in the preceding tables) is still a meaningful comparison.\nSample queries and passages\nThe following table provides sample queries and passages for each dataset. Dataset Sample query Sample passage DBPedia\nSzechwan dish food cuisine Mapo doufu (or \\\"mapo tofu\\\") is a popular Chinese dish from China's Sichuan province. It consists of tofu set in a spicy chili- and bean-based sauce, typically a thin, oily, and.... FiQA\n‚ÄúBusiness day‚Äù and ‚Äúdue date‚Äù for bills I don't believe Saturday is a business day either. When I deposit a check at a bank's drive-in after 4pm Friday, the receipt tells me it will credit as if I deposited on Monday. If a business' computer doesn't adjust their billing to have a weekday due date... CQADupStack\nWhy does Simplify[b-a] give -a+b and not b-a? `Simplify[b - a]` results in `-a + b`. I prefer `b - a`, which is a bit simpler (3 symbols instead of 4). Can I make _Mathematica_ to think the same way? I believe one needs... NFCorpus\nHow Doctors Responded to Being Named a Leading Killer By the end of graduate medical training, novice internists (collectively known as the housestaff) were initiated into the experience of either having done something to a patient which had a deleterious consequence or else having witnessed colleagues do the same. When these events occurred... Scifact\nŒ≤-sheet opening occurs during pleurotolysin pore formation. Membrane attack complex/perforin-like (MACPF) proteins comprise the largest superfamily of pore-forming proteins, playing crucial roles in immunity and pathogenesis. Soluble monomers assemble into large transmembrane... Trec-Covid\nwhat is the origin of COVID-19 Although primary genomic analysis has revealed that severe acute respiratory syndrome coronavirus (SARS CoV) is a new type of coronavirus, the different protein trees published in previous reports have provided.... ArguAna\nPoaching is becoming more advanced A stronger, militarised approach is needed as poaching is becoming ‚Ä¶ Tougher protection of Africa\\u2019s nature reserves will only result in more bloodshed. Every time the military upgrade their weaponry, tactics and logistic, the poachers improve their own methods to counter... Quora\nIs heaven a really nice place? What do you think heaven will be like? Scidocs\nCFD Analysis of Convective Heat Transfer Coefficient on External Surfaces of Buildings This paper provides an overview of the application of CFD in building performance simulation for the outdoor environment, focused on four topics... Amazon ESCI\n#1 black natural hair dye without ammonia or peroxide 6N - Sagebrush BrownNaturcolor Haircolor Hair Dye - Sagebrush Brown, 4 Fl Oz (6N)contains no ammonia, resorcinol or parabens\\navailable in 31 colors, each color... Dataset statistics\nThe following table provides statistics about passages and queries for each dataset. Dataset Average query length Median query Length Average passage length Median passage Length Number of passages Number of test queries DBPedia\n5.54\n5\n46.89\n47\n4635922\n400 Quora\n9.531\n9\n11.46\n10\n522931\n10000\nFiQA\n10.94\n10\n132.9\n90\n57638\n648\nCQADupStack\n8.53\n8\n126.59\n68\n457199\n13145\nNFCorpus\n3.29\n2\n22.098\n224\n3633\n323\nScifact\n12.51\n12\n201.81\n192\n300\n5183\nTrec-Covid\n10.6\n10\n148.64\n155\n171332\n50\nArguAna\n193.55\n174\n164.19\n147\n8674\n1406\nScidocs\n9.44\n9\n167.24\n151\n25657\n1000\nAmazon ESCI\n3.89\n4\n179.87\n137\n482105\n8956 Linear combination experiment results\nThe following table contains the results of the experiments for different linear combinations \\(s_i‚Äã=\\tilde{b_i}‚Äã+f\\cdot \\tilde{n_i}\\)‚Äã. ¬†¬†¬† BM25 TASB with factors 0.1 TASB with factor 1 (arithmetic mean) TASB with factors 2 TASB with factors 8 TASB with factors 128 TASB with factors 1024 Fine-tuned with factors 0.1 Fine-tuned with factor 1 (arithmetic mean) Fine-tuned with factors 2 Fine-tuned with factors 8 Fine-tuned with factors 128 Fine-tuned with factors 1024 NFCorpus\n0.343\n0.329\n0.346\n0.334\n0.343\n0.335\n0.324\n0.336\n0.369\n0.34\n0.322\n0.304\n0.301\nFiQA\n0.254\n0.273\n0.289\n0.3\n0.327\n0.303\n0.3\n0.311\n0.364\n0.352\n0.329\n0.314\n0.314\nArguAna\n0.472\n0.476\n0.485\n0.49\n0.485\n0.437\n0.428\n0.497\n0.527\n0.516\n0.494\n0.48\n0.479\nAmazon ESCI\n0.081\n0.082\n0.085\n0.087\n0.087\n0.075\n0.071\n0.085\n0.091\n0.089\n0.0819\n0.075\n0.074\nScifact\n0.691\n0.681\n0.686\n0.693\n0.7\n0.672\n0.65\n0.706\n0.728\n0.728\n0.671\n0.628\n0.623\nScidocs\n0.165\n0.168\n0.17\n0.172\n0.176\n0.154\n0.15\n0.175\n0.184\n0.179\n0.163\n0.155\n0.154\ntrec-covid\n0.688\n0.729\n0.732\n0.74\n0.718\n0.514\n0.486\n0.744\n0.752\n0.665\n0.542\n0.501\n0.503\nQuora\n0.789\n0.816\n0.836\n0.849\n0.867\n0.843\n0.836\n0.829\n0.874\n0.881\n0.874\n0.857\n0.855 Average peformance N/A\n1.9\n4.63\n5.8\n7.64\n-3.22\n-5.94\n6.51\n13.98\n9.88\n1.83\n-3.4\n-3.85 References\nHofst√§tter, Sebastian, et al. ‚ÄúEfficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling.‚Äù ArXiv.org, 26 May 2021, https://arxiv.org/abs/2104.06967.\nThakur, Nandan, et al. ‚ÄúBEIR: A Heterogenous Benchmark for Zero-Shot Evaluation of Information Retrieval Models.‚Äù ArXiv.org, 21 Oct. 2021, https://arxiv.org/abs/2104.08663.\nReddy, Chandan K., et al. ‚ÄúShopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving Product Search.‚Äù ArXiv.org, 14 June 2022, https://arxiv.org/abs/2206.06588.\nAlberti, Chris, et al. ‚ÄúSynthetic QA Corpora Generation with Roundtrip Consistency.‚Äù ACL Anthology, https://aclanthology.org/P19-1620/.\nLiang, Davis, et al. ‚ÄúEmbedding-Based Zero-Shot Retrieval through Query Generation.‚Äù ArXiv.org, 22 Sept. 2020, https://arxiv.org/abs/2009.10270.\nBajaj, Payal, et al. ‚ÄúMS Marco: A Human Generated Machine Reading Comprehension Dataset.‚Äù ArXiv.org, 31 Oct. 2018, https://arxiv.org/abs/1611.09268.\nKarpukhin, Vladimir, et al. ‚ÄúDense Passage Retrieval for Open-Domain Question Answering - Arxiv.‚Äù ArXiv.org, 30 Sept. 2020, https://arxiv.org/pdf/2004.04906.pdf.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/slack-workspace/",
    "title": "Meet your new OpenSearch Project Slack workspace",
    "content": "The OpenSearch Project is happy to announce a new Slack community workspace to provide open communication channels for anyone interested in OpenSearch.\nWhen we first introduced OpenSearch, we made a commitment to the community to ‚Äú‚Ä¶collaborate and empower all interested stakeholders to share in decision making.‚Äù This milestone is the next step in making the OpenSearch Project a truly community-driven open-source project. With the new Slack workspace, collaborators and community members will have a way to chat directly with project maintainers and other members of the community. You can discuss new features, critical bug reports, and project governance directly with other OpenSearch Project stakeholders. Notifications about release announcements, feature freezes, and build failures will help to raise awareness and accelerate development momentum through more expedient lines of communication. With this launch the OpenSearch Project is dedicated to establishing a healthy and vibrant open-source community around the source code so many rely on for their search and analytics needs.\nIn addition to the OpenSearch Code of Conduct, the following are guiding principles for the new Slack workspace:\nOpen Communication ‚Äì To facilitate better sharing of information, ideas, and announcements, all communication is encouraged to happen in the open project channels.\nSafe and Inclusive ‚Äì The workspace strives to be a safe and inclusive environment where everyone feels comfortable participating.\nLevel Playing Field ‚Äì All project maintainers and collaborators are created equal; feedback and proposals will be fully considered based on their benefits to the overall community‚Äînot based on who proposed them.\nFull Transparency ‚Äì All project maintainers are encouraged to use the Slack workspace along with the discussion forums and GitHub project repositories for all communication in order to discourage any closed-source discussions or development decisions.\nNo Sales Pitches ‚Äì The OpenSearch Project Slack workspace is a place to discuss development of the project‚Äînot a place to sell things.\nWe are confident that these principles will help to facilitate open and transparent communication and promote early feedback on feature designs, implementation, and project governance and encourage brainstorming of new ideas in a safe and inclusive environment.\nTo get started today, head over to the Slack workspace page and sign up! Don‚Äôt forget to check out the communications page for guidelines and tips on getting the most out of the new workspace. We are thrilled to have you continue with us on this journey. Have fun, and we look forward to continuing to build a best-in-class project together!\nAs always, the OpenSearch Project forum is available for troubleshooting, support questions, event agendas, and interaction with others in the community. Go to opensearch.org for the latest blog posts and documentation. Subscribe to our Meetup group to join the latest meetings and local user groups. Follow us on Twitter and LinkedIn to stay up to date with the latest #OpenSearch news.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/FOSDEM-Recap-Blog/",
    "title": "FOSDEM 2023 Overview",
    "content": "Wow‚Ä¶ That is all I can really say after my first experience with FOSDEM. I‚Äôve heard from others that this is an event like no other but going in person really does put things into perspective. For those who may not know, FOSDEM is a conference held every year in Brussels for open-source software developers. It‚Äôs attended by thousands of developers with 700+ talks covering all areas of open source.\nSo what can you expect from FOSDEM? Lots of great talks led by open-source developers and communities. A space free from pushy booths trying to sell you their systems. Packed devrooms and great conversations with like minded individuals. Last but not least you can expect rain. Even if it says its supposed to be sunny expect it to be overcast and drizzly ‚òîÔ∏è.\nOpenSearch at FOSDEM\nOf course you are here for the OpenSearch content right? Well I have great news! There were several talks adjacent to or utilizing OpenSearch at FOSDEM!\nBuilding an Enterprise Search Server with FESS This wonderful talk by Kumagai-san details how to use FESS to scrape your organizations documentation into a single searchable document store. He covers some of the challenges they had and how they resolved them to bring better observability across their organizations documentation. Talk FESS Building a Semantic Search Application in Python Semantic search is all the buzz at the moment. Hear first hand from Tuana on how you can build a semantic search application with Haystack. She walks through an example where travel documents for her boss are uploaded into their document store and she is able to ask it when her boss will be traveling. Not only does Haystack return the answer but it is in a format that resembles how a person would respond. Talk Haystack <!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> Python Logging Like Your Job Depends on It\nIn this talk you can hear from me on how to log in python. We start with a very simple logging setup and build all the way up to logging remotely into OpenSearch from python. It serves as an intro to python logging and covers some typical architectures for log collection and aggregation. Talk Code <!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n--> Observability-driven development with OpenTelemetry\nIn this talk by Adnan you will see how to test your systems using your OpenTelemetry traces. This is the next major step for testing distributed systems. Tracetest allows you to create assertions and build tests based off previous traces. Check it out and level up your testing! Talk Tracetest How We Gained Observability Into Our CI/CD Pipeline We all do observability on our production systems but can you say the same on your CI/CD system? Dotan talks about how Logz.io started to observe their deployment and integration systems based on Jenkins. Talk Logz.io Elasticsearch Internals\nEver wondered how Elasticsearch or OpenSearch worked on the insides? Martin takes us all on a tour from the lowest data structure in Lucene to the top level index within Elasticsearch or OpenSearch. This talk is a great foundation for anyone who wants to gain a deeper knowledge into one of these systems. Talk Elasticsearch",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/technical-roadmap-opensearch-extensibility/",
    "title": "Technical roadmap: OpenSearch extensibility",
    "content": "The primary reason users choose OpenSearch is the wide range of use cases they can address with its features, such as search or log analytics. Thus, we aim to make the OpenSearch Project the preferred platform for builders by creating a vibrant and deeply integrated ecosystem of projects, features, content packs, integrations, and tools that can be found quickly, installed securely, combined to solve problems, and monetized by many participants.\nThe existing mechanism used to extend OpenSearch and OpenSearch Dashboards is a plugin framework. It provides a useful way to extend functionality, particularly when the new functionality needs access to a significant number of internal APIs. However, the plugin framework presents a number of challenges for users and developers in the areas of administration, dependency management, security, availability, scalability, and developer velocity. To begin solving these, we‚Äôve embarked on a journey to replace the OpenSearch plugin mechanism with a new catalog of extensions. We plan to ship two new SDKs for OpenSearch and OpenSearch Dashboards and then launch a catalog of extensions.\nIn this blog post, we‚Äôll introduce the concept of extensions and outline some proposed projects in this area.\nIntroducing extensions\nFrom the product point of view, extensions are a new mechanism that provides a way to break up a monolithic, tightly coupled model for building new features in OpenSearch. Technically, extensions are a simple evolution of plugins‚Äîor plugins decoupled from their OpenSearch/Dashboards hosts. In practice, an extension is only different from a plugin in that it only depends on the OpenSearch/Dashboards SDK and works with multiple versions of OpenSearch/Dashboards. We aim for the existence of many more extensions than there are plugins today, written by many more developers. Then we will sunset plugins.\nWe want for extensions to become the preferred mechanism for providing functionality in OpenSearch/Dashboards. Having multiple competing implementations will produce extensions with high performance, improved security, and versatile features for all categories of users, administrators, and developers.\nExtensions roadmap\nThe following proposed projects are in chronological order, but many of them can be accomplished in parallel.\nProvide experimental SDKs and move extensibility concerns out of the cores\nFirst, we plan to introduce an OpenSearch SDK and an OpenSearch Dashboards SDK and refactor the OpenSearch/Dashboards cores to support them as needed. This creates both a logical and a physical separation between extensions and their hosts. You should be able to author an extension that is compatible with all minor versions of an OpenSearch/Dashboards release and to upgrade OpenSearch/Dashboards without having to upgrade an installed extension.\nThe SDK assumes current and future extensibility concerns from OpenSearch/Dashboards. It will contain the set of APIs that need to follow semver, significantly reducing the number of APIs that OpenSearch/Dashboards needs to worry about, because plugins only take a dependency on the SDK. With a semver-stable SDK, plugins can confidently declare that they work with, for example, OpenSearch/Dashboards &gt;= 2.3.0 (2.3, 2.4, ‚Ä¶ 3.0, and so on) or ~&gt; 2.5 (any 2.x after 2.5). The SDK will provide support for integration testing against broad ranges of OpenSearch/Dashboards versions. It can begin selecting common functionality that all plugins may need, such as storing credentials or saving objects, and be strongly opinionated about what constitutes a semver-compatible extension point for OpenSearch/Dashboards. Instead of importing a transitive dependency (for example, oui), developers import an SDK namespace (for example, sdk/ui).\nThe SDK will be much smaller in size than OpenSearch/Dashboards. To develop an extension on top of an SDK, you will not need to check out and build OpenSearch/Dashboards. We will publish the SDKs to maven/npm; they will follow their own semver and will have documentation of public interfaces. The SDK can also choose to implement wrappers for multiple major versions of OpenSearch/Dashboards, extending compatibility much further and enabling developers to write extensions once for several major versions of OpenSearch. Finally, extension testing can be performed against a released, downloaded, and stable version of OpenSearch/Dashboards.\nThis project is currently in progress. The SDK for OpenSearch exists, the SDK repo for OpenSearch Dashboards has been created, and some POCs for plugins as extensions exist. See OpenSearch #2447, opensearch-sdk-java #139, OpenSearch-Dashboards #2608, and OpenSearch-Dashboards #3095.\nAdd security support for extensions in the OpenSearch/Dashboards cores\nIn the plugin model, security is also a plugin. This means security can be optional, which makes it difficult for plugins to build features such as field- and document-level security (FLS/DLS), which perform data access securely. Each plugin must implement data access checks independently and correctly, which has proven to be difficult (see security #1895 for some examples). A simpler and more secure implementation would make all operations inside OpenSearch permissible, no matter the source, syncing access checks to the lower levels. The SDKs move extensibility concerns out of the cores. They will have brand-new APIs, presenting the most opportune time to require a security context in all OpenSearch API calls. It should, of course, still be possible to disable security as needed.\nWe plan to add authentication mechanisms to the OpenSearch core (every API call or new thread/process will carry an identity) and perform authorization when accessing data at the level of these APIs in a way that is backward compatible with the Security plugin. Authorization checks will be enabled using the Security plugin for core APIs exposed in the SDK, and there will be no changes required in plugins to ensure backward compatibility.\nWe currently have an active feature branch for adding security support for extensions in OpenSearch. See OpenSearch #5834.\nReplace plugins in the OpenSearch/Dashboards distribution\nOne of the main reasons for the existence of the default OpenSearch distribution is that it provides a set of secure, signed binaries along with a rich set of features (plugins). We have invested in a large automation effort in the open-source distribution to make this process safe and repeatable. However, producing this distribution still requires significant logistical and technical coordination, plus the semi-automated labor of incrementing versions, adding components to manifests, and tracking unstable upstreams. The toughest challenge to overcome is developers having to develop, build, and test plugins against moving targets of non-stable versions of the OpenSearch/Dashboards cores.\nWe aim for extensions to ship independently, either more or less often than the official distribution. Users should be able to upgrade OpenSearch clusters much more easily because they won‚Äôt need to upgrade installed extensions. Additionally, with fewer versions of released extensions containing no new features, there will be less security patching.\nFor each existing plugin that ships with the OpenSearch distribution, we will need to design a technical path forward, but this phase will present no change in user experience for the default OpenSearch/Dashboards distribution. First, we will design and implement interfaces that need to be exported via the OpenSearch/Dashboards SDK. This presents an opportunity to redesign extension points to be simpler and more coherent and an opportunity to refactor classes in OpenSearch/Dashboards. Plugins remove the dependency on OpenSearch/Dashboards and add a dependency on the SDK, reimplement the calls, and are then available as extensions. Second, to migrate the entire Security plugin into the core, we will need to add support for authorization in REST handlers, implement authorized request forwarding in the REST layer, add support for asynchronous operations and background tasks, and add system index support to allow extensions to reserve system indexes. Finally, the distribution mechanisms can begin picking up the latest available version of an extension, and releasing those artifacts as a bundle instead of rebuilding everything.\nOpenSearch Dashboards cohesion through interfaces\nOpenSearch Dashboards is a single product that includes the core platform (for example, the application chrome, data, and saved objects APIs), native plugins (for example, Home, Discover, Dev Tools, and Stack Management), and feature plugins (for example, Observability, Anomaly Detection, Maps, and Alerting).\nThe current Dashboards experience is not cohesive. As you move between core experiences and feature plugins, there are visual inconsistencies in the fonts, colors, layouts, and charts. Feature plugins are mostly siloed experiences and don‚Äôt render on dashboard pages. Feature plugins are built differently than the native plugins. For example, they often don‚Äôt leverage existing interfaces, such as saved objects, to store UI-configured metadata, or render embedded components on dashboard pages. Additionally, Dashboards currently uses six libraries for rendering visualizations and offers six visualization authoring experiences with overlapping functionality, each with its own implementation.\nConsolidating UI component and visualization rendering libraries into an SDK will reduce maintenance and cognitive burden. We‚Äôll introduce a configuration and template system and a style integrator so that common default preferences can be set once instead of on every visualization. We‚Äôll standardize and modularize configuration options so that UI components are consistent and saved visualizations are cross-compatible. We‚Äôll clearly separate data source configurations and fetching from visualization configurations. We‚Äôll define capabilities and behavior that visualizations should satisfy so that it‚Äôs quicker and easier to build new visualization type definitions that are still fully featured.\nFor more information, see OpenSearch-Dashboards #2840 and OpenSearch-Dashboards #2880.\nIn-proc support for OpenSearch extensions\nWe designed extensions to have a clear API boundary, yet lost the ability to host extensions on the same JVM, adding about 10% serialization overhead to the process. We want to give extension developers the ability to remove that overhead, at the same time providing cluster administrators with more control and improving safety and security.\nA number of extensions, such as language analyzers or storage extensions, live on the critical path of high throughput and may not be able to achieve high performance due to the overhead of additional inter-process communication. Furthermore, for any non-trivial number of extensions (about 10 or more), these extensions will be unlikely to run effectively in separate JVM or Node.js processes without negatively impacting nodes. To satisfy high performance requirements, we will need to reintroduce a way for extensions to run on the same JVM as OpenSearch or to share Node.js processes for OpenSearch Dashboards while giving administrators a way to gain performance in exchange for security isolation, support for extensions written in multiple languages, and multiple major version compatibility.\nPractically speaking, we will need to provide the ability for a subset of OpenSearch extensions to run on the same JVM, and operate on data structures without serialization or deserialization, all without the need to change anything in the implementation of the extension itself.\nSupport dependencies between extensions\nIn the current state, all dependencies are implicit, and all versions across cores and plugins must match. Therefore, we heavily rely on testing a distribution bundle. There‚Äôs no mechanism for knowing what the dependencies are, and all dependency errors are found at runtime. Any update to the dependencies requires rebuilding other plugins, even when there are no changes within the current plugin. Thus, everything is always rebuilt from scratch for every release. To solve this problem, we will add the ability for extensions to depend on other extensions, similar to the ability of an extension to depend on a semver-compatible version of OpenSearch/Dashboards.\nPublic catalog\nWe‚Äôll augment the previously built rudimentary schema for an extension‚Äôs metadata to provide additional fields beyond name, version, and compatibility. These will include such fields as well-defined categories and additional vendor/sponsor information. We will build a minimal catalog website with search functionality and deploy and maintain a public version of it. We‚Äôll ensure that the catalog system can also be run by any enterprise internally, building signing and trust into the system, but will not perform any validation beyond metadata correctness at this stage. An internal catalog will be able to sync with a public catalog with appropriate administrative controls. Alternatively, existing third-party catalog systems will use an API to import extensions. Developers will be able to sign up to publish extensions on the public instance, and we will build mechanisms that help users trust publishers. An API in OpenSearch will support installing extensions from catalogs. A view in OpenSearch Dashboards will allow browsing of available catalogs and extensions.\nDevelopers will be able to publish extensions to a public catalog; users will be able to find extensions in this catalog with a taxonomy and search functions. We‚Äôll provide detail pages with meaningful metadata and vendor information, and administrators will be able to install extensions from public or private catalogs and import a subset of a public catalog into their enterprise. Finally, we‚Äôll add a way for publishers to verify themselves, add quality controls, and possibly include publisher approvals.\nSupporting extensions in OpenSearch high-level language clients\nWe are in the process of improving support for existing plugins in clients by publishing REST interfaces in the form of OpenAPI specs. A generator will consume the spec and output parts of a complete high-level thin client for the OpenSearch distribution.\nWe will similarly support extensions in the clients by creating thin clients for each extension that will be composable with the core client in every supported programming language. Extensions will publish REST interfaces in the form of OpenAPI specs, and a generator will consume the spec and output a complete high-level thin client. The burden of m * n extensions and languages will be alleviated by automating as much of the process as possible, and providing build and test tooling such as CI workflows, so that both project-owned extensions and third-party-developed extensions benefit from uniform support. The extension owner can then take the generated clients and publish them to their package repositories of choice. The core clients will define stable low-level/raw interfaces with their transport layer such that the thin clients compose as expected and follow semver compatibility rules.\nSee opensearch-clients #19 for more information.\nRewrite most plugins as extensions\nWe will provide a way to deprecate plugins by implementing all corresponding plugin features in extensions, with the goal of minimizing the effort required to migrate. We will write a migration guide that clearly specifies the effort required to perform the initial migration and follow-up deprecated feature replacement so that you can integrate it into your own task planning, offer code that makes the migration from plugins to extensions easier, implement samples that provide one-to-one analogs with the existing plugin framework, and create assurances that external behavior in the migration has not changed. Code permitting this quick bridge may be marked deprecated but will allow you to methodically remove the deprecated code over time.\nFor more information, see opensearch-sdk-java #315.\nDeprecate plugins and unbundle distributions\nAssuming extensions have been widely adopted, we can deprecate the plugin APIs and remove them from the next major version of OpenSearch. We don‚Äôt expect this to happen earlier than OpenSearch 4.0. Older plugin versions will continue to work with older versions of OpenSearch and receive security patches.\nWe will replace the two distribution flavors of OpenSearch/Dashboards (currently a -min distribution without security or plugins) with a set of distribution manifests tailored for such purposes as log analytics and search. Each distribution will be represented by a manifest that can be assembled by downloading the artifacts from the public catalog for packaging purposes. We want to enable single-click installation of a collection of extensions, provide recommended distributions of OpenSearch for tailored purposes, let vendors create their favorite flavor of OpenSearch/Dashboards distribution easily, and add the capability to create enterprise-tailored distributions.\nFuture\nExtensions will make a lot of new, big ideas possible! Here are some of our favorites.\nHot swap extensions\nOpenSearch/Dashboards bootstrap plugins at start time, and various parts of the system assume that plugins do not change at runtime. Requiring a cluster restart for all extensions is a severely crippling limitation on the path of ecosystem adoption of any significant number of extensions, primarily because cluster restarts mean stopping inbound traffic. We will ensure that any extension point can be loaded or unloaded at runtime by making settings in the OpenSearch core dynamic and adding tools to support loading and unloading extensions at runtime without restarting OpenSearch/Dashboards nodes or the entire cluster. This creates the ability to add, upgrade, or remove an extension without the need to restart OpenSearch/Dashboards or connect a remote extension to an existing OpenSearch/Dashboards cluster.\nExtensible document parsing\nJSON is, by far, the most popular input format for OpenSearch. JSON is humanly readable, so it is fairly easy to test and use for development. Additionally, the OpenSearch ecosystem is built around JSON, with most benchmarks written in JSON and ingest connectors supporting JSON. However, JSON is much slower and more space-consuming than most binary formats, thus swapping JSON for another type may yield significant performance gains. We would like to make OpenSearch input formats extensible so that it is easier to add and test more formats. This was proposed in OpenSearch #4559.\nSecurity isolation\nWith extensions being designed to run on a separate virtual machine (VM), we can introduce multiple options for isolating extensions, such as containers and Java runtimes (for example, Firecracker, GraalVM, and EBPF). We can also provide a new secure default runtime and solve the problem of Java Security Manager (JSM) deprecation. We can further extend security across new boundaries, ensuring all messages are encrypted in transit and all data is encrypted at rest.\nSearch processors become extensions\nIn search-processor #80, we proposed a new search processor pipeline. Search processors are plugins that can become extensions before plugins are deprecated.\nStorage plugins become extensions\nThe storage API in OpenSearch has proven to be quite stable. There may be no need to change storage extensions across OpenSearch versions. New features and improvements in the respective remote storage clients (for example, Microsoft Azure or Amazon Simple Storage Service (Amazon S3)) happen separately from OpenSearch distributions, so these extensions can be upgraded and improved without needing to wait for a new OpenSearch release. We also want partners to maintain their own storage, removing the idea of ‚Äúfirst-class storage.‚Äù\nWith the addition of features like remote-backed indexes and searchable snapshots, the storage plugins (for example, Amazon S3, Azure, GCP, and HDFS) are on the critical path for both indexing and search operations. These plugins will not be able to ship with any performance penalty because it is likely to make for an unacceptable user experience. We‚Äôll use the in-process support for extensions to move these plugins to the extensions model. We could abstract just reading or writing, support multiple versions of Lucene side by side, or use different storage engines and initialize one engine per index.\nReplication as an extension\nWith the introduction of segment replication (segrep), node resources need to be allocated to perform file copy. Today there are settings that define limits on data transfer rates (for segrep and recovery) to prevent these functions from consuming valuable resources required for indexing and search. Moving this functionality to a separate node-local JVM allows us to control maximum resource consumption (CPU/memory) and avoid unnecessary throttling and any impact on read and write performance. We can therefore define new extension points on the engine to support segrep implementations that can run as a sidecar and provide the opportunity to plug in an implementation based on storage requirement (remote, node-node). This will be either in process or in a separate node-local process and will be integrated with the storage plugins to support remote store as a replication source.\nExtensions in other programming languages\nWith extensions designed to operate remotely, we can support out-of-process extensions written in other languages running on the JVM and remote extensions written in any other language hosted externally. By enabling polyglot applications or porting the extension SDKs to other programming languages, we will also lower the barrier to entry for authoring extensions in languages such as Ruby with a JRuby SDK or add support for in-proc extensions written in TypeScript running on GraalVM.\nExtensions in other technologies\nAn extension cannot be entirely implemented in an Azure Function or AWS Lambda because it must maintain a network connection with OpenSearch and share some state information. It is possible to create a client for this purpose with an internal Lambda implementation to enable Lambda extensions.\nAggregations as an extension\nAs we work toward separating search aggregations and compute, we will refactor and extract interface and SDK components for compute separately from the search phase lifecycle. Simple search aggregations, which default to the current single-pass data aggregation implementation, may still be supported for basic search use cases. Separating the compute framework through extensions and a developer SDK enables drop-in replacements from more evolved data analytics systems, such as Spark. By separating the search aggregation framework through extensions and a developer SDK, third-party contributors can leverage new kinds of search aggregation support, such as principal component analysis (PCA) or stratified sampling pipeline aggregations.\nCluster manager as an extension\nWe would like to refactor and extract cluster management interfaces to remove the existing cluster manager node limit of ~200. For large deployments, we believe we can provide alternate cluster manager implementations that have a different scalability and availability model.\nOffloading background tasks to extensions\nA number of background operations currently run asynchronously in an OpenSearch cluster, including Lucene segment merges, data tier migrations (for example, hot to warm to cold), and most operations performed by Index State Management (ISM). These can be offloaded to dedicated or isolated compute instances through the extensions mechanism, improving cluster scalability and availability.\nCommunication protocols as extensions\nBy making communication protocols extensible, we can experiment with more performant implementations such as GRPC or no- or low-garbage-collection implementations (for example, Netty) without having to modify the core engine. Check out opensearch-sdk-java #414, which prototypes protobuf serialization.\nHelp wanted\nThis blog post reflects some of our current project plans. Like all plans, they may change as we make progress, and we would love your help! The best way to start is by checking out opensearch-sdk-java. Try to implement a trivial extension on top of it, or help us port an existing plugin. You could also pick up one of the issues labeled ‚Äú good first issue ‚Äù in that project or start with one of the ideas we mentioned above. As always, please let us know how we can help by opening new issues or posting to the forums.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/ml-deployment-status/",
    "title": "You can now see ML model status in OpenSearch Dashboards",
    "content": "Before OpenSearch 2.6, checking the status of a deployed machine learning (ML) model in OpenSearch could only be done by using the ML Commons Profile API. Although the Profile API returned data organized by ML nodes, the response could not tell you the status of a specific model deployed on an ML node.\nThe ML team is happy to announce the first piece of ML functionality in OpenSearch Dashboards, the deployed models dashboard, as an experimental feature. The deployed models dashboard gives you a view into the responsiveness of each deployed model on each ML node in your cluster.\nViewing deployed ML status at the model level\nYou can find the the deployed model dashboard in the Machine Learning section of OpenSearch Dashboards, as shown in the following image. The deployed model dashboard shows the following information about your deployed models: Name: The name of the model given upon upload. Status: The number of nodes for which the model is responsive.\nWhen all nodes are responsive, the status is Green.\nWhen some nodes are responsive,the status is Yellow.\nWhen all nodes are unresponsive, the status is Red. Model ID: The model ID.\nIf you want to see even more details about your model‚Äôs responsiveness, select View status details to see a detailed view of your model on each of its ML nodes, as shown in the following image. The detailed view prevents you from having to manually call the Profile API and parse through complex return results to see which ML nodes are responsive. Instead, the detailed view tells you which ML nodes, by Node ID, are responding or not responding, which is very useful when troubleshooting deployment issues.\nSimplifying model filtering and searching\nIf you deploy a large number of models, you might find it difficult to locate the model you need with the Profile API. Luckily, you can search and filter through all deployed models by using:\nThe model name.\nThe model ID.\nThe model status.\nFurthermore, the deployed model dashboard can automatically refresh data according to a selected time cycle. To set the refresh cycle, select the Clock icon, set the refresh time, and then select Start, as shown in the following image. Next steps\nThe ML functionality in OpenSearch Dashboards is experimental, so it shouldn‚Äôt be used in a production environment. If you want to test the functionality, see Enabling ML in Dashboards or visit the ML OpenSearch Dashboard Playground.\nFor additional updates on the deployed models dashboard or to leave feedback, see the OpenSearch Forum discussion.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/blog/newsletter-vol3/",
    "title": "OpenSearch Project Newsletter - Volume 1, Issue 3",
    "content": "The OpenSearch Project is pleased to present our newsletter. Distributed monthly, the newsletter is a great way to learn more about the latest OpenSearch news, relevant content from the community, and upcoming events that you may be interested in. What‚Äôs New? OpenSearch Slack Launch: The OpenSearch public slack instance launched and is now live. Join the channel to collaborate with the OpenSearch community. OpenSearch-Benchmark 0.2.0 with python 3.10, 3.11 support release: This is the first release of OSB since it was forked from ESRally. It includes several fixes and is now available on Docker/ECR as well. After a long hiatus, there is substantial engagement and activity on this project. OpenSearch 1.3.9 has been released with Debian support. The College Contributor Initiative (CCI) raised 111 PRs: The first batch of 100 college contributors last week started onboarding the second batch of 190 new college contributors. OpenSearch SDK integrated a framework to permit plugins and extensions to trigger actions on other extensions: This enables key plugin features such as Job Scheduler executing scheduled jobs on Extensions. OpenSearch High Level Python Client (opensearch-dsl-py) has been merged into OpenSearch Python Client (opensearch-py) from version 2.2.0 of the opensearch-py client: The opensearch-dsl-py was deprecated from its 2.1.0 release. This will help keep improvements synced across both high-level and low-level clients.\nActive Issues:\nProposal for Clients Support of Extensions Add Search to Remote-backed Index Proposal to change minor release process Proposal for Protobuf in OpenSearch Proposal to move SDK to OpenSearch Repo Proposal to remove SDK dependencies on o.o.client:opensearch-java Good First Issues Add minDocs condition to the Rollover API Improve extension-from-scratch example with Get, Update, and Delete functionality Upcoming Events &amp; Meetings KubeCon + CloudNativeCon Europe, April 18th-21st, 2023 - See us in Stand P16 The Cloud Native Computing Foundation‚Äôs flagship conference gathers adopters and technologists from leading open source and cloud native communities in Amsterdam, The Netherlands. Stop by stand P16 to connect with OpenSearch technologists and get a demo of the latest functionality and features. And hear from our partners, Opster), who will showcase latest advancements in their community tools, and explore the cutting-edge Kubernetes trends. We‚Äôll also be at Observability Day before the main event, so stop by and see our team at stand T10! Please reach out to Senior Marketing Manager, Patti Juric, or Developer Advocate, Nate Boot for more details on how to meet up with the OpenSearch team. Haystack Conference, April 24th, 2023 Reach our to our Developer Advocate, David Tippett, to chat about OpenSearch Project. David will also be presenting at Haystack 2023. Haystack is the conference for organizations where search, matching, and relevance really matters to the bottom line. For search managers, developers, relevance engineers &amp; data scientists finding ways to innovate, see past the silver bullets, and share what actually has worked well for their unique problems. Open Source Summit, North America, May 10th, 2023 Open Source Summit is the premier event for open source developers, technologists, and community leaders to collaborate, share information, solve problems, and gain knowledge, furthering open source innovation and ensuring a sustainable open source ecosystem. It is the gathering place for open-source code and community contributors. Please reach out to David Tippett to connect with the OpenSearch team at Open Source Summit.\nSubscribe to our MeetUp group to join the latest meetings, local user groups.\nFollow us on Twitter and LinkedIn to stay up-to-date with the latest #OpenSearch news.\nVisit our forum to ask questions and interact with the OpenSearch community. Submit Content for the Monthly Newsletter.",
    "keywords": [
      "community"
    ],
    "type": "News"
  },
  {
    "url": "/blog/Announcing-Data-Prepper-2.2.0/",
    "title": "Announcing Data Prepper 2.2.0",
    "content": "Data Prepper 2.2.0 is now available for download!\nThis release introduces a number of changes that help with Data Prepper‚Äôs reliability and data delivery assurances.\nS3-based dead-letter queue for OpenSearch\nPrior to Data Prepper 2.2.0, the opensearch sink could only write failed events to a local file.\nThis required logging in to your cloud instance or machine to retrieve failed events as well creating a different infrastructure to export them.\nNow the opensearch sink can write documents from failed events directly into Amazon Simple Storage Service (Amazon S3) objects.\nYou can now use these objects as an alternate dead-letter-queue (DLQ).\nThis helps you analyze event failures without having to retrieve them locally.\nFurthermore, users that run on a serverless infrastructure can avoid maintaining a persistent fail state on serverless machines.\nEnd-to-end acknowledgments for S3 source\nData Prepper‚Äôs s3 source now support end-to-end acknowledgments.\nBefore end-to-end acknowledgments, the s3 source would only acknowledge event delivery with Amazon Simple Queue Service (Amazon SQS) after writing all events to a Data Prepper buffer.\nIn cases where Data Prepper was unable to write to OpenSearch, the SQS message would still be acknowledged, and Data Prepper would not read for the object.\nWith end-to-end acknowledgments, the s3 source does not acknowledge completion until all events are sent to an OpenSearch index or the opensearch sink‚Äôs DLQ.\nIf the s3 source receives no acknowledgment, the SQS message remains in the SQS queue for reprocessing.\nFor Data Prepper 2.2.0, end-to-end acknowledgments are only supported inside the s3 source because acknowledgments to s3 are asynchronous.\nHowever, we‚Äôve designed end-to-end acknowledgments so that they could be used in other sources.\nIf you would like to see additional sources added for this feature, create a GitHub issue.\nWriting to Amazon OpenSearch Serverless\nData Prepper can write events to an Amazon OpenSearch Serverless collection, giving Amazon OpenSearch Serverless users the ability to use Data Prepper to ingest log data.\nOther features\nAdded a new list_to_map processor, which converts lists of objects to maps.\nAdded support for format strings in the add_entries processor.\nAdded support to the s3 source for reading S3 objects using Amazon S3 Select. With this feature, you can read Parquet files in Data Prepper or filter the data in S3 Select before it ever even reaches Data Prepper.\nGetting started\nTo download Data Prepper, see the OpenSearch downloads page.\nFor instructions on how to get started with Data Prepper, see Getting started with Data Prepper.\nTo learn more about the work in progress for Data Prepper 2.3, see the Data Prepper roadmap.\nThanks to our contributors!\nThe following people contributed to this release. Thank you! ashoktelukuntla - Ashok Telukuntla asifsmohammed - Asif Sohail Mohammed chenqi0805 - Qi Chen cmanning09 - Christopher Manning dlvenable - David Venable engechas - Chase Engelbrecht graytaylor0 - Taylor Gray kkondaka - Krishna Kondaka KrishnanandSingh - Krishnanand Singh livekn - Toby Lam oeyh - Hai Yan roshan-dongre - Roshan Dongre udaych20 - Uday Chintala",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/get-started-opensearch-2-7-0/",
    "title": "Get Started with OpenSearch 2.7.0",
    "content": "OpenSearch 2.7.0 is ready for download! The latest version of OpenSearch offers a range of new capabilities for search, analytics, observability, and security applications, along with significant enhancements to administration and usability. This release also marks the general availability of several major features that were previously released as experimental‚Äîwe hope you‚Äôre as eager as we are to put capabilities like segment replication, searchable snapshots, and more into production! As always, the release notes provide a full view of what‚Äôs new, and you can explore OpenSearch‚Äôs visualization tools on the Playground.\nGain efficiencies at scale with searchable snapshots\nIntroduced as experimental in OpenSearch 2.4.0, searchable snapshots allow you to search indexes that are stored as snapshots within remote repositories in real time, with no need to download the entire set of indexed data to cluster storage ahead of time. Now production ready with a number of enhancements to performance, stability, and administration (as tracked here), searchable snapshots can help you take advantage of remote storage options while saving time and conserving storage capacity. With this release, phase two of the project‚Äôs storage roadmap is now generally available.\nEnhance performance with segment replication\nWith the general availability of segment replication, users can choose another strategy for replicating their data, with the potential to improve performance for high-ingestion workloads. Segment replication copies Lucene segment files from the primary shard to its replicas. Lucene‚Äôs write-once segmented architecture means that only new segment files need to be copied, offering improved indexing throughput and lower resource utilization at the expense of increased network utilization and refresh times. You can now choose between segment replication and document replication; document replication performs the same indexing operation on the primary shard and each replica in parallel whenever documents are added to, removed from, or updated within an index. Released as experimental in OpenSearch 2.3.0, segment replication received a number of contributions as it approached general availability, as tracked in the project here.\nVisualize and explore data from multiple sources\nAlso ready for production is support for multiple data sources in OpenSearch Dashboards. Now you can dynamically manage data sources across multiple OpenSearch clusters, create index patterns based on those sources, run queries against a specific data source, and combine visualizations into a single dashboard. Launched as experimental in OpenSearch 2.4.0, this feature has gained functionality in preparation for version 2.7.0, as tracked in this issue, including integration with the Dev Tools console and several usability enhancements.\nReduce overhead with flat objects\nComplex JSON objects often include a large number of subfields. As indexes grow, the overhead required to map each field can consume excessive amounts of storage and memory, potentially leading to ‚Äúmapping explosions‚Äù that can impact the performance and resilience of the cluster. With the new flat object field type, you have the option to store complex JSON objects in an index without indexing all fields separately. By defining a flat object, you can choose to store the object and all of the objects within it, avoiding the need to separately index the subfields while keeping those subfields accessible as keywords using dot notation in DSL and SQL. This means that you can you tune your index mapping to your data and better manage and utilize your resources.\nUse observability features in OpenSearch Dashboards\nWith 2.7.0, OpenSearch continues the trend of integrating observability features as core functionality within OpenSearch Dashboards. Now you can easily access observability features from the main menu, create and select observability dashboards from within Dashboards, and add event analytics visualizations (PPL) to new or existing dashboards in OpenSearch Dashboards. Simply create a new dashboard from within OpenSearch Dashboards and see observability dashboards as an option or add your favorite event analytics PPL visualization to your existing dashboard. For more on this functionality, see the documentation.\nQuery geospatial data with shape-based filters\nThis release brings another round of enhancements to the geospatial tools in OpenSearch Dashboards, with the ability to filter geospatial data against geospatial field types. In earlier versions, users could filter documents by non-geospatial field types in the document layer. Now you can filter your data by drawing a rectangle or polygon over a selected area of the map. This applies filters to geospatial data to identify spatial relationships; you can use this functionality to return documents whose geographic coordinates (geo_point) or geographic shape (geo_shape) intersect, contain, are within, or are not found within the query geometry. View OpenSearch maps in local languages\nWith 2.7.0, OpenSearch will now automatically render maps with labels and contents shown in the language for which the OpenSearch instance is configured. In earlier versions, maps are rendered using the language provided by the source library. Now you have the option to display maps in the supported language of their choice. At launch, the selected language will be defined by the OpenSearch Dashboards YAML configuration file; look out for a selectable dropdown menu in a future release.\nSimplify administration with component templates\nOpenSearch 2.7.0 simplifies the management of multiple index templates by adding component templates directly into the index management UI in OpenSearch Dashboards. In the past, users faced difficulties managing multiple index templates due to duplication, resulting in a larger cluster state. Additionally, making changes to multiple templates required manual updates for each one. Further enhancing the index management UI introduced with 2.5.0, component templates allow you to overcome these challenges by abstracting common settings, mappings, and aliases into a reusable building block. Configure tenancy dynamically in OpenSearch Dashboards\nAnother time-saving upgrade for OpenSearch administrators comes with the availability of dynamic tenant management. OpenSearch Dashboards uses tenants as spaces in which to save and share index patterns, visualizations, dashboards, and other objects, with administrative control over which users can access a tenant and the level of access provided. In earlier versions, tenant creation and mapping was supported in Dashboards, while tenant configuration was done in YAML files, requiring changes to be made within each data node to maintain consistency across nodes and necessitating a restart of Dashboards in order to take effect. With this release, administrators can view, configure, and enable or disable tenancy within Dashboards and effect those changes without needing to restart.\nMaintain performance with hot shard identification\nThis release brings hot shard identification to the collection of tools available in OpenSearch‚Äôs Performance Analyzer plugin. Hot shards consume more compute, memory, or network resources than other shards in an index; left unaddressed, they can lead to reduced query throughput and increased latency across the index, potentially impacting cluster availability. Now you can use the performance analyzer‚Äôs root cause analysis agent to identify hot shards within the cluster so they can be mitigated to the benefit of cluster performance.\nAnalyze security events with built-in correlation tools\nThe log data that comprises security events can span across multiple indexes and data streams, and visualizing relationships between connected events can provide valuable insight for security analysts. Included in this release as an experimental feature, the correlation engine lets you define correlations in your security event data, enabling high-fidelity findings across different log sources, like DNS, Netflow, and Active Directory, to name a few. This knowledge graph can be used to identify, store, and recall connected event data across multiple indexes and data streams to help you identify patterns and investigate relationships across different systems in your monitored infrastructure. As always, experimental features are only recommended for use outside of production environments.\nImprove availability for ML models\nThe experimental machine learning (ML) framework receives updates in this release, including a new automatic reloading mechanism for ML models. Now you can set up your search clusters to auto-reload deployed models when a cluster restarts after shutting down or when a node rejoins a cluster, so you can minimize recovery time and get your ML models back into production faster.\nExplore OpenSearch 2.7.0\nThe latest version of OpenSearch is ready for download. You can learn more about these features and many more in the release notes, documentation release notes, and documentation, and the OpenSearch Playground is a great place to explore the tools before downloading them. Look for upcoming blog posts that dive deeper into the new features included in OpenSearch 2.7.0.",
    "keywords": [
      "releases"
    ],
    "type": "News"
  },
  {
    "url": "/blog/segment-replication/",
    "title": "Reduce compute costs and increase throughput with segment replication, generally available in OpenSearch 2.7",
    "content": "We are excited to announce that segment replication‚Äîa new replication strategy built on Lucene‚Äôs Near-Real-Time (NRT) Segment Index Replication API and introduced as experimental in OpenSearch 2.3‚Äîis generally available in OpenSearch 2.7. Implemented as an alternative to document replication, segment replication significantly increases indexing throughput while lowering compute costs for many use cases. With document replication, all replica nodes (referred to as a replica group) perform the same indexing operation as the primary node. With segment replication, only the primary node performs the indexing operation, creating segment files that are copied remotely to each node in the replica group. In this replication design, the heavy indexing workload is performed only on the primary node, freeing up resources on the replicas for scaling out other operations. In this blog post, we dive deep into the concept of segment replication, advantages and shortcomings as compared to document replication, and planned future enhancements. To find out if segment replication is the right choice for your use case, see Segment replication or document replication.\nCore concepts\nWhen you create an index in OpenSearch, you specify its number_of_shards (the default is 1), called primary shards, and number_of_replicas (the default is 1). Each replica is a full copy of the set of primary shards. If you have 5 primary shards and 1 replica for each of them, you have 10 total shards in your cluster. The data you send for indexing is randomly hashed across the primary shards and replicated by the primary shards to the replica or replicas.\nInternally, each shard is an instance of a Lucene index‚Äîa Java library for reading and writing index structures. Lucene is a file-based, append-only search API. A segment is a portion of a Lucene index in a folder on disk. Each document you send for indexing is split across its fields, with indexed data for the fields stored in 20‚Äì30 different structures. Lucene holds these structures in RAM until they are eventually flushed to disk as a collection of files, called a segment.\nReplicas are typically used for two different purposes: durability and scalability, where replica shards provide redundant searchable copies of the data in a cluster. OpenSearch guarantees that the primary and replica shard data is allocated to different nodes in the cluster, meaning that even if you lose a node, you don‚Äôt lose data. OpenSearch can automatically recreate the missing copies of any shard that may have been lost on a faulty node. If you are running in the cloud, where the cluster spans isolated data centers (AWS Availability Zones), you can increase resiliency by having two replicas across three zones. The second and subsequent replicas provide additional query capacity. You add more nodes along with the additional replicas to provide further parallelism for query processing.\nDocument replication\nFor versions 2.7 and earlier, document replication is the default replication mode. In this mode, all write operations that affect an index (for example, adding, updating, or removing documents) are first routed to the node containing the index‚Äôs primary shard. The primary shard is responsible for validating the operation and subsequently running it locally. Once the operation has completed successfully, the operation is forwarded in parallel to each node in the replica group. Each replica node in the group runs the same operation, duplicating the processing performed on the primary. When an operation has completed on a replica (either successfully or with a failure), a response is sent to the primary. Once all replicas in the group have responded, the primary node responds to the coordinating node, which sends a response to the client with detailed information about replication success or failure (for example, how many and which replica nodes may have failed).\nThe advantage of document replication is that documents become searchable on the replicas faster because they are sent to the replicas immediately following ingestion on the primary shard. The system reaches a consistent state between primary and replica shards as quickly as possible. However, document replication consumes more CPU because indexing operations are duplicated on every primary and replica for every document.\nRefer to the following diagram of the document replication process. Segment replication\nWith segment replication, documents are indexed only on the node containing the primary shard. The resulting segment files are then copied directly to all replicas in a group and made searchable. Segment replication reduces the compute cost of adding, updating, or deleting documents by performing the CPU work only on the primary node. The underlying Lucene append-only index makes copying segments possible: as documents are added, updated, or deleted, Lucene creates new segments, but the existing segments are left untouched (deletes are soft and handled with tombstones and docvalue fields).\nThe advantage of segment replication is that it reduces the overall CPU usage in your cluster by removing the duplicated effort of parsing and processing the data in your documents. However, because all indexing and networking originates on the nodes with primary shards, those nodes become more heavily loaded. Additionally, nodes with primary shards spend time waiting for segment creation (this amount of time is controlled by the refresh_interval) and sending the segments to the replica, increasing the amount of time before a particular document is consistently searchable on every shard.\nRefer to the following diagram of the segment replication process. Segment repication test results\nDuring benchmark ingestion testing with 10 primary shards and 1 replica on the stackoverflow dataset, segment replication provided an increased ingestion rate throughput of up to 25% as compared to document replication. For detailed benchmarking results, see the Benchmarks section.\nOur experimental release users reported up to 40% higher throughput with segment replication than with document replication for the same cluster setup. With segment replication, you can get the same ingestion throughput with 9 nodes in a cluster as you would get with 15 nodes with document replication.\nUnderstanding the tradeoffs\nSegment replication trades CPU usage for time and networking. The primary shard sends larger blocks of data to its replicas less frequently. As replica count increases, the primary shard becomes a bottleneck, performing all indexing work and replicating all segments. In our testing, we saw consistent improvement for a replica count of one. As replica count grows, the improvement decreases linearly. Performance improvement in your cluster depends on the workload, instance types, and configuration. Be sure to test segment replication with your own data and queries to determine the benefits for your workload.\nFor higher replica counts, remote storage integration works better. With remote storage integration, the primary shard writes segments to an object store, such as Amazon Simple Storage Service (Amazon S3), Google Cloud Storage, or Azure Blob Storage. Replicas then load the segments from the object store in parallel, freeing the node with the primary shard from sending out large data blocks to all replicas. We are planning to introduce remote storage integration in a future release.\nAs with any distributed system, some cluster nodes can fall behind the tolerable or expected throughput levels. Nodes may not be able to catch up to the primary node for various reasons, such as heavy local search loads or network congestion. To monitor segment replication performance, see OpenSearch benchmark.\nSegment replication or document replication Segment replication is best suited for the following configurations:\nYour cluster deployment has low replica counts (1‚Äì2 replicas). This is typically true for log analytics deployments.\nYour deployment has a high ingestion rate and relatively low search volume.\nYour application is not sensitive to replication lag.\nThe network bandwidth between the nodes is ample for the high volume of data transfer between nodes required for segment replication.\nWe recommend using document replication in the following use cases, where segment replication does not work well:\nYour cluster deployment has high replica counts (more than 3) and you value low replication lag. This is typically true of search deployments.\nYour deployment cannot tolerate replication lag. In deployments such as search deployments, where the data consistency between all replicas is critical, we do not recommend segment replication because of its high latency.\nYour deployment has insufficient network bandwidth for expedient data transfer for the number of replicas.\nYou can validate the replication lag across your cluster with the CAT Segment Replication API.\nSee the Benchmarks section for benchmarking test results.\nSegment replication backpressure\nIn addition to the existing shard indexing backpressure, OpenSearch 2.7 introduces a new segment replication backpressure rejection mechanism that is disabled by default.\nShard indexing backpressure is a shard-level smart rejection mechanism that dynamically rejects indexing requests when your cluster is under strain. It transfers requests from an overwhelmed node or shard to other nodes or shards that are still healthy.\nSegment replication backpressure monitors the replicas to ensure they are not falling behind the primary shard. If a replica has not synchronized to the primary shard within a set time limit, the primary shard will start rejecting requests when ingesting new documents in an attempt to slow down the indexing.\nEnabling segment replication\nTo enable segment replication for your index, follow the step-by-step instructions in the documentation.\nBenchmarks\nThe following benchmarks were collected with OpenSearch-benchmark using the stackoverflow and nyc_taxi datasets.\nThe benchmarks demonstrate the effect of the following configurations on segment replication: The workload size The number of primary shards The number of replicas Note: Your results may vary based on the cluster topology, hardware used, shard count, and merge settings.\nIncreasing the workload size\nThe following table lists benchmarking results for the nyc_taxi dataset with the following configuration:\n10 m5.xlarge data nodes\n40 primary shards, 1 replica each (80 shards total)\n4 primary shards and 4 replica shards per node 40 GB primary shard, 80 GB total 240 GB primary shard, 480 GB total Document Replication\nSegment Replication\nPercent difference\nDocument Replication\nSegment Replication\nPercent difference\nStore size\n85.2781\n91.2268\nN/A\n515.726\n558.039\nN/A\nIndex throughput (number of requests per second)\nMinimum\n148,134\n185,092\n24.95%\n100,140\n168,335\n68.10%\nMedian\n160,110\n189,799\n18.54%\n106,642\n170,573\n59.95%\nMaximum\n175,196\n190,757\n8.88%\n108,583\n172,507\n58.87%\nError rate\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nAs the size of the workload increases, the benefits of segment replication are amplified because the replicas are not required to index the larger dataset. In general, segment replication leads to higher throughput at lower resource costs than document replication in all cluster configurations, not accounting for replication lag.\nIncreasing the number of primary shards\nThe following table lists benchmarking results for the nyc_taxi dataset for 40 and 100 primary shards. 40 primary shards, 1 replica 100 primary shards, 1 replica Document Replication\nSegment Replication\nPercent difference\nDocument Replication\nSegment Replication\nPercent difference\nIndex throughput (number of requests per second)\nMinimum\n148,134\n185,092\n24.95%\n151,404\n167,391\n9.55%\nMedian\n160,110\n189,799\n18.54%\n154,796\n172,995\n10.52%\nMaximum\n175,196\n190,757\n8.88%\n166,173\n174,655\n4.86%\nError rate\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nAs the number of primary shards increases, the benefits of segment replication over document replication decrease. While segment replication is still beneficial with a larger number of primary shards, the difference in performance becomes less pronounced because there are more primary shards per node that must copy segment files across the cluster.\nIncreasing the number of replicas\nThe following table lists benchmarking results for the stackoverflow dataset for 1 and 9 replicas. 10 primary shards, 1 replica 10 primary shards, 9 replicas Document Replication\nSegment Replication\nPercent difference\nDocument Replication\nSegment Replication\nPercent difference\nIndex throughput (number of requests per second)\nMedian\n72,598.10\n90,776.10\n25.04%\n16,537.00\n14,429.80\n&minus;12.74%\nMaximum\n86,130.80\n96,471.00\n12.01%\n21,472.40\n38,235.00\n78.07%\nCPU usage (%)\np50\n17\n18.857\n10.92%\n69.857\n8.833\n&minus;87.36%\np90\n76\n82.133\n8.07%\n99\n86.4\n&minus;12.73%\np99\n100\n100\n0%\n100\n100\n0%\np100\n100\n100\n0%\n100\n100\n0%\nMemory usage (%)\np50\n35\n23\n&minus;34.29%\n42\n40\n&minus;4.76%\np90\n59\n57\n&minus;3.39%\n59\n63\n6.78%\np99\n69\n61\n&minus;11.59%\n66\n70\n6.06%\np100\n72\n62\n&minus;13.89%\n69\n72\n4.35%\nError rate\n0.00%\n0.00%\n0.00%\n0.00%\n2.30%\n2.30%\nAs the number of replicas increases, the amount of time required for primary shards to keep replicas up to date (known as the replication lag) also increases. This is because segment replication copies the segment files directly from primary shards to replicas.\nThe benchmarking results show a non-zero error rate as the number of replicas increases. The error rate indicates that the segment replication backpressure mechanism is initiated when replicas cannot keep up with the primary shard. However, the error rate is offset by the significant CPU and memory gains that segment replication provides.\nOther considerations\nThe following considerations apply to segment replication in the 2.7 release: Read-after-write guarantees: The wait_until refresh policy is not compatible with segment replication. If you use the wait_until refresh policy while ingesting documents, you‚Äôll get a response only after the primary node has refreshed and made those documents searchable. Replica shards will respond only after having written to their local translog. We are exploring other mechanisms for providing read-after-write guarantees. For more information, see the corresponding GitHub issue. System indexes will continue to use document replication internally until read-after-write guarantees are available. In this case, document replication does not hinder the overall performance because there are few system indexes. Enabling segment replication for an existing index requires reindexing. Rolling upgrades are not yet supported. Upgrading to new versions of OpenSearch requires a full cluster restart.\nWhat‚Äôs next?\nThe OpenSearch 2.7 release provides a peer-to-peer (node-to-node) implementation of segment replication. With this release, you can choose to use either document replication or segment replication based on your cluster configuration and workloads. In the coming releases, OpenSearch remote storage, our next-generation storage architecture, will use segment replication as the single replication mechanism. Segment-replication-enabled remote storage will eliminate network bottlenecks on primary shards for clusters with higher replica counts. We are also exploring a chain replication strategy to further alleviate the load on primary shards. For better usability, we are planning to integrate segment replication with OpenSearch Dashboards so that you can enable the feature using the Dashboards UI. We are also planning to provide quick support for rolling upgrades, making it easier to migrate to new versions without downtime.",
    "keywords": [
      "technical-post"
    ],
    "type": "News"
  },
  {
    "url": "/authors/aalkouz/",
    "title": "Aalkouz",
    "content": "Anas Alkouz is a software development manager at AWS working on OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/abandeji/",
    "title": "Abandeji",
    "content": "Arpit Bandejiya is a Software Engineer working on OpenSearch. He is interested in solving problems related to large-scale systems and is an active contributor to the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/achiojha/",
    "title": "Achiojha",
    "content": "Achit Ojha is a Software engineer working on search services at Amazon Web Services. His primary interests are distributed systems, performance and data processing. He is an active contributor to OpenSearch Benchmark.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/aditjind/",
    "title": "Aditjind",
    "content": "Aditya Jindal is a Software engineer working on search services at Amazon Web Services. His primary interests are distributed systems, deep learning and networking. He is an active contributor to Open Distro for Elasticsearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/adnapibar/",
    "title": "Adnapibar",
    "content": "Rabi Panda is a Software Engineer at AWS working on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/alicejw/",
    "title": "Alicejw",
    "content": "Alice Williams is an AWS Technical Writer working on the OpenSearch project documentation.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/alolitas/",
    "title": "Alolitas",
    "content": "Alolita Sharma is a Principal Technologist at AWS where she leads open source observability engagement and development of OpenTelemetry, Prometheus components focused on metrics and log based observability. Alolita also serves as a member of the OpenTelemetry Governance Committee and as a board director of the Unicode Consortium. She contributes to open standards on the Unicode Technical Committee and W3C. She has served on the boards of the OSI and SFLC.in. Alolita has built and led engineering teams at Wikipedia, Twitter, PayPal and IBM. Two decades of doing open source continue to inspire her.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/amistrn/",
    "title": "Amistrn",
    "content": "Amitai Stern is a software engineer &amp; tech lead of the Log Analytics team at Logz.io. Amitai works on Big data, SaaS projects, leading feature development from design to production monitoring. As an artist, he leverages his creative side to find innovative solutions to engineering problems.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/amitgalitz/",
    "title": "Amitgalitz",
    "content": "Amit Galitzky is a software engineer at Amazon Web Services. He focuses mostly on the Anomaly Detection plugin for OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/anandhi/",
    "title": "Anandhi",
    "content": "Anandhi Bumstead is the Director of Software Development at the OpenSearch Project",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ananzh/",
    "title": "Ananzh",
    "content": "Anan is an engineer at AWS working on the OpenSearch Dashboards and OpenSearch-js project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/andhopp/",
    "title": "Andhopp",
    "content": "Andrew Hopp is the Senior Technical Product Manager for OpenSearch and OpenSearch Dashboards.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/andrross/",
    "title": "Andrross",
    "content": "Andrew Ross is a Senior Software Developer at AWS working on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/anurag-gup/",
    "title": "Anurag_gup",
    "content": "Anurag Gupta is a maintainer of the Fluentd and Fluent Bit project as well as a co-founder of Calyptia. Previously he has worked at Elastic, driving cloud products and helping create the Elastic Kubernetes Operator. He has also worked at Treasure Data heading enterprise open source with Fluentd and Microsoft Azure Log Analytics working on Observability as a cloud provider.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/aoguan/",
    "title": "Aoguan",
    "content": "Aozixuan Guan is a Software Engineer at Amazon Web Services working on the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/apasun/",
    "title": "Apasun",
    "content": "Dr. Aparna Sundar is a senior UX researcher at AWS covering all areas of the OpenSearch UI. She has over 20 years of experience in the field of research and design and actively publishes in the area of cognitive science.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/armarble/",
    "title": "Armarble",
    "content": "Ariana Marble is an AWS technical writer focusing on OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/arubin/",
    "title": "Arubin",
    "content": "Anton Rubin is a technical support engineer at Eliatra. Having worked on the front lines of Symantec and other large companies, Anton has developed a passion for security in distributed systems and search engines. Combined with strong desire to share knowledge, he is eager to make security simple to follow and implement across any OpenSearch environment",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ashisagr/",
    "title": "Ashisagr",
    "content": "Ashish Agrawal is a software engineer at Amazon Web Services working on OpenSearch. Ashish has worked on OpenSearch Alerting and Notifications.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ashwinkumar12345/",
    "title": "Ashwinkumar12345",
    "content": "Ashwin Kumar is a Senior Technical Writer at AWS. Ashwin focuses on OpenSearch and OpenSearch Dashboards.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ashwinpc/",
    "title": "Ashwinpc",
    "content": "Ashwin Pc is a Frontend Enginner at AWS. Ashwin focuses on OpenSearch Dashboards Visualizations.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ausgeraci/",
    "title": "Ausgeraci",
    "content": "Austin Geraci is a subject matter expert in F5 Networks Technology, and has worked in the ADC space for 20 years. When he‚Äôs not working with &amp; evangelizing F5‚Äôs cutting edge technology, you can find him on the squash courts, going for a ride around Lady Bird Lake, or listening to some live music in ATX.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/billbeckler/",
    "title": "Billbeckler",
    "content": "Bill Beckler is a Sr. Software Manager at the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/bpavani/",
    "title": "Bpavani",
    "content": "Pavani Baddepudi is a senior product manager working on search services at AWS. Her interests are distributed systems, networking and security.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/bukhtawa/",
    "title": "Bukhtawa",
    "content": "Bukhtawar is a Senior Software Engineer working on Amazon OpenSearch Service. He is interested in distributed and autonomous systems and is an active contributor to the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/carlmeadows/",
    "title": "Carlmeadows",
    "content": "Carl Meadows is Director, Product Management at AWS",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/csw/",
    "title": "Csw",
    "content": "Chris Swierczewski is an applied scientist at AWS. He enjoys hiking and backpacking with his wife and their dogs, River and Bosco.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/daichen/",
    "title": "Daichen",
    "content": "Chen Dai is a Senior Software Engineer at Amazon Web Services, with a focus on OpenSearch SQL and PPL project. He is passionate about distributed computation and database internals.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/daryllswager/",
    "title": "Daryllswager",
    "content": "Daryll Swager is the Principal Product Marketer for the OpenSearch Project; his hobbies include farming, fishing, and developing his skills as a chef.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/davidlago/",
    "title": "Davidlago",
    "content": "Dave Lago is an AWS Software Development Manager working on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/dblock/",
    "title": "Dblock",
    "content": "Daniel (dB.) Doubrovkine is a Principal Engineer at AWS, focusing on OpenSearch plugins.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ddpowers/",
    "title": "Ddpowers",
    "content": "David Powers is a Software Development Engineer at AWS working in search services. He is a maintainer on the Data Prepper project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/dhrubo/",
    "title": "Dhrubo",
    "content": "Dhrubo Saha is a machine learning engineer at Amazon Web Services (AWS) interested in machine learning algorithms, large language models, and distributed systems.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/dhruvdas/",
    "title": "Dhruvdas",
    "content": "Dhrubajyoti Das is an Engineering Manager working on OpenSearch at Amazon Web Services.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/dlv/",
    "title": "Dlv",
    "content": "David is a senior software engineer working on observability in OpenSearch at Amazon Web Services.\nHe is a maintainer on the Data Prepper project. Prior to working at Amazon, he was the CTO at\nAllogy Interactive - a start-up creating mobile-learning solutions for healthcare.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/dtaivpp/",
    "title": "Dtaivpp",
    "content": "David Tippett is a Senior Developer Advocate at AWS, focusing on open source OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/elifish/",
    "title": "Elifish",
    "content": "Eli Fisher is a Senior Technical Product Manager at AWS. Eli focuses on OpenSearch and OpenSearch Dashboards.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/eugenesk/",
    "title": "Eugenesk",
    "content": "Eugene Lee is a Software Development Engineer at Amazon Web Services working on OpenSearch focusing on Observability.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ftisiot/",
    "title": "Ftisiot",
    "content": "Francesco Tisiot comes from Verona, Italy and works as a Senior Developer Advocate at Aiven. With his many years of experience as a data engineer, he has stories to tell and advice for data-wranglers everywhere. Francesco loves sharing knowledge with others as a speaker and writer, and is on a mission to defend the world from bad Italian food!",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/gbh/",
    "title": "Gbh",
    "content": "Bharathwaj is a Software Engineer working on OpenSearch. He is interested in distributed systems and is an active contributor to OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/gbinlong/",
    "title": "Gbinlong",
    "content": "Binlong Gao is an AWS software development engineer working on the OpenSearch Project. He is a previous contributor to the Elasticsearch project and is experienced in operating and optimizing Elasticsearch. Currently, he focuses on the Index Management plugin and other OpenSearch plugins.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/gopalak/",
    "title": "Gopalak",
    "content": "Gopala Krishna is a Senior Software Engineer working on Search Services at Amazon Web Services. He is passionate about solving problems in the large-scale distributed systems‚Äô space. He is an active contributor to OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/gteu/",
    "title": "Gteu",
    "content": "Shunsuke Goto is a Prototyping Engineer working at AWS. He works closely with customers to build their prototypes and also helps customers to build search and streaming systems.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/handalm/",
    "title": "Handalm",
    "content": "Marc Handalian is a senior software development engineer at AWS working on OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/handler/",
    "title": "Handler",
    "content": "Jon Handler is a principal solutions architect at AWS based in Palo Alto, CA. Jon works on the OpenSearch Project, providing help and guidance to a broad range of customers who have search workloads that they want to move to the AWS Cloud. Prior to joining AWS, Jon‚Äôs career as a software developer included four years of coding a large-scale eCommerce search engine. Jon holds a Bachelor of Arts from the University of Pennsylvania and a Master of Science and PhD in computer science and artificial intelligence from Northwestern University.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/hdhalter/",
    "title": "Hdhalter",
    "content": "Heather Halter is the Documentation Manager for OpenSearch Project. She is passionate about creating great content that contributes to a great user experience.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/henkle/",
    "title": "Henkle",
    "content": "Charlotte is a Senior Manager at AWS working on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/hvamsi/",
    "title": "Hvamsi",
    "content": "Harsha Vamsi Kalluri is a Software Development Engineer at AWS working on the OpenSearch Clients team.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ihailong/",
    "title": "Ihailong",
    "content": "Hailong Cui is an AWS software development engineer working on the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/jadhanir/",
    "title": "Jadhanir",
    "content": "Anirudha Jadhav is an Engineering Manager at Amazon Web Services working on OpenSearch focusing on Query Infrastructure (SQL/PPL), Observability, Reporting, Notifications, and Notebooks.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/jaindhir/",
    "title": "Jaindhir",
    "content": "Dhiraj is a Software Development Engineer at Amazon Web Services. He is a Kubernetes enthusiast and his primary interests includes distributed systems and blockchain. Outside work he likes sketching and playing badminton.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/jamesmcintyre/",
    "title": "Jamesmcintyre",
    "content": "James McIntyre is a senior product marketing manager with AWS serving the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/jdbright/",
    "title": "Jdbright",
    "content": "Joshua Bright is the senior technical product manager for OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/jmazane/",
    "title": "Jmazane",
    "content": "Jack Mazanec is a software engineer working on OpenSearch plugins. His primary interests include machine learning and distributed systems. Outside of work, he enjoys skiing and watching sports.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/joshtok/",
    "title": "Joshtok",
    "content": "Joshua Tokle is a senior software engineer at Amazon and has worked on AWS Glue and Kinesis Analytics. Prior to joining Amazon Joshua worked as a data scientist and statistician with a specialization in record linkage algorithms. His current interests include statistical and machine learning algorithms on streams. Joshua is a contributor to the Random Cut Forest library.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/joydees/",
    "title": "Joydees",
    "content": "Joydeep Sinha is a Senior Software Engineer working on search services at Amazon Web Services. He is interested in distributed and autonomous systems. He is an active contributor to OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/junqui/",
    "title": "Junqui",
    "content": "Junqiu Lei is a software development engineer at AWS specializing in the development of OpenSearch map visualizations and backend map services.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kaituo/",
    "title": "Kaituo",
    "content": "Kaituo Li is an engineer in Amazon OpenSearch Service. He has worked on distributed systems, applied machine learning, monitoring, and database storage in Amazon. Before Amazon, Kaituo was a PhD student in Computer Science at University of Massachusetts, Amherst. He likes reading and sports.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/karanas/",
    "title": "Karanas",
    "content": "Saikumar Karanam is a Software engineer working on Search Services at Amazon Web Services. His interests are distributed systems, networking and machine learning. He is an active contributor to Open Distro for Elasticsearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kazabdu/",
    "title": "Kazabdu",
    "content": "Owais Kazi is a Software Engineer at AWS, focusing on OpenSearch and OpenSearch plugins.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kbalaji/",
    "title": "Kbalaji",
    "content": "Balaji Kannan is an Engineering Manager working on search services at Amazon Web Services. He spent most of his career building vertical search engine and big data platforms.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/khushbr/",
    "title": "Khushbr",
    "content": "Khushboo Rajput is a Software engineer working on search services at Amazon Web Services. Her primary interests are distributed systems and search. She is an active contributor to Open Distro for Elasticsearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kkondaka/",
    "title": "Kkondaka",
    "content": "Krishna is a senior software engineer working on observability in OpenSearch at Amazon Web Services. He is a contributor to the Data Prepper project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kkumargu/",
    "title": "Kkumargu",
    "content": "Karthik Kumarguru is a Software Engineer working on search services at Amazon Web Services. His primary interests are distributed systems and networking. He is an active contributor to Open Distro for Elasticsearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kolchfa/",
    "title": "Kolchfa",
    "content": "Fanit Kolchina is a technical writer at AWS focusing on OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kotwanikunal/",
    "title": "Kotwanikunal",
    "content": "Kunal Kotwani is a Software Engineer at AWS working on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/krisfreedain/",
    "title": "Krisfreedain",
    "content": "Kris Freedain is the OpenSearch Project Community Manager; his hobbies include gardening, garage gym powerlifting, and meditation.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kroosh/",
    "title": "Kroosh",
    "content": "Kroosh is a Sr. UX Designer at Amazon Web Services working on the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kschnitter/",
    "title": "Kschnitter",
    "content": "Karsten is a Software Development Expert at SAP. He works on various logging services on the SAP Business Technology Platform. He focusses on observability and integrations topics.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kuberkaul/",
    "title": "Kuberkaul",
    "content": "Kuber Kaul is a Distinguished Software Engineer in the Engineering Productivity team at Dow Jones. His primary interests are automation, continuous delivery pipelines, docker, k8s, service meshes, observability, large scale distributed systems, and cloud infrastructure. Kuber holds a Master of Science in Computer Science from Columbia University.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kvngar/",
    "title": "Kvngar",
    "content": "Kevin Garcia is a UX Manager at Amazon Web Services working on the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/kyledvs/",
    "title": "Kyledvs",
    "content": "Kyle Davis is the Senior Developer Advocate for Bottlerocket and Amazon Linux. Previously, Kyle was the Senior Developer Advocate for OpenSearch and Open Distro for Elasticsearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/laijiang/",
    "title": "Laijiang",
    "content": "Lai Jiang is a software engineer working on machine learning and Elasticsearch at Amazon Web Services. His primary interests are algorithms and math. He is an active contributor to Open Distro for Elasticsearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/laneholl/",
    "title": "Laneholl",
    "content": "Lane Holloway is a Software Development Engineer at Amazon Web Services and enjoys discussing the two hardest things in Computer Science: naming, cache invalidation, and off-by-one errors.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/lijshu/",
    "title": "Lijshu",
    "content": "Joshua Li is an AWS software development engineer working on the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/lioperry/",
    "title": "Lioperry",
    "content": "Lior Perry is an AWS software development engineer focusing on OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/liutaoaz/",
    "title": "Liutaoaz",
    "content": "Tao Liu is a Software Engineer at Amazon Web Services, working on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/lornajane/",
    "title": "Lornajane",
    "content": "Lorna Mitchell is Head of Developer Relations at Aiven as well as a software developer, published author, and open source enthusiast.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/lxuesong/",
    "title": "Lxuesong",
    "content": "Xuesong Luo is an AWS engineering manager working on the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/macrakis/",
    "title": "Macrakis",
    "content": "Stavros Macrakis is the senior technical product manager for OpenSearch focusing on document and e-commerce search. He has worked on search for almost 20 years and is passionate about search relevance.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/maharup/",
    "title": "Maharup",
    "content": "Rupal Mahajan is an AWS software development engineer working on the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/matheusnogueira/",
    "title": "Matheusnogueira",
    "content": "Matheus Nogueira is a senior software engineer at Kubeshop and is one the maintainers of Tracetest. Prior to joining the Tracetest team, Matheus helped companies to improve automated tests and tooling to improve developer experience.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/mnkugler/",
    "title": "Mnkugler",
    "content": "Monica Kugler is a Senior Technical Product Manager at AWS working on OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/movvaam/",
    "title": "Movvaam",
    "content": "Ajay is a Software Engineer working on OpenSearch at Amazon Web Services. He is passionate about solving problems in large-scale distributed systems.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/mqureshi/",
    "title": "Mqureshi",
    "content": "Mohammad Qureshi is a software engineer at Amazon Web Services working on OpenSearch. Mohammad has worked on OpenSearch Alerting, Index State Management, Rollups, and Notifications.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/mshyani/",
    "title": "Mshyani",
    "content": "Milind Shyani is an applied scientist at Amazon Web Services working on machine learning algorithms and large language models.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/mtimmerm/",
    "title": "Mtimmerm",
    "content": "Matt Timmermans was a Senior Principal Software Engineer at Tripadvisor, integrating OpenSearch into a low-latency CQRS backend. He currently is a Principle Engineer at Amazon.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/naarcha/",
    "title": "Naarcha",
    "content": "Nate Archer is a Technical Writer at AWS focusing on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/nataliebroman/",
    "title": "Nataliebroman",
    "content": "Natalie Broman is an Event Program Manager at AWS focusing on the OpenSearch Project and the OpenSource community.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/nateboot/",
    "title": "Nateboot",
    "content": "At work, I‚Äôm a Developer Advocate for OpenSearch. At home, I‚Äôm a devoted husband and father. No matter where I am, I am at your service.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/navneev/",
    "title": "Navneev",
    "content": "Navneet Verma is a senior software engineer at AWS working on geospatial and vector search in OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/nbower/",
    "title": "Nbower",
    "content": "Nate Bower is the Senior Technical Editor for OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/nknize/",
    "title": "Nknize",
    "content": "Nick Knize is a Principal Engineer at AWS, and a committer and PMC member for the Apache Lucene project. He is a core maintainer for OpenSearch and OpenSearch Dashboards.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/nmishra/",
    "title": "Nmishra",
    "content": "Nina Mishra is a scientist at Amazon interested in machine learning algorithms and healthcare.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/nsifmoh/",
    "title": "Nsifmoh",
    "content": "Asif is a Software Development Engineer at Amazon Web Services working on Observability. He is a maintainer of the Data Prepper project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/oeyh/",
    "title": "Oeyh",
    "content": "Hai Yan is a Software Development Engineer at AWS working in search services. He is a contributor to the Data Prepper project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ohltyler/",
    "title": "Ohltyler",
    "content": "Tyler Ohlsen is a Software Engineer at AWS, focusing on anomaly detection in OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/pakshirajan/",
    "title": "Pakshirajan",
    "content": "Pakshi Rajan is the Vice President of Products at Titaniam. Titaniam uses data-in-use encryption to breach-proof enterprise search platforms.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/pallp/",
    "title": "Pallp",
    "content": "Pallavi Priyadarshini is an Engineering Manager at Amazon Web Services, leading the design and development of high-performing and at-scale analytics technologies.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/pattijuric/",
    "title": "Pattijuric",
    "content": "Patti Juric is a senior marketing content manager for the OpenSearch Project; her hobbies include watching her kids play soccer &amp; volleyball, reading, and enjoying life.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/paulaubrey/",
    "title": "Paulaubrey",
    "content": "Paul Aubrey is an VP of Product Management at Instaclustr and Product Manager for OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/penghuo/",
    "title": "Penghuo",
    "content": "Peng Huo is a Senior Engineer at Amazon Web Services working on OpenSearch focusing on Query and Observability.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/peternied/",
    "title": "Peternied",
    "content": "Peter Nied is a software engineer at Amazon Web Services focusing on OpenSearch security.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/plewis/",
    "title": "Plewis",
    "content": "Phil Lewis is CTO and co-founder of Pureinsights. He is a search industry veteran with extensive experience in the design and implementation of open source search projects.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/pprost/",
    "title": "Pprost",
    "content": "Pierrick Prost is an observability and infrastructure engineer. He tries to be the unicorn with three heads but it‚Äôs not easy. He loves mountains, bike and beer.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/prashagr/",
    "title": "Prashagr",
    "content": "Prashant is a Search Specialist Solutions Architect with Amazon OpenSearch Service. He works closely with team members to help customers migrate their workloads to the cloud. Before joining AWS, he helped various customers use OpenSearch for their search and analytics use cases. Outside of work he enjoys EAT -&gt; TRAVEL -&gt; REPEAT.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/praveensameneni/",
    "title": "Praveensameneni",
    "content": "Praveen Sameneni is an Engineering Manager at AWS. Praveen is focused on plugin development and all things extensible for OpenSearch",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/prudhvigodithi/",
    "title": "Prudhvigodithi",
    "content": "Prudhvi Godithi is an AWS System Engineer working on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/rajtaori/",
    "title": "Rajtaori",
    "content": "Rajiv Taori is Principal Product Manager - Technical at Amazon Web Services working on OpenSearch with a focus on observability, security, data ingestion, and OpenTelemetry",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ramaran/",
    "title": "Ramaran",
    "content": "Ranjith is an Engineering Manager working on OpenSearch at Amazon Web Services.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/reta/",
    "title": "Reta",
    "content": "Andriy Redko is a seasoned software developer with a great passion to code, extensively working with JVM platform using Java, Groovy, Scala as well as other languages and technologies (Ruby, Grails, Play!, Akka, MySQL, PostreSQL, MongoDB, Redis, JUnit, etc.).",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/rohin/",
    "title": "Rohin",
    "content": "Rohin Bhargava is a Sr. Product Manager at Amazon Web Services for Search Services. Rohin‚Äôs focus is on Cross-domain experience for OpenSearch, search improvements and performance.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ruizhen/",
    "title": "Ruizhen",
    "content": "Ruizhen Guo is a Software engineer working on search services at Amazon Web Services. His primary interests are distributed systems and low level database primitives. He is an active contributor to Open Distro for Elasticsearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ryanbogan/",
    "title": "Ryanbogan",
    "content": "Ryan Bogan is an Engineer at AWS.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ryanparas/",
    "title": "Ryanparas",
    "content": "Director of Observability at WorldTech IT",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/sai/",
    "title": "Sai",
    "content": "Saikumar Karanam is a Software engineer working on Search Services at Amazon Web Services. His interests are distributed systems, networking and machine learning. He is an active contributor to OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/satnandi/",
    "title": "Satnandi",
    "content": "Satish Nandi is a senior technical product manager for OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/sayaligaikawad/",
    "title": "Sayaligaikawad",
    "content": "Sayali Gaikawad is a Software Engineer at Amazon Web Services working on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/sbayer55/",
    "title": "Sbayer55",
    "content": "Steven Bayer is a Software Development Engineer at AWS working in search services. He is a maintainer on the Data Prepper project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/sdharms/",
    "title": "Sdharms",
    "content": "Dharmesh is Software Engineer at OpenSearch. His primary interests are distributed systems and performance.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/seanzheng/",
    "title": "Seanzheng",
    "content": "Sean Zheng is an engineering manager at Amazon Web Services working on OpenSearch, with a focus on machine-learning-based plugins, including Anomaly Detection, k-NN, and ML Commons.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/setiah/",
    "title": "Setiah",
    "content": "Himanshu Setia is a Senior Software Engineer at Amazon Web Services. He is passionate about distributed systems and solving problems at scale. He is an active contributor to OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/shahar/",
    "title": "Shahar",
    "content": "Shahar Shaked - is the VP R&amp;D and Co-Founder at Opster.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/shivamdhar/",
    "title": "Shivamdhar",
    "content": "Shivam Dhar is an AWS Software Engineer working on OpenSearch and geospatial (UI) visualizations.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/shuttie/",
    "title": "Shuttie",
    "content": "An independent search engineer, working on relevancy, personalization and recommendations. A pragmatic fan of open-source software, functional programming, learn-to-rank models and performance tuning.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/sisurab/",
    "title": "Sisurab",
    "content": "Saurabh is a Senior Software Engineer working on OpenSearch at Amazon Web Services. He is passionate about solving problems in the large-scale distributed systems. He is an active contributor to OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/skkosuri-amzn/",
    "title": "Skkosuri Amzn",
    "content": "Sriram Kosuri is a Senior Software Engineer at AWS. Sriram focuses on OpenSearch alerting, security and distributed systems.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/smortex/",
    "title": "Smortex",
    "content": "Romain Tarti√®re is a Hacker, FLOSS Mercenary, FreeBSD developer (romain@), BOFH; he loves stuff from the 70‚Äôs (e.g. UNIX, Japanese hard-rock) and Ruby.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/snistala/",
    "title": "Snistala",
    "content": "Surya is a Software Engineer working on OpenSearch. His primary interests are distributed systems and cloud computing.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ssayakci/",
    "title": "Ssayakci",
    "content": "Soner Sayakci is working as Technical Specialist at Shopware AG and maintains the OpenSearch PHP SDK amongst other projects.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/sudipto/",
    "title": "Sudipto",
    "content": "Sudipto Guha is a principal scientist at Amazon Web Services where he studies the design and implementation of a wide range of computational systems: from resource-constrained devices such as sensors, to massively parallel and distributed systems. Sudipto is the prime contributor to the Random Cut Forest library.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/suzhou/",
    "title": "Suzhou",
    "content": "Zhou Su is an AWS frontend engineer working on the Index Management plugin.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/tetianaftv/",
    "title": "Tetianaftv",
    "content": "Tetiana Fydorenchyk is a Director of Global Product Marketing at Virtuozzo, an author of technical and business articles, speaker, and developers community enthusiast.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/theotr/",
    "title": "Theotr",
    "content": "Theo Truong is a Software Engineer at AWS working on OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/tkharban/",
    "title": "Tkharban",
    "content": "Tushar is Software Engineer at OpenSearch. He is an active contributor to OpenSearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/tlfeng/",
    "title": "Tlfeng",
    "content": "Tianli Feng is a Software Development Engineer at AWS working on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/tyarong/",
    "title": "Tyarong",
    "content": "Kristen Tian is a Software Engineer at Amazon Web Services working on the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/tylgry/",
    "title": "Tylgry",
    "content": "Taylor Gray is a Software Development Engineer at AWS with a focus on OpenSearch and observability. He is a maintainer of the Data Prepper project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/uselmh/",
    "title": "Uselmh",
    "content": "Hannah Uselman is a social media specialist for the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/vachshah/",
    "title": "Vachshah",
    "content": "Vacha Shah is a Software Engineer at AWS working on the OpenSearch project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/vagimeli/",
    "title": "Vagimeli",
    "content": "Melissa Vagi is an AWS Technical Writer working on the OpenSearch project documentation.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/vajsatya/",
    "title": "Vajsatya",
    "content": "Satya Vajrapu is a DevOps Consultant at Amazon Web Services (AWS). He works with AWS customers to help design and develop various practices and tools in the DevOps toolchain.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/vamshin/",
    "title": "Vamshin",
    "content": "Vamshi Vijay Nakkirtha is a Software Engineering Manager working on the OpenSearch Project and Amazon OpenSearch Service. His primary interests include distributed systems. He is an active contributor to various plugins, like k-NN, GeoSpatial, and dashboard-maps.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/vemsarat/",
    "title": "Vemsarat",
    "content": "Sarat Vemulapalli is a Software Engineer at AWS, focusing on OpenSearch and OpenSearch plugins.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/vigyas/",
    "title": "Vigyas",
    "content": "Vigya Sharma is a Senior Software Engineer at Amazon Web Services. His projects focus on providing a managed service experience to Amazon Elasticsearch Service customers. Vigya is passionate about distributed systems and likes to solve problems around large scale systems. Vigya holds a Masters degree in Computer Science from IIT Delhi.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/vijayanb/",
    "title": "Vijayanb",
    "content": "Vijayan Balasubramanian is a software engineer working on OpenSearch k-NN, geospatial data, and maps. His interests include distributed systems, big data, data mining, and information retrieval.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/virajph/",
    "title": "Virajph",
    "content": "Viraj Phanse is a Senior Product Manager at Amazon Web Services for Search Services.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/wonglam/",
    "title": "Wonglam",
    "content": "Lin Wang is an AWS web development engineer working on the OpenSearch Project. His interests include machine learning and data visualization.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/x-adithya-chandra/",
    "title": "X_adithya Chandra",
    "content": "Adithya Chandra is a Senior Software Engineer working on search services at Amazon Web Services. He actively presents his work on root cause analysis and performance engineering most recently at Devoxx and is also an active contributor to Open Distro for Elasticsearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/x-partha-kanuparthy/",
    "title": "X_partha Kanuparthy",
    "content": "Partha Kanuparthy is a Principal Engineer working on database services at Amazon Web Services. His work spans distributed systems and databases, networking and machine learning. He actively contributes to open source software, and most recently, Open Distro for Elasticsearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/x-sid-narayan/",
    "title": "X_sid_narayan",
    "content": "Sid Narayan is a Software engineer working on search services at Amazon Web Services. His primary interests are distributed systems and observability frameworks. He is an active contributor to Open Distro for Elasticsearch.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/xtansia/",
    "title": "Xtansia",
    "content": "Thomas Farr is a Software Development Engineer at AWS working on the OpenSearch Clients team, with a primary focus on the.NET and Rust clients.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/ylwu/",
    "title": "Ylwu",
    "content": "Yaliang Wu is a software engineer at Amazon Web Services. He primarily works on OpenSearch machine learning plugins (anomaly-detection/ml-commons). He is passionate about distributed systems, machine learning, and big data.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/yych/",
    "title": "Yych",
    "content": "Charlie Yang is an AWS engineering manager working on the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/zengyan/",
    "title": "Zengyan",
    "content": "Yan Zeng is a Senior Software Engineer at Amazon Web Services working on the OpenSearch Project.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/zhujiaxiang/",
    "title": "Zhujiaxiang",
    "content": "Jiaxiang (Peter) Zhu is a System Engineer at Amazon Web Services. He mainly works on infrastructure, automation, and release of the OpenSearch Project. He likes ‚Äô80s books, ‚Äô70s music, and ‚Äô60s movies.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/authors/zmre/",
    "title": "Zmre",
    "content": "Patrick Walsh is the CEO of IronCore Labs, a data privacy platform for protecting data that lives in the cloud. The platform secures data stored in OpenSearch and other services without sacrificing the ability to use that data. Patrick has more than 20 years of experience building security products and enterprise SaaS solutions. Prior to IronCore Labs, Patrick ran an engineering division in the Oracle Cloud.",
    "keywords": [

    ],
    "type": "Authors"
  },
  {
    "url": "/events/2021-early-july/",
    "title": "OpenSearch Community Meeting - Early July",
    "content": "Join us for our our biweekly online community meeting. Agenda TBD.\nWe have a collaborative agenda on HackMD. Feel free to comment on the agenda before the meeting if you want to add an item or have a question. During the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-early-june/",
    "title": "OpenSearch Community Meeting - Early June",
    "content": "Join us for our our biweekly online community meeting. Agenda:\nPublic Roadmap\nNew Branding\nGraph on OpenSearch / YangDB\nQ&amp;A New: This meeting we‚Äôre going to try out a collaborative agenda on HackMD. Feel free to comment on the agenda before the meeting if you want to add an item or have a question. During the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-late-july/",
    "title": "OpenSearch Community Meeting - Late July",
    "content": "Join us for our our biweekly online community meeting.\nObservability Features in OpenSearch\nWe have a collaborative agenda on HackMD. Feel free to comment on the agenda before the meeting if you want to add an item or have a question. During the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-late-june/",
    "title": "OpenSearch Community Meeting - Late June",
    "content": "Join us for our our biweekly online community meeting.\nAgenda:\nExploring &amp; Visualizing Logs Using OpenSearch and Apache Superset\n(more items TBA)\nWe have a collaborative agenda on HackMD. Feel free to comment on the agenda before the meeting if you want to add an item or have a question. During the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-may-late/",
    "title": "OpenSearch Community Meeting - Late May",
    "content": "Join us for our our biweekly online community meeting. Agenda to be determined.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-mid-june/",
    "title": "OpenSearch Community Meeting - Mid June",
    "content": "Join us for our our biweekly online community meeting. Agenda:\nRelease Candidate -&gt; GA\nBeats &amp; Logstash with OpenSearch\nQ&amp;A\nWe have a collaborative agenda on HackMD. Feel free to comment on the agenda before the meeting if you want to add an item or have a question. During the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-opensearch_community_meeting_dec/",
    "title": "OpenSearch Community Meeting - Dec",
    "content": "Join us for our our biweekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-opensearch_community_meeting_early_august/",
    "title": "OpenSearch Community Meeting - Early August",
    "content": "Join us for our our biweekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-opensearch_community_meeting_early_nov/",
    "title": "OpenSearch Community Meeting - Early Nov",
    "content": "Join us for our our biweekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-opensearch_community_meeting_early_oct/",
    "title": "OpenSearch Community Meeting - Early Oct",
    "content": "Join us for our our biweekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-opensearch_community_meeting_early_sept/",
    "title": "OpenSearch Community Meeting - Early Sept",
    "content": "Join us for our our biweekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-opensearch_community_meeting_late_august/",
    "title": "OpenSearch Community Meeting - Late August",
    "content": "Join us for our our biweekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-opensearch_community_meeting_late_nov/",
    "title": "OpenSearch Community Meeting - Late Nov",
    "content": "Join us for our our biweekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-opensearch_community_meeting_late_oct/",
    "title": "OpenSearch Community Meeting - Late Oct",
    "content": "Join us for our our biweekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2021-opensearch_community_meeting_late_sept/",
    "title": "OpenSearch Community Meeting - Late Sept",
    "content": "Join us for our our biweekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0104/",
    "title": "OpenSearch Community Meeting - 2022-01-04",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0111/",
    "title": "OpenSearch Community Meeting - 2022-01-11 - open office hour",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0118/",
    "title": "OpenSearch Community Meeting - 2022-01-18",
    "content": "Join us for our weekly online community meeting. (meeting moved from 2022-01-17 to accommodate MLK Day)\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0124/",
    "title": "OpenSearch Community Meeting - 2022-01-24 - open office hour",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0201/",
    "title": "OpenSearch Community Meeting - 2022-02-01",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0208/",
    "title": "OpenSearch Community Meeting - 2022-02-08 - open office hour",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0214/",
    "title": "OpenSearch Community Meeting - 2022-02-14",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0221/",
    "title": "OpenSearch Community Meeting - 2022-02-21 - open office hour",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0301/",
    "title": "OpenSearch Community Meeting - 2022-03-01",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0308/",
    "title": "OpenSearch Community Meeting - 2022-03-08 - open office hour",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0314/",
    "title": "OpenSearch Community Meeting - 2022-03-14",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0321/",
    "title": "OpenSearch Community Meeting - 2022-03-21 - open office hour",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0329/",
    "title": "OpenSearch Community Meeting - 2022-03-29",
    "content": "Join us for our weekly online community meeting.\nMeeting topics are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0405/",
    "title": "OpenSearch Community Meeting - 2022-04-05 - open office hour",
    "content": "Join us for an open office hour community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0411/",
    "title": "OpenSearch Community Meeting - 2022-04-11",
    "content": "Join us for our weekly online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0418/",
    "title": "OpenSearch Community Meeting - 2022-04-18 - open office hour",
    "content": "Join us for an open office hour community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0426/",
    "title": "OpenSearch Community Meeting - 2022-04-26",
    "content": "Join us for our weekly online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0503/",
    "title": "OpenSearch Community Meeting - 2022-05-03",
    "content": "Join us for Q&amp;A at an open office community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0509/",
    "title": "OpenSearch Community Meeting - 2022-05-09",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0516/",
    "title": "OpenSearch Community Meeting - 2022-05-16",
    "content": "Join us for Q&amp;A at an open office community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0524/",
    "title": "OpenSearch Community Meeting - 2022-05-24",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0531/",
    "title": "OpenSearch Community Meeting - 2022-05-31",
    "content": "Join us for Q&amp;A at an open office community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0606/",
    "title": "OpenSearch Community Meeting - 2022-06-06",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0613/",
    "title": "OpenSearch Community Meeting - 2022-06-13",
    "content": "Join us for Q&amp;A at an open office community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0621/",
    "title": "OpenSearch Community Meeting - 2022-06-22",
    "content": "Join us for our online community meeting. Please note - we‚Äôve updated the date from 6/21 to the new date of 6/22 due to unforeseen circumstances. Thank you for your understanding.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0628/",
    "title": "OpenSearch Community Meeting - 2022-06-28",
    "content": "Join us for Q&amp;A at an open office community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on HackMD.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nDuring the meeting the agenda will be unlocked for collaborative editing / note taking. After the meeting the agenda will be set to read-only mode.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0705/",
    "title": "OpenSearch Community Meeting - 2022-07-05",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0712/",
    "title": "OpenSearch Community Meeting - 2022-07-12",
    "content": "Join us for an open office discussion.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0719/",
    "title": "OpenSearch Community Meeting - 2022-07-19",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0802/",
    "title": "OpenSearch Community Meeting - 2022-08-02",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0816/",
    "title": "OpenSearch Community Meeting - 2022-08-16",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0830/",
    "title": "OpenSearch Community Meeting - 2022-08-30",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0913/",
    "title": "OpenSearch Community Meeting - 2022-09-13",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0921-opensearchcon/",
    "title": "OpenSearchCon - 2022-09-21",
    "content": "OpenSearchCon - A one-day conference for the community!\nJoin the OpenSearch community for a day of learning, collaboration, and innovation with the people and organizations that are shaping the future of the OpenSearch project. This free-to-attend, one-day conference brings together users, developers, and technologists across the OpenSearch community to explore real-world successes and new applications. Connect with peers and partners who can help you solve today‚Äôs search challenges and unlock the next phase of your OpenSearch journey.\nWhen: September 21, 2022, 10:00 a.m. ‚Äì 6:30 p.m.\nWhere: Seattle ‚Äì Fremont Studios, 155 N 35th St, 98103\nWho: OpenSearch users, developers, and other community members",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-0927/",
    "title": "OpenSearch Community Meeting - 2022-09-27",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1011/",
    "title": "OpenSearch Community Meeting - 2022-10-11",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1017-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2022-10-17",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1024-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2022-10-24",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1024-open-observability-day/",
    "title": "Kubecon Open Observability Day - 2022-10-24",
    "content": "Join the OpenSearch team at Open Observability Day.\nJoin David Tippett, and Prudhvi Godithi for a booth talk on getting started with the OpenSearch operator!",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1025/",
    "title": "OpenSearch Community Meeting - 2022-10-25",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1031-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2022-10-31",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1107-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2022-11-07",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1107-ubuntu-summit/",
    "title": "Ubuntu Summit - 2022-11-07 through 11-09",
    "content": "Join the OpenSearch team at Ubuntu Summit 2022.\nJoin Nate Boot, David Tippett, and Kris Freedain for their talk Measuring Data with OpenSearch to Better Serve the Community.\nExact schedule TBD - be sure to watch the Ubuntu Summit 2022 for more details.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1114-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2022-11-14",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1122/",
    "title": "OpenSearch Community Meeting - 2022-11-22",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1128-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2022-11-28",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1205-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2022-12-05",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1206/",
    "title": "OpenSearch Community Meeting - 2022-12-06",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1212-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2022-12-12",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1219-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2022-12-19",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2022-1220/",
    "title": "OpenSearch Community Meeting - 2022-12-20",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0109-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-01-09",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0116-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-01-17",
    "content": "PLEASE NOTE - This was originally scheduled for Monday the 16th, but that is a US Holiday so we are moving this to Tuesday the 17th. Thank you for your understanding.\nJoin the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0117/",
    "title": "OpenSearch Community Meeting - 2023-01-17",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0123-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-01-23",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0124-finding-the-right-things/",
    "title": "Data Ops Poland - Finding the Right Things with OpenSearch",
    "content": "Check out Data Ops Poland where David will be presenting, ‚ÄúFinding the Right Things with OpenSearch‚Äù:\nAs our data grows it‚Äôs become increasingly challenging to find what you need. In comes OpenSearch to help out. Whether you are searching through terabytes of logs, hundreds of thousands of products online, or even security events OpenSearch can help you find the right information. Learn how OpenSearch uses search techniques to find you the data that is relevant to you.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0130-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-01-30",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0131/",
    "title": "OpenSearch Community Meeting - 2023-01-31",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0205-opensearch-at-fosdem23-copy/",
    "title": "FOSDEM - Python Logging Like Your Job Depends on It",
    "content": "Come visit with us at FOSDEM, Brussels! David will be presenting, ‚ÄúPython Logging Like Your Job Depends on It‚Äù. It is a fast track to understanding logging in Python that ends in a demo of how to log into OpenSearch with Python.\nIf you‚Äôd like to meetup at FOSDEM or chat about the event we can all use this forum thread! Hope to see everyone there.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0206-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-02-06",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0213-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-02-13",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0214/",
    "title": "OpenSearch Community Meeting - 2023-02-14",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0220-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-02-20",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0227-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-02-27",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0228/",
    "title": "OpenSearch Community Meeting - 2023-02-28",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0306-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-03-06",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0307-awsopensearch-unplugged/",
    "title": "Partner Event - OpenSearch Unplugged - Observability and Search",
    "content": "Join this partner event with AWS\nIn this session, learn how you can use Amazon OpenSearch Service as log analytics and Search solution including machine learning services with two hands on labs. We will also introduce you to the Amazon OpenSearch Serverless option that makes it easier to run search and analytics workloads without having to think about infrastructure management. Speakers Jon Handler - Senior Principal Solution Architect, OpenSearch\nKarla Ferry - Head of Analytics Sales for US East\nKevin Fallis - Principal Solution Architect, OpenSearch\nManish Arora - WW GTM, OpenSearch\nNate Boot - Developer Advocate, OpenSearch Project - Nate Boot has been given a unique opportunity to share our fully open source community with users of the AWS managed service. Learn how you can maximize your impact on the future of OpenSearch itself.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0313-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-03-13",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0314/",
    "title": "OpenSearch Community Meeting - 2023-03-14",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0320-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-03-20",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0323-fluent-community/",
    "title": "Fluent Community Meeting",
    "content": "The Fluent Community meeting is an online meetup centered around the Fluentd and Fluent Bit open source projects that are part of the Cloud Native Computing Foundation (CNCF). We also sometimes branch off into discussions of observability in general.\nUsers who are interested in discussing the projects, communicating with maintainers, or who are eager to learn about the technologies should join. New users are welcomed.\nOur agenda for the meeting is an open document.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0327-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-03-27",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0328/",
    "title": "OpenSearch Community Meeting - 2023-03-28",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0403-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-04-03",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0406-dev-integrations-apache-spark/",
    "title": "Planning for Simple Schema Based Integrations and Apache Spark - 2023-0406",
    "content": "Join the OpenSearch team for their Integrations and Apache Spark planning meeting.\n(hosts: Joshua Bright - GitHub, forum, &amp; Ani Jadhav - GitHub, forum) Agenda: Review progress made on integrations backend and front end work [FEATURE] Community driven Integration repository #1457 Review proposed CLI to make creation of integrations easier [FEATURE] Integration Templating CLI #1451 Apache Spark indexing progress with covered and skipping indexes OpenSearch and Spark Integration P0 Demo #1465 Reviewing community use cases Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0406-fluent-community/",
    "title": "Fluent Community Meeting",
    "content": "The Fluent Community meeting is an online meetup centered around the Fluentd and Fluent Bit open source projects that are part of the Cloud Native Computing Foundation (CNCF). We also sometimes branch off into discussions of observability in general.\nUsers who are interested in discussing the projects, communicating with maintainers, or who are eager to learn about the technologies should join. New users are welcomed.\nOur agenda for the meeting is an open document.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0410-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-04-10",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0411/",
    "title": "OpenSearch Community Meeting - 2023-04-11",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0417-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-04-17",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0420-dev-integrations-apache-spark/",
    "title": "Planning for Simple Schema Based Integrations and Apache Spark - 2023-0420",
    "content": "Join the OpenSearch team for their Integrations and Apache Spark planning meeting.\n(hosts: Joshua Bright - GitHub, forum, &amp; Ani Jadhav - GitHub, forum) Agenda: Review progress made on integrations backend and front end work [FEATURE] Community driven Integration repository #1457 Review proposed CLI to make creation of integrations easier [FEATURE] Integration Templating CLI #1451 Apache Spark indexing progress with covered and skipping indexes OpenSearch and Spark Integration P0 Demo #1465 Reviewing community use cases Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0424-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-04-24",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0424-haystack-us-2023/",
    "title": "Haystack US 2023 - The Search Relevance Conference",
    "content": "Haystack is the conference for improving search relevance. If you‚Äôre like us, you work to understand the shiny new tools or dense academic papers out there that promise the moon. Then you puzzle how to apply those insights to your search problem, in your search stack. But the path isn‚Äôt always easy, and the promised gains don‚Äôt always materialize.\nHaystack is the conference for organizations where search, matching, and relevance really matters to the bottom line. For search managers, developers, relevance engineers &amp; data scientists finding ways to innovate, see past the silver bullets, and share what actually has worked well for their unique problems. Please come share and learn!\nFeaturing talks from AWS OpenSearch, Getty Images, Elsevier Health, Delivery Hero, Pinecone, Weaviate &amp; a raft of other search experts!",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0425/",
    "title": "OpenSearch Community Meeting - 2023-04-25",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0426-rta-sumit/",
    "title": "Upgrading Log-Analytics Clusters to OpenSearch",
    "content": "Join Amitai Stern at the Real-Time Analytics Summit for the session ‚ÄúUpgrading Log-Analytics Clusters to OpenSearch‚Äù\nHere at Logz.io, the open-source observability and security company, we manage observability data for over 1300 companies in highly scalable multi-cloud deployments. With the Elasticsearch license changes of 2021 we needed to migrate to an open-source platform, and OpenSearch was where we were going to contribute and what we wanted to run in production.\nMany equate upgrading to OpenSearch from Elasticsearch in production as changing the tires on a moving bus. Upgrading has many risks, and if the cluster is in continuous production use, ingesting terabytes of data daily, the risks can seem overbearing.\nIn this talk, we will cover multiple upgrade strategies, including version requirements, and their pros and cons. Additionally, we will cover a different option, which is the way we, at Logz.io, upgraded all our clusters to OpenSearch without significant extra costs while minimizing risk. Not only did we upgrade to OpenSearch, but we also migrated our AWS workloads to Graviton2 instances.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0501-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-05-01",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0504-dev-integrations-apache-spark/",
    "title": "Planning for Simple Schema Based Integrations and Apache Spark - 2023-0504",
    "content": "Join the OpenSearch team for their Integrations and Apache Spark planning meeting.\n(hosts: Joshua Bright - GitHub, forum, &amp; Ani Jadhav - GitHub, forum) Agenda: Review progress made on integrations backend and front end work [FEATURE] Community driven Integration repository #1457 Review proposed CLI to make creation of integrations easier [FEATURE] Integration Templating CLI #1451 Apache Spark indexing progress with covered and skipping indexes OpenSearch and Spark Integration P0 Demo #1465 Reviewing community use cases Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0504-dev-officehours-dashboards/",
    "title": "OpenSearch Dashboards Developer Office Hours - 2023-05-04",
    "content": "Join the OpenSearch Dashboards team for developer office hours.\n(host: Josh Romero) Agenda: Visit the forum post to sign-up and view planned topics. The meeting is divided into four 15-minute slots for community developers to chat with OpenSearch Dashboards project maintainers. Priority will be given to topics that are signed-up in advance, but ad-hoc discussions are welcome in any remaining time. If there are no sign-ups, maintainers will present a brief knowledge-sharing session or demo and the meeting may end early. Please see Meetup link for URL and required passcode. After the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0508-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-05-08",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0509/",
    "title": "OpenSearch Community Meeting - 2023-05-09",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0510-devoxxuk/",
    "title": "Using Apache Kafka and OpenSearch to explore Mastodon",
    "content": "Join Olena Kutsenko at DevoxxUK for the session ‚ÄúUsing Apache Kafka and OpenSearch to explore Mastodon‚Äù - https://www.devoxx.co.uk/speaker-details/?id=7861\nApache Kafka is a powerful tool to connect multiple systems together, allowing the data to flow across multiple services and be reused for multiple purposes. This can be useful in many scenarios both for mission-critical applications, as well as for fast data explorations.\nIn this talk I‚Äôll show one such data exploration. Mastodon, as a tool for microblogging, is rising in popularity in recent months. If you just recently joined Mastodon and are still exploring it, you might find that scrolling the timeline has its limits to understand all that is happening there. That being the case, applying some engineering skills will give a better overview on topics and discussions happening on the platform.\nSince Mastodon‚Äôs timeline is nothing more than a collection of continuously arriving events, its feed is well-suited for Apache Kafka. Adding Kafka connectors on top of that opens multiple opportunities to use data for aggregations and visualizations.\nDuring this talk you‚Äôll learn how to bring data from Mastodon to Kafka using TypeScript and a couple of helpful libraries. Once the data is in the topic, we‚Äôll use Kafka Connect to bring the data into OpenSearch and use it for search, aggregations and visualizations.\nThis talk is for both beginners in Apache Kafka and intermediate users. We‚Äôll use some more advanced concepts, but will keep it all simple, so that everyone can follow along and experiment with Mastodon data!",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0515-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-05-15",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0518-dev-integrations-apache-spark/",
    "title": "Planning for Simple Schema Based Integrations and Apache Spark - 2023-0518",
    "content": "Join the OpenSearch team for their Integrations and Apache Spark planning meeting.\n(hosts: Joshua Bright - GitHub, forum, &amp; Ani Jadhav - GitHub, forum) Agenda: Review progress made on integrations backend and front end work [FEATURE] Community driven Integration repository #1457 Review proposed CLI to make creation of integrations easier [FEATURE] Integration Templating CLI #1451 Apache Spark indexing progress with covered and skipping indexes OpenSearch and Spark Integration P0 Demo #1465 Reviewing community use cases Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0518-dev-officehours-dashboards/",
    "title": "OpenSearch Dashboards Developer Office Hours - 2023-05-18",
    "content": "Join the OpenSearch Dashboards team for developer office hours.\n(host: Josh Romero) Agenda: Visit the forum post to sign-up and view planned topics. The meeting is divided into four 15-minute slots for community developers to chat with OpenSearch Dashboards project maintainers. Priority will be given to topics that are signed-up in advance, but ad-hoc discussions are welcome in any remaining time. If there are no sign-ups, maintainers will present a brief knowledge-sharing session or demo and the meeting may end early. Please see Meetup link for URL and required passcode. After the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0522-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-05-22",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0523/",
    "title": "OpenSearch Community Meeting - 2023-05-23",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0529-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-05-29",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0601-dev-integrations-apache-spark/",
    "title": "Planning for Simple Schema Based Integrations and Apache Spark - 2023-0601",
    "content": "Join the OpenSearch team for their Integrations and Apache Spark planning meeting.\n(hosts: Joshua Bright - GitHub, forum, &amp; Ani Jadhav - GitHub, forum) Agenda: Review progress made on integrations backend and front end work [FEATURE] Community driven Integration repository #1457 Review proposed CLI to make creation of integrations easier [FEATURE] Integration Templating CLI #1451 Apache Spark indexing progress with covered and skipping indexes OpenSearch and Spark Integration P0 Demo #1465 Reviewing community use cases Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0601-dev-officehours-dashboards/",
    "title": "OpenSearch Dashboards Developer Office Hours - 2023-06-01",
    "content": "Join the OpenSearch Dashboards team for developer office hours.\n(host: Josh Romero) Agenda: Visit the forum post to sign-up and view planned topics. The meeting is divided into four 15-minute slots for community developers to chat with OpenSearch Dashboards project maintainers. Priority will be given to topics that are signed-up in advance, but ad-hoc discussions are welcome in any remaining time. If there are no sign-ups, maintainers will present a brief knowledge-sharing session or demo and the meeting may end early. Please see Meetup link for URL and required passcode. After the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0605-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-06-05",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0606/",
    "title": "OpenSearch Community Meeting - 2023-06-06",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0612-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-06-12",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0615-dev-integrations-apache-spark/",
    "title": "Planning for Simple Schema Based Integrations and Apache Spark - 2023-0615",
    "content": "Join the OpenSearch team for their Integrations and Apache Spark planning meeting.\n(hosts: Joshua Bright - GitHub, forum, &amp; Ani Jadhav - GitHub, forum) Agenda: Review progress made on integrations backend and front end work [FEATURE] Community driven Integration repository #1457 Review proposed CLI to make creation of integrations easier [FEATURE] Integration Templating CLI #1451 Apache Spark indexing progress with covered and skipping indexes OpenSearch and Spark Integration P0 Demo #1465 Reviewing community use cases Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0615-dev-officehours-dashboards/",
    "title": "OpenSearch Dashboards Developer Office Hours - 2023-06-15",
    "content": "Join the OpenSearch Dashboards team for developer office hours.\n(host: Josh Romero) Agenda: Visit the forum post to sign-up and view planned topics. The meeting is divided into four 15-minute slots for community developers to chat with OpenSearch Dashboards project maintainers. Priority will be given to topics that are signed-up in advance, but ad-hoc discussions are welcome in any remaining time. If there are no sign-ups, maintainers will present a brief knowledge-sharing session or demo and the meeting may end early. Please see Meetup link for URL and required passcode. After the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0619-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-06-19",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0620/",
    "title": "OpenSearch Community Meeting - 2023-06-20",
    "content": "Join us for our online community meeting.\nMeeting topics and passcode are listed in the collaborative agenda on the forum.\nFeel free to comment on the agenda before the meeting if you want to add an item or have a question. Would you like to present? Reach out on the forumn thread - we welcome community presentations.\nAfter the meeting, we will post the chat log and any meeting notes. We welcome you to keep the conversation going on the forum. By joining the OpenSearch Community Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the OpenSearch Community Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0626-dev-triage-security/",
    "title": "Development Backlog & Triage Meeting - Security - 2023-06-26",
    "content": "Join the OpenSearch Security team for their next backlog &amp; triage planning meeting.\n(hosts: Dave Lago, Peter Nied, &amp; Stephen Crawford) Agenda: Triage issues (add the triaged label once reviewed/ready. They can be also labelled as sprint backlog if we are looking to queueing them up next, or good first issue / help wanted when appropriate.) Backend security Dashboards security Sprint backlog (Examine if it still reflects the work that we are committing to doing and is it in the right priority order) Backend security Dashboards security Backlog (anything we should move to sprint backlog? anything we should tag asking for help from the community?) Backend security Dashboards security Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0629-dev-integrations-apache-spark/",
    "title": "Planning for Simple Schema Based Integrations and Apache Spark - 2023-0629",
    "content": "Join the OpenSearch team for their Integrations and Apache Spark planning meeting.\n(hosts: Joshua Bright - GitHub, forum, &amp; Ani Jadhav - GitHub, forum) Agenda: Review progress made on integrations backend and front end work [FEATURE] Community driven Integration repository #1457 Review proposed CLI to make creation of integrations easier [FEATURE] Integration Templating CLI #1451 Apache Spark indexing progress with covered and skipping indexes OpenSearch and Spark Integration P0 Demo #1465 Reviewing community use cases Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0713-dev-integrations-apache-spark/",
    "title": "Planning for Simple Schema Based Integrations and Apache Spark - 2023-0713",
    "content": "Join the OpenSearch team for their Integrations and Apache Spark planning meeting.\n(hosts: Joshua Bright - GitHub, forum, &amp; Ani Jadhav - GitHub, forum) Agenda: Review progress made on integrations backend and front end work [FEATURE] Community driven Integration repository #1457 Review proposed CLI to make creation of integrations easier [FEATURE] Integration Templating CLI #1451 Apache Spark indexing progress with covered and skipping indexes OpenSearch and Spark Integration P0 Demo #1465 Reviewing community use cases Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/events/2023-0727-dev-integrations-apache-spark/",
    "title": "Planning for Simple Schema Based Integrations and Apache Spark - 2023-0727",
    "content": "Join the OpenSearch team for their Integrations and Apache Spark planning meeting.\n(hosts: Joshua Bright - GitHub, forum, &amp; Ani Jadhav - GitHub, forum) Agenda: Review progress made on integrations backend and front end work [FEATURE] Community driven Integration repository #1457 Review proposed CLI to make creation of integrations easier [FEATURE] Integration Templating CLI #1451 Apache Spark indexing progress with covered and skipping indexes OpenSearch and Spark Integration P0 Demo #1465 Reviewing community use cases Please see Meetup link for URL and required passcode. By joining the Development Backlog &amp; Triage Meeting, you grant OpenSearch, and our affiliates the right to record, film, photograph, and capture your voice and image during the Development Backlog &amp; Triage Meeting (the ‚ÄúRecordings‚Äù). You grant to us an irrevocable, nonexclusive, perpetual, worldwide, royalty-free right and license to use, reproduce, modify, distribute, and translate, for any purpose, all or any part of the Recordings and Your Materials. For example, we may distribute Recordings or snippets of Recordings via our social media outlets.",
    "keywords": [

    ],
    "type": "Events"
  },
  {
    "url": "/versions/opensearch-1-0-0.html",
    "title": "Opensearch 1.0.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-0-0-rc1.html",
    "title": "Opensearch 1.0.0 Rc1",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-0-1.html",
    "title": "Opensearch 1.0.1",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-1-0.html",
    "title": "Opensearch 1.1.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-2-0.html",
    "title": "Opensearch 1.2.0",
    "content": "Note: Please use version 1.2.1 or higher. OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-2-1.html",
    "title": "Opensearch 1.2.1",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-2-2.html",
    "title": "Opensearch 1.2.2",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-2-3.html",
    "title": "Opensearch 1.2.3",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-2-4.html",
    "title": "Opensearch 1.2.4",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-3-0.html",
    "title": "Opensearch 1.3.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-3-1.html",
    "title": "Opensearch 1.3.1",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-3-2.html",
    "title": "Opensearch 1.3.2",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-3-3.html",
    "title": "Opensearch 1.3.3",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-3-4.html",
    "title": "Opensearch 1.3.4",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-3-5.html",
    "title": "Opensearch 1.3.5",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-3-6.html",
    "title": "Opensearch 1.3.6",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-3-7.html",
    "title": "Opensearch 1.3.7",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-3-8.html",
    "title": "Opensearch 1.3.8",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-1-3-9.html",
    "title": "Opensearch 1.3.9",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-0-0.html",
    "title": "Opensearch 2.0.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-0-0-rc1.html",
    "title": "Opensearch 2.0.0 Rc1",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-0-1.html",
    "title": "Opensearch 2.0.1",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-1-0.html",
    "title": "Opensearch 2.1.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-2-0.html",
    "title": "Opensearch 2.2.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-2-1.html",
    "title": "Opensearch 2.2.1",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-3-0.html",
    "title": "Opensearch 2.3.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-4-0.html",
    "title": "Opensearch 2.4.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-4-1.html",
    "title": "Opensearch 2.4.1",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-5-0.html",
    "title": "Opensearch 2.5.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-6-0.html",
    "title": "Opensearch 2.6.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/versions/opensearch-2-7-0.html",
    "title": "Opensearch 2.7.0",
    "content": "OpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.",
    "keywords": [

    ],
    "type": "Downloads"
  },
  {
    "url": "/testimonials/coralogix.html",
    "title": "Coralogix",
    "content": "At Coralogix, we believe in making observability fast, cost effective, and accessible, without limitations. That‚Äôs why we believe in building with open source tools. OpenSearch gives us a powerful, flexible, open source foundation to enable full-stack observability and real-time streaming analytics at scale for customers everywhere.",
    "keywords": [

    ],
    "type": "Testimonials"
  },
  {
    "url": "/testimonials/dow-jones.html",
    "title": "Dow Jones",
    "content": "At Dow Jones we use a variety of open source software projects to solve challenging technology problems. We are excited by the recent launch of OpenSearch and how it might enhance some of our key managed Observability services. We look forward to its advancement, and participating in the OpenSearch community.",
    "keywords": [

    ],
    "type": "Testimonials"
  },
  {
    "url": "/testimonials/goldman-sachs.html",
    "title": "Goldman Sachs",
    "content": "Open source solutions have changed the way people are thinking about software and hardware alike. The recent launch of OpenSearch marks an exciting time, and we look forward to empowering our research content platform to achieve an enhanced client, curator and developer experience.",
    "keywords": [

    ],
    "type": "Testimonials"
  },
  {
    "url": "/testimonials/openinfrafoundation.html",
    "title": "Openinfrafoundation",
    "content": "We in the OpenInfra community are energized by how OpenSearch has engaged with the vision of an open development model, based on the four principles of open source, open design, open development, and open community. And, OpenSearch‚Äôs support as an infrastructure donor to the OpenStack community is a tangible proof point of how our communities are working together to deliver open source software that runs in production.",
    "keywords": [

    ],
    "type": "Testimonials"
  },
  {
    "url": "/testimonials/pinterest.html",
    "title": "Pinterest",
    "content": "Pinterest makes extensive use of open source software in our product, which brings inspiration to hundreds of millions of people around the world. And we contribute software to communities that are working together to advance the state of the art. Because we value open source communities, we are excited that OpenSearch provides a fully open source search and analytics suite.",
    "keywords": [

    ],
    "type": "Testimonials"
  },
  {
    "url": "/testimonials/quantiphi.html",
    "title": "Quantiphi",
    "content": "We embrace cloud technologies that are built on open-source services and technologies so that our customer use cases are well supported and agnostic of vendors. Earlier, we relied centrally on Elasticsearch for source tracking, content search, and log analytics because of its ability to ingest and log large volumes of data. With the launch of OpenSearch, we can serve customers across industries through its additional capabilities such as Advanced security and alerting, KNN Search, and anomaly detection. It also facilitates faster decision-making for the stakeholders through its integrated visualization tool, OpenSearch Dashboards. In conclusion, we‚Äôre excited to leverage OpenSearch in our future engagements.",
    "keywords": [

    ],
    "type": "Testimonials"
  },
  {
    "url": "/testimonials/rackspace.html",
    "title": "Rackspace",
    "content": "At Rackspace Technology, we welcome more open-source contribution and collaboration as it underpins modern analytics platforms and technologies. Our engineers utilize the open-source spectrum across our data platform implementations and are excited by the increased options and flexibility OpenSearch offers when building modern data solutions for our customers. We look forward to supporting its advancement and participating in the OpenSearch community.",
    "keywords": [

    ],
    "type": "Testimonials"
  },
  {
    "url": "/testimonials/sap.html",
    "title": "Sap",
    "content": "SAP customers expect a unified, business-centric and open SAP Business Technology Platform. Our observability strategy uses Elasticsearch as a major enabler. OpenSearch provides a true open source path and community-driven approach to move this forward.",
    "keywords": [

    ],
    "type": "Testimonials"
  },
  {
    "url": "/testimonials/wipro.html",
    "title": "Wipro",
    "content": "More and more businesses consider open source to be an important part of their technology strategy. Wipro has been a pioneer and thought leader in Open Source with deep foundations into open source community and development process. We are delighted to see OpenSearch provide an open source search and analytics suite that could enhance some of our key services around log aggregation, operational monitoring and data observability. We look forward to actively contributing and participating in the OpenSearch community.",
    "keywords": [

    ],
    "type": "Testimonials"
  },
  {
    "url": "/testimonials/zoom.html",
    "title": "Zoom",
    "content": "At Zoom we recognize the importance of open source software as it allows a diverse community of developers to solve problems together. We are excited to see OpenSearch provide an open source solution for search, log analytics and more.",
    "keywords": [

    ],
    "type": "Testimonials"
  },
  {
    "url": "/tutorials/data-prepper-jaeger.html",
    "title": "Data Prepper + Jaeger Tutorial and Demo",
    "content": "In this tutorial, you‚Äôll use Data Prepper with Jaeger to demonstrate the ability for OpenSearch and OpenSearch Dashboards to capture and visualize trace data.\n1. Get your demo environment ready\nLaunch OpenSearch Dashboards with Docker Compose [todo]\nüõë Please wait while OpenSearch Dashboards is loading. This may take a few moments.\n‚úÖ OpenSearch Dashboards is ready, you can proceed to the next step.\n2. Prepare OpenSearch Dashboards and the Jaeger Hot R.O.D. Demo\nThis tutorial will use three browser windows. One for this tutorial content, another for OpenSearch Dashboards, and a third for the Jaeger Hot R.O.D. demo.\nPoint the second browser window at the following URL to launch OpenSearch Dashboards:\nüõë OpenSearch Dashboards is still loading, URLs will be visible once it is complete. http://localhost:5601/ http://localhost:5601/ Now, point the third browser window at the following URL to launch the Jaeger Hot R.O.D. demo:\nüõë OpenSearch Dashboards is still loading, URLs will be visible once it is complete. http://localhost:8080/ http://localhost:8080/ You should have three windows open like this: 3. Login to OpenSearch Dashboards for the first time At the login screen, use the following credentials:\nUsername: admin Password: admin Click ‚ÄúLog In‚Äù Next, you‚Äôll be presented with the Welcome page. You should click ‚ÄúExplore on my own‚Äù since the demo is only using data from the Hot Rod demo. Now you‚Äôll see the tenant selection window. In this tutorial, tenants will not be used, so just select ‚ÄúGlobal‚Äù then click ‚ÄúConfirm‚Äù On the home screen, click the menu button in the upper left to reveal the menu along the left side. Click ‚ÄúTrace Analytics‚Äù under the heading ‚ÄúOpenSearch Plugins.‚Äù\n4. Generate sample traces with the Jaeger Hot R.O.D. demo application\nSwitch to the window that is running the Jaeger Hot R.O.D. demo. Click several times on the customer buttons - even click rapidly. This application will simulate a microservice architecture and will generate trace data that is being read by Data Prepper and stored in OpenSearch.\n5. View the trace data in OpenSearch Dashboards\nSwitch back to OpenSearch Dashboards. You‚Äôll see the panels have been populated with some data based on your clicks (if not, click the ‚ÄúRefresh‚Äù button on the right). On the top, you have a listing of latency by trace groups, then below you have the service map, trace error rate and trace over time charts. Feel free to explore these panels,\nNow, view the traces of any trace group by clicking on the number of traces in the right most column of the ‚ÄúLatency by trace group‚Äù panel. This brings you to the traces page where you can click on individual trace IDs. Click on any trace ID from the left most column. This shows the detail for that individual trace. Make sure and look at both the ‚ÄúTime spent by service‚Äù bar chart and the ‚ÄúSpan‚Äù chart to see how this trace moved through the architecture. Go back to the main Trace Analytics panel by selecting ‚ÄúTrace Analytics‚Äù in the upper left next to the menu button. Then select ‚ÄúServices‚Äù from the left hand menu. Here you can inspect individual services and their performance. Make sure to take a look at the service map and it‚Äôs options as well as the span table at the bottom of the page.\nWhat did you learn and what‚Äôs next?\nIn this tutorial, you logged into OpenSearch Dashboards, started an application that generated traces and started Data Prepper to read and store those traces and saw how OpenSearch Dashboards presents the trace data for your analysis.\nFrom here you can continue to explore the interface of OpenSearch Dashboards, read the documentation for trace analytics ( https://opensearch.org/docs/latest/monitoring-plugins/trace/index/) or Data Prepper ( https://opensearch.org/docs/latest/monitoring-plugins/trace/data-prepper/).",
    "keywords": [

    ],
    "type": "Tutorials"
  },
  {
    "url": "/tutorials/loading.html",
    "title": "Loading",
    "content": "Loading your OpenSearch Tutorial\nPlease be patient, your live environment is being setup. This page will go away once it‚Äôs ready.",
    "keywords": [

    ],
    "type": "Tutorials"
  },
  {
    "url": "/tutorials/opn305.html",
    "title": "OPN305 OpenSearch: Securing your OpenSearch Data",
    "content": "Today‚Äôs builder‚Äôs session will leverage Gitpod (or Docker Compose, if you wish) to launch a container instance that hosts a single node of an OpenSearch cluster coupled with OpenSearch Dashboards. The container image comes preinstalled with multiple plugins from the OpenSearch project. One of those plugins is the Security plugin. The session will focus specifically on the security plugin and how you can leverage the plugin for fine grained access controls on your indexes.\nTasks\nThe builder‚Äôs session is divided into specific tasks to help you understand how you can leverage the security plugin to help control the scope of data and the operations that one user or group can perform on your data in your cluster. Many customers find that the fine grained access controls for the plugin can give customers a mechanism that ensures data access is scoped to the desired users. The plugin includes integrations with SAML providers and other authentication sources and it also has a local user database. This session focuses on the provisioning of a local user database to simulate entitlements such as those that are normally implemented in production deployments. You will perform the following activities with these instructions and you will have an OpenSearch person at your table to help you through any issues that may arise while going through these instructions. The task are broken down into the following actions:\nGet setup and create the cluster\nLoad data into the cluster using Python scripts and other Linux tools like wget Create users in the local user database with basic authentication\nConfigure audit logging so you can see the actions of your users and verify compliance\nConfigure roles at the index level and the document and field levels\nAssign the roles to users created in basic authentication\nExercise queries in OpenSearch Dashboards to view the data and the effects of the role\nassignments for particular users\nLet‚Äôs create the cluster!\nToday‚Äôs session will leverage Gitpod or Docker Compose to launch a container instance that hosts a single node of an OpenSearch cluster coupled with OpenSearch Dashboards. The container image comes preinstalled with multiple plugins from the OpenSearch project. One of those plugins is the Security plugin. The session will focus specifically on this plugin and how you can leverage the plugin for fine grained access controls on your indexes.\nStarting the cluster with Gitpod\nClick on the ‚ÄúOpen in Gitpod‚Äù button below and it will launch an instance of OpenSearch in your browser account. Once you click on the button, the container will launch in Gitpod. You should observe the following as\nthe Gitpod environment spins up: Be patient while the container launches. You will see these instructions come up in the preview window on top and terminal on the bottom: Looks like you‚Äôre running in GitPod - here is the status of OpenSearch Dashboards:\nWhen this document is viewed in Gitpod, it will indicate when OpenSearch Dashboards is ready for use.\nüõë Please wait while OpenSearch Dashboards is loading. This may take a few moments.\n‚úÖ OpenSearch Dashboards is ready, you can proceed to the next step.\nNow, let‚Äôs launch OpenSearch Dashboards. Right click on the link below and open it in a new window: ‚û°Ô∏è Launch OpenSearch Dashboards.\nStarting the cluster with Docker Compose\nIf you are using Docker Desktop directly on your machine, keep in mind that this will download several hundred megabytes of container images before you can begin, so it‚Äôs suggested that you spin up a cloud instance with Docker installed or use Gitpod as mentioned above, especially if you are on an unreliable connection.\nSetup your Docker host environment\nmacOS &amp; Windows: In Docker Preferences &gt; Resources, set RAM to at least 4 GB.\nLinux: Ensure vm.max_map_count is set to at least 262144 as per the documentation.\nIn a terminal window, run download the Docker compose file: wget https://raw.githubusercontent.com/opensearch-project/project-website/opn305-demo/_demo/docker-compose.yml In a terminal window, start the cluster by running: docker-compose up Be patient while everything starts up\nNavigate to http://localhost:5601/ (or the address/hostname of your instance) for OpenSearch Dashboards in a web browser. This may take a few moments to full be ready.\nOnce the container has started up, you will observe an event that states ‚Äúhttp server running at\nhttp://0:5601‚Äù. Port 5601 is where your OpenSearch dashboard is listening for requests.\nLog into OpenSearch Dashboards\nYou should see a sign on screen for OpenSearch Dashboards. The user name is admin and the password\nis admin for this image. Keep in mind, this password is only for demonstration purposes and you should\ncreate passwords that are complex and not easy to figure out. Your organization will most likely have\nstandards and you should adhere to a strong password to keep things secure even in private\nenvironments. Once you have signed in to OpenSearch Dashboards, you will be presented with the following screen: Select the tenant\nYou will be prompted to select a tenant. Chose the Global tenant and click on the confirm button. Let‚Äôs configure audit logs!\nAudit logs give you visibility into what users are doing on your cluster and the logs help you understand how you are staying compliant for your organizational requirements. For this builder‚Äôs session, the audit logs are already enabled. Your focus will be on setting up the types of logs and users that you are going to monitor. To do this, you need to navigate to the security plugin. If you look at your console, there is an expand button next to the Home label. Click on this to get to the menu. Navigate to the Security plugin\nThe Security plugin manages the audit logs configuration. Click on the Security option. Navigate to the Audit logs menu\nThe Audit logs by default will be contained in a series of indexes for this walkthrough. You can adjust your opensearch.yml file to change the location. For demo purposes, this will be in an index. I suggest using logs instead if you deploy this in any production environment. Once you click into General settings you will be presented with a series of options. In the Layer settings, enable the REST layer logging. Leave transport layer alone for now. This REST layer will show you all requests over GET, PUT, POST, etc that come into the cluster. This can help you see what queries are being run by your users so there is visibility into user activities. Some settings are already provisioned for you and we are going to verify that certain settings are adjusted as seen below: In the Attribute settings, enable Request body and Resolve indices. These two settings show the query body and will show the actual index for which a query was executed. Save your settings. You will review what gets logged here once you run through all the exercises for the builder‚Äôs session. You won‚Äôt need to concern yourself with the compliance settings for this builder‚Äôs session.\nImport the movies data into the OpenSearch cluster\nNow, switch over to the terminal.\nIn Gitpod, this will be in the lower section of your workspace.\nIf you are using Docker Compose, you‚Äôll need to use your favourite terminal emulator. If running Docker on your local machine, you can do this directly. If you are running OpenSearch in a cloud instance, you‚Äôll need SSH into that instance.\nDownload data and scripts\nThe first thing that you are going to do is pull the data set from the location in which it is hosted. You\nwill use wget to pull the dataset to your local storage on the container. Execute the following command\nin your bash shell: wget http://search-sa-log-solutions.s3-us-east-2.amazonaws.com/fluentd-kinesis-logstash/data/2013Imdb.txt Observe the following: Next, you need to download two different scripts. The first script will add the index template to the\ncluster by leveraging the _template endpoint. The second script will parse the 2013imdb.txt file that\nyou downloaded and it will convert it into _bulk API calls against the cluster.\nGo ahead and download the two files with the following commands: wget http://search-sa-log-solutions.s3-us-east-2.amazonaws.com/builders/put-mappings.py\nwget http://search-sa-log-solutions.s3-us-east-2.amazonaws.com/builders/put-data.py Observe the following output: Push the data and mappings to the OpenSearch cluster\nVerify all three files have been downloaded to your work area. Use the following command: ls -al Now that the scripts and the IMDB data is local for your container, you are going to push mappings and the data using a series of _bulk API calls. Since the data is not in the proper format for OpenSearch, you will use a Python script that reads from the file and writes to your cluster.\nCreate the index template and mappings using the put_mappings.py script\nOf the three files you downloaded, one creates an index template used to determine the mappings of the data being added to the cluster for the session. It also defines sharding strategies and other index level settings.\nYou will use the admin user for the initial data seeding. The admin user is already associated with the all_access role on the cluster. You can verify this in OpenSearch Dashboards in the Security plugin. The cluster is secured with basic authentication and only authenticated and authorized users can interact with the OpenSearch cluster. For example, if you execute the put_mappings script with an invalid user, you will get an HTTP code of 401 which means the user either does not exist or the user lacks credentials to interact with the APIs. Run the following command for a non-existent user: python put-mappings.py --endpoint localhost:9200 --username john --password jinglehiemer The Python code is calling the OpenSearch cluster using invalid credentials. And it will fail as seen below: Using the admin user, lets get the template installed with the appropriate credentials. Execute the following command: python put-mappings.py --endpoint localhost:9200 --username admin --password admin Observe a successful HTTP 200 code: You can verify the template is in the cluster by navigating to Dev Tools in OpenSearch Dashboards. If you are not already logged into the OpenSearch Dashboards, log in and navigate to Dev Tools. The home page has a breadcrumb that you can follow. Once you have clicked on the Dev Tools link, you will be presented with a command line, auto complete, editor that enables you to interact with all the APIs on the cluster as long as you have permissions as defined in the Role Mappings. Type in the following command and press the play button (click to send request) as seen in the image below: GET _template/movies_template So now you have a template for the data you will ingest using the next command. This data will be used to show you have to control access to data in an index on the OpenSearch cluster.\nLoad the IMDB data into the cluster using the put_data.py script\nAnother file you uploaded is a Python script that loads data into the cluster. It bundles the data after parsing the file and sends it to the cluster using the _bulk API. The _bulk API is the most efficient way to add data to an index at scale. Execute the following command: python put-data.py --endpoint localhost:9200 --username admin --password admin Navigate to OpenSearch Dashboards and in Dev Tools, execute the following command and observe the output: GET _cat/indices Now run a query to grab the first 5 documents. We will use this output to compare how all access permissions compare to limited access permissions. GET movies/_search?size=5\n{\n\"query\": {\n\"match_all\": {}\n},\n\"_source\": [\"title\",\"directors\",\"actors\",\"genres\"]\n} Observe the following output: At this point, you have data to query and you now have data to demonstrate how to lock that data down. Let‚Äôs prepare the data just a little bit more. Let‚Äôs split the data into to indexes using a reindex API and a delete_by_query. Go back to Dev Tools and issue the following request: POST _reindex\n{\n\"source\": {\n\"index\": \"movies\",\n\"query\": {\n\"match\": {\n\"genres\": \"comedy\"\n}\n}\n},\n\"dest\": {\n\"index\": \"movies_subset\"\n}\n} Observe the following: Clean up the transferred data from the movies index so we have separated indexes to show fine grained access controls. Issue the following request: POST movies/_delete_by_query\n{\n\"query\": {\n\"match\": {\n\"genres\": \"comedy\"\n}\n}\n} Observe the following behavior: Now we have two different indexes for which we can apply fine grained access controls at the index, document and field levels. With the data, you can now setup the permissions for the users you are going to create next.\nLet‚Äôs create some users!\nOnce you are signed in on the cluster, you should see the following screen. Navigate to the Security\nplugin. For our session on security, lets lay a solid foundation for entitlements; otherwise known as permissions or roles that have permissions to do things on a cluster. You create these users so that you, as an admin, can monitor behaviours and ensure that users only have the permissions they need. Audit logs help you verify the permissions you create are performing as intended. The role of an admin user typically is limited in the fact that the admin user can issue entitlements and revoke them. Outside of that, admin users should never be used for programmatic access outside of the scope of entitlements as a best practice.\nYou are going to create the following users:\nA read only limited access user assigned to one index ‚Äì this user can only read data on a specific\nindex and the user can see all fields in the documents.\nA read only document level and field level security user ‚Äì this user has access to the same single\nindex the limited access user has and the user is further limited by specific documents matching\na specific genre of movies. The user will not have access to the actors field in the movies index\nthat you will create shortly.\nOnce you click on the Security link, you should see the following page. Navigate to the ‚ÄòInternal users‚Äô\nlink found on the left hand side of the browser window and click on the link. Create the read only index level user\nCreate a user called read_only_index_level user with a password of pa$$word. Scroll to the bottom of the form, leave the other fields alone for now. Do not map them. Scroll down and click the Create button. Observe user is created. Create the read only document level field level user\nNext, you are going to exercise the APIs for OpenSearch security plugin and add a user through the command line. Many organizations automate user mappings and provisioning of users. Understanding that you can use the APIs to do this work will help with your automation needs.\nCreate a user called read_only_dls_fls user using the following command: curl -XPUT https://localhost:9200/_opendistro/_security/api/internalusers/read_only_dls_fls -u admin:admin -k -H 'Content-Type: application/json' -d '{\"password\":\"pa$$w0rd\"}' It should respond back with {\"status\":\"CREATED\",\"message\":\"'read_only_dls_fls' created.\"} Observe the following output: Observe two users are created\nIn the overview, you should see two users created. Let‚Äôs create some roles!\nWithout a role assignment, a user is not able to do anything on the cluster. To prove this point, logout of your OpenSearch Dashboards and log in as the read_only_index_level user. Navigate to the Dev Tools. You will be presented with a preformed query. Change add the movies index to the query: When you execute the query by clicking the play button (Click to send request), you will see that this user has no permissions to query the movies index. Let‚Äôs go ahead and give that user minimal permissions to execute this query.\nNavigate to the Security plugin and click on the roles menu\nMake sure you sign out of Dashboards and sign back in as admin. Once you have clicked on the ‚ÄòRoles‚Äô option, you should see the following screen: Click on the Create role button.\nCreate the read_only_index_level role\nAssign the following name ‚Äúread_only_index_level‚Äù to the role in the Create Role form. Give cluster_composite_ops_ro permissions to the role. Scroll down to Index permissions and add a pattern mov* for index permissions. Allow read, get and search permissions. Assign the global_tenant in Tenant permissions and create the role by clicking the Create button at the bottom right of the form. Observe the role gets created. Navigate back to the ‚ÄòRoles‚Äô overview page and find the newly created role‚Äôs breadcrumb. Click on the link labeled read_only_index_level. After clicking the link, you will see the following: Click on ‚ÄúMap users‚Äù as there should be no users / processes / groups assigned to this role. Assign the read_only_index_level user to the read_only_index_level role. Observe the mappings have been updated: Once you have the role mapped, you can sign out as admin and sign back in as read_only_index_level user. Enter the read_only_index_level credentials from your prior activities. Once you have signed in, navigate to Dev Tools and issue the following query: GET movies/_search\n{\n\"query\": {\n\"match_all\": {}\n}\n} Observe that you are able to query the indexes that are mapped. Keep in mind, you mapped this user to a pattern. That means you will be able to read any indexes that begin with ‚Äúmov‚Äù. Before you test the other index, grab the document ID from the first document that shows up in your results. Cut and paste that _id field into the following command: DELETE movies/_doc/&lt;your_pasted_id&gt; Issue that command in Dev Tools and observe that you have no permissions to delete the document. Feel free to play with other commands to test the entitlements. As you can see, you will not be able to delete any documents with the current permissions. Repeat this exercise for the movies_subset index. You should observe similar behaviors. Next, you are going to narrow the scope for the read_only_dls_fls user.\nCreate the read_only_dls_fls_role role\nNext, you are going to work with document and field level permissions. This part of the exercise will leverage scripting so that you can see how to provision roles and role mapping over APIs. The role will have the following properties:\nLimit the scope to only the movies_subset index (index level filtering)\nFurther narrow the scope to documents in that index that match only the genre ‚ÄúFantasy‚Äù (document level filtering)\nMask the directors field in the results (field level anonymization)\nRemove actors from the results (field level elimination)\nGo to the command line in bash and execute the following command: curl -XPUT https://localhost:9200/_opendistro/_security/api/roles/read_only_dls_fls_role -u admin:admin -k -H 'Content-Type: application/json' -d '{\"cluster_permissions\":[\"cluster_composite_ops_ro\"],\"index_permissions\":[{\"index_patterns\":[\"movies_subset\"],\"dls\":\"{\\\"bool\\\":{\\\"must\\\": {\\\"match\\\": {\\\"genres\\\": \\\"Fantasy\\\"}}}}\",\"fls\":[\"~actors\"],\"masked_fields\":[\"directors\"],\"allowed_actions\":[\"read\",\"get\",\"search\"]}],\"tenant_permissions\":[{\"tenant_patterns\":[\"global_tenant\"],\"allowed_actions\":[\"kibana_all_read\"]}]}' Observe the role gets created: Now you need to map the role to the user. Leveraging the role mapping API, issue the following command: curl -XPUT https://localhost:9200/_opendistro/_security/api/rolesmapping/read_only_dls_fls_role -u admin:admin -k -H 'Content-Type: application/json' -d '{\"backend_roles\":[],\"hosts\": [], \"users\": [ \"read_only_dls_fls\"]}' Once the command responds with success, query the rolesmapping API to view the final set of mappings. There should be only one user. curl -XGET https://localhost:9200/_opendistro/_security/api/rolesmapping/read_only_dls_fls_role -u admin:admin -k Observe the results: At this point in time you now have two new users on the cluster that are mapped to distinct roles that limit the scope of what each user can perform. The entitlements go from wide (index level only) to narrow (document and field level permissions).\nLet‚Äôs setup the tests and assert the entitlements!\nThe remainder of this exercise will have you issue commands from bash that are queries against the movies and movies_subset indexes. You will also observe that you cannot query other indexes.\nCreate another index that does not fall under the patterns specified for the read only users\nLet‚Äôs create yet another index that just refers to drama genres. As the admin user, issue the following command: curl -XPOST https://localhost:9200/_reindex -u admin:admin -k -H 'Content-Type: application/json' -d '{\"source\":{\"index\": \"movies\",\"query\": {\"match\": {\"genres\": \"Drama\"}}},\"dest\": {\"index\": \"shows\"}}' Observe the results: Verify only admin user can query shows index\nIssue the following command in bash:\nAs admin user curl -XGET https://localhost:9200/shows/_search?size=1\\&amp;pretty=true\\&amp;filter_path=hits -u admin:admin -k -H 'Content-Type: application/json' -d '{\"query\": {\"match_all\": {}},\"_source\": [\"title\",\"directors\",\"actors\",\"genres\"]}' Observe success: As read_only_index_level user\nIssue the following command in bash: curl -XGET https://localhost:9200/shows/_search?size=1\\&amp;pretty=true\\&amp;filter_path=hits -u read_only_index_level:pa\\$\\$w0rd -k -H 'Content-Type: application/json' -d '{\"query\": {\"match_all\": {}},\"_source\": [\"title\",\"directors\",\"actors\",\"genres\"]}' Observe that the user has no permissions for this index which is in line with the settings you allowed in the role for read_only_index_level_role that gave access to movies and movies_subset indexes. As read_only_dls_fls user\nIssue the following command in bash: curl -XGET https://localhost:9200/shows/_search?size=1\\&amp;pretty=true\\&amp;filter_path=hits -u read_only_dls_fls:pa\\$\\$w0rd -k -H 'Content-Type: application/json' -d '{\"query\": {\"match_all\": {}},\"_source\": [\"title\",\"directors\",\"actors\",\"genres\"]}' Observe that the user has no permissions for this index which is in line with the settings you allowed in the role for read_only_dls_fls_role that gave access to movies_subset index. shows index summary\nOverall, you have proven that at the index level, both read only users have no access to the shows index. This is because you specified that each read_only* user has a narrow scope of access that does not include the shows index.\nVerify the read_only* user‚Äôs access against the ‚Äúmovies‚Äù index\nNext, lets pivot to the movies index. You know that the admin user has ‚Äúgod‚Äù access. It can create indexes, assign permissions and do other cluster level entitlements and actions. For the remainder of the builder‚Äôs session, you will work with only the read_only* users for observations.\nAs read_only_index_level user\nIssue the following command in bash: curl -XGET https://localhost:9200/movies/_search?size=1\\&amp;pretty=true\\&amp;filter_path=hits -u read_only_index_level:pa\\$\\$w0rd -k -H 'Content-Type: application/json' -d '{\"query\": {\"match_all\": {}},\"_source\": [\"title\",\"directors\",\"actors\",\"genres\"]}' Observe the following behavior. The user can query. The index is defined in the role: As read_only_dls_fls user\nIssue the following command in bash: curl -XGET https://localhost:9200/movies/_search?size=1\\&amp;pretty=true\\&amp;filter_path=hits -u read_only_dls_fls:pa\\$\\$w0rd -k -H 'Content-Type: application/json' -d '{\"query\": {\"match_all\": {}},\"_source\": [\"title\",\"directors\",\"actors\",\"genres\"]}' Observe the following behavior. The user can‚Äôt query. The index is not defined in the role: Verify the read_only* user‚Äôs access against the movies_subset index\nNext, lets pivot to the movies_subset index. Keep in mind that now, we have entitlements for the read_only_dls_fls user that state the following:\nAccess to only the movies_subset index\nDocument level permissions to the filter that must match the genres of ‚ÄòFantasy‚Äô\nField level masking of the directors field\nExclusion of the actors field\nAs read_only_index_level user\nIssue the following command in bash: curl -XGET https://localhost:9200/movies_subset/_search?size=1\\&amp;pretty=true\\&amp;filter_path=hits -u read_only_index_level:pa\\$\\$w0rd -k -H 'Content-Type: application/json' -d '{\"query\": {\"match\": {\"title.keyword\": \"Don Jon\"}},\"_source\": [\"title\",\"directors\",\"actors\",\"genres\"]}' Observe the following behavior. The user can query. The index is defined in the role and there is no document level security: As read_only_dls_fls user\nIssue the following command in bash: curl -XGET https://localhost:9200/movies_subset/_search?size=1\\&amp;pretty=true\\&amp;filter_path=hits -u read_only_dls_fls:pa\\$\\$w0rd -k -H 'Content-Type: application/json' -d '{\"query\": {\"match\": {\"title.keyword\": \"Don Jon\"}},\"_source\": [\"title\",\"directors\",\"actors\",\"genres\"]}' Observe the following behavior. The user can‚Äôt query documents that match ‚ÄúDon Jon‚Äù even though you specifically reindexed documents to the movies_subset index that match ‚ÄúComedy‚Äù. Only documents that match genres ‚ÄúFantasy‚Äù are allowed. Luckily, the genres field, like the others, can be expressed as an array. Each movie can be associated with multiple genres. Let‚Äôs see what the results are: Nothing is found because the user can only pull back documents matching ‚ÄúFantasy‚Äù in the genres field.\nVerify that the read_only_dls_fls user can actually query something\nIssue the following command in bash: curl -XGET https://localhost:9200/movies_subset/_search?size=1\\&amp;pretty=true\\&amp;filter_path=hits -u read_only_dls_fls:pa\\$\\$w0rd -k -H 'Content-Type: application/json' -d '{\"query\": {\"match_all\": {}},\"_source\": [\"title\",\"directors\",\"actors\",\"genres\"]}' Observe that you get data back. You will see that the genres field has multiple values (array) and in fact ‚ÄúFantasy‚Äù is in the result set. There is NO field called actors and the directors field has been masked (your result might be different): So as the final result indicates, THIS IS THE END üòâ. Have fun! Keep in mind that the OpenSearch experts are here to assist in your questions. Thanks so much for your time today!\nIn summary, you built a set of users with varying permissions. You have asserted those permissions in fact do work. The Security plugin uses a variety of authentication sources and basic authentication is just one of those typically used for programmatic access. Consider SAML or other more secure authentication methods for your activities.\nBut wait! How do I validate and audit my users???\nNavigate to the Discover function as admin user\nLog back in as admin. Since this is the first index you query, it will force you to create an index pattern. Click on the create index pattern. Type in security* and click Next step. Select the @timestamp field since this is a time series index and click on the Create index pattern button. Now navigate to the Discover function by invoking the menu on the left side of the Dashboards browser. In the search bar, type movies_subset. Click around in the results and you will discover a robust set of details about the queries, users and IP addresses from which the queries originated. This feature gives you the ability to audit what goes on in your cluster. As stated before, its best to have these logs go to local disk instead of an index as we have setup for this builder‚Äôs session.\nAll the commands\nIf you want to see all the commands used in this tutorial, take a look at the command scratchpad.\nStay Connected!\nFind Kyle on Twitter at @stockholmux, on LinkedIn, or reach out via email at kyledvs@amazon.com\nFind Kevin on LinkedIn, or reach out via email at kffallis@amazon.com",
    "keywords": [

    ],
    "type": "Tutorials"
  },
  {
    "url": "/OpenSearchCon2023",
    "title": "Save The Date for OpenSearchCon 2023!",
    "content": "Save the Date for OpenSearchCon 2023!\nSeptember 28, 2023 @ the Sheraton Grand, Seattle\nGet ready to join us at OpenSearchCon 2023 as we evolve what it means to be a builder. That‚Äôs right, this year‚Äôs event revolves around you‚Äîour builders‚Äîas we expand your opportunity to connect, collaborate, and grow as part of the open source community.\nThis free-to-attend, one-day conference brings together users, developers, and technologists across the OpenSearch Project to explore real-world successes and new applications. And don‚Äôt miss the chance to connect with peers and partners who can help you solve today‚Äôs search, analytics, and observability challenges and unlock the next phase of your OpenSearch journey.\nMark your calendars and plan to join us for OpenSearchCon 2023, September 28 at the Sheraton Grand in Seattle! And don‚Äôt forget to click the link below and let us know if you‚Äôre interested in attending. When you do, we‚Äôll make sure you‚Äôre one of the first to know when we share more details!\nCome back here and check in the forums for any updates!\nMissed OpenSearchCon 2022?\n<!--\nCopyright (c) 2020 Nathan Lam\nhttps://github.com/nathancy/jekyll-embed-video\n-->",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/about.html",
    "title": "About",
    "content": "Updated December 20, 2022 Principles for development ¬∑ Founding Documents ¬∑ OpenSearch disambiguation ¬∑\nOpenSearch is a community-driven, open-source search and analytics suite used by developers to ingest, search, visualize, and analyze data. OpenSearch consists of a data store and search engine (OpenSearch), a visualization and user interface (OpenSearch Dashboards), and a server-side data collector (Data Prepper). Users can extend the functionality of OpenSearch with a selection of plugins that enhance search, analytics, observability, security, machine learning, and more.\nThe OpenSearch Project was first announced in January 2021 as an open-source fork of Elasticsearch and Kibana to provide a secure, high-quality, fully open-source search and analytics suite with a rich feature roadmap. In July 2021, the project released OpenSearch 1.0 for production under the Apache License, Version 2.0 (ALv2), with the codebase published to GitHub and open to contribution from the OpenSearch community. A comprehensive project roadmap is maintained here.\nSince the start of the project, the OpenSearch community has grown to 100s of contributors, 1000s of pull requests, 1000s of issues closed, and is organized across more than 90 repositories. In November 2022, OpenSearch released version 2.4 of the project with the introduction of Windows distributions and enhancements to cluster resiliency, search functionality, analytics tools, and more.\nThe latest release of OpenSearch is available for download here.\nAs a fully open source solution, OpenSearch offers you the freedom to modify, extend, monetize, and resell the product as you see fit, as well as the flexibility to deploy on a variety of infrastructures. At the same time, the OpenSearch project provides a secure, high-quality search and analytics suite with a rich roadmap of new and innovative functionality.\nOpenSearch is built with your input. The project is maintained and advanced by a community, including a network of partners, and is open to contribution. We invite you to get involved.\nPrinciples for development\nWhen we (the contributors) are successful, OpenSearch will be:\nGreat software.\nIf it doesn‚Äôt solve your problems, everything else is moot. It‚Äôs going to be software you love to use.\nOpen source like we mean it.\nWe are invested in this being a successful open-source project for the long term. It‚Äôs all Apache 2.0. There‚Äôs no Contributor License Agreement. Easy.\nA level playing field.\nWe will not tweak the software so that it runs better for any vendor (including AWS) at the expense of others. If this happens, call it out and we will fix it as a community.\nUsed everywhere.\nOur goal is for as many people as possible to use it in their business, their software, and their projects. Use it however you want. Surprise us!\nMade with your input.\nWe will ask for public input on direction, requirements, and implementation for any feature we build.\nOpen to contributions.\nGreat open-source software is built together, with a diverse community of contributors. If you want to get involved at any level - big, small, or huge - we will find a way to make that happen. We don‚Äôt know what that looks like yet, and we look forward to figuring it out together.\nRespectful, approachable, and friendly.\nThis will be a community where you will be heard, accepted, and valued, whether you are a new or experienced user or contributor.\nA place to invent.\nYou will be able to innovate rapidly. This project will have a stable and predictable foundation that is modular, making it easy to extend.\nFounding Documents Introducing OpenSearch Stepping up for a truly open source Elasticsearch Keeping Open Source Open OpenSearch disambiguation\nAt the 2005 O‚ÄôReilly Emerging Technology Conference, Jeff Bezos showed the world the OpenSearch syndication protocol. You can find more details on Wikipedia. This specification is maintained in GitHub at github.com/dewitt/opensearch.",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/codeofconduct.html",
    "title": "Code of Conduct",
    "content": "This code of conduct applies to all spaces provided by the OpenSource project including in code, documentation, issue trackers, mailing lists, chat channels, wikis, blogs, social media and any other communication channels used by the project. Our open source communities endeavor to: Be Inclusive: We are committed to being a community where everyone can join and contribute. This means using inclusive and welcoming language.\nBe Welcoming: We are committed to maintaining a safe space for everyone to be able to contribute.\nBe Respectful: We are committed to encouraging differing viewpoints, accepting constructive criticism and work collaboratively towards decisions that help the project grow. Disrespectful and unacceptable behavior will not be tolerated.\nBe Collaborative: We are committed to supporting what is best for our community and users. When we build anything for the benefit of the project, we should document the work we do and communicate to others on how this affects their work. Our Responsibility. As contributors, members, or bystanders we each individually have the responsibility to behave professionally and respectfully at all times. Disrespectful and unacceptable behaviors include, but are not limited to: The use of violent threats, abusive, discriminatory, or derogatory language;\nOffensive comments related to gender, gender identity and expression, sexual orientation, disability, mental illness, race, political or religious affiliation;\nPosting of sexually explicit or violent content;\nThe use of sexualized language and unwelcome sexual attention or advances;\nPublic or private harassment of any kind;\nPublishing private information, such as physical or electronic address, without permission;\nOther conduct which could reasonably be considered inappropriate in a professional setting;\nAdvocating for or encouraging any of the above behaviors.\nEnforcement and Reporting Code of Conduct Issues:\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported. Contact us. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances.",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/connect.html",
    "title": "Connect with the Community",
    "content": "Connect with the Community\nThere are several ways to connect with the OpenSearch community.\nOpenSearch has active forums, which are the fastest way to get help using, troubleshooting, and developing with OpenSearch. As a bonus, questions and discussions on the forums are easy for others to learn from in the future.\nIf you want to share a feature idea or submit a bug report, open an issue in the relevant repo in the GitHub project.\nJoin the OpenSearch Project Slack workspace for open communication with maintainers, contributors, and the community.\nIf you want to present or have a question for an upcoming community meeting, add a comment to the agenda for that meeting.\nBusinesses partnering in the development and advancement of OpenSearch can propose to add themselves to the partners page with a pull request.\nTo feature a plug-in or project you developed with OpenSearch, add it to the community projects page with a pull request.\nInformation about using the OpenSearch brand and logo are available on the Brand Guidelines page.\nTo report or inquire about a potential security vulnerability, see the Reporting a Vulnerability guidelines.\nLastly, you can email the OpenSearch maintainers with other inquiries that don‚Äôt fit into any of the above categories.\nThank you for being part of the OpenSearch community.",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/disambiguation.html",
    "title": "OpenSearch Disambiguation",
    "content": "OpenSearch has a long history at Amazon. The trademark for OpenSearch is held by Amazon.\nOpenSearch Syndication Protocol\nAt the 2005 O‚ÄôReilly Emerging Technology Conference, Jeff Bezos showed the world the OpenSearch syndication protocol. This protocol was born out of A9, the search arm of Amazon.com, and allowed for the syndication of search results across services. This protocol has lived on and is still valid today. You can find more details on Wikipedia. This specification is currently being maintained in GitHub by an ex-A9 employee, at github.com/dewitt/opensearch.",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/downloads.html",
    "title": "Download & Get Started",
    "content": "OpenSearch Download About OpenSearch Releases FAQ Community Blog Forum Slack Events Partners Projects Documentation Live Demo! Try OpenSearch with Docker Compose\nThe best way to try out OpenSearch is to use Docker Compose. These steps will set up a two node cluster of OpenSearch plus OpenSearch Dashboards:\nSet up your Docker host environment macOS &amp; Windows: In Docker Preferences &gt; Resources, set RAM to at least 4 GB. Linux: Ensure vm.max_map_count is set to at least 262144 as per the documentation.\nDownload docker-compose.yml into your desired directory\nRun docker-compose up Have a nice coffee while everything is downloading and starting up\nNavigate to http://localhost:5601/ for OpenSearch Dashboards\nLogin with the default username ( admin) and password ( admin)\nOpenSearch\nOpenSearch is a distributed search and analytics engine based on Apache Lucene. After adding your data to OpenSearch, you can perform full-text searches on it with all of the features you might expect: search by field, search multiple indices, boost fields, rank results by score, sort results by field, and aggregate results. Platform: Linux Docker FreeBSD Windows Package: x64\n/.tar.gz x64\n/\ndeb x64\n/\nrpm x64\n/\nyum ARM64\n/.tar.gz ARM64\n/\ndeb ARM64\n/\nrpm ARM64\n/\nyum x64\n/\ndocker ARM64\n/\ndocker x64\n/\nSystem package ARM64\n/\nSystem package x64\n/.zip Download File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Download File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Install from FreeBSD packages:\n# pkg install opensearch Get it Install from FreeBSD packages:\n# pkg install opensearch Get it Download File Signature Signature verification how to View on Docker Hub Docker Hub docker pull opensearchproject/opensearch:2.7.0 View on Amazon ECR Amazon ECR docker pull public.ecr.aws/opensearchproject/opensearch:2.7.0 View on Docker Hub Docker Hub docker pull opensearchproject/opensearch:2.7.0 View on Amazon ECR Amazon ECR docker pull public.ecr.aws/opensearchproject/opensearch:2.7.0 OpenSearch Dashboards\nOpenSearch Dashboards is the default visualization tool for data in OpenSearch. It also serves as a user interface for many of the OpenSearch plugins, including security, alerting, Index State Management, SQL, and more. Platform: Linux Docker FreeBSD Windows Package: x64\n/.tar.gz x64\n/\ndeb x64\n/\nrpm x64\n/\nyum ARM64\n/.tar.gz ARM64\n/\ndeb ARM64\n/\nrpm ARM64\n/\nyum x64\n/\ndocker ARM64\n/\ndocker x64\n/\nSystem package ARM64\n/\nSystem package x64\n/.zip Download File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Download File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Download Install Guide File Signature Signature verification how to Install from FreeBSD packages:\n# pkg install opensearch-dashboards Get it Install from FreeBSD packages:\n# pkg install opensearch-dashboards Get it Download File Signature Signature verification how to View on Docker Hub Docker Hub docker pull opensearchproject/opensearch-dashboards:2.7.0 View on Amazon ECR Amazon ECR docker pull public.ecr.aws/opensearchproject/opensearch-dashboards:2.7.0 View on Docker Hub Docker Hub docker pull opensearchproject/opensearch-dashboards:2.7.0 View on Amazon ECR Amazon ECR docker pull public.ecr.aws/opensearchproject/opensearch-dashboards:2.7.0 Ingest Tools\nOpenSearch is compatible with a variety of ingestion and processing tools including beats, fluentbit and fluentd. The project also maintains specific ingestion tools:\nLogstash OSS with OpenSearch Output Plugin\nThis package includes open source Logstash bundled with the OpenSearch output plugin (v2.0.1). The output plugin is compatible with OpenSearch and Open Source versions of Elasticsearch (7.10.2 or lower). The output plugin is also available as a Ruby Gem. Platform: Linux Docker macOS Package: x64\n/.tar.gz ARM64\n/.tar.gz x64\n/\ndocker ARM64\n/\ndocker x64\n/.tar.gz Download File Signature Signature verification how to Download File Signature Signature verification how to Download File Signature Signature verification how to View on Docker Hub docker pull opensearchproject/logstash-oss-with-opensearch-output-plugin:8.6.1 View on Amazon ECR docker pull public.ecr.aws/opensearchproject/logstash-oss-with-opensearch-output-plugin:8.6.1 View on Docker Hub docker pull opensearchproject/logstash-oss-with-opensearch-output-plugin:8.6.1 View on Amazon ECR docker pull public.ecr.aws/opensearchproject/logstash-oss-with-opensearch-output-plugin:8.6.1 Data Prepper\nData Prepper is a component of the OpenSearch project that accepts, filters, transforms, enriches, and routes data at scale.\nDistributions without a bundled JDK are also available in the artifacts directory. Platform: Docker Linux Package: x64\n/\ndocker x64\n/.tar.gz Download File Signature Signature verification how to View on Docker Hub docker pull opensearchproject/data-prepper:2.1.1 View on Amazon ECR docker pull public.ecr.aws/opensearchproject/data-prepper:2.1.1 Command Line Tools\nThe OpenSearch command line interface ( opensearch-cli) lets you manage your cluster from the command line and automate tasks. Platform: Linux macOS Windows Package: x64\n/.zip ARM64\n/.zip x64\n/\nInstaller (pkg) ARM64\n/\nInstaller (pkg) x64\n/.zip x86\n/.zip Download Download Download Download Download Download Drivers\nOpenSearch provides SQL drivers like ODBC and JDBC that let you integrate OpenSearch with business intelligence software and other tools.\nODBC Driver\nThe Open Database Connectivity (ODBC) driver is a read-only ODBC driver for Windows and macOS that lets you connect business intelligence (BI) and data visualization applications like Tableau, Microsoft Excel, and Power BI to the SQL plugin. Platform: macOS Windows Package: x64\n/.zip x64\n/\nInstaller (msi) x86\n/\nInstaller (msi) Download Download Download JDBC Driver\nThe Java Database Connectivity (JDBC) driver is a read-only driver for connecting business intelligence and data visualization applications with OpenSearch using SQL. Platform: Java Package: Java Virtual Machine\n/.jar Download File Signature Signature verification how to Minimal Distributions\nThe following distributions include the bare minimum features required for a functioning OpenSearch and OpenSearch Dashboards. It is intended for those who already have custom plugins or are intending to integrate/embed OpenSearch with other services. If you are an end user, you probably do not want these distributions.\nOpenSearch Minimum\nThis distribution lacks important security features and should be only used in carefully controlled environments. Platform: Linux Package: x64\n/.tar.gz ARM64\n/.tar.gz Download File Signature Signature verification how to Download File Signature Signature verification how to OpenSearch Dashboards Minimum\nThis distribution lacks important security features and should be only used in carefully controlled environments. Platform: Linux Package: x64\n/.tar.gz ARM64\n/.tar.gz Download File Signature Signature verification how to Download File Signature Signature verification how to For artifacts of all components of the project, see our artifacts directory for a complete list or a list just for this version.\nReleases in the 2.x line: 2.7.0\n/ 2.6.0 / 2.5.0 / 2.4.1 / 2.4.0 / 2.3.0 / 2.2.1 / 2.2.0 / 2.1.0 / 2.0.1 / 2.0.0 / 2.0.0-rc1 Release sequence (all release lines): Previous Release <!-- END #content-main -->\nOpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration. Version: 2.7.0 Release Date: May 02, 2023 Release Notes Version Permalink This is a release in the 2.x line. Available release lines: 2.x 1.x Try OpenSearch with Docker Compose OpenSearch OpenSearch Dashboards Ingest Tools Command Line Tools Drivers Minimal Distributions <!-- END #content-related -->\n<!-- END #subwrap -->\n<!-- END #content-extra -->\nGet Involved Code of Conduct Forum Github Slack Resources About Release Schedule Maintenance Policy FAQ Testimonials Trademark and Brand Policy Privacy Contact Us Connect Twitter LinkedIn YouTube Meetup Facebook &copy; OpenSearch contributors, 2023. OpenSearch is a registered trademark of Amazon Web Services. OpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V. ¬© 2005-2021 Django Software Foundation and individual contributors. Django is a registered trademark of the Django Software Foundation. This website was forked from the BSD-licensed djangoproject.com originally designed by Threespot &amp; andrevv. We ‚ô° Django and the Django community. If you need a high-level Python framework, check it out.",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/partners/",
    "title": "Partners",
    "content": "OpenSearch is a team effort, we are grateful to all our partners.\nWant to join the partner program? This guide describes how to join as an OpenSearch partner. Aiven Arenadata AWS Barefoot Coders BigData Boutique Bitergia Bonsai Calyptia Canonical | Ubuntu Cloud Scale Coralogix Dattell deepset DigitalPebble ElastiFlow Eliatra Graylog GSI Technology H&A Security Solutions Hidora Highlight Hopsworks Hossted Hyland Improving INFINI Labs initMAX s.r.o. Instaclustr IronCore Labs, Inc. KMW Technology Logit.io Logz.io Mach5 Software, Inc. maxcluster Metarank netimate OpenSource Connections Opster Oracle Pureinsights ReactiveSearch Recon InfoSec ReflexSOAR Seacom SearchBlox Software, Inc. Searchium.ai Sease Serverless Operations, Inc. Smart Search Tools SnappyFlow SquareShift Stellar Cyber Titaniam Tornis Tecnologia Tracetest | Kubeshop Virtuozzo Wazuh Weblink Technology WPSOLR WorldTech IT",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/artifacts/by-version/",
    "title": "Project Artifacts by Version",
    "content": "Release 2.7.0\nopensearch 2.7.0 arm64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-arm64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-arm64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.7.0/opensearch-2.7.0-windows-x64.zip.sig) opensearch-dashboards 2.7.0 arm64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-arm64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-arm64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-x64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-x64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.7.0/opensearch-dashboards-2.7.0-windows-x64.zip.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.1.1 x64 https://artifacts.opensearch.org/data-prepper/2.1.1/opensearch-data-prepper-jdk-2.1.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.1.1/opensearch-data-prepper-jdk-2.1.1-linux-x64.tar.gz.sig) opensearch-min 2.7.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.7.0/opensearch-min-2.7.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.7.0/opensearch-min-2.7.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.7.0/opensearch-min-2.7.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.7.0/opensearch-min-2.7.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.7.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.7.0/opensearch-dashboards-min-2.7.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.7.0/opensearch-dashboards-min-2.7.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.7.0/opensearch-dashboards-min-2.7.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.7.0/opensearch-dashboards-min-2.7.0-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.3.9\nopensearch 1.3.9 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.9\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-arm64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-arm64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.9\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-x64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-x64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.9/opensearch-1.3.9-windows-x64.zip.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.3.9 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.9\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-arm64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-arm64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.9\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-x64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-x64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.9/opensearch-dashboards-1.3.9-windows-x64.zip.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.3.9 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.9/opensearch-min-1.3.9-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.9/opensearch-min-1.3.9-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.9/opensearch-min-1.3.9-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.9/opensearch-min-1.3.9-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.3.9 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.9/opensearch-dashboards-min-1.3.9-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.9/opensearch-dashboards-min-1.3.9-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.9/opensearch-dashboards-min-1.3.9-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.9/opensearch-dashboards-min-1.3.9-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.6.0\nopensearch 2.6.0 arm64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-arm64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-arm64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-x64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-x64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.6.0/opensearch-2.6.0-windows-x64.zip.sig) opensearch-dashboards 2.6.0 arm64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-arm64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-arm64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-x64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-x64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.6.0/opensearch-dashboards-2.6.0-windows-x64.zip.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 2.6.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.6.0/opensearch-min-2.6.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.6.0/opensearch-min-2.6.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.6.0/opensearch-min-2.6.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.6.0/opensearch-min-2.6.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.6.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.6.0/opensearch-dashboards-min-2.6.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.6.0/opensearch-dashboards-min-2.6.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.6.0/opensearch-dashboards-min-2.6.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.6.0/opensearch-dashboards-min-2.6.0-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.3.8\nopensearch 1.3.8 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.8\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.8/opensearch-1.3.8-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.8/opensearch-1.3.8-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.8/opensearch-1.3.8-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.8/opensearch-1.3.8-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.8\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.8/opensearch-1.3.8-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.8/opensearch-1.3.8-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.8/opensearch-1.3.8-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.8/opensearch-1.3.8-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.8/opensearch-1.3.8-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.8/opensearch-1.3.8-windows-x64.zip.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.3.8 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.8\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.8/opensearch-dashboards-1.3.8-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.8/opensearch-dashboards-1.3.8-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.8/opensearch-dashboards-1.3.8-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.8/opensearch-dashboards-1.3.8-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.8\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.8/opensearch-dashboards-1.3.8-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.8/opensearch-dashboards-1.3.8-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.8/opensearch-dashboards-1.3.8-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.8/opensearch-dashboards-1.3.8-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.8/opensearch-dashboards-1.3.8-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.8/opensearch-dashboards-1.3.8-windows-x64.zip.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.3.8 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.8/opensearch-min-1.3.8-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.8/opensearch-min-1.3.8-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.8/opensearch-min-1.3.8-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.8/opensearch-min-1.3.8-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.3.8 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.8/opensearch-dashboards-min-1.3.8-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.8/opensearch-dashboards-min-1.3.8-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.8/opensearch-dashboards-min-1.3.8-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.8/opensearch-dashboards-min-1.3.8-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.5.0\nopensearch 2.5.0 arm64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-arm64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-arm64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-x64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-x64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.5.0/opensearch-2.5.0-windows-x64.zip.sig) opensearch-dashboards 2.5.0 arm64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-arm64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-arm64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-x64.deb (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-x64.deb.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.5.0/opensearch-dashboards-2.5.0-windows-x64.zip.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 2.5.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.5.0/opensearch-min-2.5.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.5/opensearch-min-2.5.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.5.0/opensearch-min-2.5.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.5.0/opensearch-min-2.5.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.5.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.5.0/opensearch-dashboards-min-2.5.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.5.0/opensearch-dashboards-min-2.5.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.5.0/opensearch-dashboards-min-2.5.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.5.0/opensearch-dashboards-min-2.5.0-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.4.1\nopensearch 2.4.1 arm64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.4.1/opensearch-2.4.1-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.1/opensearch-2.4.1-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.1/opensearch-2.4.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.1/opensearch-2.4.1-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.4.1/opensearch-2.4.1-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.1/opensearch-2.4.1-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.1/opensearch-2.4.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.1/opensearch-2.4.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.1/opensearch-2.4.1-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.1/opensearch-2.4.1-windows-x64.zip.sig) opensearch-dashboards 2.4.1 arm64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.1/opensearch-dashboards-2.4.1-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.1/opensearch-dashboards-2.4.1-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.1/opensearch-dashboards-2.4.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.1/opensearch-dashboards-2.4.1-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.1/opensearch-dashboards-2.4.1-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.1/opensearch-dashboards-2.4.1-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.1/opensearch-dashboards-2.4.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.1/opensearch-dashboards-2.4.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.1/opensearch-dashboards-2.4.1-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.1/opensearch-dashboards-2.4.1-windows-x64.zip.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 2.4.1 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.4.1/opensearch-min-2.4.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.4.1/opensearch-min-2.4.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.4.1/opensearch-min-2.4.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.4.1/opensearch-min-2.4.1-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.4.1 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.4.1/opensearch-dashboards-min-2.4.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.4.1/opensearch-dashboards-min-2.4.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.4.1/opensearch-dashboards-min-2.4.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.4.1/opensearch-dashboards-min-2.4.1-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.3.7\nopensearch 1.3.7 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.7\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.7/opensearch-1.3.7-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.7/opensearch-1.3.7-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.7/opensearch-1.3.7-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.7/opensearch-1.3.7-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.7\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.7/opensearch-1.3.7-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.7/opensearch-1.3.7-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.7/opensearch-1.3.7-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.7/opensearch-1.3.7-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.7/opensearch-1.3.7-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.7/opensearch-1.3.7-windows-x64.zip.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.3.7 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.7\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.7/opensearch-dashboards-1.3.7-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.7/opensearch-dashboards-1.3.7-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.7/opensearch-dashboards-1.3.7-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.7/opensearch-dashboards-1.3.7-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.7\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.7/opensearch-dashboards-1.3.7-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.7/opensearch-dashboards-1.3.7-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.7/opensearch-dashboards-1.3.7-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.7/opensearch-dashboards-1.3.7-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.7/opensearch-dashboards-1.3.7-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.7/opensearch-dashboards-1.3.7-windows-x64.zip.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.3.7 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.7/opensearch-min-1.3.7-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.7/opensearch-min-1.3.7-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.7/opensearch-min-1.3.7-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.7/opensearch-min-1.3.7-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.3.7 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.7/opensearch-dashboards-min-1.3.7-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.7/opensearch-dashboards-min-1.3.7-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.7/opensearch-dashboards-min-1.3.7-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.7/opensearch-dashboards-min-1.3.7-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.4.0\nopensearch 2.4.0 arm64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.4.0/opensearch-2.4.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.0/opensearch-2.4.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.0/opensearch-2.4.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.0/opensearch-2.4.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.4.0/opensearch-2.4.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.0/opensearch-2.4.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.0/opensearch-2.4.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.0/opensearch-2.4.0-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.0/opensearch-2.4.0-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.4.0/opensearch-2.4.0-windows-x64.zip.sig) opensearch-dashboards 2.4.0 arm64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.0/opensearch-dashboards-2.4.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.0/opensearch-dashboards-2.4.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.0/opensearch-dashboards-2.4.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.0/opensearch-dashboards-2.4.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.0/opensearch-dashboards-2.4.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.0/opensearch-dashboards-2.4.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.0/opensearch-dashboards-2.4.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.0/opensearch-dashboards-2.4.0-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.0/opensearch-dashboards-2.4.0-windows-x64.zip (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.4.0/opensearch-dashboards-2.4.0-windows-x64.zip.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 2.4.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.4.0/opensearch-min-2.4.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.4.0/opensearch-min-2.4.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.4.0/opensearch-min-2.4.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.4.0/opensearch-min-2.4.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.4.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.4.0/opensearch-dashboards-min-2.4.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.4.0/opensearch-dashboards-min-2.4.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.4.0/opensearch-dashboards-min-2.4.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.4.0/opensearch-dashboards-min-2.4.0-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.3.6\nopensearch 1.3.6 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.6\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.6/opensearch-1.3.6-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.6/opensearch-1.3.6-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.6/opensearch-1.3.6-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.6/opensearch-1.3.6-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.6\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.6/opensearch-1.3.6-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.6/opensearch-1.3.6-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.6/opensearch-1.3.6-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.6/opensearch-1.3.6-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.3.6 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.6\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.6/opensearch-dashboards-1.3.6-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.6/opensearch-dashboards-1.3.6-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.6/opensearch-dashboards-1.3.6-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.6/opensearch-dashboards-1.3.6-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.6\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.6/opensearch-dashboards-1.3.6-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.6/opensearch-dashboards-1.3.6-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.6/opensearch-dashboards-1.3.6-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.6/opensearch-dashboards-1.3.6-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.3.6 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.6/opensearch-min-1.3.6-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.6/opensearch-min-1.3.6-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.6/opensearch-min-1.3.6-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.6/opensearch-min-1.3.6-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.3.6 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.6/opensearch-dashboards-min-1.3.6-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.6/opensearch-dashboards-min-1.3.6-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.6/opensearch-dashboards-min-1.3.6-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.6/opensearch-dashboards-min-1.3.6-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.3.0\nopensearch 2.3.0 arm64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.3.0/opensearch-2.3.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.3.0/opensearch-2.3.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.3.0/opensearch-2.3.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.3.0/opensearch-2.3.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.3.0/opensearch-2.3.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.3.0/opensearch-2.3.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.3.0/opensearch-2.3.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.3.0/opensearch-2.3.0-linux-x64.tar.gz.sig) opensearch-dashboards 2.3.0 arm64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.3.0/opensearch-dashboards-2.3.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.3.0/opensearch-dashboards-2.3.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.3.0/opensearch-dashboards-2.3.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.3.0/opensearch-dashboards-2.3.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.3.0/opensearch-dashboards-2.3.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.3.0/opensearch-dashboards-2.3.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.3.0/opensearch-dashboards-2.3.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.3.0/opensearch-dashboards-2.3.0-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 2.3.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.3.0/opensearch-min-2.3.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.3.0/opensearch-min-2.3.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.3.0/opensearch-min-2.3.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.3.0/opensearch-min-2.3.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.3.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.3.0/opensearch-dashboards-min-2.3.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.3.0/opensearch-dashboards-min-2.3.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.3.0/opensearch-dashboards-min-2.3.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.3.0/opensearch-dashboards-min-2.3.0-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.3.5\nopensearch 1.3.5 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.5\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.5/opensearch-1.3.5-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.5/opensearch-1.3.5-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.5/opensearch-1.3.5-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.5/opensearch-1.3.5-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.5\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.5/opensearch-1.3.5-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.5/opensearch-1.3.5-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.5/opensearch-1.3.5-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.5/opensearch-1.3.5-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.3.5 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.5\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.5/opensearch-dashboards-1.3.5-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.5/opensearch-dashboards-1.3.5-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.5/opensearch-dashboards-1.3.5-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.5/opensearch-dashboards-1.3.5-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.5\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.5/opensearch-dashboards-1.3.5-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.5/opensearch-dashboards-1.3.5-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.5/opensearch-dashboards-1.3.5-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.5/opensearch-dashboards-1.3.5-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.3.5 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.5/opensearch-min-1.3.5-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.5/opensearch-min-1.3.5-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.5/opensearch-min-1.3.5-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.5/opensearch-min-1.3.5-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.3.5 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.5/opensearch-dashboards-min-1.3.5-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.5/opensearch-dashboards-min-1.3.5-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.5/opensearch-dashboards-min-1.3.5-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.5/opensearch-dashboards-min-1.3.5-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.2.1\nopensearch 2.2.1 arm64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.2.1/opensearch-2.2.1-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.1/opensearch-2.2.1-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.1/opensearch-2.2.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.1/opensearch-2.2.1-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.2.1/opensearch-2.2.1-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.1/opensearch-2.2.1-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.1/opensearch-2.2.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.1/opensearch-2.2.1-linux-x64.tar.gz.sig) opensearch-dashboards 2.2.1 arm64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.1/opensearch-dashboards-2.2.1-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.1/opensearch-dashboards-2.2.1-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.1/opensearch-dashboards-2.2.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.1/opensearch-dashboards-2.2.1-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.1/opensearch-dashboards-2.2.1-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.1/opensearch-dashboards-2.2.1-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.1/opensearch-dashboards-2.2.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.1/opensearch-dashboards-2.2.1-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 2.2.1 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.2.1/opensearch-min-2.2.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.2.1/opensearch-min-2.2.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.2.1/opensearch-min-2.2.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.2.1/opensearch-min-2.2.1-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.2.1 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.2.1/opensearch-dashboards-min-2.2.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.2.1/opensearch-dashboards-min-2.2.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.2.1/opensearch-dashboards-min-2.2.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.2.1/opensearch-dashboards-min-2.2.1-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.2.0\nopensearch 2.2.0 arm64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.2.0/opensearch-2.2.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.0/opensearch-2.2.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.0/opensearch-2.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.0/opensearch-2.2.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.2.0/opensearch-2.2.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.0/opensearch-2.2.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.0/opensearch-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.2.0/opensearch-2.2.0-linux-x64.tar.gz.sig) opensearch-dashboards 2.2.0 arm64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.0/opensearch-dashboards-2.2.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.0/opensearch-dashboards-2.2.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.0/opensearch-dashboards-2.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.0/opensearch-dashboards-2.2.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.0/opensearch-dashboards-2.2.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.0/opensearch-dashboards-2.2.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.0/opensearch-dashboards-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.2.0/opensearch-dashboards-2.2.0-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 2.2.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.2.0/opensearch-min-2.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.2.0/opensearch-min-2.2.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.2.0/opensearch-min-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.2.0/opensearch-min-2.2.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.2.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.2.0/opensearch-dashboards-min-2.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.2.0/opensearch-dashboards-min-2.2.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.2.0/opensearch-dashboards-min-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.2.0/opensearch-dashboards-min-2.2.0-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.3.4\nopensearch 1.3.4 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.4\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.4/opensearch-1.3.4-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.4/opensearch-1.3.4-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.4/opensearch-1.3.4-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.4/opensearch-1.3.4-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.4\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.4/opensearch-1.3.4-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.4/opensearch-1.3.4-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.4/opensearch-1.3.4-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.4/opensearch-1.3.4-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.3.4 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.4\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.4/opensearch-dashboards-1.3.4-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.4/opensearch-dashboards-1.3.4-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.4/opensearch-dashboards-1.3.4-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.4/opensearch-dashboards-1.3.4-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.4\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.4/opensearch-dashboards-1.3.4-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.4/opensearch-dashboards-1.3.4-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.4/opensearch-dashboards-1.3.4-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.4/opensearch-dashboards-1.3.4-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.3.4 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.4/opensearch-min-1.3.4-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.4/opensearch-min-1.3.4-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.4/opensearch-min-1.3.4-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.4/opensearch-min-1.3.4-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.3.4 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.4/opensearch-dashboards-min-1.3.4-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.4/opensearch-dashboards-min-1.3.4-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.4/opensearch-dashboards-min-1.3.4-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.4/opensearch-dashboards-min-1.3.4-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.1.0\nopensearch 2.1.0 arm64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.1.0/opensearch-2.1.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.1.0/opensearch-2.1.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.1.0/opensearch-2.1.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.1.0/opensearch-2.1.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.1.0/opensearch-2.1.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.1.0/opensearch-2.1.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.1.0/opensearch-2.1.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.1.0/opensearch-2.1.0-linux-x64.tar.gz.sig) opensearch-dashboards 2.1.0 arm64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.1.0/opensearch-dashboards-2.1.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.1.0/opensearch-dashboards-2.1.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.1.0/opensearch-dashboards-2.1.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.1.0/opensearch-dashboards-2.1.0-linux-arm64.tar.gz.sig) x64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.1.0/opensearch-dashboards-2.1.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.1.0/opensearch-dashboards-2.1.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.1.0/opensearch-dashboards-2.1.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.1.0/opensearch-dashboards-2.1.0-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 2.1.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.1.0/opensearch-min-2.1.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.1.0/opensearch-min-2.1.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.1.0/opensearch-min-2.1.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.1.0/opensearch-min-2.1.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.1.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.1.0/opensearch-dashboards-min-2.1.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.1.0/opensearch-dashboards-min-2.1.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.1.0/opensearch-dashboards-min-2.1.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.1.0/opensearch-dashboards-min-2.1.0-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.0.1\nopensearch 2.0.1 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.0.1/opensearch-2.0.1-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.1/opensearch-2.0.1-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.1/opensearch-2.0.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.1/opensearch-2.0.1-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.0.1/opensearch-2.0.1-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.1/opensearch-2.0.1-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.1/opensearch-2.0.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.1/opensearch-2.0.1-linux-x64.tar.gz.sig) opensearch-dashboards 2.0.1 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.1/opensearch-dashboards-2.0.1-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.1/opensearch-dashboards-2.0.1-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.1/opensearch-dashboards-2.0.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.1/opensearch-dashboards-2.0.1-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.1/opensearch-dashboards-2.0.1-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.1/opensearch-dashboards-2.0.1-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.1/opensearch-dashboards-2.0.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.1/opensearch-dashboards-2.0.1-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 2.0.1 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.0.1/opensearch-min-2.0.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.0.1/opensearch-min-2.0.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.0.1/opensearch-min-2.0.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.0.1/opensearch-min-2.0.1-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.0.1 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.1/opensearch-dashboards-min-2.0.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.1/opensearch-dashboards-min-2.0.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.1/opensearch-dashboards-min-2.0.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.1/opensearch-dashboards-min-2.0.1-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.3.3\nopensearch 1.3.3 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.3\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.3/opensearch-1.3.3-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.3/opensearch-1.3.3-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.3/opensearch-1.3.3-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.3/opensearch-1.3.3-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.3\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.3/opensearch-1.3.3-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.3/opensearch-1.3.3-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.3/opensearch-1.3.3-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.3/opensearch-1.3.3-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.3.3 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.3\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.3/opensearch-dashboards-1.3.3-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.3/opensearch-dashboards-1.3.3-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.3/opensearch-dashboards-1.3.3-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.3/opensearch-dashboards-1.3.3-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.3\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.3/opensearch-dashboards-1.3.3-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.3/opensearch-dashboards-1.3.3-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.3/opensearch-dashboards-1.3.3-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.3/opensearch-dashboards-1.3.3-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.3.3 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.3/opensearch-min-1.3.3-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.3/opensearch-min-1.3.3-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.3/opensearch-min-1.3.3-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.3/opensearch-min-1.3.3-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.3.3 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.3/opensearch-dashboards-min-1.3.3-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.3/opensearch-dashboards-min-1.3.3-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.3/opensearch-dashboards-min-1.3.3-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.3/opensearch-dashboards-min-1.3.3-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.0.0\nopensearch 2.0.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.0\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0/opensearch-2.0.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0/opensearch-2.0.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0/opensearch-2.0.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0/opensearch-2.0.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.0\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0/opensearch-2.0.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0/opensearch-2.0.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0/opensearch-2.0.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0/opensearch-2.0.0-linux-x64.tar.gz.sig) opensearch-dashboards 2.0.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.0\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0/opensearch-dashboards-2.0.0-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0/opensearch-dashboards-2.0.0-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0/opensearch-dashboards-2.0.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0/opensearch-dashboards-2.0.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.0\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0/opensearch-dashboards-2.0.0-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0/opensearch-dashboards-2.0.0-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0/opensearch-dashboards-2.0.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0/opensearch-dashboards-2.0.0-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 2.0.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.0.0/opensearch-min-2.0.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.0.0/opensearch-min-2.0.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.0.0/opensearch-min-2.0.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.0.0/opensearch-min-2.0.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.0.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.0/opensearch-dashboards-min-2.0.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.0/opensearch-dashboards-min-2.0.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.0/opensearch-dashboards-min-2.0.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.0/opensearch-dashboards-min-2.0.0-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.3.2\nopensearch 1.3.2 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.2\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.2/opensearch-1.3.2-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.2/opensearch-1.3.2-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.2/opensearch-1.3.2-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.2/opensearch-1.3.2-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.2\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.2/opensearch-1.3.2-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.2/opensearch-1.3.2-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.x/opensearch-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.2/opensearch-1.3.2-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.2/opensearch-1.3.2-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.3.2 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.2\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.2/opensearch-dashboards-1.3.2-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.2/opensearch-dashboards-1.3.2-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.2/opensearch-dashboards-1.3.2-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.2/opensearch-dashboards-1.3.2-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.2\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.2/opensearch-dashboards-1.3.2-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.2/opensearch-dashboards-1.3.2-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.x/opensearch-dashboards-1.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.2/opensearch-dashboards-1.3.2-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.2/opensearch-dashboards-1.3.2-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.3.2 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.2/opensearch-min-1.3.2-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.2/opensearch-min-1.3.2-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.2/opensearch-min-1.3.2-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.2/opensearch-min-1.3.2-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.3.2 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.2/opensearch-dashboards-min-1.3.2-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.2/opensearch-dashboards-min-1.3.2-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.2/opensearch-dashboards-min-1.3.2-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.2/opensearch-dashboards-min-1.3.2-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 2.0.0-rc1\nopensearch 2.0.0-rc1 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.0-rc1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0-rc1/opensearch-2.0.0-rc1-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0-rc1/opensearch-2.0.0-rc1-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0-rc1/opensearch-2.0.0-rc1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0-rc1/opensearch-2.0.0-rc1-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.0-rc1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0-rc1/opensearch-2.0.0-rc1-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0-rc1/opensearch-2.0.0-rc1-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.x/opensearch-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0-rc1/opensearch-2.0.0-rc1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/2.0.0-rc1/opensearch-2.0.0-rc1-linux-x64.tar.gz.sig) opensearch-dashboards 2.0.0-rc1 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.0-rc1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-2.0.0-rc1-linux-arm64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-2.0.0-rc1-linux-arm64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-2.0.0-rc1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-2.0.0-rc1-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.0-rc1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-2.0.0-rc1-linux-x64.rpm (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-2.0.0-rc1-linux-x64.rpm.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.x/opensearch-dashboards-2.x.repo.sig) https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-2.0.0-rc1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-2.0.0-rc1-linux-x64.tar.gz.sig) opensearch-min 2.0.0-rc1 arm64 https://artifacts.opensearch.org/releases/core/opensearch/2.0.0-rc1/opensearch-min-2.0.0-rc1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.0.0-rc1/opensearch-min-2.0.0-rc1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/2.0.0-rc1/opensearch-min-2.0.0-rc1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/2.0.0-rc1/opensearch-min-2.0.0-rc1-linux-x64.tar.gz.sig) opensearch-dashboards-min 2.0.0-rc1 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-min-2.0.0-rc1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-min-2.0.0-rc1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-min-2.0.0-rc1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/2.0.0-rc1/opensearch-dashboards-min-2.0.0-rc1-linux-x64.tar.gz.sig) Release 1.3.1\nopensearch 1.3.1 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.1\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.1/opensearch-1.3.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.1/opensearch-1.3.1-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.1\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.1/opensearch-1.3.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.1/opensearch-1.3.1-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.3.1 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.1/opensearch-dashboards-1.3.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.1/opensearch-dashboards-1.3.1-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.1/opensearch-dashboards-1.3.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.1/opensearch-dashboards-1.3.1-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.3.1 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.1/opensearch-min-1.3.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.1/opensearch-min-1.3.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.1/opensearch-min-1.3.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.1/opensearch-min-1.3.1-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.3.1 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.1/opensearch-dashboards-min-1.3.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.1/opensearch-dashboards-min-1.3.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.1/opensearch-dashboards-min-1.3.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.1/opensearch-dashboards-min-1.3.1-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.3.0\nopensearch 1.3.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.0\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.0/opensearch-1.3.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.0/opensearch-1.3.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.0\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.3.0/opensearch-1.3.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.3.0/opensearch-1.3.0-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.3.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.0/opensearch-dashboards-1.3.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.0/opensearch-dashboards-1.3.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.0/opensearch-dashboards-1.3.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.3.0/opensearch-dashboards-1.3.0-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch-dashboards\nopensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.3.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.0/opensearch-min-1.3.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.0/opensearch-min-1.3.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.3.0/opensearch-min-1.3.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.3.0/opensearch-min-1.3.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.3.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.0/opensearch-dashboards-min-1.3.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.0/opensearch-dashboards-min-1.3.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.0/opensearch-dashboards-min-1.3.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.3.0/opensearch-dashboards-min-1.3.0-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.2.4\nopensearch 1.2.4 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.4\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.2.4/opensearch-1.2.4-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.4/opensearch-1.2.4-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.4\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.2.4/opensearch-1.2.4-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.4/opensearch-1.2.4-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.2.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch-dashboards\nopensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.2.4 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.2.4/opensearch-min-1.2.4-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.2.4/opensearch-min-1.2.4-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.2.4/opensearch-min-1.2.4-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.2.4/opensearch-min-1.2.4-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.2.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-x64.tar.gz.sig) opensearch-sql-odbc 1.1.0.1 x64 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-mac-1.1.0.1.zip\nhttps://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win64-1.1.0.1.msi\nx86 https://artifacts.opensearch.org/opensearch-clients/odbc/signed_opensearch-sql-odbc-win32-1.1.0.1.msi\nopensearch-sql-jdbc 1.1.0.1 jvm https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar (sig: https://artifacts.opensearch.org/opensearch-clients/jdbc/opensearch-sql-jdbc-1.1.0.1.jar.asc) Release 1.2.3\nopensearch 1.2.3 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.3\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.2.3/opensearch-1.2.3-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.3/opensearch-1.2.3-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.3\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.2.3/opensearch-1.2.3-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.3/opensearch-1.2.3-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.2.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch-dashboards\nopensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.2.3 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.2.3/opensearch-min-1.2.3-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.2.3/opensearch-min-1.2.3-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.2.3/opensearch-min-1.2.3-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.2.3/opensearch-min-1.2.3-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.2.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-x64.tar.gz.sig) Release 1.2.2\nopensearch 1.2.2 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.2\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.2.2/opensearch-1.2.2-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.2/opensearch-1.2.2-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.2\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.2.2/opensearch-1.2.2-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.2/opensearch-1.2.2-linux-x64.tar.gz.sig) opensearch-dashboards 1.2.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch-dashboards\nopensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.2.2 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.2.2/opensearch-min-1.2.2-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.2.2/opensearch-min-1.2.2-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.2.2/opensearch-min-1.2.2-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.2.2/opensearch-min-1.2.2-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.2.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-x64.tar.gz.sig) Release 1.2.1\nopensearch 1.2.1 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.1\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.2.1/opensearch-1.2.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.1/opensearch-1.2.1-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.1\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.2.1/opensearch-1.2.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.1/opensearch-1.2.1-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.2.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch-dashboards\nopensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.2.1 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.2.1/opensearch-min-1.2.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.2.1/opensearch-min-1.2.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.2.1/opensearch-min-1.2.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.2.1/opensearch-min-1.2.1-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.2.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-x64.tar.gz.sig) Release 1.2.0\nopensearch 1.2.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.2.0/opensearch-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.0/opensearch-1.2.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.2.0/opensearch-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.0/opensearch-1.2.0-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.2.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.2.0/opensearch-dashboards-1.2.0-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch-dashboards\nopensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.2.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.2.0/opensearch-min-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.2.0/opensearch-min-1.2.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.2.0/opensearch-min-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.2.0/opensearch-min-1.2.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.2.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.2.0/opensearch-dashboards-min-1.2.0-linux-x64.tar.gz.sig) Release 1.1.0\nopensearch 1.1.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.1.0\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.1.0/opensearch-1.1.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.1.0/opensearch-1.1.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.1.0\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.1.0/opensearch-1.1.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.1.0/opensearch-1.1.0-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.1.0 arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.1.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.1.0/opensearch-dashboards-1.1.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.1.0/opensearch-dashboards-1.1.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.1.0\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.1.0/opensearch-dashboards-1.1.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.1.0/opensearch-dashboards-1.1.0-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch-dashboards\nopensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.1.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.1.0/opensearch-min-1.1.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.1.0/opensearch-min-1.1.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.1.0/opensearch-min-1.1.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.1.0/opensearch-min-1.1.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.1.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.1.0/opensearch-dashboards-min-1.1.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.1.0/opensearch-dashboards-min-1.1.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.1.0/opensearch-dashboards-min-1.1.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.1.0/opensearch-dashboards-min-1.1.0-linux-x64.tar.gz.sig) Release 1.0.1\nopensearch 1.0.1 arm64 https://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.0.1/opensearch-1.0.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.0.1/opensearch-1.0.1-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.0.1\nhttps://www.freshports.org/textproc/opensearch\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.0.1/opensearch-1.0.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.0.1/opensearch-1.0.1-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch\nopensearch-dashboards 1.0.1 arm64 https://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.0.1/opensearch-dashboards-1.0.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.0.1/opensearch-dashboards-1.0.1-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.0.1\nhttps://www.freshports.org/textproc/opensearch-dashboards\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.0.1/opensearch-dashboards-1.0.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.0.1/opensearch-dashboards-1.0.1-linux-x64.tar.gz.sig) x86 https://www.freshports.org/textproc/opensearch-dashboards\nopensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.0.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.0.0/opensearch-min-1.0.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.0.0/opensearch-min-1.0.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.0.0/opensearch-min-1.0.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.0.0/opensearch-min-1.0.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.0.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.0.0/opensearch-dashboards-min-1.0.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.0.0/opensearch-dashboards-min-1.0.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.0.0/opensearch-dashboards-min-1.0.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.0.0/opensearch-dashboards-min-1.0.0-linux-x64.tar.gz.sig) Release 1.0.0\nopensearch 1.0.0 arm64 https://artifacts.opensearch.org/releases/bundle/opensearch/1.0.0/opensearch-1.0.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.0.0/opensearch-1.0.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.0.0\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.0.0/opensearch-1.0.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.0.0/opensearch-1.0.0-linux-x64.tar.gz.sig) opensearch-dashboards 1.0.0 arm64 https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.0.0/opensearch-dashboards-1.0.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.0.0/opensearch-dashboards-1.0.0-linux-arm64.tar.gz.sig) x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.0.0\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.0.0/opensearch-dashboards-1.0.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.0.0/opensearch-dashboards-1.0.0-linux-x64.tar.gz.sig) opensearch-cli 1.1.0 arm64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-arm64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-arm64.pkg\nx64 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-linux-x64.zip\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-macos-x64.pkg\nhttps://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x64.zip\nx86 https://artifacts.opensearch.org/opensearch-clients/opensearch-cli/opensearch-cli-1.1.0-windows-x86.zip\nlogstash-oss-with-opensearch-output-plugin 8.6.1 arm64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz.sig) https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz (sig: https://artifacts.opensearch.org/logstash/logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz.sig) data-prepper data-prepper-2.2.0 x64 https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/data-prepper/2.2.0/opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz.sig) opensearch-min 1.0.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch/1.0.0/opensearch-min-1.0.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.0.0/opensearch-min-1.0.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch/1.0.0/opensearch-min-1.0.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch/1.0.0/opensearch-min-1.0.0-linux-x64.tar.gz.sig) opensearch-dashboards-min 1.0.0 arm64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.0.0/opensearch-dashboards-min-1.0.0-linux-arm64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.0.0/opensearch-dashboards-min-1.0.0-linux-arm64.tar.gz.sig) x64 https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.0.0/opensearch-dashboards-min-1.0.0-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/core/opensearch-dashboards/1.0.0/opensearch-dashboards-min-1.0.0-linux-x64.tar.gz.sig) Release 1.0.0 (Release Candidate 1)\nopensearch 1.0.0-rc1 x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.0.0-rc1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch/1.0.0-rc1/opensearch-1.0.0-rc1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch/1.0.0-rc1/opensearch-1.0.0-rc1-linux-x64.tar.gz.sig) opensearch-dashboards 1.0.0-rc1 x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.0.0-rc1\nhttps://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.0.0-rc1/opensearch-dashboards-1.0.0-rc1-linux-x64.tar.gz (sig: https://artifacts.opensearch.org/releases/bundle/opensearch-dashboards/1.0.0-rc1/opensearch-dashboards-1.0.0-rc1-linux-x64.tar.gz.sig)",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/artifacts/",
    "title": "Project Artifacts",
    "content": "data-prepper\ndocker-x64 https://hub.docker.com/r/opensearchproject/data-prepper/tags?page=1&ordering=last_updated&name=1.0.0 [External] https://hub.docker.com/r/opensearchproject/data-prepper/tags?page=1&ordering=last_updated&name=1.1.0 [External] https://hub.docker.com/r/opensearchproject/data-prepper/tags?page=1&ordering=last_updated&name=1.1.1 [External] https://hub.docker.com/r/opensearchproject/data-prepper/tags?page=1&ordering=last_updated&name=1.2.0 [External] https://hub.docker.com/r/opensearchproject/data-prepper/tags?page=1&ordering=last_updated&name=1.2.1 [External] https://hub.docker.com/r/opensearchproject/data-prepper/tags?page=1&ordering=last_updated&name=1.3.0 [External] https://hub.docker.com/r/opensearchproject/data-prepper/tags?page=1&ordering=last_updated&name=1.4.0 [External] https://hub.docker.com/r/opensearchproject/data-prepper/tags?page=1&ordering=last_updated&name=1.5.0 [External]\nlinux-x64 opensearch-data-prepper-jdk-1.2.1-linux-x64.tar.gz opensearch-data-prepper-jdk-1.3.0-linux-x64.tar.gz opensearch-data-prepper-jdk-1.4.0-linux-x64.tar.gz opensearch-data-prepper-jdk-1.5.0-linux-x64.tar.gz opensearch-data-prepper-jdk-1.5.1-linux-x64.tar.gz opensearch-data-prepper-jdk-1.5.2-linux-x64.tar.gz opensearch-data-prepper-jdk-2.0.0-linux-x64.tar.gz opensearch-data-prepper-jdk-2.0.1-linux-x64.tar.gz opensearch-data-prepper-jdk-2.1.0-linux-x64.tar.gz opensearch-data-prepper-jdk-2.1.1-linux-x64.tar.gz opensearch-data-prepper-jdk-2.2.0-linux-x64.tar.gz data-prepper-no-jdk\nlinux-x64 opensearch-data-prepper-1.2.1-linux-x64.tar.gz opensearch-data-prepper-1.3.0-linux-x64.tar.gz opensearch-data-prepper-1.4.0-linux-x64.tar.gz opensearch-data-prepper-1.5.0-linux-x64.tar.gz opensearch-data-prepper-1.5.1-linux-x64.tar.gz opensearch-data-prepper-1.5.2-linux-x64.tar.gz opensearch-data-prepper-2.0.0-linux-x64.tar.gz opensearch-data-prepper-2.0.1-linux-x64.tar.gz opensearch-data-prepper-2.1.0-linux-x64.tar.gz opensearch-data-prepper-2.1.1-linux-x64.tar.gz opensearch-data-prepper-2.2.0-linux-x64.tar.gz logstash-oss-with-opensearch-output-plugin\ndocker-arm64 https://hub.docker.com/r/opensearchproject/logstash-oss-with-opensearch-output-plugin [External] https://hub.docker.com/r/opensearchproject/logstash-oss-with-opensearch-output-plugin [External] https://hub.docker.com/r/opensearchproject/logstash-oss-with-opensearch-output-plugin [External]\ndocker-x64 https://hub.docker.com/r/opensearchproject/logstash-oss-with-opensearch-output-plugin [External] https://hub.docker.com/r/opensearchproject/logstash-oss-with-opensearch-output-plugin [External] https://hub.docker.com/r/opensearchproject/logstash-oss-with-opensearch-output-plugin [External]\nlinux-arm64 logstash-oss-with-opensearch-output-plugin-7.13.2-linux-arm64.tar.gz logstash-oss-with-opensearch-output-plugin-7.16.1-linux-arm64.tar.gz logstash-oss-with-opensearch-output-plugin-7.16.2-linux-arm64.tar.gz logstash-oss-with-opensearch-output-plugin-7.16.3-linux-arm64.tar.gz logstash-oss-with-opensearch-output-plugin-8.4.0-linux-arm64.tar.gz logstash-oss-with-opensearch-output-plugin-8.6.1-linux-arm64.tar.gz linux-x64 logstash-oss-with-opensearch-output-plugin-7.13.2-linux-x64.tar.gz logstash-oss-with-opensearch-output-plugin-7.16.1-linux-x64.tar.gz logstash-oss-with-opensearch-output-plugin-7.16.2-linux-x64.tar.gz logstash-oss-with-opensearch-output-plugin-7.16.3-linux-x64.tar.gz logstash-oss-with-opensearch-output-plugin-8.4.0-linux-x64.tar.gz logstash-oss-with-opensearch-output-plugin-8.6.1-linux-x64.tar.gz macos-x64 logstash-oss-with-opensearch-output-plugin-7.13.2-macos-x64.tar.gz logstash-oss-with-opensearch-output-plugin-7.16.1-macos-x64.tar.gz logstash-oss-with-opensearch-output-plugin-7.16.2-macos-x64.tar.gz logstash-oss-with-opensearch-output-plugin-7.16.3-macos-x64.tar.gz logstash-oss-with-opensearch-output-plugin-8.4.0-macos-x64.tar.gz logstash-oss-with-opensearch-output-plugin-8.6.1-macos-x64.tar.gz opensearch\ndocker-x64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.0.0 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.0.0-rc1 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.0.1 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.1.0 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.0 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.1 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.2 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.3 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.4 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.0 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.1 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.2 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.3 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.4 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.5 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.6 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.7 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.8 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.9 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.0 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.0-rc1 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.1 [External]\ndocker-arm64 https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.1.0 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.0 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.1 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.2 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.3 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.2.4 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.0 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.1 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.2 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.3 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.4 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.5 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.6 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.7 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.8 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=1.3.9 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.0 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.0-rc1 [External] https://hub.docker.com/r/opensearchproject/opensearch/tags?page=1&ordering=last_updated&name=2.0.1 [External]\nfreebsd-arm64 https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External]\nfreebsd-x64 https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External]\nfreebsd-x86 https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External] https://www.freshports.org/textproc/opensearch [External]\nlinux-arm64 opensearch-1.0.0-linux-arm64.tar.gz opensearch-1.0.1-linux-arm64.tar.gz opensearch-1.1.0-linux-arm64.tar.gz opensearch-1.2.0-linux-arm64.tar.gz opensearch-1.2.1-linux-arm64.tar.gz opensearch-1.2.2-linux-arm64.tar.gz opensearch-1.2.3-linux-arm64.tar.gz opensearch-1.2.4-linux-arm64.tar.gz opensearch-1.3.0-linux-arm64.tar.gz opensearch-1.3.1-linux-arm64.tar.gz opensearch-1.3.2-linux-arm64.rpm opensearch-1.x.repo opensearch-1.3.2-linux-arm64.tar.gz opensearch-1.3.3-linux-arm64.rpm opensearch-1.x.repo opensearch-1.3.3-linux-arm64.tar.gz opensearch-1.3.4-linux-arm64.rpm opensearch-1.x.repo opensearch-1.3.4-linux-arm64.tar.gz opensearch-1.3.5-linux-arm64.rpm opensearch-1.x.repo opensearch-1.3.5-linux-arm64.tar.gz opensearch-1.3.6-linux-arm64.rpm opensearch-1.x.repo opensearch-1.3.6-linux-arm64.tar.gz opensearch-1.3.7-linux-arm64.rpm opensearch-1.x.repo opensearch-1.3.7-linux-arm64.tar.gz opensearch-1.3.8-linux-arm64.rpm opensearch-1.x.repo opensearch-1.3.8-linux-arm64.tar.gz opensearch-1.3.9-linux-arm64.deb opensearch-1.3.9-linux-arm64.rpm opensearch-1.x.repo opensearch-1.3.9-linux-arm64.tar.gz opensearch-2.0.0-linux-arm64.rpm opensearch-2.x.repo opensearch-2.0.0-linux-arm64.tar.gz opensearch-2.0.0-rc1-linux-arm64.rpm opensearch-2.x.repo opensearch-2.0.0-rc1-linux-arm64.tar.gz opensearch-2.0.1-linux-arm64.rpm opensearch-2.x.repo opensearch-2.0.1-linux-arm64.tar.gz opensearch-2.1.0-linux-arm64.rpm opensearch-2.x.repo opensearch-2.1.0-linux-arm64.tar.gz opensearch-2.2.0-linux-arm64.rpm opensearch-2.x.repo opensearch-2.2.0-linux-arm64.tar.gz opensearch-2.2.1-linux-arm64.rpm opensearch-2.x.repo opensearch-2.2.1-linux-arm64.tar.gz opensearch-2.3.0-linux-arm64.rpm opensearch-2.x.repo opensearch-2.3.0-linux-arm64.tar.gz opensearch-2.4.0-linux-arm64.rpm opensearch-2.x.repo opensearch-2.4.0-linux-arm64.tar.gz opensearch-2.4.1-linux-arm64.rpm opensearch-2.x.repo opensearch-2.4.1-linux-arm64.tar.gz opensearch-2.5.0-linux-arm64.deb opensearch-2.5.0-linux-arm64.rpm opensearch-2.x.repo opensearch-2.5.0-linux-arm64.tar.gz opensearch-2.6.0-linux-arm64.deb opensearch-2.6.0-linux-arm64.rpm opensearch-2.x.repo opensearch-2.6.0-linux-arm64.tar.gz opensearch-2.7.0-linux-arm64.deb opensearch-2.7.0-linux-arm64.rpm opensearch-2.x.repo opensearch-2.7.0-linux-arm64.tar.gz linux-x64 opensearch-1.0.0-linux-x64.tar.gz opensearch-1.0.0-rc1-linux-x64.tar.gz opensearch-1.0.1-linux-x64.tar.gz opensearch-1.1.0-linux-x64.tar.gz opensearch-1.2.0-linux-x64.tar.gz opensearch-1.2.1-linux-x64.tar.gz opensearch-1.2.2-linux-x64.tar.gz opensearch-1.2.3-linux-x64.tar.gz opensearch-1.2.4-linux-x64.tar.gz opensearch-1.3.0-linux-x64.tar.gz opensearch-1.3.1-linux-x64.tar.gz opensearch-1.3.2-linux-x64.rpm opensearch-1.x.repo opensearch-1.3.2-linux-x64.tar.gz opensearch-1.3.3-linux-x64.rpm opensearch-1.x.repo opensearch-1.3.3-linux-x64.tar.gz opensearch-1.3.4-linux-x64.rpm opensearch-1.x.repo opensearch-1.3.4-linux-x64.tar.gz opensearch-1.3.5-linux-x64.rpm opensearch-1.x.repo opensearch-1.3.5-linux-x64.tar.gz opensearch-1.3.6-linux-x64.rpm opensearch-1.x.repo opensearch-1.3.6-linux-x64.tar.gz opensearch-1.3.7-linux-x64.rpm opensearch-1.x.repo opensearch-1.3.7-linux-x64.tar.gz opensearch-1.3.8-linux-x64.rpm opensearch-1.x.repo opensearch-1.3.8-linux-x64.tar.gz opensearch-1.3.9-linux-x64.deb opensearch-1.3.9-linux-x64.rpm opensearch-1.x.repo opensearch-1.3.9-linux-x64.tar.gz opensearch-2.0.0-linux-x64.rpm opensearch-2.x.repo opensearch-2.0.0-linux-x64.tar.gz opensearch-2.0.0-rc1-linux-x64.rpm opensearch-2.x.repo opensearch-2.0.0-rc1-linux-x64.tar.gz opensearch-2.0.1-linux-x64.rpm opensearch-2.x.repo opensearch-2.0.1-linux-x64.tar.gz opensearch-2.1.0-linux-x64.rpm opensearch-2.x.repo opensearch-2.1.0-linux-x64.tar.gz opensearch-2.2.0-linux-x64.rpm opensearch-2.x.repo opensearch-2.2.0-linux-x64.tar.gz opensearch-2.2.1-linux-x64.rpm opensearch-2.x.repo opensearch-2.2.1-linux-x64.tar.gz opensearch-2.3.0-linux-x64.rpm opensearch-2.x.repo opensearch-2.3.0-linux-x64.tar.gz opensearch-2.4.0-linux-x64.rpm opensearch-2.x.repo opensearch-2.4.0-linux-x64.tar.gz opensearch-2.4.1-linux-x64.rpm opensearch-2.x.repo opensearch-2.4.1-linux-x64.tar.gz opensearch-2.5.0-linux-x64.deb opensearch-2.5.0-linux-x64.rpm opensearch-2.x.repo opensearch-2.5.0-linux-x64.tar.gz opensearch-2.6.0-linux-x64.deb opensearch-2.6.0-linux-x64.rpm opensearch-2.x.repo opensearch-2.6.0-linux-x64.tar.gz opensearch-2.7.0-linux-x64.deb opensearch-2.7.0-linux-x64.rpm opensearch-2.x.repo opensearch-2.7.0-linux-x64.tar.gz windows-x64 opensearch-1.3.7-windows-x64.zip opensearch-1.3.8-windows-x64.zip opensearch-1.3.9-windows-x64.zip opensearch-2.4.0-windows-x64.zip opensearch-2.4.1-windows-x64.zip opensearch-2.5.0-windows-x64.zip opensearch-2.6.0-windows-x64.zip opensearch-2.7.0-windows-x64.zip opensearch-cli\nlinux-arm64 opensearch-cli-1.0.0-linux-arm64.zip opensearch-cli-1.1.0-linux-arm64.zip linux-x64 opensearch-cli-1.0.0-linux-x64.zip opensearch-cli-1.1.0-linux-x64.zip macos-arm64 opensearch-cli-1.0.0-macos-arm64.pkg opensearch-cli-1.1.0-macos-arm64.pkg macos-x64 opensearch-cli-1.0.0-macos-x64.pkg opensearch-cli-1.1.0-macos-x64.pkg windows-x64 opensearch-cli-1.0.0-windows-x64.zip opensearch-cli-1.1.0-windows-x64.zip windows-x86 opensearch-cli-1.0.0-windows-x86.zip opensearch-cli-1.1.0-windows-x86.zip opensearch-dashboards\ndocker-x64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.0.0 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.0.0-rc1 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.0.1 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.1.0 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.0 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.1 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.2 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.3 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.4 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.5 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.6 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.7 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.8 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.9 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.0 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.0-rc1 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.1 [External]\ndocker-arm64 https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.1.0 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.2.0 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.0 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.1 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.2 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.3 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.4 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.5 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.6 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.7 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.8 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=1.3.9 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.0 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.0-rc1 [External] https://hub.docker.com/r/opensearchproject/opensearch-dashboards/tags?page=1&ordering=last_updated&name=2.0.1 [External]\nfreebsd-arm64 https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External]\nfreebsd-x64 https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External]\nfreebsd-x86 https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External] https://www.freshports.org/textproc/opensearch-dashboards [External]\nlinux-arm64 opensearch-dashboards-1.0.0-linux-arm64.tar.gz opensearch-dashboards-1.0.1-linux-arm64.tar.gz opensearch-dashboards-1.1.0-linux-arm64.tar.gz opensearch-dashboards-1.2.0-linux-arm64.tar.gz opensearch-dashboards-1.3.0-linux-arm64.tar.gz opensearch-dashboards-1.3.1-linux-arm64.tar.gz opensearch-dashboards-1.3.2-linux-arm64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.2-linux-arm64.tar.gz opensearch-dashboards-1.3.3-linux-arm64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.3-linux-arm64.tar.gz opensearch-dashboards-1.3.4-linux-arm64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.4-linux-arm64.tar.gz opensearch-dashboards-1.3.5-linux-arm64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.5-linux-arm64.tar.gz opensearch-dashboards-1.3.6-linux-arm64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.6-linux-arm64.tar.gz opensearch-dashboards-1.3.7-linux-arm64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.7-linux-arm64.tar.gz opensearch-dashboards-1.3.8-linux-arm64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.8-linux-arm64.tar.gz opensearch-dashboards-1.3.9-linux-arm64.deb opensearch-dashboards-1.3.9-linux-arm64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.9-linux-arm64.tar.gz opensearch-dashboards-2.0.0-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.0.0-linux-arm64.tar.gz opensearch-dashboards-2.0.0-rc1-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.0.0-rc1-linux-arm64.tar.gz opensearch-dashboards-2.0.1-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.0.1-linux-arm64.tar.gz opensearch-dashboards-2.1.0-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.1.0-linux-arm64.tar.gz opensearch-dashboards-2.2.0-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.2.0-linux-arm64.tar.gz opensearch-dashboards-2.2.1-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.2.1-linux-arm64.tar.gz opensearch-dashboards-2.3.0-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.3.0-linux-arm64.tar.gz opensearch-dashboards-2.4.0-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.4.0-linux-arm64.tar.gz opensearch-dashboards-2.4.1-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.4.1-linux-arm64.tar.gz opensearch-dashboards-2.5.0-linux-arm64.deb opensearch-dashboards-2.5.0-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.5.0-linux-arm64.tar.gz opensearch-dashboards-2.6.0-linux-arm64.deb opensearch-dashboards-2.6.0-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.6.0-linux-arm64.tar.gz opensearch-dashboards-2.7.0-linux-arm64.deb opensearch-dashboards-2.7.0-linux-arm64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.7.0-linux-arm64.tar.gz linux-x64 opensearch-dashboards-1.0.0-linux-x64.tar.gz opensearch-dashboards-1.0.0-rc1-linux-x64.tar.gz opensearch-dashboards-1.0.1-linux-x64.tar.gz opensearch-dashboards-1.1.0-linux-x64.tar.gz opensearch-dashboards-1.2.0-linux-x64.tar.gz opensearch-dashboards-1.3.0-linux-x64.tar.gz opensearch-dashboards-1.3.1-linux-x64.tar.gz opensearch-dashboards-1.3.2-linux-x64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.2-linux-x64.tar.gz opensearch-dashboards-1.3.3-linux-x64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.3-linux-x64.tar.gz opensearch-dashboards-1.3.4-linux-x64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.4-linux-x64.tar.gz opensearch-dashboards-1.3.5-linux-x64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.5-linux-x64.tar.gz opensearch-dashboards-1.3.6-linux-x64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.6-linux-x64.tar.gz opensearch-dashboards-1.3.7-linux-x64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.7-linux-x64.tar.gz opensearch-dashboards-1.3.8-linux-x64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.8-linux-x64.tar.gz opensearch-dashboards-1.3.9-linux-x64.deb opensearch-dashboards-1.3.9-linux-x64.rpm opensearch-dashboards-1.x.repo opensearch-dashboards-1.3.9-linux-x64.tar.gz opensearch-dashboards-2.0.0-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.0.0-linux-x64.tar.gz opensearch-dashboards-2.0.0-rc1-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.0.0-rc1-linux-x64.tar.gz opensearch-dashboards-2.0.1-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.0.1-linux-x64.tar.gz opensearch-dashboards-2.1.0-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.1.0-linux-x64.tar.gz opensearch-dashboards-2.2.0-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.2.0-linux-x64.tar.gz opensearch-dashboards-2.2.1-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.2.1-linux-x64.tar.gz opensearch-dashboards-2.3.0-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.3.0-linux-x64.tar.gz opensearch-dashboards-2.4.0-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.4.0-linux-x64.tar.gz opensearch-dashboards-2.4.1-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.4.1-linux-x64.tar.gz opensearch-dashboards-2.5.0-linux-x64.deb opensearch-dashboards-2.5.0-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.5.0-linux-x64.tar.gz opensearch-dashboards-2.6.0-linux-x64.deb opensearch-dashboards-2.6.0-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.6.0-linux-x64.tar.gz opensearch-dashboards-2.7.0-linux-x64.deb opensearch-dashboards-2.7.0-linux-x64.rpm opensearch-dashboards-2.x.repo opensearch-dashboards-2.7.0-linux-x64.tar.gz windows-x64 opensearch-dashboards-1.3.7-windows-x64.zip opensearch-dashboards-1.3.8-windows-x64.zip opensearch-dashboards-1.3.9-windows-x64.zip opensearch-dashboards-2.4.0-windows-x64.zip opensearch-dashboards-2.4.1-windows-x64.zip opensearch-dashboards-2.5.0-windows-x64.zip opensearch-dashboards-2.6.0-windows-x64.zip opensearch-dashboards-2.7.0-windows-x64.zip opensearch-dashboards-min\nlinux-arm64 opensearch-dashboards-min-1.0.0-linux-arm64.tar.gz opensearch-dashboards-min-1.1.0-linux-arm64.tar.gz opensearch-dashboards-min-1.2.0-linux-arm64.tar.gz opensearch-dashboards-min-1.3.0-linux-arm64.tar.gz opensearch-dashboards-min-1.3.1-linux-arm64.tar.gz opensearch-dashboards-min-1.3.2-linux-arm64.tar.gz opensearch-dashboards-min-1.3.3-linux-arm64.tar.gz opensearch-dashboards-min-1.3.4-linux-arm64.tar.gz opensearch-dashboards-min-1.3.5-linux-arm64.tar.gz opensearch-dashboards-min-1.3.6-linux-arm64.tar.gz opensearch-dashboards-min-1.3.7-linux-arm64.tar.gz opensearch-dashboards-min-1.3.8-linux-arm64.tar.gz opensearch-dashboards-min-1.3.9-linux-arm64.tar.gz opensearch-dashboards-min-2.0.0-linux-arm64.tar.gz opensearch-dashboards-min-2.0.0-rc1-linux-arm64.tar.gz opensearch-dashboards-min-2.0.1-linux-arm64.tar.gz opensearch-dashboards-min-2.1.0-linux-arm64.tar.gz opensearch-dashboards-min-2.2.0-linux-arm64.tar.gz opensearch-dashboards-min-2.2.1-linux-arm64.tar.gz opensearch-dashboards-min-2.3.0-linux-arm64.tar.gz opensearch-dashboards-min-2.4.0-linux-arm64.tar.gz opensearch-dashboards-min-2.4.1-linux-arm64.tar.gz opensearch-dashboards-min-2.5.0-linux-arm64.tar.gz opensearch-dashboards-min-2.6.0-linux-arm64.tar.gz opensearch-dashboards-min-2.7.0-linux-arm64.tar.gz linux-x64 opensearch-dashboards-min-1.0.0-linux-x64.tar.gz opensearch-dashboards-min-1.1.0-linux-x64.tar.gz opensearch-dashboards-min-1.2.0-linux-x64.tar.gz opensearch-dashboards-min-1.3.0-linux-x64.tar.gz opensearch-dashboards-min-1.3.1-linux-x64.tar.gz opensearch-dashboards-min-1.3.2-linux-x64.tar.gz opensearch-dashboards-min-1.3.3-linux-x64.tar.gz opensearch-dashboards-min-1.3.4-linux-x64.tar.gz opensearch-dashboards-min-1.3.5-linux-x64.tar.gz opensearch-dashboards-min-1.3.6-linux-x64.tar.gz opensearch-dashboards-min-1.3.7-linux-x64.tar.gz opensearch-dashboards-min-1.3.8-linux-x64.tar.gz opensearch-dashboards-min-1.3.9-linux-x64.tar.gz opensearch-dashboards-min-2.0.0-linux-x64.tar.gz opensearch-dashboards-min-2.0.0-rc1-linux-x64.tar.gz opensearch-dashboards-min-2.0.1-linux-x64.tar.gz opensearch-dashboards-min-2.1.0-linux-x64.tar.gz opensearch-dashboards-min-2.2.0-linux-x64.tar.gz opensearch-dashboards-min-2.2.1-linux-x64.tar.gz opensearch-dashboards-min-2.3.0-linux-x64.tar.gz opensearch-dashboards-min-2.4.0-linux-x64.tar.gz opensearch-dashboards-min-2.4.1-linux-x64.tar.gz opensearch-dashboards-min-2.5.0-linux-x64.tar.gz opensearch-dashboards-min-2.6.0-linux-x64.tar.gz opensearch-dashboards-min-2.7.0-linux-x64.tar.gz opensearch-min\nlinux-arm64 opensearch-min-1.0.0-linux-arm64.tar.gz opensearch-min-1.1.0-linux-arm64.tar.gz opensearch-min-1.2.0-linux-arm64.tar.gz opensearch-min-1.2.1-linux-arm64.tar.gz opensearch-min-1.2.2-linux-arm64.tar.gz opensearch-min-1.2.3-linux-arm64.tar.gz opensearch-min-1.2.4-linux-arm64.tar.gz opensearch-min-1.3.0-linux-arm64.tar.gz opensearch-min-1.3.1-linux-arm64.tar.gz opensearch-min-1.3.2-linux-arm64.tar.gz opensearch-min-1.3.3-linux-arm64.tar.gz opensearch-min-1.3.4-linux-arm64.tar.gz opensearch-min-1.3.5-linux-arm64.tar.gz opensearch-min-1.3.6-linux-arm64.tar.gz opensearch-min-1.3.7-linux-arm64.tar.gz opensearch-min-1.3.8-linux-arm64.tar.gz opensearch-min-1.3.9-linux-arm64.tar.gz opensearch-min-2.0.0-linux-arm64.tar.gz opensearch-min-2.0.0-rc1-linux-arm64.tar.gz opensearch-min-2.0.1-linux-arm64.tar.gz opensearch-min-2.1.0-linux-arm64.tar.gz opensearch-min-2.2.0-linux-arm64.tar.gz opensearch-min-2.2.1-linux-arm64.tar.gz opensearch-min-2.3.0-linux-arm64.tar.gz opensearch-min-2.4.0-linux-arm64.tar.gz opensearch-min-2.4.1-linux-arm64.tar.gz opensearch-min-2.5.0-linux-arm64.tar.gz opensearch-min-2.6.0-linux-arm64.tar.gz opensearch-min-2.7.0-linux-arm64.tar.gz linux-x64 opensearch-min-1.0.0-linux-x64.tar.gz opensearch-min-1.1.0-linux-x64.tar.gz opensearch-min-1.2.0-linux-x64.tar.gz opensearch-min-1.2.1-linux-x64.tar.gz opensearch-min-1.2.2-linux-x64.tar.gz opensearch-min-1.2.3-linux-x64.tar.gz opensearch-min-1.2.4-linux-x64.tar.gz opensearch-min-1.3.0-linux-x64.tar.gz opensearch-min-1.3.1-linux-x64.tar.gz opensearch-min-1.3.2-linux-x64.tar.gz opensearch-min-1.3.3-linux-x64.tar.gz opensearch-min-1.3.4-linux-x64.tar.gz opensearch-min-1.3.5-linux-x64.tar.gz opensearch-min-1.3.6-linux-x64.tar.gz opensearch-min-1.3.7-linux-x64.tar.gz opensearch-min-1.3.8-linux-x64.tar.gz opensearch-min-1.3.9-linux-x64.tar.gz opensearch-min-2.0.0-linux-x64.tar.gz opensearch-min-2.0.0-rc1-linux-x64.tar.gz opensearch-min-2.0.1-linux-x64.tar.gz opensearch-min-2.1.0-linux-x64.tar.gz opensearch-min-2.2.0-linux-x64.tar.gz opensearch-min-2.2.1-linux-x64.tar.gz opensearch-min-2.3.0-linux-x64.tar.gz opensearch-min-2.4.0-linux-x64.tar.gz opensearch-min-2.4.1-linux-x64.tar.gz opensearch-min-2.5.0-linux-x64.tar.gz opensearch-min-2.6.0-linux-x64.tar.gz opensearch-min-2.7.0-linux-x64.tar.gz opensearch-sql-jdbc\njava-jvm opensearch-sql-jdbc-1.1.0.1.jar opensearch-sql-odbc\nmacos-x64 signed_opensearch-sql-odbc-mac-1.1.0.1.zip opensearch-sql-odbc-mac-1.4.0.0.pkg windows-x86 signed_opensearch-sql-odbc-win32-1.1.0.1.msi opensearch-sql-odbc-win32-1.4.0.0.msi windows-x64 signed_opensearch-sql-odbc-win64-1.1.0.1.msi opensearch-sql-odbc-win64-1.4.0.0.msi",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/testimonials/",
    "title": "Testimonials",
    "content": "OpenSearch is helping developers solve a range of problems. This page highlights what people are saying about OpenSearch and how they are participating in the project.\nAt Coralogix, we believe in making observability fast, cost effective, and accessible, without limitations. That‚Äôs why we believe in building with open source tools. OpenSearch gives us a powerful, flexible, open source foundation to enable full-stack observability and real-time streaming analytics at scale for customers everywhere.\nOded David R&D Director Coralogix At Dow Jones we use a variety of open source software projects to solve challenging technology problems. We are excited by the recent launch of OpenSearch and how it might enhance some of our key managed Observability services. We look forward to its advancement, and participating in the OpenSearch community.\nChris Nelligan VP & CTO, Enterprise Products Dow Jones Open source solutions have changed the way people are thinking about software and hardware alike. The recent launch of OpenSearch marks an exciting time, and we look forward to empowering our research content platform to achieve an enhanced client, curator and developer experience.\nElizabeth Byrnes Managing Director Global Investment Research Division Goldman Sachs We in the OpenInfra community are energized by how OpenSearch has engaged with the vision of an open development model, based on the four principles of open source, open design, open development, and open community. And, OpenSearch‚Äôs support as an infrastructure donor to the OpenStack community is a tangible proof point of how our communities are working together to deliver open source software that runs in production.\nThierry Carrez General Manager OpenInfra Foundation Pinterest makes extensive use of open source software in our product, which brings inspiration to hundreds of millions of people around the world. And we contribute software to communities that are working together to advance the state of the art. Because we value open source communities, we are excited that OpenSearch provides a fully open source search and analytics suite.\nDavid Chaiken Chief Architect Pinterest We embrace cloud technologies that are built on open-source services and technologies so that our customer use cases are well supported and agnostic of vendors. Earlier, we relied centrally on Elasticsearch for source tracking, content search, and log analytics because of its ability to ingest and log large volumes of data. With the launch of OpenSearch, we can serve customers across industries through its additional capabilities such as Advanced security and alerting, KNN Search, and anomaly detection. It also facilitates faster decision-making for the stakeholders through its integrated visualization tool, OpenSearch Dashboards. In conclusion, we‚Äôre excited to leverage OpenSearch in our future engagements.\nSanchit Jain Practice Lead and AWS APN Ambassador Quantiphi At Rackspace Technology, we welcome more open-source contribution and collaboration as it underpins modern analytics platforms and technologies. Our engineers utilize the open-source spectrum across our data platform implementations and are excited by the increased options and flexibility OpenSearch offers when building modern data solutions for our customers. We look forward to supporting its advancement and participating in the OpenSearch community.\nVikram Reddy Kosanam Director/Head of Data, Analytics, and AI/ML Professional Services Rackspace Technology SAP customers expect a unified, business-centric and open SAP Business Technology Platform. Our observability strategy uses Elasticsearch as a major enabler. OpenSearch provides a true open source path and community-driven approach to move this forward.\nJan Schaffner SVP and Head of BTP Foundational Plane SAP More and more businesses consider open source to be an important part of their technology strategy. Wipro has been a pioneer and thought leader in Open Source with deep foundations into open source community and development process. We are delighted to see OpenSearch provide an open source search and analytics suite that could enhance some of our key services around log aggregation, operational monitoring and data observability. We look forward to actively contributing and participating in the OpenSearch community.\nRakesh Verma Global Practice Head, Data Engineering & Platforms Wipro At Zoom we recognize the importance of open source software as it allows a diverse community of developers to solve problems together. We are excited to see OpenSearch provide an open source solution for search, log analytics and more.\nYasin Mohammed Engineering Manager, Cloud Operations Zoom Video Communications Interested in sharing your OpenSearch story? To add your testimonial to this page, please create a fork the project website, add a markdown file for your project under /_testimonials/your-testimonial.md, and submit a pull request.",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/authors/",
    "title": "Authors",
    "content": "Anas Alkouz Arpit Bandejiya Achit Ojha Aditya Jindal Rabi Panda Alice Williams Alolita Sharma Amitai Stern Amit Galitzky Anandhi Bumstead Anan Zhuang Andrew Hopp Andrew Ross Anurag Gupta Aozixuan Guan Aparna Sundar Ariana Marble Anton Rubin Ashish Agrawal Ashwin Kumar Ashwin Pc Austin Geraci Bill Beckler Pavani Baddepudi Bukhtawar Khan Carl Meadows Chris Swierczewski Chen Dai Daryll Swager Dave Lago Daniel (dB.) Doubrovkine David Powers Dhrubo Saha Dhrubajyoti Das David Venable David Tippett Eli Fisher Eugene Lee Francesco Tisiot Bharathwaj G Binlong Gao Gopala Krishna Shunsuke Goto Marc Handalian Jon Handler Heather Halter Charlotte Henkle Harsha Vamsi Kalluri Hailong Cui Anirudha Jadhav Dhiraj Kumar Jain James McIntyre Joshua Bright Jack Mazanec Joshua Tokle Joydeep Sinha Junqiu Lei Kaituo Li Saikumar Karanam Owais Kazi Balaji Kannan Khushboo Rajput Krishna Kondaka Karthik Kumarguru Fanit Kolchina Kunal Kotwani Kris Freedain Kroosh Karsten Schnitter Kuber Kaul Kevin Garcia Kyle Davis Lai Jiang Lane Holloway Joshua Li Lior Perry Tao Liu Lorna Mitchell Xuesong Luo Stavros Macrakis Rupal Mahajan Matheus Nogueira Monica Kugler Ajay Kumar Movva Mohammad Qureshi Milind Shyani Matt Timmermans Nate Archer Natalie Broman Nate Boot Navneet Verma Nate Bower Nicholas Knize Nina Mishra Asif Sohail Mohammed Hai Yan Tyler Ohlsen Pakshi Rajan Pallavi Priyadarshini Patti Juric Paul Aubrey Peng Huo Peter Nied Phil Lewis Pierrick Prost Prashant Agrawal Praveen Sameneni Prudhvi Godithi Rajiv Taori Ranjith Ramachandra Andriy Redko Rohin Bhargava Ruizhen Guo Ryan Bogan Ryan Paras Saikumar Karanam Satish Nandi Sayali Gaikawad Steven Bayer Dharmesh Singh Sean Zheng Himanshu Setia Shahar Shaked Shivam Dhar Roman Grebennikov Saurabh Singh Sriram Kosuri Romain Tarti√®re Surya Sashank Nistala Soner Sayakci Sudipto Guha Zhou Su Tetiana Fydorenchyk Theo Truong Tushar Kharbanda Tianli Feng Kristen Tian Tayor Gray Hannah Uselman Vacha Shah Melissa Vagi Satya Vajrapu Vamshi Vijay Nakkirtha Sarat Vemulapalli Vigya Sharma Vijayan Balasubramanian Viraj Phanse Lin Wang Adithya Chandra Partha Kanuparthy Sid Narayan Thomas Farr Yaliang Wu Charlie Yang Yan Zeng Jiaxiang (Peter) Zhu Patrick Walsh",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/faq/",
    "title": "Frequently Asked Questions",
    "content": "Frequently Asked Questions\nCategories: General &middot; Tools and Plugins &middot; Upgrading to OpenSearch &middot; Community and Collaboration 1. General 1.1 What is OpenSearch? OpenSearch is a fully open source search and analytics suite. OpenSearch includes OpenSearch (derived from Elasticsearch 7.10.2) and OpenSearch Dashboards (derived from Kibana 7.10.2), and a range of features like built-in security, anomaly detection, alerting, observability experiences, and more. 1.2 Why was OpenSearch created? Developers embrace open source software for many reasons, one of the most important is the freedom to use that software where and how they wish. Elastic ceased making open source options available for Elasticsearch and Kibana, releasing them under the Elastic license, with source code available under the Elastic License or SSPL. These are not open source and do not offer users the same freedoms. Because of this, we made the decision to create a fork from the last Apache 2.0 version of Elasticsearch and Kibana and provide OpenSearch under the Apache\nLicense, Version 2.0 (ALv2).\nALv2 grants well-understood and permissive usage rights that match the freedoms people expect with open source software; freedoms such as being able to use, modify, extend, monetize, and resell the open source software where and how they want. For OpenSearch, we believe this will enable broad adoption and contributions benefiting all members of the community. 1.3 Why should I use OpenSearch? OpenSearch enables people to easily ingest, secure, search, aggregate, view, and analyze data. These capabilities have led it to be popular for use cases such as application search, log analytics, and more. With OpenSearch you benefit from having an open source product you can use, modify, extend, monetize, and resell how you want. At the same time, we will continue to provide a secure, high-quality search and analytics suite with a rich roadmap of new and innovative functionality. 1.4 Is OpenSearch suitable for production use? OpenSearch became ready for production use in July of 2021 with the generally available release of OpenSearch 1.0. 1.5 What license is OpenSearch released under? All of the software in the OpenSearch project is released under the Apache License, Version 2.0 (ALv2). The ALv2 license grants you well-understood usage rights for OpenSearch. You can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. We have also published permissive usage guidelines for the OpenSearch trademark, so you can use the name to promote your offerings. 1.6 Is AWS equipped to maintain and advance a project like OpenSearch? When AWS decides to offer a service based on an open source project, we ensure that we are equipped and prepared to maintain it ourselves, if necessary. We bring years of experience working with Elasticsearch and Kibana codebases and have made upstream code contributions to both Elasticsearch and Apache Lucene (the core search library that Elasticsearch is built on). We have added several features in open source like security, alerting, anomaly detection, index state management, and trace analytics that are widely used and deployed in production by our community and customers. We are well equipped to maintain and advance the project ourselves. Also, the community-backed codebase will help accelerate new innovations and will allow everyone to move faster in improving stability, scalability, resiliency, and performance. Already many organizations including SAP, CapitalOne, RedHat, Logz.io, Aiven.io, Bonsai, Logit.io, Search Guard, and BAInsight have publicly backed OpenSearch. 1.7 What is the relationship between the existing Open Distro for Elasticsearch project and OpenSearch? The final version of Open Distro for Elasticsearch is version 1.13. Future versions of the plugins and advanced features distributed with Open Distro for Elasticsearch will be available in the OpenSearch project. 1.8 Is OpenSearch wire-compatible with Elasticsearch? Yes. OpenSearch is a fork of open source Elasticsearch 7.10. As such, it provides backwards REST APIs for ingest, search, and management. The query syntax and responses are also the same. In addition, OpenSearch can use indices from Elasticsearch versions 6.0 up to 7.10. We also aim to support the existing Elasticsearch clients that work with Elasticsearch 7.10.\nNote that while the OpenSearch API is backwards compatible, some clients or tools may include code, such as version checks, that may cause the client or tool to not work with OpenSearch.\nFor more information on backwards compatibility, see upgrading FAQs. 1.9 Will the OpenSearch API change in future versions? All future OpenSearch 1.x releases will be backwards compatible with Elasticsearch 7.10, although some APIs will be deprecated. If functionality requires a breaking change, we will introduce a new major version of OpenSearch and provide tooling to make migrating to the new major version simple. When new features require adding APIs, we will work with the community to add support for these features in popular clients.\nFor more details on deprecated APIs, see upgrading FAQs. 1.10 Do Elasticsearch clients like Logstash and Beats work with OpenSearch? Since OpenSearch is wire-compatible with Elasticsearch 7.10, any clients that currently work\nwith Elasticsearch 7.10 should also work with OpenSearch. For details specific about open\nsource Logstash and Beats compatibility please see the documentation.\nPlease report any issues you experience with these clients\nor other clients in our project GitHub issues.\nFor more questions related to upgrading clients, see upgrading FAQs. 1.11 Does OpenSearch include forks of Logstash and Beats? No. There is documentation on which versions of Logstash and Beats work with OpenSearch. We will be providing, more documentation on how to use them with OpenSearch\nand download links to OpenSearch compatible versions. 1.12 Can I upgrade from Elasticsearch to OpenSearch? Yes. You can upgrade using a rolling upgrade (one node at a time) process when upgrading from Elasticsearch versions 7.0 - 7.10 to OpenSearch. For Elasticsearch versions 6.x you will be required to perform a cluster restart upgrade. OpenSearch can use indices from Elasticsearch versions 6.0 up to 7.10. Indices on versions prior to Elasticsearch 6.0 or after 7.10 will need to be removed from a cluster being upgraded to OpenSearch or reindexed into a compatible version of Elasticsearch then upgraded to OpenSearch. 1.13 Which features of Elasticsearch do you plan to offer in the future? Is OpenSearch going to keep pace with the upstream Elasticsearch releases? How will this evolve? The future investments and plans for OpenSearch should be viewed as independent of Elasticsearch. Our roadmap will be driven by the needs of the community. Where possible, if new features in OpenSearch are similar to Elasticsearch, we will strive to make the APIs similar. The two projects, however, are distinct. The OpenSearch project is open source and is focused on providing the innovations that our community and customers ask for. 1.14 Can OpenSearch read indices from previous versions of Elasticsearch? Yes. OpenSearch is compatible with indices created from Elasticsearch versions 6.0 up to 7.10. 1.15 Will you be making contributions back to Elasticsearch open-source? No. Open source Elasticsearch development ended with 7.10 when it was moved to a non-open source license. OpenSearch software will move forward according to the needs of the community. That said, all OpenSearch software built for its customers and the community is released under the ALv2 license and we welcome anyone to use the software under the ALv2 terms and conditions. 1.16 I want to contribute, what can I do? We welcome contributions in multiple forms. You can submit pull requests, open issues, and leave comments on any of the OpenSearch GitHub repositories. You can join our community meetings. You can also leave comments in the community forum. If you have a specific way you would like to contribute and don‚Äôt know what steps to take, please leave a comment in our community forum. 1.17 How are new features, bug fixes, and other development decisions made? New software developed for OpenSearch is made based on project‚Äôs principles for development. To learn more, visit our website. 1.18 What will your governance model be? Are you looking at any foundations? At this time, there is not a plan to move OpenSearch in to a foundation. As we work together in the open, we expect to uncover the best ways to collaborate and empower all interested stakeholders to share in decision making. Cultivating the right governance approach for an open source project requires thoughtful deliberation with the community. We‚Äôre confident that we can find the best approach together over time. For now, AWS is the steward of OpenSearch. The principles of development define the guidelines for how decisions about the project are made. These principles will continue to be iterated and refined based on feedback. 1.19 How do I contribute functionality and get it on the public roadmap? If you want to add something to OpenSearch that is not in the public roadmap, that‚Äôs a perfect opportunity to contribute! There are a couple of things you can do.\nYou can create a feature request in the relevant GitHub repo for the feature and socialize the request. People are always looking for in-demand features to build. A maintainer or someone else in the community may pick this feature up and work on it. As progress is made the maintainers of the repo will help get the feature onto the roadmap.\nAnother option is to build the feature yourself. To do this create a proposal as a GitHub issue in the relevant repo and use the proposal template (thanks jkowall for contributing the template!). Offer your commitment to build it. The maintainers of the repo will work with you to figure out how best to proceed. That could be further discussion, design docs, or just starting to write the code. As the feature is developed, the maintainers of the repo will also work with you to incorporate it into the roadmap. 1.20 What tools do you recommend for log and metrics collection? OpenSearch is supported by a range of tools like Beats, Fluentd, Fluent Bit, and OpenTelemetry Collector. Moving forward, we will focus effort on improving Data Prepper, Fluentd, and FluentBit. Users who are using Beats &lt;= 7.12.x as an agent tool, and considering open source alternatives, should migrate to Fluent Bit &gt;= 1.9 or Open Telemetry Collector. Beats version &gt;= 7.13 does not support OpenSearch. 1.21 What tools do you recommend for log aggregation? OpenSearch is supported by a range of tools like Data Prepper, Fluentd, Logstash, and Kafka. OpenSearch believes in multiple open source options and will focus on improving the Data Prepper, Fluentd, and Kafka support going forward.\n2. Tools and Plugins 2.1 Can I use OpenSearch plugins with the proprietary Elastic stack? The plugins are tested to work with OpenSearch. They have not been tested with Elastic‚Äôs proprietary software. As these plugins are open source, we do welcome anyone who wants to test them out with the Elastic Stack. However, we do not plan to invest in making the OpenSearch plugins work on the Elastic stack. 2.2 Can I install an OpenSearch plugin as a standalone plugin? Yes. You can install an OpenSearch plugin independently of the other plugins. For example, if you would like to use OpenSearch with only our security plugin installed, you can remove the other plugins using the OpenSearch plugin remove command. 2.3 Will older versions of Open Distro for Elasticsearch plugins still be available for open source Elasticsearch? No. Open Distro has been archived. 2.4 Does OpenSearch include forks of Logstash and Beats? No. For more information on which versions of Logstash and Beats work with OpenSearch, see the Compatibility Matrix for Logstash. 2.5 What is OpenSearch Data Prepper? Data Prepper is a server-side data collector capable of filtering, enriching, transforming, normalizing and aggregating data for downstream analytics and visualization. Also, Data Prepper lets users build custom pipelines to improve the operational view of applications.\nTwo common uses of Data Prepper are trace and log analytics. Trace analytics help you visualize the flow of events and identify performance problems. Log analytics can improve search functionality, as well as help you analyze and provide insights into your application.\nTo get started building your own custom pipelines with Data Prepper, see the Data Prepper Get Started guide. 2.6 What is Fluentd and Fluent Bit? Fluentd is a data collector for log data collection, processing, and forwarding. It‚Äôs written in Ruby and supports over 500 plugins including data sources, data output, parsers, formatters, and filters. Fluent Bit is an agent that collects data from different sources, enriches them with filters, and sends them to multiple destinations. It‚Äôs designed with performance in mind, meaning it is optimized for high throughput and low CPU and memory usage. It‚Äôs written in C and has an architecture that supports more than 70 plugins for inputs, filters, and outputs.\n3. Upgrading to OpenSearch 3.1 How can I upgrade an Elasticsearch OSS and Kibana OSS cluster with multiple nodes to OpenSearch and OpenSearch Dashboards? OpenSearch supports rolling upgrades in the same way as Elasticsearch OSS. You can deploy OpenSearch into a mixed cluster with Elasticsearch OSS or Open Distro for Elasticsearch nodes. One by one you can replace the legacy nodes with little to no additional manual work.\nIn the same way as Kibana OSS, OpenSearch Dashboards does not support rolling upgrades, but it supports restart upgrades. You are able to stop all Kibana OSS instances, deploy a new OpenSearch Dashboards instance and direct traffic to it. 3.2 How can I upgrade a single node deployment¬†of Elasticsearch OSS and Kibana OSS to OpenSearch and OpenSearch Dashboards? You are able to stop Elasticsearch OSS and Kibana OSS, install OpenSearch and OpenSearch Dashboards, manually configure those to point to your Elasticsearch OSS and Kibana OSS data, review and potentially update settings, then start OpenSearch with OpenSearch Dashboards. 3.3 Which versions of Elasticsearch OSS and Kibana OSS can I upgrade from to OpenSearch and OpenSearch Dashboards, directly? You can directly upgrade to OpenSearch 1.x from Elasticsearch OSS and Kibana OSS 6.8.0-7.10.2, and Open Distro 1.x. For more information about upgrades, please see the documentation. 3.4 Can I upgrade older versions of Elasticsearch OSS and Kibana OSS to OpenSearch and OpenSearch Dashboards? Elasticsearch OSS and Kibana OSS 5.x up to 6.7.2 can be first upgraded to 6.8.0, then it is recommended to upgrade to Elasticsearch OSS 7.10.2 or ODFE 1.13, before upgrading to OpenSearch and OpenSearch Dashboards.\nNote that the minimum supported index version for OpenSearch is 6.0. So, all the 5.x indices have to be re-indexed, before upgrading to OpenSearch. 3.5 Can I upgrade a non-OSS Elasticsearch and Kibana cluster to OpenSearch? Yes. However, functionality not available in Elasticsearch OSS and Kibana OSS continues to not be available. We recommend evaluating additional features available in OpenSearch that may provide similar functionality. 3.6 Can I upgrade from Elasticsearch and Kibana 7.11 or other versions released after the fork? No, this will not be supported in OpenSearch 1.0. 3.7 Will my ODFE plugins continue to work after upgrade? OpenSearch plugins based on the Open Distro for Elasticsearch (ODFE) plugins are included in OpenSearch 1.0, and are functionally backwards compatible with their predecessors. 3.8 At what point is the upgrade process complete? When all nodes are running OpenSearch and OpenSearch Dashboards 1.0. 3.9 Does an upgrade to OpenSearch require downtime? You can perform rolling upgrades from Elasticsearch OSS to OpenSearch, which does not require downtime. Kibana OSS upgrades require a restart which will cause downtime for Kibana OSS and OpenSearch Dashboards. If you have a single-node deployment and wish to upgrade it in-place, you will incur downtime. 3.10 Can I do a Blue/Green upgrade with data migration? Yes. You can choose to do a Blue/Green upgrade instead of a rolling upgrade. 3.11 Can I install elasticsearch-xyz-plugin in OpenSearch? An Elasticsearch plugin that depends on Elasticsearch JARs will not work without code changes to depend on OpenSearch JARs. 3.12 Can I rollback an upgrade half-way in progress (e.g. 2/4 nodes are running OpenSearch)? This is possible and has caveats. For example, upgrading Kibana OSS to OpenSearch Dashboards migrates the Kibana OSS indexes and does not allow rollback. 3.13 Are there any changes required for my existing Elasticsearch OSS clients to continue to work? While the OpenSearch API is backwards compatible, some clients or tools may include code, such as version checks, that may cause the client or tool to not work with OpenSearch. 3.14 Can I run a heterogeneous mixed OpenSearch and Elasticsearch OSS cluster? This only supported in OpenSearch for the purpose of a rolling upgrade. 3.15 Can I run a heterogeneous mixed OpenSearch Dashboards and Kibana cluster? No. This is not supported. 3.16 Is OpenSearch security enabled by default after an upgrade? Yes. Security is enabled by default. 3.17 What settings change during the upgrade? Environment variables that contain branded words such as ES or OPENDISTRO have been renamed.\nCluster settings that contain branded words such as es. or opendistro. can continue to be used but have been deprecated. 3.18 What REST APIs change during the upgrade? REST APIs that contain branded words such as ES or OPENDISTRO have been deprecated. 3.19 How do I upgrade or migrate secure or system indices? There is no need to migrate secure or system indices for the upgrade to OpenSearch 1.0. This is because theses indices are not being renamed. In future upgrades, index aliases or migrations may be introduced for secure or system indices. 3.20 Is there binary compatibility between Elasticsearch-OSS and OpenSearch? While an OpenSearch node is able to join an Elasticsearch OSS cluster, namespaces and class names in OpenSearch have been changed. If your plugin code depends on Elasticsearch OSS JARs, you will need to upgrade those dependencies to OpenSearch JARs. 3.21 Where can I find more information and track progress on ensuring backwards compatibility? The umbrella issue for backwards-compatibility is OpenSearch#671. Issues across opensearch-project repositories are labeled backwards-compatibility. 3.22 Will Kibana work on top of OpenSearch 1.0? No. You will need to upgrade to OpenSearch Dashboards. 3.23 I have built a custom plugin for Elasticsearch, will it run without modification on OpenSearch? Because OpenSearch is backwards compatible, it will. If you find identify an incompatibility, please open an issue in OpenSearch. 3.24 Can I install kibana-xyz-plugin that only uses REST APIs in OpenSearch Dashboards 1.0? While the APIs have not changed, we have not tested kibana-xyz-plugin. 3.25 I have built a custom visualization for Kibana, will it be usable with OpenSearch Dashboards without modification? Because OpenSearch Dashboards is backwards compatible, it will. If you find identify an incompatibility, please open an issue in OpenSearch Dashboards. 3.26 Can OpenSearch read indices from previous versions of Elasticsearch? Yes. OpenSearch is compatible with indices created from Elasticsearch versions 6.0 up to 7.10. 3.27 Will the OpenSearch API change in future versions? All future OpenSearch 1.x releases will be backwards compatible with open source Elasticsearch 7.10. If functionality requires a breaking change, we will introduce a new major version of OpenSearch and provide tooling to make migrating to the new major version simple. When new features require adding APIs, we will work with the community to add support for these features in popular clients. 3.28 Which version of OpenSearch should I use? For OpenSearch and other software in the OpenSearch project, new features and active development always takes place against the newest version. The OpenSearch project follows the semantic versioning specification for assigning version numbers to releases, so in most cases you should be able to upgrade to the latest version of any software without encountering incompatible changes.\nSometimes an incompatible change is unavoidable. When this happens, the software‚Äôs maintainers will increment the major version number (e.g. increment from OpenSearch 1.y.z to OpenSearch 2.0.0), and the most recent release of the previous major version will be put into maintenance. During the maintenance window, the software will continue to receive bug fixes and security patches, but no new features.\nThe duration of the maintenance window will vary from product to product and release to release. It will be based on the patching and support schedules for dependencies the software includes, community input, the scope of the changes introduced by the new version, and estimates for the effort required to continue maintenance of the previous version. 3.29 Can I use tools that require a particular version of Elasticsearch? By default, OpenSearch reports its version number, however OpenSearch can be configured to report 7.10.2 for compatibility with existing tools designed for Elasticsearch versioning. This configuration option is deprecated and will log a deprecation message and will be removed in the future in a major version of OpenSearch (3.0.0 or later). 3.30 Does OpenSearch support version downgrades? OpenSearch does not support direct version downgrades. If your environment must be downgraded, we recommend using snapshots to create a restore point, then restoring the snapshot to a cluster built with the target version.\n4. Community and Collaboration 4.1 How do I add a project to the community project page on opensearch.org? To add a project:\nFork the opensearch website repository.\nCreate a branch. The recommended naming convention is adds-&lt;your project name&gt;-to-community-projects Add a markdown file for your project to. /_community_projects/your-project.md Last create a pull request and one of the website maintainers will review it and work with you to merge it.\nTo see an example pull request, look here). 4.2 What kinds of projects are welcome on the community project page? Any project that benefits the OpenSearch community is welcome. These can be both open source licensed projects and proprietary licensed projects. 4.3 What are the requirements for a project before it can be moved into the OpenSearch Project GitHub organization? There are three requirements:\nThe project is licensed under a permissive open source license, like Apache 2.0.\nThe project has broad utility for the OpenSearch community\nThe project is currently actively maintained 4.4 What are the benefits of moving a project under the OpenSearch Project GitHub organization? The main benefits include increased visibility, and if you no longer can or want to maintain the project some day, the project will help find new maintainers to transition the ownership. Everything else remains the same, including that you retain the administrator permissions of your GitHub repository 4.5 How can I get my project into the OpenSearch Project GitHub organization? If you are interested in moving your project, create an issue in your repository, visibly confirm with your co-maintainers that you would like to make the move, and tag dblock and elfisher to initiate the process of moving your project. 4.6 What is an OpenSearch Project Github organization repository administrator? An admin owns stewardship of the repository and its settings. Admins have admin-level permissions on a repository. Use those privileges to serve the community and protect the repository as follows. Administrator responsibilities can be found in the OpenSearch.github repository admins file. 4.7 What is an OpenSearch Project Github organization repository maintainer? Maintainers are active and visible members of the community, and have maintain-level permissions on a repository. They use those privileges to serve the community and evolve the software in the repository they maintain. Maintainer responsibilities can be found in the OpenSearch.github repository maintainer file. 4.8 What are my security obligations when developing in OpenSearch Project Github organization repositories? Security is your number one priority. Maintainer‚Äôs Github keys must be password protected securely and any reported security vulnerabilities are addressed before features or bugs. Note that repositories in the OpenSearch GitHub organization are monitored and supported 24/7 by Amazon Security, see Reporting a Vulnerability for details. 4.9 How should an OpenSearch Project Github organization repository roadmap be managed? Maintainers can manage the repository roadmap in the way that best works for them. An example roadmap/project board can be found here. 4.10 How do I contribute to an OpenSearch Project Github organization repository? It‚Äôs strongly recommend that you follow the contributors guide.",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/community_projects/",
    "title": "Community Projects",
    "content": "This page was made to highlight projects built by the community for the community.\nWant your project on this page? Check out this guide on adding a new community project.\nProjects Apache Flink OpenSearch Connector The connector provides Apache Flink sinks that can request document actions to an OpenSearch indices\nLicense: Apache License 2.0 Owner: Apache Software Foundation Arkime Arkime is a large-scale, open-source, indexed full packet capture system. Arkime can be used to store and analyze network traffic data, which can be used for various purposes such as security analysis, network performance optimization, and troubleshooting.\nLicense: Apache License 2.0 Owner: Arkime Download Haystack Haystack is an open source NLP framework that leverages Transformer models. Haystack enables the developers to implement production-ready neural search, question answering, semantic document search, summarization for a wide range of applications.\nLicense: Apache License 2.0 Owner: deepset GmbH Diggie Professional GUI client for AWS OpenSearch, desktop app of Mac / Windows that makes operating with OpenSearch easier. This project is currently a developer preview version. Feedbacks and opinions are welcome.\nLicense: Diggie EULA (developer preview) Owner: Serverless Operations, Inc. Download ElastiFlow&trade; ElastiFlow&trade; instantly transforms OpenSearch into a turnkey network traffic analytics solution. It provides deep insights into performance and availability issues, as well as security threats, for environments at any scale.\nLicense: ElastiFlow EULA Owner: ElastiFlow Inc. Encrypted Repository Plugin for OpenSearch Encrypted Repository for OpenSearch is a plugin that offers client-side encryption for snapshotting repositories and could be combined with all official supported repository plugins including File Systsem, Google Cloud Storage, Amazon S3 and Microsoft Azure.\nLicense: Apache License 2.0 Owner: Aiven Oy and project maintainers Download Fess Fess is very powerful and easily deployable Enterprise Search Server. It provides Web UI and a crawler, which can crawl documents on a web server, file system, or Database. Many file formats are supported, including Microsoft Office, PDF, and zip.\nLicense: Apache License 2.0 Owner: CodeLibs Project Download Kubernetes Logging stack using Opensearch This helm chart deploys a scalable containerized logging stack, enabling log observability for kubernetes applications with Opensearch. The deployment may take various forms, from a single node setup usable for a local development up to scaled multi nodes opensearch deployments used in production environments. It is an easy way to provision a managed Opensearch cluster with optional kafka brokers, fluentbits and fluentd(s) supplying additional initialization steps for the various components.\nLicense: Apache License 2.0 Owner: Niki Dokovski Malcolm Malcolm is a powerful, easily deployable network traffic analysis tool suite.\nLicense: Permissive (BSD-ish) Open-source License Owner: Idaho National Laboratory OpenSIEM Logstash Parsing Configurations Logstash parsing for more than a hundred technologies. We welcome contributions for security and non-security based technologies.\nLicense: Apache License 2.0 Owner: Cargill, Inc. OpenSearch Checkup The OpenSearch check-up analyzes architecture and configuration to provide you with specific recommendations for improvement and optimization. The Check-Up is free forever and based on JSON files so you don‚Äôt need to download or install anything.\nLicense: Free Owner: Opster ltd. OpenSearch Connector for Apache Kafka OpenSearch Connector for Apache Kafka¬Æ for copying data from Kafka to OpenSearch.\nLicense: Apache License 2.0 Owner: Aiven.io and project maintainers Download Kubernetes Operator for OpenSearch The Kubernetes OpenSearch Operator is used for automating the deployment, provisioning, management, and orchestration of OpenSearch clusters and OpenSearch dashboards.\nLicense: Apache License 2.0 Owner: Opster ltd. Management Console The Management Console makes it easy to deploy and manage OpenSearch clusters on Kubernetes. You can deploy multiple clusters, configure node roles, scale cluster resources, manage certificates and more ‚Äì all from a single interface, for free.\nLicense: Opster license Owner: Shahar Shaked OpenSearch Plugin Template Java Template repo for creating OpenSearch plugins\nLicense: Apache License 2.0 Owner: Amitai Stern OpenSearch Terraform module OpenSearch Terraform module by Opster allows you to setup an OpenSearch cluster on AWS EC2 instances. Using this module you can deploy a new cluster, control its settings, configuration and discovery settings. The module deploys both OpenSearch cluster and OpenSearch Dashboards server.\nLicense: Apache License 2.0 Owner: Opster ltd. PGSync PGSync is a middleware for syncing data from Postgres to OpenSearch\nLicense: GNU General Public License v3.0 Owner: Tolu Aina Prometheus exporter plugin for OpenSearch The Prometheus¬Æ exporter plugin for OpenSearch exposes many OpenSearch metrics in Prometheus format.\nLicense: Apache License 2.0 Owner: Aiven.io and project maintainers Download Python Logger OpenSearch logger for Python sends standard logs directly to OpenSearch. It provides direct and easy to use logging handler. Supports out of the box Python logging.\nLicense: Apache License 2.0 Owner: Vagiz Duseev Shuffle Shuffle is an open source automation platform for security professionals. With a graphical app- and workflow creator, it allows for fast testing, development, deployment and sharing of usecases with the broader industry. Using OpenSearch, Shuffle is able to provide responsive and easy to use search- and storage capabilities at scale.\nLicense: GNU AFFERO GENERAL PUBLIC LICENSE Owner: Shuffle AS SnappyFlow SnappyFlow is a simplified observability platform that brings together metrics, logs, distributed tracing, synthetics monitoring and alert management in an integrated workflow. SnappyFlow uses OpenSearch for storing, aggregating and querying all datapoints collected by Snappyflow.\nLicense: SnappyFlow EULA Owner: MapleLabs, Inc. Telicent Synonym Plugin OpenSearch plugin to store the synonyms resources in an index instead of a file\nLicense: Apache 2.0 Owner: Telicent Transform plugin for OpenSearch Dashboards An OpenSearch Dashboards visualization plugin that allows arbitrary queries results to be processed by a Mustache transform. You can also call any external JS library to build new visualisations such as Google Chart, d3js,...\nLicense: Apache License 2.0 Owner: Lionel Guillaud Wazuh Wazuh is a free and open source platform used for threat prevention, detection, and response. It protects workloads across on-premises, virtualized, containerized and cloud-based environments. Wazuh has been fully integrated with OpenSearch, providing a search engine and a user interface to navigate through security alerts.\nLicense: GNU General Public License v2.0 Owner: Wazuh Inc.",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/javadocs/",
    "title": "JavaDocs",
    "content": "There is an ongoing effort to improve OpenSearch JavaDocs.\nArranged by version: 1.1.0 1.0.0 1.0.0-rc1 1.0.0-beta1",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/new-community-project.html",
    "title": "New Community Project",
    "content": "Adding a new community project is accomplished through a pull request to opensearch-project/project-website in the _community_projects folder. The suggested method is to fork the repo, add the community project markdown file and build the website locally to verify it looks correct then contribute the changed file back to opensearch-project/project-website.\nAlternately, this form will help you make the pull request and properly sign the DCO in the GitHub web interface by creating the pull request for you. You will need to have a GitHub account and be logged in for this form to work properly. Due to limitations in this process, you will need to add images through another method.\nThis visual guide can help you understand how a community project tile connects to the rendered HTML. Project Name Link to Project (URL) Project Description Owner Owner Link (URL, Optional) Project License (MIT, Apache 2.0, BSD 3 Clause, etc.) Project License File URL Commit Message Your Full Name (must match GitHub name exactly) Your E-mail Address (must match GitHub email address exactly) Filename (usually lowercase, no spaces version of your project name; should end in.markdown) Create PR",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/new-partner.html",
    "title": "New Partner",
    "content": "Joining the partner program is accomplished through a pull request to opensearch-project/project-website in the _partners folder. The suggested method is to fork the repo, add the partner markdown file and build the website locally to verify it looks correct then contribute the changed file back to opensearch-project/project-website.\nAlternately, this form will help you make the pull request and properly sign the DCO in the GitHub web interface by creating the pull request for you. You will need to have a GitHub account and be logged in for this form to work properly. Due to limitations in this process, you will need to add your logo through another method like GitHub Desktop or command-line Git.\nThis visual guide can help you understand how a partner tile connects to the rendered HTML. Company Name Company Website Link (URL) Logo URL Note: this form does not support uploading images. You will need to add the image through another method like GitHub Desktop or command-line Git. Commit Message Your Full Name (must match GitHub name exactly) Your E-mail Address (must match GitHub email address exactly) Filename (usually lowercase, no spaces version of your company name; should end in.markdown) Create PR",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/old_downloads.html",
    "title": "Downloads",
    "content": "How to get started with OpenSearch\nOpenSearch is open source software that uses the Apache License version 2 (ALv2). ALv2 grants you well-understood usage rights; you can use, modify, extend, embed, monetize, resell, and offer OpenSearch as part of your products and services. The source for the entire project is available on GitHub and you‚Äôre welcome to build from source for customized deployments. Downloadable artifacts for OpenSearch and OpenSearch Dashboards include plugins and tools, ready for you to use with minimal configuration.\nOpenSearch\nLinux, x64,.tgz ( signature) Current version: Release Candidate 1 Download OpenSearch OpenSearch on Docker OpenSearch Dashboards\nLinux, x64,.tgz ( signature) Current version: Release Candidate 1 Download OpenSearch Dashboards OpenSearch Dashboards on Docker Try OpenSearch with Docker Compose\nThe best way to try out OpenSearch is to use Docker Compose. These steps will setup a two node cluster of OpenSearch plus OpenSearch Dashboards:\nDownload docker-compose.yml into your desired directory\nRun docker-compose up Have a nice coffee while everything is downloading and starting up\nNavigate to http://localhost:5601/ for OpenSearch Dashboards\nLogin with the default username ( admin) and password ( admin)\n<!-- END #content-main -->\nFound a bug?\nReport any unexpected behaviours or bugs on the OpenSearch GitHub repo. The team will make sure your issue gets the attention of the right folks. Report a bug Need Open Distro?\nOpen Distro for Elasticsearch can still be downloaded. Get it here How to verify signatures\nDownload our PGP key using the link below and import it. If you‚Äôre using gpg, you just need to run gpg --import /path/to/key. You can then verify the signature by downloading it into the same directory where you downloaded the tarball, and running gpg --verify /path/to/signature /path/to/tarball. It should show a good signature signed by opensearch@amazon.com. Our current PGP key fingerprint is C5B7 4989 65EF D1C2 924B A9D5 39D3 1987 9310 D3FC Get Our PGP key <!-- END #content-related -->\n<!-- END #subwrap -->\n<!-- END #content-extra -->",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/releases.html",
    "title": "Release Schedule and Maintenance Policy",
    "content": "Updated April 26, 2023 Release Schedule ¬∑ Maintenance Policy Release Schedule\nFor more information on the changes planned for each release, please see the Project Roadmap 2023\nNote: We have not added a major release to the 2023 schedule yet. If/when we add one, it will replace a minor release in the 2.x line. See below for criteria for a major releases. Release Number Code Freeze Date Release Date 2.5.0\nJanuary 10th\nJanuary 24th\n1.3.8\nJanuary 26th\nFebruary 2nd\n2.6.0\nFebruary 21th\nFebruary 28th\n1.3.9\nMarch 9th\nMarch 16th\n2.7.0\nApril 17th\nMay 2nd\n1.3.10\nMay 11th\nMay 18th\n2.8.0\nMay 30th\nJune 6th\n1.3.11\nJune 22nd\nJune 29th\n2.9.0\nJuly 11th\nJuly 18th\n1.3.12\nAugust 3rd\nAugust 10th\n2.10.0\nAugust 22nd\nAugust 29th\n1.3.13\nSeptember 14th\nSeptember 21st\n2.11.0\nOctober 10th\nOctober 17th\n1.3.14\nNovember 9th\nNovember 16th\n2.12.0\nNovember 30th\nDecember 7th OpenSearch follows semver, which means we will only release breaking changes in major versions. All minor versions are compatible with every other minor version for that major. For example, 1.2.0 will work with 1.3.2, 1.4.1, etc, but may not work with 2.0.\nFor minor version releases, OpenSearch uses a ‚Äúrelease-train‚Äù model. Approximately every six weeks we release a new minor version which includes all the new features and fixes that are ready to go. Having a set release schedule makes sure OpenSearch is releasing in a predictable way and prevents a backlog of unreleased changes.\nIn contrast, OpenSearch releases new major versions only when there are a critical mass of breaking changes (e.g. changes that are incompatible with existing APIs). These tend to be tied to Lucene major version releases, and will be announced in the forums at least 4 weeks prior to the release date.\nPlease note: Both the roadmap and the release dates reflect intentions rather than firm commitments and may change as we learn more or encounters unexpected issues. If dates do need to change, we will be as transparent as possible, and log all changes in the changelog at the bottom of this page.\nMaintenance Policy\nFor OpenSearch and other software in the OpenSearch project, new features and active development always takes place against the newest version. The OpenSearch project follows the semantic versioning specification for assigning version numbers to releases, so you should be able to upgrade to the latest minor version of that same major version of the software without encountering incompatible changes (e.g., 1.1.0 ‚Üí 1.3.x).\nSometimes an incompatible change is unavoidable. When this happens, the software‚Äôs maintainers will increment the major version number (e.g., increment from OpenSearch 1.3.z to OpenSearch 2.0.0). The last minor version of the previous major version of the software will then enter a maintenance window (e.g., 1.3.x). During the maintenance window, the software will continue to receive bug fixes and security patches, but no new features.\nWe follow OpenSSF‚Äôs best practices for patching publicly known vulnerabilities and we make sure that there are no unpatched vulnerabilities of medium or higher severity that have been publicly known for more than 60 days in our actively maintained versions.\nThe duration of the maintenance window will vary from product to product and release to release. By default, versions will remain under maintenance until the next major version enters maintenance, or 1 year passes, whichever is longer. Therefore, at any given time, the current major version and previous major version will both be supported, as well as older major versions that have been in maintenance for less than 12 months. Please note that, maintenance windows are influenced by the support schedules for dependencies the software includes, community input, the scope of the changes introduced by the new version, and estimates for the effort required to continue maintenance of the previous version.\nThe software maintainers will not back-port fixes or features to versions outside of the maintenance window. That said, PRs with said back-ports are welcome and will follow the project‚Äôs review process. No new releases will result from these changes, but interested parties can create their own distribution from the updated source after the PRs are merged. Major Version Latest Minor Version Status Initial Release Maintenance Window Start Maintenance Window End 1\n1.3.x\nMaintenance\nJuly 12, 2021\nMay 26, 2022\nDecember 31, 2023\n2\n2.0.0\nCurrent\nMay 26, 2022\nN/A\nN/A *Note that the length of the maintenance window is an estimated minimum and the project may, at its discretion, extend it to a later date\nRelease History\n2022 Release Number Code Freeze Date Release Date 2.1\nJune 30, 2022\nJuly 7, 2022\n1.3.4\nJuly 8, 2022\nJuly 14, 2022\n2.2\nAugust 4, 2022\nAugust 11, 2022\n1.3.5\nAugust 16, 2022\nSeptember 1, 2022\n2.2.1\nAugust 19, 2022\nSeptember 1, 2022\n2.3\nSeptember 7, 2022\nSeptember 14, 2022\n1.3.6\nSeptember 30, 2022\nOctober 6, 2022\n2.4\nNovember 8, 2022\nNovember 15, 2022\n1.3.7\nDecember 6, 2022\nDecember 13, 2022 Change Log Date Change Reason July 1, 2022\n¬†\nInitial Version\nOctober 20,2022\nIncreased time between 2.5 code freeze and release\n7 days is standard, and there were only 2 days for 2.5\nOctober 20,2022\nAdded Initial 2023 schedule\nCurrent schedule was running out\nJanuary 13, 2023\nUpdate to 2.5.0 release date\nMaps team found last minute issue, moving to accommodate resolution\nJanuary 19, 2023\nUpdate to 2.5.0 release date\nDocs team due diligence, moving to accommodate\nApril 26, 2023\nUpdate to 2.7.0 release date\nFound CVE to resolve, fix issues found in regression tests",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/slack.html",
    "title": "Slack workspace",
    "content": "Updated April 3, 2023 The OpenSearch Project has a public Slack to enable brainstorming, code reviews, and other ad hoc collaboration discussions with the community.\nIn addition to the OpenSearch Code of Conduct, the following are guiding principles for the new Slack workspace:\nOpen Communication ‚Äì To facilitate better sharing of information, ideas, and announcements, all communication is encouraged to happen in the open project channels.\nSafe and Inclusive ‚Äì The workspace strives to be a safe and inclusive environment where everyone feels comfortable participating.\nLevel Playing Field ‚Äì All project maintainers and collaborators are created equal; feedback and proposals will be fully considered based on their benefits to the overall community‚Äînot based on who proposed them.\nFull Transparency ‚Äì All project maintainers are encouraged to use the Slack workspace along with the discussion forums and GitHub project repositories for all communication in order to discourage any closed-source discussions or development decisions.\nNo Sales Pitches ‚Äì The OpenSearch Project Slack workspace is a place to discuss development of the project‚Äînot a place to sell things. Contributors, maintainers, and community members can use this communication channel to collaborate.\nRead more on Slack communications here.",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/source.html",
    "title": "Source Code",
    "content": "OpenSearch and OpenSearch Dashboards as well as the plugins and tools included in the OpenSearch Project are open source software with an Apache 2.0 license. We encourage you to use it all to the full extent of this license.",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/trademark-brand-policy.html",
    "title": "Trademark and Brand Policy",
    "content": "Trademark and Brand Policy Updated May 18, 2021 Trademark Trademark Policy ¬∑ Overall Guidelines ¬∑ Uses that do not require permission ¬∑ Uses that require permission ¬∑ Questions ¬∑ Trademark FAQ Brand Brand Guidelines ¬∑ Logo ¬∑ Mark ¬∑ More file formats Trademark Policy\nThis Policy provides guidelines for use of the ‚ÄúOpenSearch‚Äù name and logos (the ‚ÄúOpenSearch Trademarks‚Äù) to identify the OpenSearch software. Amazon.com, Inc. or its affiliates (‚ÄúAmazon‚Äù) strives to be a steward of the OpenSearch brand for the entire OpenSearch Community and is the owner of the OpenSearch Trademarks. As such, the law obligates us to police and protect the trademarks. Therefore, we require use of the OpenSearch Trademarks to be in accordance with this policy. Indeed, Amazon‚Äôs own use is designed to be consistent with this Policy.\nOur goal is to ensure, on behalf of the OpenSearch Community, that the OpenSearch Trademark remain reliable indicators of quality and security while also permitting community members, software distributors and others to discuss OpenSearch and to accurately describe their products‚Äô affiliation with OpenSearch or the OpenSearch Community, as well as exercise their rights given OpenSearch‚Äôs open source nature. Note that this policy only applies to use of the OpenSearch Trademarks.\nOverall Guidelines\nYou may use the ‚ÄúOpenSearch‚Äù Trademarks to refer to the OpenSearch software provided that your use is in compliance with this policy. Any other use of the OpenSearch Trademarks requires prior written permission.\nOverall, your use of the OpenSearch Trademarks must not be confusing, misleading, false, or damaging to the OpenSearch software, the OpenSearch Community or to the OpenSearch Trademarks themselves.\nPeople should always know who they are dealing with, and where the software they are downloading and using came from. You may not use the OpenSearch Trademarks in any manner that implies approval or endorsement by, or association with, the OpenSearch project or the OpenSearch Community. When using the OpenSearch Trademarks, your branding should be distinguishable from OpenSearch trade dress.\nYou may not use the OpenSearch Trademarks in a manner that may diminish or otherwise damage the goodwill in the OpenSearch Trademarks. The ‚ÄúOpenSearch‚Äù word mark should be used in its exact form, and not abbreviated or combined with any other word or words (e.g., ‚ÄúOpenSearch‚Äù software rather than ‚ÄúOPNSRCH‚Äù or ‚ÄúOpenSearch-ified‚Äù). Similarly, the OpenSearch logos should not be modified or integrated with your logos or other designs. You may create a lockup with your logo and an OpenSearch logo side by side so long as your logo is not confusingly similar to the OpenSearch logo and appears first and so long as your use complies with our Brand Guidelines and this policy.\nYour use of the OpenSearch Trademarks does not transfer rights in the trademarks or goodwill to you. Uses that do not require permission\nProvided your use complies with this policy and our Brand Guidelines, you may use the OpenSearch logos to link to the OpenSearch website, to indicate that your software or service uses the OpenSearch software, in architecture diagrams to show how your software or service integrates with OpenSearch, and in presentations, social media posts (but not as your account image or avatar), whitepapers, blog posts, and similar content as a reference to the OpenSearch project itself. It should be clear what role the OpenSearch project or software plays in the context of your software or services. The OpenSearch logos should not be more prominent than your own branding.\nUse the official versions of the OpenSearch logos available for download here. You may transform the file format itself for ease of use and modify the colors in accordance with the Brand Guidelines. Provided your use complies with this policy, you may use the ‚ÄúOpenSearch‚Äù word mark to accurately reference the OpenSearch software, including on your website, in presentations and publications, at events, in advertising and marketing material, etc., for commercial and noncommercial purposes.\nYou may use the ‚ÄúOpenSearch‚Äù word mark and any logos we placed on the software in connection with a redistribution of an official distribution of the OpenSearch software that has not been modified or changed in any way.\nThose taking full advantage of the open source nature of the OpenSearch code may make modifications in accordance with the applicable open source license to create Derivative Works (as defined in the Apache License, version 2.0) of OpenSearch. You may use the ‚ÄúOpenSearch‚Äù word mark to refer to your Derivative Works of OpenSearch provided (a) you include an additional identifier indicating you as the source of the Derivative Works (e.g., ‚ÄúFoocorp‚Äôs OpenSearch Derivative‚Äù), (b) you clearly identify your modifications and indicate you are the source of the modifications, (c) your use does not suggest any affiliation between OpenSearch or the OpenSearch Community and you or your Derivative Works of OpenSearch, and (d) your use of the ‚ÄúOpenSearch‚Äù word mark should not be more prominent than your additional identifier.\nThose taking advantage of the open source nature of the OpenSearch code may also offer services for, or software that works with, OpenSearch or Derivative Works of OpenSearch, such as cloud management services. Users should not be confused as to the source of your software or services. With that in mind, you may use the ‚ÄúOpenSearch‚Äù word mark to refer to services for, or software that works with, OpenSearch or Derivative Works of OpenSearch provided (a) you include an additional identifier indicating you as the source of the software or services (e.g., ‚ÄúFoocorp‚Äôs OpenSearch Tool‚Äù or ‚ÄúFoocorp OpenSearch Service‚Äù), (b) if your services or software works with Derivative Works of OpenSearch, you clearly identify the modifications and indicate the source of the modifications, (c) your use does not suggest any affiliation between OpenSearch or the OpenSearch Community and you or your work, and (d) your use of the ‚ÄúOpenSearch‚Äù mark should not be more prominent than your additional identifier.\nYou may also use the ‚ÄúOpenSearch‚Äù word mark to make accurate statements about compatibility and interoperability using relational phrases such as ‚Äúworks with,‚Äù ‚Äúruns on,‚Äù ‚Äúcompatible with,‚Äù and the like (e.g., ‚ÄúFoocorp Software powered by OpenSearch‚Äù or ‚ÄúFoocorp Software for OpenSearch‚Äù or ‚ÄúFoocorp Software with OpenSearch compatibility‚Äù).\nUses that require permission\nThe following uses of the OpenSearch Trademarks require our prior written approval:\nUse of the OpenSearch logos in any way other than as expressly authorized by this Policy;\nUse as part of a domain name, except that you may use the OpenSearch Trademarks in a subdomain name provided your use otherwise complies with this Policy (e.g., opensearch. foocorp.com);\nUse with non-software goods or services (e.g., physical products like devices or services\nthat do not directly use the OpenSearch software), except that you may use the OpenSearch Trademarks with a limited number of swag or promotional items not for sale such as t-shirts, lanyards, stickers, mugs, or pens; and\nUse that does not comply with the terms of this Policy.\nYou may not use the OpenSearch Trademarks in connection with use or distribution of the OpenSearch software that is not in compliance with the OpenSearch software copyright license (Apache License, version 2.0).\nQuestions\nIf you are unsure whether your use of the OpenSearch Trademarks is permitted under this policy, feel free to contact us and ask. If you have questions about these guidelines or use of any other Amazon trademark, please contact trademarks@amazon.com for assistance, or write to us at: Amazon.com, Inc.\nAttention: Trademarks\nPO Box 81226\nSeattle, WA 98108-1226 This policy is based in part on the open source trademark policy defined by the Mozilla organization, therefore, the text of this policy (and not the OpenSearch Trademarks themselves) is licensed under the Creative Commons ‚ÄúAttribution-ShareAlike 2.0‚Äù license.\nTrademark FAQ Can I create and redistribute my own builds of OpenSearch? If you build OpenSearch from unmodified source and redistribute the results, you may use ‚ÄúOpenSearch‚Äù only if it is clear in both the name of your distribution and the content associated with it that your distribution is your build of OpenSearch and not the official build, and you must identify the commit from which it is built, including the commit date. What can I do if I see abuse of the OpenSearch Trademarks? If you are aware of confusing or misleading use or other misuse of the OpenSearch Trademarks, you may contact us as described above at trademarks@amazon.com so we can investigate further. This Policy requires modifications and their source to be identified for Derivative Works of OpenSearch, where should I put this information? You may put this information in any location that is commonly used to convey differences from an upstream open source project, such as a NOTICE text or end-user documentation. What‚Äôs new in this update to the Trademark Policy? We‚Äôve got logos! Our talented creative team designed logos the OpenSearch project. We also worked hard on Brand Guidelines to help ensure use of the OpenSearch Trademarks is unified, cohesive, and, simply put, looks good. Please use them! Brand Guidelines\nThese guidelines were developed to help keep the OpenSearch brand unified and cohesive. Please reference this guide for all communication and brand design across all formats.\nDownload: Brand Guidelines PDF Logos &amp; Marks\nLogo Default: PNG ¬∑ SVG Dark Mode: PNG ¬∑ SVG Monochrome: PNG ¬∑ SVG Mark Default: PNG ¬∑ SVG Dark Mode: PNG ¬∑ SVG Monochrome: PNG ¬∑ SVG More file formats\nThe full set of PNG, SVG and EPS Logos, Marks, &amp; Logotypes are available as downloadable.zip files below.\nDownload: SVG ¬∑ PNG ¬∑ EPS",
    "keywords": [

    ],
    "type": null
  },
  {
    "url": "/verify-signatures.html",
    "title": "How to verify signatures",
    "content": "How to verify signatures for downloadable artifacts\nDownload our PGP key using the link below and import it.\nIf you‚Äôre using gpg, you just need to run: gpg --import /path/to/key You can then verify the signature by downloading it into the same directory where you downloaded the tarball, and running: gpg --verify /path/to/signature /path/to/tarball It should show a good signature signed by opensearch@amazon.com.\nOur current PGP key fingerprint is C5B7 4989 65EF D1C2 924B A9D5 39D3 1987 9310 D3FC Get our PGP Key Note: If you see ‚Äúgpg: Note: This key has expired!‚Äù as originally noted in Issue 2040, please download the newest key. See change log for dates. Change Log Date Issue Created Expires 2022-05-11 Issue 2040 2022-05-12\n2023-05-12\n2023-05-04 Issue 2136 2023-05-03\n2024-05-12",
    "keywords": [

    ],
    "type": null
  }
]
